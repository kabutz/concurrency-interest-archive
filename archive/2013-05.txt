From hallerp at gmail.com  Wed May  1 04:07:55 2013
From: hallerp at gmail.com (Philipp Haller)
Date: Wed, 1 May 2013 10:07:55 +0200
Subject: [concurrency-interest] Critical fix in jsr166y for Scala 2.10.2
Message-ID: <EAFE4F7A-BAF7-4677-83D1-F12F676ED67E@gmail.com>

Hi all,

Scala's standard library ships with a fork of jsr166y to provide a high-performance thread pool on Java6.

We've recently discovered a performance/scalability bug in our fork which lead to underutilized worker threads (see https://issues.scala-lang.org/browse/SI-7438).

The most recent versions of jsr166y and jsr166e do not suffer from this bug.

A workaround using the revision of jsr166y shipped in Scala is not to use `fork` in our global "execution context". However, this seems like a drastic measure, since we would loose the benefit of submitting to local work queues if possible. (In the past we have seen tangible scalability improvements in actor-based code.)

Since this is a critical issue, we're planning to provide a fix in Scala 2.10.2. Our questions are:
- Given that Scala 2.10.2 will be binary compatible with Scala 2.10.1, is it safe to update to the latest revision of jsr166y? In other words, are later revisions of jsr166y binary compatible with earlier revisions?
- Besides the above fix, are there any other observable changes to be expected when updating jsr166y to the latest revision?
- How much performance would we loose by avoiding the use of `fork`? How much scalability?
- Would it make sense to cherry-pick the commit that fixes the issue? (We haven't pinpointed, yet, which commit fixes the issue.)

The jsr166y fork shipped in Scala 2.10.1 is based on revision 1.89 (Apr 9, 2012).

Any advice would be much appreciated.

Thanks,
Philipp

--
See you at Scala Days (June 10?12, NYC)!

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130501/efd9f4c1/attachment.html>

From oleksandr.otenko at oracle.com  Wed May  1 04:42:56 2013
From: oleksandr.otenko at oracle.com (oleksandr otenko)
Date: Wed, 01 May 2013 09:42:56 +0100
Subject: [concurrency-interest] JLS 17.7 Non-atomic treatment of double
 and long : Android
In-Reply-To: <7EC0C469-F8AA-4419-94D5-592305C62293@cox.net>
References: <CAChYfd-8DO0YCu+cEHj_MGVNzyoPewD+HSwuaF_HmAr_6K-31A@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD236BB8242@G9W0725.americas.hpqcorp.net>
	<7EC0C469-F8AA-4419-94D5-592305C62293@cox.net>
Message-ID: <5180D590.2070808@oracle.com>

I don't know what everyone means by "sequential consistency".

Hashtable<K,Integer> t = new Hashtable<>();

...

t.put(k, t.get(k)+1);

is this sequentially consistent?


Alex


On 01/05/2013 03:21, Gregg Wonderly wrote:
> The problem is that people can trivially create data races.  It's 
> happened everywhere.  There is so much legacy Java code that has data 
> races.   That's no the large problem.  Data races are one thing.  The 
> large problem is when the JIT reorders code and optimizes code to not 
> be sequentially consistent.  It then lands on the developers shoulders 
> to understand every possible, undocumented optimization that might be 
> done by the compiler to try and understand why the code is 
> misbehaving.  Only then, can they possibly formulate which changes 
> will actually correct the behavior of their code, without making it 
> "slower" than it needs to be.  To me, that is exactly the wrong way 
> for a developer to test and optimize their code.  Visible data races 
> that are only about "the data", are trivial to reason about.
>
> If I saw the Thread.setName/getName lack of synchronization, out in 
> the wild, create a corrupted thread name, I'd be writing a bug report 
> about the JVM creating memory corruption before I would consider that 
> the array had been allocated and made visible before it was filled. 
>  That's not how the code is written, and that's not how it should 
> execute without the developer deciding that such an optimization is 
> safe for their application to see.
>
> Suggesting that denying the visibility of the optimization is selected 
> by the developer not doing anything, is where I respectfully, cannot 
> agree with such reasoning.  This is not a new issue.  The design of 
> many functional languages is based on the simple fact that developers 
> don't need to be burdened with designating that they want correctly 
> executing code.  They should get that by default, and have to work at 
> "faster" or "more performant" or "less latency", by explicit actions 
> they take in code structure.
>
> Thread.setName/getName is a great example of "no thought given" to 
> concurrency in the original design.  That code, as said before, is 
> only executed in a concurrent environment.  Why didn't the original 
> code have the correct concurrency design.  Why hasn't it been visible 
> to enough people that a bug report written and it already fixed?
>
> The answer is, because that racy code, doesn't "act" wrong, in general 
> usage patterns.  But optimizations due to how volatile works and how 
> JIT developers exploit lack of "concurrency selected" coding, could, 
> at any point, break that code.
>
> This is how all of that legacy code, laying around on the internet, is 
> going to come down.  It's going to start randomly breaking and causing 
> completely unexplainable bugs.  People will not remember enough 
> details about this code to actually understand that the JIT is causing 
> them problems with reordering, loop hoisting or other "nifty" 
> optimizations that provide a .1% improvement in speed.   It will be a 
> giant waste of peoples time trying to reconcile what is actually going 
> wrong, and in some cases, there will be real impact on the users 
> lives, welfare and/or safety, potentially.
>
> Call my position extreme, but I think it's vital for the Java 
> community and Oracle in particular to understand that the path we are 
> going down is absolutely a perilous disaster without something very 
> specific being visible to developers to allow them to understand how 
> their code is being executed so that they can see exactly what parts 
> of the code need to be fixed.
>
> Gregg Wonderly
>
> On Apr 30, 2013, at 7:48 PM, "Boehm, Hans" <hans.boehm at hp.com 
> <mailto:hans.boehm at hp.com>> wrote:
>
>> A nice simple example to consider here is a user application that 
>> declares an array, and calls a library to sort it.  The sort library 
>> uses a parallel sort that relies on a sequential sort to sort small 
>> sections of the array.  In a sequential-by-default world, how would 
>> you declare the parallel sections?  Would it be any different than 
>> what we do now?  The user application that declares the array may 
>> never know that there is any parallel code involved.  Nor should it.
>> Applications such as this are naturally data-race-free, thus there is 
>> no issue with the compiler "breaking" code.  And the compiler can 
>> apply nearly all sequentially valid transformations on 
>> synchronization-free code, such as the sequential sort operations.  
>> If you accidentally introduce a data race bug, it's unlikely your 
>> code would run correctly even if the compiler guaranteed sequential 
>> consistency. Your code may be a bit easier to debug with a 
>> hypothetical compiler that ensures sequential consistency.  But so 
>> long as you avoided intentional (unannotated) data races, I think a 
>> data race detector would also make this fairly easy.
>> Hans
>> *From:*concurrency-interest-bounces at cs.oswego.edu 
>> <mailto:concurrency-interest-bounces at cs.oswego.edu>[mailto:concurrency-interest-bounces at cs.oswego.edu 
>> <mailto:interest-bounces at cs.oswego.edu>]*On Behalf Of*Martin Thompson
>> *Sent:*Tuesday, April 30, 2013 5:57 AM
>> *To:*Kirk Pepperdine
>> *Cc:*Gregg Wonderly;concurrency-interest at cs.oswego.edu 
>> <mailto:concurrency-interest at cs.oswego.edu>
>> *Subject:*Re: [concurrency-interest] JLS 17.7 Non-atomic treatment of 
>> double and long : Android
>> I agree with Kirk here and would take it further.  By default the 
>> vast majority of code should be single threaded and concurrent 
>> programming is only utilized in regions of data exchange.
>> If all code was sequentially consistent then most hardware and 
>> compiler optimizations would be defeated.  A default position that 
>> all code is concurrent is sending the industry the wrong way in my 
>> view.  It makes a more sense to explicitly define the regions of data 
>> exchange in our programs and therefore what ordering semantics are 
>> required in those regions.
>> Martin...
>>
>>     ------------------------------
>>     Message: 3
>>     Date: Tue, 30 Apr 2013 07:38:01 +0200
>>     From: Kirk Pepperdine <kirk at kodewerk.com <mailto:kirk at kodewerk.com>>
>>     To: Gregg Wonderly <gergg at cox.net <mailto:gergg at cox.net>>
>>     Cc: concurrency-interest at cs.oswego.edu
>>     <mailto:concurrency-interest at cs.oswego.edu>Subject: Re:
>>     [concurrency-interest] JLS 17.7 Non-atomic treatment of
>>             double  and long : Android
>>     Message-ID: <5D85887E-8BFE-4F09-AFA8-53FE1DFD52D3 at kodewerk.com
>>     <mailto:5D85887E-8BFE-4F09-AFA8-53FE1DFD52D3 at kodewerk.com>>
>>     Content-Type: text/plain; charset="windows-1252"
>>     Sorry but making thing volatile by default would be a horrible
>>     thing to do. Code wouldn't be a bit slower, it would be a lot
>>     slower and then you'd end up with the same problem in reverse!
>>     Regards,
>>     Kirk
>>     On 2013-04-30, at 12:10 AM, Gregg Wonderly <gergg at cox.net
>>     <mailto:gergg at cox.net>> wrote:
>>     > This code exists everywhere on the Java JVM now, because no one
>>     expects the loop hoist?   People are living with it, or
>>     eventually declaring the loop variable volatile after finding
>>     these discussions.
>>     >
>>     > Java, by default, should of used nothing but volatile
>>     variables, and developers should of needed to add non-volatile
>>     declarations via annotations, without the 'volatile' keyword
>>     being used, at all.
>>     >
>>     > That would of made it hard to "break" code without actually
>>     looking up what you were doing, because the added verbosity would
>>     only be tolerated when it actually accomplished a performance
>>     improvement.  Today, code is "Faster" without "volatile".   If
>>     everything was "volatile" by default, then code would be slower
>>     to start with, and proper "concurrency programming" would then
>>     make your code faster, as concurrency should.
>>     >
>>     > Gregg
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu 
>> <mailto:Concurrency-interest at cs.oswego.edu>
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130501/2e817369/attachment-0001.html>

From aph at redhat.com  Wed May  1 04:58:43 2013
From: aph at redhat.com (Andrew Haley)
Date: Wed, 01 May 2013 09:58:43 +0100
Subject: [concurrency-interest] JLS 17.7 Non-atomic treatment of double
 and long : Android
In-Reply-To: <CAHjP37H81-uT=jwRmS2C1UYL3+unPkOgG-N53p5n9aQn88jK7g@mail.gmail.com>
References: <CAMyLHFynogmafOgx8Eb6_nvXwqmC6SWUXcZNiZrkJ_DM5psV7g@mail.gmail.com>
	<517EDB36.1090508@oracle.com>
	<CAMyLHFwNqcwwqK=uWbOZNDtSnZH0r1C62LtxSx=bWNdQTdyr0w@mail.gmail.com>
	<517EDF4E.3040302@oracle.com>
	<CAMyLHFypYbY=iL4D9+CH+DGYRhvCYLYdj6nwV=RE+D=mrtC-uQ@mail.gmail.com>
	<CAHjP37H81-uT=jwRmS2C1UYL3+unPkOgG-N53p5n9aQn88jK7g@mail.gmail.com>
Message-ID: <5180D943.6090307@redhat.com>

On 04/30/2013 04:56 PM, Vitaly Davidovich wrote:
> I don't see how 32bit Hotspot can *not * split 64 bit values given
> registers are not wide enough.
> 
> Easiest would be to hook up hsdis to hotspot and look at the disassembly.

In the open 32-bit ARM port I do this:

void Thumb2_load_long(Thumb2_Info *jinfo, Reg r_lo, Reg r_hi, Reg base,
                      int field_offset,
                      bool is_volatile = false)
{
  CodeBuf *codebuf = jinfo->codebuf;
  if (is_volatile) {
    Reg r_addr = base;
    if (field_offset) {
      r_addr = Thumb2_Tmp(jinfo, (1<<r_lo) | (1<<r_hi) | (1<<base));
      add_imm(jinfo->codebuf, r_addr, base, field_offset);
    }
    ldrexd(codebuf, r_lo, r_hi, r_addr);
  } else {
    ldrd_imm(codebuf, r_lo, r_hi, base, field_offset, 1, 0);
  }
}

// Generate code for a store of a jlong.  If the operand is volatile,
// generate a sequence of the form
//
// .Ldst
//      ldrexd  r2, r3, [dst]
//      strexd  r2, r0, r1, [dst]
//      cmp     r2, #0
//      bne     .Ldst

void Thumb2_store_long(Thumb2_Info *jinfo, Reg r_lo, Reg r_hi, Reg base,
                      int field_offset,
                      bool is_volatile = false)
{
  CodeBuf *codebuf = jinfo->codebuf;
  if (is_volatile) {
    Reg r_addr = base;
    Reg tmp1 = Thumb2_Tmp(jinfo, (1<<r_lo) | (1<<r_hi) | (1<<base));
    Reg tmp2 = Thumb2_Tmp(jinfo, (1<<r_lo) | (1<<r_hi) | (1<<base) | (1<<tmp1));
    if (field_offset) {
      r_addr = Thumb2_Tmp(jinfo, (1<<r_lo) | (1<<r_hi) | (1<<base) | (1<<tmp1) | (1<<tmp2));
      add_imm(jinfo->codebuf, r_addr, base, field_offset);
    }
    int loc = out_loc(codebuf);
    ldrexd(codebuf, tmp1, tmp2, r_addr);
    strexd(codebuf, tmp1, r_lo, r_hi, r_addr);
    cmp_imm(codebuf, tmp1, 0);
    branch(codebuf, COND_NE, loc);
  } else {
    strd_imm(codebuf, r_lo, r_hi, base, field_offset, 1, 0);
  }
}


From peter.firmstone at zeus.net.au  Wed May  1 05:36:53 2013
From: peter.firmstone at zeus.net.au (Peter Firmstone)
Date: Wed, 01 May 2013 19:36:53 +1000
Subject: [concurrency-interest] JLS 17.7 Non-atomic treatment of double
	and long
In-Reply-To: <mailman.397.1367374876.2479.concurrency-interest@cs.oswego.edu>
References: <mailman.397.1367374876.2479.concurrency-interest@cs.oswego.edu>
Message-ID: <5180E235.2010500@zeus.net.au>

A data race detector in visualvm or perhaps a flag in the jvm to have 
the jvm detect data races at runtime and throw an Error early would be 
very useful.

That would make it so much easer to track down these bugs that 
ultimately need to be fixed.

Regards,

Peter.


On 1/05/2013 12:21 PM, concurrency-interest-request at cs.oswego.edu wrote:
> Send Concurrency-interest mailing list submissions to
> 	concurrency-interest at cs.oswego.edu
>
> To subscribe or unsubscribe via the World Wide Web, visit
> 	http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> or, via email, send a message with subject or body 'help' to
> 	concurrency-interest-request at cs.oswego.edu
>
> You can reach the person managing the list at
> 	concurrency-interest-owner at cs.oswego.edu
>
> When replying, please edit your Subject line so it is more specific
> than "Re: Contents of Concurrency-interest digest..."
>
>
> Today's Topics:
>
>     1. Re: JLS 17.7 Non-atomic treatment of double and long :
>        Android (Boehm, Hans)
>     2. Re: JLS 17.7 Non-atomic treatment of double	and long :
>        Android (Gregg Wonderly)
>
>
> ----------------------------------------------------------------------
>
> Message: 1
> Date: Wed, 1 May 2013 00:48:01 +0000
> From: "Boehm, Hans"<hans.boehm at hp.com>
> To: Martin Thompson<mjpt777 at gmail.com>, Kirk Pepperdine
> 	<kirk at kodewerk.com>
> Cc: Gregg Wonderly<gergg at cox.net>,
> 	"concurrency-interest at cs.oswego.edu"
> 	<concurrency-interest at cs.oswego.edu>
> Subject: Re: [concurrency-interest] JLS 17.7 Non-atomic treatment of
> 	double and long : Android
> Message-ID:
> 	<A3E67C2071F49C4CBC4F17E6D77CDDD236BB8242 at G9W0725.americas.hpqcorp.net>
> 	
> Content-Type: text/plain; charset="us-ascii"
>
> A nice simple example to consider here is a user application that declares an array, and calls a library to sort it.  The sort library uses a parallel sort that relies on a sequential sort to sort small sections of the array.  In a sequential-by-default world, how would you declare the parallel sections?  Would it be any different than what we do now?  The user application that declares the array may never know that there is any parallel code involved.  Nor should it.
>
> Applications such as this are naturally data-race-free, thus there is no issue with the compiler "breaking" code.  And the compiler can apply nearly all sequentially valid transformations on synchronization-free code, such as the sequential sort operations.  If you accidentally introduce a data race bug, it's unlikely your code would run correctly even if the compiler guaranteed sequential consistency. Your code may be a bit easier to debug with a hypothetical compiler that ensures sequential consistency.  But so long as you avoided intentional (unannotated) data races, I think a data race detector would also make this fairly easy.
>
> Hans
>
> From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Martin Thompson
> Sent: Tuesday, April 30, 2013 5:57 AM
> To: Kirk Pepperdine
> Cc: Gregg Wonderly; concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] JLS 17.7 Non-atomic treatment of double and long : Android
>
> I agree with Kirk here and would take it further.  By default the vast majority of code should be single threaded and concurrent programming is only utilized in regions of data exchange.
>
> If all code was sequentially consistent then most hardware and compiler optimizations would be defeated.  A default position that all code is concurrent is sending the industry the wrong way in my view.  It makes a more sense to explicitly define the regions of data exchange in our programs and therefore what ordering semantics are required in those regions.
>
> Martin...
> ------------------------------
> Message: 3
> Date: Tue, 30 Apr 2013 07:38:01 +0200
> From: Kirk Pepperdine<kirk at kodewerk.com<mailto:kirk at kodewerk.com>>
> To: Gregg Wonderly<gergg at cox.net<mailto:gergg at cox.net>>
> Cc: concurrency-interest at cs.oswego.edu
> <mailto:concurrency-interest at cs.oswego.edu>Subject: Re: [concurrency-interest] JLS 17.7 Non-atomic treatment of
>          double  and long : Android
> Message-ID:<5D85887E-8BFE-4F09-AFA8-53FE1DFD52D3 at kodewerk.com<mailto:5D85887E-8BFE-4F09-AFA8-53FE1DFD52D3 at kodewerk.com>>
> Content-Type: text/plain; charset="windows-1252"
> Sorry but making thing volatile by default would be a horrible thing to do. Code wouldn't be a bit slower, it would be a lot slower and then you'd end up with the same problem in reverse!
> Regards,
> Kirk
> On 2013-04-30, at 12:10 AM, Gregg Wonderly<gergg at cox.net<mailto:gergg at cox.net>>  wrote:
>> This code exists everywhere on the Java JVM now, because no one expects the loop hoist?   People are living with it, or eventually declaring the loop variable volatile after finding these discussions.
>>
>> Java, by default, should of used nothing but volatile variables, and developers should of needed to add non-volatile declarations via annotations, without the 'volatile' keyword being used, at all.
>>
>> That would of made it hard to "break" code without actually looking up what you were doing, because the added verbosity would only be tolerated when it actually accomplished a performance improvement.  Today, code is "Faster" without "volatile".   If everything was "volatile" by default, then code would be slower to start with, and proper "concurrency programming" would then make your code faster, as concurrency should.
>>
>> Gregg
> -------------- next part --------------
> An HTML attachment was scrubbed...
> URL:<http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130501/fcdf203a/attachment-0001.html>
>
> ------------------------------
>
> Message: 2
> Date: Tue, 30 Apr 2013 21:21:02 -0500
> From: Gregg Wonderly<gergg at cox.net>
> To: "Boehm, Hans"<hans.boehm at hp.com>
> Cc: "concurrency-interest at cs.oswego.edu"
> 	<concurrency-interest at cs.oswego.edu>,	Martin Thompson
> 	<mjpt777 at gmail.com>
> Subject: Re: [concurrency-interest] JLS 17.7 Non-atomic treatment of
> 	double	and long : Android
> Message-ID:<7EC0C469-F8AA-4419-94D5-592305C62293 at cox.net>
> Content-Type: text/plain; charset="windows-1252"
>
> The problem is that people can trivially create data races.  It's happened everywhere.  There is so much legacy Java code that has data races.   That's no the large problem.  Data races are one thing.  The large problem is when the JIT reorders code and optimizes code to not be sequentially consistent.  It then lands on the developers shoulders to understand every possible, undocumented optimization that might be done by the compiler to try and understand why the code is misbehaving.  Only then, can they possibly formulate which changes will actually correct the behavior of their code, without making it "slower" than it needs to be.  To me, that is exactly the wrong way for a developer to test and optimize their code.  Visible data races that are only about "the data", are trivial to reason about.
>
> If I saw the Thread.setName/getName lack of synchronization, out in the wild, create a corrupted thread name, I'd be writing a bug report about the JVM creating memory corruption before I would consider that the array had been allocated and made visible before it was filled.  That's not how the code is written, and that's not how it should execute without the developer deciding that such an optimization is safe for their application to see.
>
> Suggesting that denying the visibility of the optimization is selected by the developer not doing anything, is where I respectfully, cannot agree with such reasoning.  This is not a new issue.  The design of many functional languages is based on the simple fact that developers don't need to be burdened with designating that they want correctly executing code.  They should get that by default, and have to work at "faster" or "more performant" or "less latency", by explicit actions they take in code structure.
>
> Thread.setName/getName is a great example of "no thought given" to concurrency in the original design.  That code, as said before, is only executed in a concurrent environment.  Why didn't the original code have the correct concurrency design.  Why hasn't it been visible to enough people that a bug report written and it already fixed?
>
> The answer is, because that racy code, doesn't "act" wrong, in general usage patterns.  But optimizations due to how volatile works and how JIT developers exploit lack of "concurrency selected" coding, could, at any point, break that code.
>
> This is how all of that legacy code, laying around on the internet, is going to come down.  It's going to start randomly breaking and causing completely unexplainable bugs.  People will not remember enough details about this code to actually understand that the JIT is causing them problems with reordering, loop hoisting or other "nifty" optimizations that provide a .1% improvement in speed.   It will be a giant waste of peoples time trying to reconcile what is actually going wrong, and in some cases, there will be real impact on the users lives, welfare and/or safety, potentially.
>
> Call my position extreme, but I think it's vital for the Java community and Oracle in particular to understand that the path we are going down is absolutely a perilous disaster without something very specific being visible to developers to allow them to understand how their code is being executed so that they can see exactly what parts of the code need to be fixed.
>
> Gregg Wonderly
>
> On Apr 30, 2013, at 7:48 PM, "Boehm, Hans"<hans.boehm at hp.com>  wrote:
>
>> A nice simple example to consider here is a user application that declares an array, and calls a library to sort it.  The sort library uses a parallel sort that relies on a sequential sort to sort small sections of the array.  In a sequential-by-default world, how would you declare the parallel sections?  Would it be any different than what we do now?  The user application that declares the array may never know that there is any parallel code involved.  Nor should it.
>>
>> Applications such as this are naturally data-race-free, thus there is no issue with the compiler ?breaking? code.  And the compiler can apply nearly all sequentially valid transformations on synchronization-free code, such as the sequential sort operations.  If you accidentally introduce a data race bug, it?s unlikely your code would run correctly even if the compiler guaranteed sequential consistency. Your code may be a bit easier to debug with a hypothetical compiler that ensures sequential consistency.  But so long as you avoided intentional (unannotated) data races, I think a data race detector would also make this fairly easy.
>>
>> Hans
>>
>> From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Martin Thompson
>> Sent: Tuesday, April 30, 2013 5:57 AM
>> To: Kirk Pepperdine
>> Cc: Gregg Wonderly; concurrency-interest at cs.oswego.edu
>> Subject: Re: [concurrency-interest] JLS 17.7 Non-atomic treatment of double and long : Android
>>
>> I agree with Kirk here and would take it further.  By default the vast majority of code should be single threaded and concurrent programming is only utilized in regions of data exchange.
>>
>> If all code was sequentially consistent then most hardware and compiler optimizations would be defeated.  A default position that all code is concurrent is sending the industry the wrong way in my view.  It makes a more sense to explicitly define the regions of data exchange in our programs and therefore what ordering semantics are required in those regions.
>>
>> Martin...
>> ------------------------------
>> Message: 3
>> Date: Tue, 30 Apr 2013 07:38:01 +0200
>> From: Kirk Pepperdine<kirk at kodewerk.com>
>> To: Gregg Wonderly<gergg at cox.net>
>> Cc: concurrency-interest at cs.oswego.edu
>> Subject: Re: [concurrency-interest] JLS 17.7 Non-atomic treatment of
>>          double  and long : Android
>> Message-ID:<5D85887E-8BFE-4F09-AFA8-53FE1DFD52D3 at kodewerk.com>
>> Content-Type: text/plain; charset="windows-1252"
>> Sorry but making thing volatile by default would be a horrible thing to do. Code wouldn't be a bit slower, it would be a lot slower and then you'd end up with the same problem in reverse!
>> Regards,
>> Kirk
>> On 2013-04-30, at 12:10 AM, Gregg Wonderly<gergg at cox.net>  wrote:
>>> This code exists everywhere on the Java JVM now, because no one expects the loop hoist?   People are living with it, or eventually declaring the loop variable volatile after finding these discussions.
>>>
>>> Java, by default, should of used nothing but volatile variables, and developers should of needed to add non-volatile declarations via annotations, without the 'volatile' keyword being used, at all.
>>>
>>> That would of made it hard to "break" code without actually looking up what you were doing, because the added verbosity would only be tolerated when it actually accomplished a performance improvement.  Today, code is "Faster" without "volatile".   If everything was "volatile" by default, then code would be slower to start with, and proper "concurrency programming" would then make your code faster, as concurrency should.
>>>
>>> Gregg
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> -------------- next part --------------
> An HTML attachment was scrubbed...
> URL:<http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130430/e1ffd03e/attachment.html>
>
> ------------------------------
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> End of Concurrency-interest Digest, Vol 99, Issue 81
> ****************************************************


From dl at cs.oswego.edu  Wed May  1 08:03:59 2013
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed, 01 May 2013 08:03:59 -0400
Subject: [concurrency-interest] Critical fix in jsr166y for Scala 2.10.2
In-Reply-To: <EAFE4F7A-BAF7-4677-83D1-F12F676ED67E@gmail.com>
References: <EAFE4F7A-BAF7-4677-83D1-F12F676ED67E@gmail.com>
Message-ID: <518104AF.4010909@cs.oswego.edu>

On 05/01/13 04:07, Philipp Haller wrote:
> Hi all,
>
> Scala's standard library ships with a fork of jsr166y to provide a
> high-performance thread pool on Java6.
>
> We've recently discovered a performance/scalability bug in our fork which lead
> to underutilized worker threads (see https://issues.scala-lang.org/browse/SI-7438).
>
> The most recent versions of jsr166y and jsr166e do not suffer from this bug.
>
> A workaround using the revision of jsr166y shipped in Scala is not to use `fork`
> in our global "execution context". However, this seems like a drastic measure,

Right, don't do this.

> - Given that Scala 2.10.2 will be binary compatible with Scala 2.10.1, is it
> safe to update to the latest revision of jsr166y? In other words, are later
> revisions of jsr166y binary compatible with earlier revisions?

To the best of my knowledge, yes. The jsr166{x,y,e} builds are
not guaranteed to be stable, but jsr166y (i.e., updates of jdk7
functionality usable on JDK6+) hasn't changed incompatibly recently.

> - Besides the above fix, are there any other observable changes to be expected
> when updating jsr166y to the latest revision?

Nothing I know of that would impact existing usages.

-Doug



From hallorant at gmail.com  Wed May  1 10:53:17 2013
From: hallorant at gmail.com (Tim Halloran)
Date: Wed, 1 May 2013 10:53:17 -0400
Subject: [concurrency-interest] JLS 17.7 Non-atomic treatment of double
 and long : Android
In-Reply-To: <5180D590.2070808@oracle.com>
References: <CAChYfd-8DO0YCu+cEHj_MGVNzyoPewD+HSwuaF_HmAr_6K-31A@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD236BB8242@G9W0725.americas.hpqcorp.net>
	<7EC0C469-F8AA-4419-94D5-592305C62293@cox.net>
	<5180D590.2070808@oracle.com>
Message-ID: <CAMyLHFyGUCpVqb9pZS=NX-WBkHK79mESuQ9Ykvx7uRBa7O9kcw@mail.gmail.com>

On Wed, May 1, 2013 at 4:42 AM, oleksandr otenko <
oleksandr.otenko at oracle.com> wrote:

>  I don't know what everyone means by "sequential consistency".
>

There are several links on Doug Lea's JSR-166 Cookbook page, but this is
probably the one to have a look at:

Sarita Adve and Kourosh Gharachorloo, Shared Memory Consistency Models: A
Tutorial <http://rsim.cs.uiuc.edu/~sadve/>


>
> Hashtable<K,Integer> t = new Hashtable<>();
>
> ...
>
> t.put(k, t.get(k)+1);
>
> is this sequentially consistent?
>
>
> Alex
>
>
>
> On 01/05/2013 03:21, Gregg Wonderly wrote:
>
> The problem is that people can trivially create data races.  It's happened
> everywhere.  There is so much legacy Java code that has data races.
> That's no the large problem.  Data races are one thing.  The large problem
> is when the JIT reorders code and optimizes code to not be sequentially
> consistent.  It then lands on the developers shoulders to understand every
> possible, undocumented optimization that might be done by the compiler to
> try and understand why the code is misbehaving.  Only then, can they
> possibly formulate which changes will actually correct the behavior of
> their code, without making it "slower" than it needs to be.  To me, that is
> exactly the wrong way for a developer to test and optimize their code.
>  Visible data races that are only about "the data", are trivial to reason
> about.
>
>  If I saw the Thread.setName/getName lack of synchronization, out in the
> wild, create a corrupted thread name, I'd be writing a bug report about the
> JVM creating memory corruption before I would consider that the array had
> been allocated and made visible before it was filled.  That's not how the
> code is written, and that's not how it should execute without the developer
> deciding that such an optimization is safe for their application to see.
>
>  Suggesting that denying the visibility of the optimization is selected
> by the developer not doing anything, is where I respectfully, cannot agree
> with such reasoning.  This is not a new issue.  The design of many
> functional languages is based on the simple fact that developers don't need
> to be burdened with designating that they want correctly executing code.
>  They should get that by default, and have to work at "faster" or "more
> performant" or "less latency", by explicit actions they take in code
> structure.
>
>  Thread.setName/getName is a great example of "no thought given" to
> concurrency in the original design.  That code, as said before, is only
> executed in a concurrent environment.  Why didn't the original code have
> the correct concurrency design.  Why hasn't it been visible to enough
> people that a bug report written and it already fixed?
>
>  The answer is, because that racy code, doesn't "act" wrong, in general
> usage patterns.  But optimizations due to how volatile works and how JIT
> developers exploit lack of "concurrency selected" coding, could, at any
> point, break that code.
>
>  This is how all of that legacy code, laying around on the internet, is
> going to come down.  It's going to start randomly breaking and causing
> completely unexplainable bugs.  People will not remember enough details
> about this code to actually understand that the JIT is causing them
> problems with reordering, loop hoisting or other "nifty" optimizations that
> provide a .1% improvement in speed.   It will be a giant waste of peoples
> time trying to reconcile what is actually going wrong, and in some cases,
> there will be real impact on the users lives, welfare and/or safety,
> potentially.
>
>  Call my position extreme, but I think it's vital for the Java community
> and Oracle in particular to understand that the path we are going down is
> absolutely a perilous disaster without something very specific being
> visible to developers to allow them to understand how their code is being
> executed so that they can see exactly what parts of the code need to be
> fixed.
>
>  Gregg Wonderly
>
>   On Apr 30, 2013, at 7:48 PM, "Boehm, Hans" <hans.boehm at hp.com> wrote:
>
>   A nice simple example to consider here is a user application that
> declares an array, and calls a library to sort it.  The sort library uses a
> parallel sort that relies on a sequential sort to sort small sections of
> the array.  In a sequential-by-default world, how would you declare the
> parallel sections?  Would it be any different than what we do now?  The
> user application that declares the array may never know that there is any
> parallel code involved.  Nor should it.****
>
> Applications such as this are naturally data-race-free, thus there is no
> issue with the compiler ?breaking? code.  And the compiler can apply nearly
> all sequentially valid transformations on synchronization-free code, such
> as the sequential sort operations.  If you accidentally introduce a data
> race bug, it?s unlikely your code would run correctly even if the compiler
> guaranteed sequential consistency. Your code may be a bit easier to debug
> with a hypothetical compiler that ensures sequential consistency.  But so
> long as you avoided intentional (unannotated) data races, I think a data
> race detector would also make this fairly easy.****
>
> Hans****
>
>   *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency<concurrency>
> -interest-bounces at cs.oswego.edu] *On Behalf Of *Martin Thompson
> *Sent:* Tuesday, April 30, 2013 5:57 AM
> *To:* Kirk Pepperdine
> *Cc:* Gregg Wonderly; concurrency-interest at cs.oswego.edu
> *Subject:* Re: [concurrency-interest] JLS 17.7 Non-atomic treatment of
> double and long : Android****
>  ** **
>  I agree with Kirk here and would take it further.  By default the vast
> majority of code should be single threaded and concurrent programming is
> only utilized in regions of data exchange.****
>  ** **
>  If all code was sequentially consistent then most hardware and compiler
> optimizations would be defeated.  A default position that all code is
> concurrent is sending the industry the wrong way in my view.  It makes a
> more sense to explicitly define the regions of data exchange in our
> programs and therefore what ordering semantics are required in those
> regions.****
>  ** **
>  Martin...****
>
> ------------------------------
> Message: 3
> Date: Tue, 30 Apr 2013 07:38:01 +0200
> From: Kirk Pepperdine <kirk at kodewerk.com>
> To: Gregg Wonderly <gergg at cox.net>
> Cc: concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] JLS 17.7 Non-atomic treatment of
>         double  and long : Android
> Message-ID: <5D85887E-8BFE-4F09-AFA8-53FE1DFD52D3 at kodewerk.com>
> Content-Type: text/plain; charset="windows-1252"
> Sorry but making thing volatile by default would be a horrible thing to
> do. Code wouldn't be a bit slower, it would be a lot slower and then you'd
> end up with the same problem in reverse!
> Regards,
> Kirk
> On 2013-04-30, at 12:10 AM, Gregg Wonderly <gergg at cox.net> wrote:
> > This code exists everywhere on the Java JVM now, because no one expects
> the loop hoist?   People are living with it, or eventually declaring the
> loop variable volatile after finding these discussions.
> >
> > Java, by default, should of used nothing but volatile variables, and
> developers should of needed to add non-volatile declarations via
> annotations, without the 'volatile' keyword being used, at all.
> >
> > That would of made it hard to "break" code without actually looking up
> what you were doing, because the added verbosity would only be tolerated
> when it actually accomplished a performance improvement.  Today, code is
> "Faster" without "volatile".   If everything was "volatile" by default,
> then code would be slower to start with, and proper "concurrency
> programming" would then make your code faster, as concurrency should.
> >
> > Gregg****
>
>   _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130501/63dad708/attachment-0001.html>

From oleksandr.otenko at oracle.com  Wed May  1 11:31:41 2013
From: oleksandr.otenko at oracle.com (oleksandr otenko)
Date: Wed, 01 May 2013 16:31:41 +0100
Subject: [concurrency-interest] JLS 17.7 Non-atomic treatment of double
 and long : Android
In-Reply-To: <CAMyLHFyGUCpVqb9pZS=NX-WBkHK79mESuQ9Ykvx7uRBa7O9kcw@mail.gmail.com>
References: <CAChYfd-8DO0YCu+cEHj_MGVNzyoPewD+HSwuaF_HmAr_6K-31A@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD236BB8242@G9W0725.americas.hpqcorp.net>
	<7EC0C469-F8AA-4419-94D5-592305C62293@cox.net>
	<5180D590.2070808@oracle.com>
	<CAMyLHFyGUCpVqb9pZS=NX-WBkHK79mESuQ9Ykvx7uRBa7O9kcw@mail.gmail.com>
Message-ID: <5181355D.6050300@oracle.com>

That is kind of my point, actually.

I can't claim expert understanding of sequential consistency, but the 
example piece of code can be shown broken even if we conclude it is 
sequentially consistent. Which then leads to the next question to Gregg 
- why fight for sequential consistency so hard, if we care about 
something else, about a deeper consistency.


Alex


On 01/05/2013 15:53, Tim Halloran wrote:
> On Wed, May 1, 2013 at 4:42 AM, oleksandr otenko 
> <oleksandr.otenko at oracle.com <mailto:oleksandr.otenko at oracle.com>> wrote:
>
>     I don't know what everyone means by "sequential consistency".
>
>
> There are several links on Doug Lea's JSR-166 Cookbook page, but this 
> is probably the one to have a look at:
>
> Sarita Adve and Kourosh Gharachorloo, Shared Memory Consistency 
> Models: A Tutorial <http://rsim.cs.uiuc.edu/%7Esadve/>
>
>
>     Hashtable<K,Integer> t = new Hashtable<>();
>
>     ...
>
>     t.put(k, t.get(k)+1);
>
>     is this sequentially consistent?
>
>
>     Alex
>
>
>
>     On 01/05/2013 03:21, Gregg Wonderly wrote:
>>     The problem is that people can trivially create data races.  It's
>>     happened everywhere.  There is so much legacy Java code that has
>>     data races.   That's no the large problem.  Data races are one
>>     thing.  The large problem is when the JIT reorders code and
>>     optimizes code to not be sequentially consistent.  It then lands
>>     on the developers shoulders to understand every possible,
>>     undocumented optimization that might be done by the compiler to
>>     try and understand why the code is misbehaving.  Only then, can
>>     they possibly formulate which changes will actually correct the
>>     behavior of their code, without making it "slower" than it needs
>>     to be.  To me, that is exactly the wrong way for a developer to
>>     test and optimize their code.  Visible data races that are only
>>     about "the data", are trivial to reason about.
>>
>>     If I saw the Thread.setName/getName lack of synchronization, out
>>     in the wild, create a corrupted thread name, I'd be writing a bug
>>     report about the JVM creating memory corruption before I would
>>     consider that the array had been allocated and made visible
>>     before it was filled.  That's not how the code is written, and
>>     that's not how it should execute without the developer deciding
>>     that such an optimization is safe for their application to see.
>>
>>     Suggesting that denying the visibility of the optimization is
>>     selected by the developer not doing anything, is where I
>>     respectfully, cannot agree with such reasoning.  This is not a
>>     new issue.  The design of many functional languages is based on
>>     the simple fact that developers don't need to be burdened with
>>     designating that they want correctly executing code.  They should
>>     get that by default, and have to work at "faster" or "more
>>     performant" or "less latency", by explicit actions they take in
>>     code structure.
>>
>>     Thread.setName/getName is a great example of "no thought given"
>>     to concurrency in the original design.  That code, as said
>>     before, is only executed in a concurrent environment.  Why didn't
>>     the original code have the correct concurrency design.  Why
>>     hasn't it been visible to enough people that a bug report written
>>     and it already fixed?
>>
>>     The answer is, because that racy code, doesn't "act" wrong, in
>>     general usage patterns.  But optimizations due to how volatile
>>     works and how JIT developers exploit lack of "concurrency
>>     selected" coding, could, at any point, break that code.
>>
>>     This is how all of that legacy code, laying around on the
>>     internet, is going to come down.  It's going to start randomly
>>     breaking and causing completely unexplainable bugs.  People will
>>     not remember enough details about this code to actually
>>     understand that the JIT is causing them problems with reordering,
>>     loop hoisting or other "nifty" optimizations that provide a .1%
>>     improvement in speed.   It will be a giant waste of peoples time
>>     trying to reconcile what is actually going wrong, and in some
>>     cases, there will be real impact on the users lives, welfare
>>     and/or safety, potentially.
>>
>>     Call my position extreme, but I think it's vital for the Java
>>     community and Oracle in particular to understand that the path we
>>     are going down is absolutely a perilous disaster without
>>     something very specific being visible to developers to allow them
>>     to understand how their code is being executed so that they can
>>     see exactly what parts of the code need to be fixed.
>>
>>     Gregg Wonderly
>>
>>     On Apr 30, 2013, at 7:48 PM, "Boehm, Hans" <hans.boehm at hp.com
>>     <mailto:hans.boehm at hp.com>> wrote:
>>
>>>     A nice simple example to consider here is a user application
>>>     that declares an array, and calls a library to sort it.  The
>>>     sort library uses a parallel sort that relies on a sequential
>>>     sort to sort small sections of the array. In a
>>>     sequential-by-default world, how would you declare the parallel
>>>     sections?  Would it be any different than what we do now? The
>>>     user application that declares the array may never know that
>>>     there is any parallel code involved.  Nor should it.
>>>     Applications such as this are naturally data-race-free, thus
>>>     there is no issue with the compiler ?breaking? code.  And the
>>>     compiler can apply nearly all sequentially valid transformations
>>>     on synchronization-free code, such as the sequential sort
>>>     operations. If you accidentally introduce a data race bug, it?s
>>>     unlikely your code would run correctly even if the compiler
>>>     guaranteed sequential consistency. Your code may be a bit easier
>>>     to debug with a hypothetical compiler that ensures sequential
>>>     consistency.  But so long as you avoided intentional
>>>     (unannotated) data races, I think a data race detector would
>>>     also make this fairly easy.
>>>     Hans
>>>     *From:*concurrency-interest-bounces at cs.oswego.edu
>>>     <mailto:concurrency-interest-bounces at cs.oswego.edu>[mailto:concurrency-interest-bounces at cs.oswego.edu
>>>     <mailto:interest-bounces at cs.oswego.edu>]*On Behalf Of*Martin
>>>     Thompson
>>>     *Sent:*Tuesday, April 30, 2013 5:57 AM
>>>     *To:*Kirk Pepperdine
>>>     *Cc:*Gregg Wonderly;concurrency-interest at cs.oswego.edu
>>>     <mailto:concurrency-interest at cs.oswego.edu>
>>>     *Subject:*Re: [concurrency-interest] JLS 17.7 Non-atomic
>>>     treatment of double and long : Android
>>>     I agree with Kirk here and would take it further.  By default
>>>     the vast majority of code should be single threaded and
>>>     concurrent programming is only utilized in regions of data exchange.
>>>     If all code was sequentially consistent then most hardware and
>>>     compiler optimizations would be defeated.  A default position
>>>     that all code is concurrent is sending the industry the wrong
>>>     way in my view.  It makes a more sense to explicitly define the
>>>     regions of data exchange in our programs and therefore what
>>>     ordering semantics are required in those regions.
>>>     Martin...
>>>
>>>         ------------------------------
>>>         Message: 3
>>>         Date: Tue, 30 Apr 2013 07:38:01 +0200
>>>         From: Kirk Pepperdine <kirk at kodewerk.com
>>>         <mailto:kirk at kodewerk.com>>
>>>         To: Gregg Wonderly <gergg at cox.net <mailto:gergg at cox.net>>
>>>         Cc: concurrency-interest at cs.oswego.edu
>>>         <mailto:concurrency-interest at cs.oswego.edu>Subject: Re:
>>>         [concurrency-interest] JLS 17.7 Non-atomic treatment of
>>>                 double  and long : Android
>>>         Message-ID:
>>>         <5D85887E-8BFE-4F09-AFA8-53FE1DFD52D3 at kodewerk.com
>>>         <mailto:5D85887E-8BFE-4F09-AFA8-53FE1DFD52D3 at kodewerk.com>>
>>>         Content-Type: text/plain; charset="windows-1252"
>>>         Sorry but making thing volatile by default would be a
>>>         horrible thing to do. Code wouldn't be a bit slower, it
>>>         would be a lot slower and then you'd end up with the same
>>>         problem in reverse!
>>>         Regards,
>>>         Kirk
>>>         On 2013-04-30, at 12:10 AM, Gregg Wonderly <gergg at cox.net
>>>         <mailto:gergg at cox.net>> wrote:
>>>         > This code exists everywhere on the Java JVM now, because
>>>         no one expects the loop hoist?   People are living with it,
>>>         or eventually declaring the loop variable volatile after
>>>         finding these discussions.
>>>         >
>>>         > Java, by default, should of used nothing but volatile
>>>         variables, and developers should of needed to add
>>>         non-volatile declarations via annotations, without the
>>>         'volatile' keyword being used, at all.
>>>         >
>>>         > That would of made it hard to "break" code without
>>>         actually looking up what you were doing, because the added
>>>         verbosity would only be tolerated when it actually
>>>         accomplished a performance improvement.  Today, code is
>>>         "Faster" without "volatile".   If everything was "volatile"
>>>         by default, then code would be slower to start with, and
>>>         proper "concurrency programming" would then make your code
>>>         faster, as concurrency should.
>>>         >
>>>         > Gregg
>>>
>>>     _______________________________________________
>>>     Concurrency-interest mailing list
>>>     Concurrency-interest at cs.oswego.edu
>>>     <mailto:Concurrency-interest at cs.oswego.edu>
>>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>>     _______________________________________________
>>     Concurrency-interest mailing list
>>     Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130501/727af840/attachment-0001.html>

From gergg at cox.net  Wed May  1 12:02:05 2013
From: gergg at cox.net (Gregg Wonderly)
Date: Wed, 1 May 2013 11:02:05 -0500
Subject: [concurrency-interest] JLS 17.7 Non-atomic treatment of double
	and long : Android
In-Reply-To: <5181355D.6050300@oracle.com>
References: <CAChYfd-8DO0YCu+cEHj_MGVNzyoPewD+HSwuaF_HmAr_6K-31A@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD236BB8242@G9W0725.americas.hpqcorp.net>
	<7EC0C469-F8AA-4419-94D5-592305C62293@cox.net>
	<5180D590.2070808@oracle.com>
	<CAMyLHFyGUCpVqb9pZS=NX-WBkHK79mESuQ9Ykvx7uRBa7O9kcw@mail.gmail.com>
	<5181355D.6050300@oracle.com>
Message-ID: <8090E541-735A-4D74-8AC2-2565681A0220@cox.net>


On May 1, 2013, at 10:31 AM, oleksandr otenko <oleksandr.otenko at oracle.com> wrote:

> That is kind of my point, actually.
> 
> I can't claim expert understanding of sequential consistency, but the example piece of code can be shown broken even if we conclude it is sequentially consistent. Which then leads to the next question to Gregg - why fight for sequential consistency so hard, if we care about something else, about a deeper consistency.

Because software is written as an ordered stream of instructions, in Java, developers are going to expect an ordered stream of results, unless and until they actually become aware of what the JMM allows the JIT to do.  I do not believe for a minute, that Java developers read through the JLS and the JMM, and take a massive test to demonstrate that they know everything that is not "correct".  They know little about code structures the IDE/editor will accept, and the compiler will compile to .class files, vs what results they can expect out of each processor/OS/JVM that their code might run on.

In various places/mailing lists, where I have interacted with developers over the years, most don't believe that data races are ever a "nasty" thing.  What they use synchronized for, is just atomic controls for multi-faceted data updates.  They know that volatile exists, but few have actually really read the JMM and completely grasp how fragile Java software can be, because of what the JIT can choose to do based on what the JMM allows.

This year, at JavaOne, Oracle ought to create a closed booth "Java Programming Test" that would allow you to evaluate how much developers actually know about concurrency in Java.  There should be code fragments, like the Thread.setName()/getName() pair.  Ask developers if the code can cause an exception, or result in incomplete data becoming visible to the program.   Be creative and see just what might actually, really be going on in peoples minds.  Don't ask everyone the same question.  Have many different examples of the same issue, and mix it up so that people can't talk to each other and learn the right answers before they see the "test".

We have a lot of knowledgable people in the Java community.   Whether it was James Gosling or one of his small team that worked on the initial Java implementation, someone didn't use synchronized on those methods (the only control that was available then), because early java provided sequential consistency.   There was no JIT.  There's lots of code, just like that laying around because "synchronized" was expensive.   People looked at their code, like setName()/getName(), and said there is nothing atomic that needs to happen here, so no synchronization is necessary. 

Gregg Wonderly


> 
> Alex
> 
> 
> On 01/05/2013 15:53, Tim Halloran wrote:
>> On Wed, May 1, 2013 at 4:42 AM, oleksandr otenko <oleksandr.otenko at oracle.com> wrote:
>> I don't know what everyone means by "sequential consistency".
>> 
>> There are several links on Doug Lea's JSR-166 Cookbook page, but this is probably the one to have a look at: 
>> 
>> Sarita Adve and Kourosh Gharachorloo, Shared Memory Consistency Models: A Tutorial
>>  
>> 
>> Hashtable<K,Integer> t = new Hashtable<>();
>> 
>> ...
>> 
>> t.put(k, t.get(k)+1);
>> 
>> is this sequentially consistent?
>> 
>> 
>> Alex
>> 
>> 
>> 
>> On 01/05/2013 03:21, Gregg Wonderly wrote:
>>> The problem is that people can trivially create data races.  It's happened everywhere.  There is so much legacy Java code that has data races.   That's no the large problem.  Data races are one thing.  The large problem is when the JIT reorders code and optimizes code to not be sequentially consistent.  It then lands on the developers shoulders to                       understand every possible, undocumented optimization that might be done by the compiler to try and understand why the code is misbehaving.  Only then, can they possibly formulate which changes will actually correct the behavior of their code, without making it "slower" than it needs to be.  To me, that is exactly the wrong way for a developer to test and optimize their code.  Visible data races that are only about "the data", are trivial to reason about.  
>>> 
>>> If I saw the Thread.setName/getName lack of synchronization, out in the wild, create a corrupted thread name, I'd be writing a bug report about the JVM creating memory corruption before I would consider that the array had been allocated and made visible before it was filled.  That's not how the code is written, and that's not how it should execute without the developer deciding that such an optimization is safe for their application to see.
>>> 
>>> Suggesting that denying the visibility of the optimization is selected by the developer not doing anything, is where I respectfully, cannot agree with such reasoning.  This is not a new issue.  The design of many functional languages is based on the simple fact that developers don't need to be burdened with designating that they want correctly executing code.  They should get that by default, and have to work at "faster" or "more performant" or "less latency", by explicit actions they take in code structure.
>>> 
>>> Thread.setName/getName is a great example of "no thought given" to concurrency in the original design.  That code, as said before, is only executed in a concurrent environment.  Why didn't the original code have the correct concurrency design.  Why hasn't it been visible to enough people that a bug report written and it already fixed?
>>> 
>>> The answer is, because that racy code, doesn't "act" wrong, in general usage patterns.  But optimizations due to how volatile works and how JIT developers exploit lack of "concurrency selected" coding, could, at any point, break that code.
>>> 
>>> This is how all of that legacy code, laying around on the internet, is going to come down.  It's going to start randomly breaking and causing completely unexplainable bugs.  People will not remember enough details about this code to actually understand that the JIT is causing them problems with reordering, loop hoisting or other "nifty" optimizations that provide a .1% improvement in speed.   It will be a giant waste of peoples time trying to reconcile what is actually going wrong, and in some cases, there will be real impact on the users lives, welfare and/or safety, potentially.
>>> 
>>> Call my position extreme, but I think it's vital for the Java community and Oracle in particular to understand that the path we are going down is absolutely a perilous disaster without something very specific being visible to developers to allow them to understand how their code is being executed so that they can see exactly what parts of the code need to be fixed.
>>> 
>>> Gregg Wonderly
>>> 
>>> On Apr 30, 2013, at 7:48 PM, "Boehm, Hans" <hans.boehm at hp.com> wrote:
>>> 
>>>> A nice simple example to consider here is a user application that declares an array, and calls a library to sort it.  The sort library uses a parallel sort that relies on a sequential sort to sort small sections of the array.  In a sequential-by-default world, how would you declare the parallel sections?  Would it be any different than what we do now?  The user application that declares the array may never know that there is any parallel code involved.  Nor should it.
>>>>  
>>>> Applications such as this are naturally data-race-free, thus there is no issue with the compiler ?breaking? code.  And the compiler can apply nearly all sequentially valid transformations on synchronization-free code, such as the sequential sort operations.  If you accidentally introduce a data race bug, it?s unlikely your code would run correctly even if the compiler guaranteed sequential consistency. Your code may be a bit easier to debug with a hypothetical compiler that ensures sequential consistency.  But so long as you avoided intentional (unannotated) data races, I think a data race detector would also make this fairly easy.
>>>>  
>>>> Hans
>>>>  
>>>> From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Martin Thompson
>>>> Sent: Tuesday, April 30, 2013 5:57 AM
>>>> To: Kirk Pepperdine
>>>> Cc: Gregg Wonderly; concurrency-interest at cs.oswego.edu
>>>> Subject: Re: [concurrency-interest] JLS 17.7 Non-atomic treatment of double and long : Android
>>>>  
>>>> I agree with Kirk here and would take it further.  By default the vast majority of code should be single threaded and concurrent programming is only utilized in regions of data exchange.
>>>>  
>>>> If all code was sequentially consistent then most hardware and compiler optimizations would be defeated.  A default position that all code is concurrent is sending the industry the wrong way in my view.  It makes a more sense to explicitly define the regions of data exchange in our programs and therefore what ordering semantics are required in those regions.
>>>>  
>>>> Martin...
>>>> ------------------------------
>>>> Message: 3
>>>> Date: Tue, 30 Apr 2013 07:38:01 +0200
>>>> From: Kirk Pepperdine <kirk at kodewerk.com>
>>>> To: Gregg Wonderly <gergg at cox.net>
>>>> Cc: concurrency-interest at cs.oswego.edu
>>>> Subject: Re: [concurrency-interest] JLS 17.7 Non-atomic treatment of
>>>>         double  and long : Android
>>>> Message-ID: <5D85887E-8BFE-4F09-AFA8-53FE1DFD52D3 at kodewerk.com>
>>>> Content-Type: text/plain; charset="windows-1252"
>>>> Sorry but making thing volatile by default would be a horrible thing to do. Code wouldn't be a bit slower, it would be a lot slower and then you'd end up with the same problem in reverse!
>>>> Regards,
>>>> Kirk
>>>> On 2013-04-30, at 12:10 AM, Gregg Wonderly <gergg at cox.net> wrote:
>>>> > This code exists everywhere on the Java JVM now, because no one expects the loop hoist?   People are living with it, or eventually declaring the loop variable volatile after finding these discussions.
>>>> >
>>>> > Java, by default, should of used nothing but volatile variables, and developers should of needed to add non-volatile declarations via annotations, without the 'volatile' keyword being used, at all.
>>>> >
>>>> > That would of made it hard to "break" code without actually looking up what you were doing, because the added verbosity would only be tolerated when it actually accomplished a performance improvement.  Today, code is "Faster" without "volatile".   If everything was "volatile" by default, then code would be slower to start with, and proper "concurrency programming" would then make your code faster, as concurrency should.
>>>> >
>>>> > Gregg
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>> 
>>> 
>>> 
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> 
>> 
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> 
>> 
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130501/69a675b4/attachment-0001.html>

From oleksandr.otenko at oracle.com  Wed May  1 12:03:36 2013
From: oleksandr.otenko at oracle.com (oleksandr otenko)
Date: Wed, 01 May 2013 17:03:36 +0100
Subject: [concurrency-interest] JLS 17.7 Non-atomic treatment of double
 and long : Android
In-Reply-To: <CAMyLHFyGUCpVqb9pZS=NX-WBkHK79mESuQ9Ykvx7uRBa7O9kcw@mail.gmail.com>
References: <CAChYfd-8DO0YCu+cEHj_MGVNzyoPewD+HSwuaF_HmAr_6K-31A@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD236BB8242@G9W0725.americas.hpqcorp.net>
	<7EC0C469-F8AA-4419-94D5-592305C62293@cox.net>
	<5180D590.2070808@oracle.com>
	<CAMyLHFyGUCpVqb9pZS=NX-WBkHK79mESuQ9Ykvx7uRBa7O9kcw@mail.gmail.com>
Message-ID: <51813CD8.90509@oracle.com>

Also, it would be good to find a good discussion of sequential 
consistency of algorithms, not just hardware platform, like it is 
treated by M. Herlihy 20 years ago in the paper on linearizability. Is 
this a common way to discuss algorithms?

In particular, M. Herlihy demonstrates that observing that a sub-history 
of events is sequentially consistent is not enough - one needs the view 
of the entire system to determine whether the system as a whole is 
sequentially consistent. This sort of consistency analysis simply 
doesn't scale. What is needed, is a consistency requirement that is 
local to the component in question.

He doesn't give an example of an algorithm, only a history of events; 
but I think the below t.put(k, t.get(k)+1) is one such example. If we 
only look at one thread, the behaviour will look sequentially 
consistent. But if we consider two threads at the same time, it will 
become apparent that the logic is broken.

Alex

On 01/05/2013 15:53, Tim Halloran wrote:
> On Wed, May 1, 2013 at 4:42 AM, oleksandr otenko 
> <oleksandr.otenko at oracle.com <mailto:oleksandr.otenko at oracle.com>> wrote:
>
>     I don't know what everyone means by "sequential consistency".
>
>
> There are several links on Doug Lea's JSR-166 Cookbook page, but this 
> is probably the one to have a look at:
>
> Sarita Adve and Kourosh Gharachorloo, Shared Memory Consistency 
> Models: A Tutorial <http://rsim.cs.uiuc.edu/%7Esadve/>
>
>
>     Hashtable<K,Integer> t = new Hashtable<>();
>
>     ...
>
>     t.put(k, t.get(k)+1);
>
>     is this sequentially consistent?
>
>
>     Alex
>
>
>
>     On 01/05/2013 03:21, Gregg Wonderly wrote:
>>     The problem is that people can trivially create data races.  It's
>>     happened everywhere.  There is so much legacy Java code that has
>>     data races.   That's no the large problem.  Data races are one
>>     thing.  The large problem is when the JIT reorders code and
>>     optimizes code to not be sequentially consistent.  It then lands
>>     on the developers shoulders to understand every possible,
>>     undocumented optimization that might be done by the compiler to
>>     try and understand why the code is misbehaving.  Only then, can
>>     they possibly formulate which changes will actually correct the
>>     behavior of their code, without making it "slower" than it needs
>>     to be.  To me, that is exactly the wrong way for a developer to
>>     test and optimize their code.  Visible data races that are only
>>     about "the data", are trivial to reason about.
>>
>>     If I saw the Thread.setName/getName lack of synchronization, out
>>     in the wild, create a corrupted thread name, I'd be writing a bug
>>     report about the JVM creating memory corruption before I would
>>     consider that the array had been allocated and made visible
>>     before it was filled.  That's not how the code is written, and
>>     that's not how it should execute without the developer deciding
>>     that such an optimization is safe for their application to see.
>>
>>     Suggesting that denying the visibility of the optimization is
>>     selected by the developer not doing anything, is where I
>>     respectfully, cannot agree with such reasoning.  This is not a
>>     new issue.  The design of many functional languages is based on
>>     the simple fact that developers don't need to be burdened with
>>     designating that they want correctly executing code.  They should
>>     get that by default, and have to work at "faster" or "more
>>     performant" or "less latency", by explicit actions they take in
>>     code structure.
>>
>>     Thread.setName/getName is a great example of "no thought given"
>>     to concurrency in the original design.  That code, as said
>>     before, is only executed in a concurrent environment.  Why didn't
>>     the original code have the correct concurrency design.  Why
>>     hasn't it been visible to enough people that a bug report written
>>     and it already fixed?
>>
>>     The answer is, because that racy code, doesn't "act" wrong, in
>>     general usage patterns.  But optimizations due to how volatile
>>     works and how JIT developers exploit lack of "concurrency
>>     selected" coding, could, at any point, break that code.
>>
>>     This is how all of that legacy code, laying around on the
>>     internet, is going to come down.  It's going to start randomly
>>     breaking and causing completely unexplainable bugs.  People will
>>     not remember enough details about this code to actually
>>     understand that the JIT is causing them problems with reordering,
>>     loop hoisting or other "nifty" optimizations that provide a .1%
>>     improvement in speed.   It will be a giant waste of peoples time
>>     trying to reconcile what is actually going wrong, and in some
>>     cases, there will be real impact on the users lives, welfare
>>     and/or safety, potentially.
>>
>>     Call my position extreme, but I think it's vital for the Java
>>     community and Oracle in particular to understand that the path we
>>     are going down is absolutely a perilous disaster without
>>     something very specific being visible to developers to allow them
>>     to understand how their code is being executed so that they can
>>     see exactly what parts of the code need to be fixed.
>>
>>     Gregg Wonderly
>>
>>     On Apr 30, 2013, at 7:48 PM, "Boehm, Hans" <hans.boehm at hp.com
>>     <mailto:hans.boehm at hp.com>> wrote:
>>
>>>     A nice simple example to consider here is a user application
>>>     that declares an array, and calls a library to sort it.  The
>>>     sort library uses a parallel sort that relies on a sequential
>>>     sort to sort small sections of the array. In a
>>>     sequential-by-default world, how would you declare the parallel
>>>     sections?  Would it be any different than what we do now? The
>>>     user application that declares the array may never know that
>>>     there is any parallel code involved.  Nor should it.
>>>     Applications such as this are naturally data-race-free, thus
>>>     there is no issue with the compiler ?breaking? code.  And the
>>>     compiler can apply nearly all sequentially valid transformations
>>>     on synchronization-free code, such as the sequential sort
>>>     operations. If you accidentally introduce a data race bug, it?s
>>>     unlikely your code would run correctly even if the compiler
>>>     guaranteed sequential consistency. Your code may be a bit easier
>>>     to debug with a hypothetical compiler that ensures sequential
>>>     consistency.  But so long as you avoided intentional
>>>     (unannotated) data races, I think a data race detector would
>>>     also make this fairly easy.
>>>     Hans
>>>     *From:*concurrency-interest-bounces at cs.oswego.edu
>>>     <mailto:concurrency-interest-bounces at cs.oswego.edu>[mailto:concurrency-interest-bounces at cs.oswego.edu
>>>     <mailto:interest-bounces at cs.oswego.edu>]*On Behalf Of*Martin
>>>     Thompson
>>>     *Sent:*Tuesday, April 30, 2013 5:57 AM
>>>     *To:*Kirk Pepperdine
>>>     *Cc:*Gregg Wonderly;concurrency-interest at cs.oswego.edu
>>>     <mailto:concurrency-interest at cs.oswego.edu>
>>>     *Subject:*Re: [concurrency-interest] JLS 17.7 Non-atomic
>>>     treatment of double and long : Android
>>>     I agree with Kirk here and would take it further.  By default
>>>     the vast majority of code should be single threaded and
>>>     concurrent programming is only utilized in regions of data exchange.
>>>     If all code was sequentially consistent then most hardware and
>>>     compiler optimizations would be defeated.  A default position
>>>     that all code is concurrent is sending the industry the wrong
>>>     way in my view.  It makes a more sense to explicitly define the
>>>     regions of data exchange in our programs and therefore what
>>>     ordering semantics are required in those regions.
>>>     Martin...
>>>
>>>         ------------------------------
>>>         Message: 3
>>>         Date: Tue, 30 Apr 2013 07:38:01 +0200
>>>         From: Kirk Pepperdine <kirk at kodewerk.com
>>>         <mailto:kirk at kodewerk.com>>
>>>         To: Gregg Wonderly <gergg at cox.net <mailto:gergg at cox.net>>
>>>         Cc: concurrency-interest at cs.oswego.edu
>>>         <mailto:concurrency-interest at cs.oswego.edu>Subject: Re:
>>>         [concurrency-interest] JLS 17.7 Non-atomic treatment of
>>>                 double  and long : Android
>>>         Message-ID:
>>>         <5D85887E-8BFE-4F09-AFA8-53FE1DFD52D3 at kodewerk.com
>>>         <mailto:5D85887E-8BFE-4F09-AFA8-53FE1DFD52D3 at kodewerk.com>>
>>>         Content-Type: text/plain; charset="windows-1252"
>>>         Sorry but making thing volatile by default would be a
>>>         horrible thing to do. Code wouldn't be a bit slower, it
>>>         would be a lot slower and then you'd end up with the same
>>>         problem in reverse!
>>>         Regards,
>>>         Kirk
>>>         On 2013-04-30, at 12:10 AM, Gregg Wonderly <gergg at cox.net
>>>         <mailto:gergg at cox.net>> wrote:
>>>         > This code exists everywhere on the Java JVM now, because
>>>         no one expects the loop hoist?   People are living with it,
>>>         or eventually declaring the loop variable volatile after
>>>         finding these discussions.
>>>         >
>>>         > Java, by default, should of used nothing but volatile
>>>         variables, and developers should of needed to add
>>>         non-volatile declarations via annotations, without the
>>>         'volatile' keyword being used, at all.
>>>         >
>>>         > That would of made it hard to "break" code without
>>>         actually looking up what you were doing, because the added
>>>         verbosity would only be tolerated when it actually
>>>         accomplished a performance improvement.  Today, code is
>>>         "Faster" without "volatile".   If everything was "volatile"
>>>         by default, then code would be slower to start with, and
>>>         proper "concurrency programming" would then make your code
>>>         faster, as concurrency should.
>>>         >
>>>         > Gregg
>>>
>>>     _______________________________________________
>>>     Concurrency-interest mailing list
>>>     Concurrency-interest at cs.oswego.edu
>>>     <mailto:Concurrency-interest at cs.oswego.edu>
>>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>>     _______________________________________________
>>     Concurrency-interest mailing list
>>     Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130501/a2c9bd7c/attachment-0001.html>

From hans.boehm at hp.com  Wed May  1 15:41:16 2013
From: hans.boehm at hp.com (Boehm, Hans)
Date: Wed, 1 May 2013 19:41:16 +0000
Subject: [concurrency-interest] JLS 17.7 Non-atomic treatment of double
 and long : Android
In-Reply-To: <7EC0C469-F8AA-4419-94D5-592305C62293@cox.net>
References: <CAChYfd-8DO0YCu+cEHj_MGVNzyoPewD+HSwuaF_HmAr_6K-31A@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD236BB8242@G9W0725.americas.hpqcorp.net>
	<7EC0C469-F8AA-4419-94D5-592305C62293@cox.net>
Message-ID: <A3E67C2071F49C4CBC4F17E6D77CDDD236BB8631@G9W0725.americas.hpqcorp.net>

I think this is the wrong programming model.  Even if the implementation guaranteed sequential consistency, data races would usually indicate a bug.  Sequential consistency helps with Thread.setName() only because the natural implementation of the operation is indivisible.  If it involved setting more than one field, the racy code would still break, even in a sequentially consistent implementation.

I claim that in reality programmers expect not just sequential consistency (i.e. as if the threads were simply interleaved), but they also expect that code sections that appear sequential, i.e. those that contain no synchronization, execute sequentially, without interference from other threads.  I can't read a piece of code if any field can change out from under me at any time.  The only way I can get that required guarantee is by writing data-race-free code, which requires me to annotate synchronization variables as volatile.  Once I do that, we know how to built tools that will report data races in real code, admittedly with 10x-100x slowdown (in the absence of hardware support).  If such a tool reports no data races, then I know that the execution is sequential consistency, even without forcing my compiler to treat everything as volatile.  And since I didn't declare everything as volatile, I also get the equally important guarantee that synchronization-free code executes sequentially, i.e. atomically, and thus I don't need to worry about possible interleaving at  every instruction.  I don't need to worry about whether synchronization-free library calls are implemented with a single assignment or three.  I think this is a much more practical programming model than just sequential consistency.  And it's the one that motivates various current language memory models, including the Java one.

In this world, which requires better tools, and somewhat more disciplined code, but not a sequentially consistent memory model, I expect the Thread.setName() problem would have been fairly easy to debug.  We unfortunately still rely on testing with a race detector.  But if we can get the code to fail under those conditions, the error message would have clearly pointed at the problem before you ever saw a corrupt thread name.  (The most natural fix in this case may still involve adding synchronization to the library, and fixing the documentation to reflect that, but that's a separate issue.)

(Reality is slightly more complicated, and I think we'd need a way other than "volatile" to annotate races on variables when we don't expect enforcement of sequential consistency, but just want the current. tricky and misspecified, Java variable semantics, such as they are.  But I don't think that mechanism should be used by, or interfere with, the vast majority of programmers, whom I believe Gregg is addressing.)

Hans

From: Gregg Wonderly [mailto:gergg at cox.net]
Sent: Tuesday, April 30, 2013 7:21 PM
To: Boehm, Hans
Cc: Martin Thompson; Kirk Pepperdine; concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] JLS 17.7 Non-atomic treatment of double and long : Android

The problem is that people can trivially create data races.  It's happened everywhere.  There is so much legacy Java code that has data races.   That's no the large problem.  Data races are one thing.  The large problem is when the JIT reorders code and optimizes code to not be sequentially consistent.  It then lands on the developers shoulders to understand every possible, undocumented optimization that might be done by the compiler to try and understand why the code is misbehaving.  Only then, can they possibly formulate which changes will actually correct the behavior of their code, without making it "slower" than it needs to be.  To me, that is exactly the wrong way for a developer to test and optimize their code.  Visible data races that are only about "the data", are trivial to reason about.

If I saw the Thread.setName/getName lack of synchronization, out in the wild, create a corrupted thread name, I'd be writing a bug report about the JVM creating memory corruption before I would consider that the array had been allocated and made visible before it was filled.  That's not how the code is written, and that's not how it should execute without the developer deciding that such an optimization is safe for their application to see.

Suggesting that denying the visibility of the optimization is selected by the developer not doing anything, is where I respectfully, cannot agree with such reasoning.  This is not a new issue.  The design of many functional languages is based on the simple fact that developers don't need to be burdened with designating that they want correctly executing code.  They should get that by default, and have to work at "faster" or "more performant" or "less latency", by explicit actions they take in code structure.

Thread.setName/getName is a great example of "no thought given" to concurrency in the original design.  That code, as said before, is only executed in a concurrent environment.  Why didn't the original code have the correct concurrency design.  Why hasn't it been visible to enough people that a bug report written and it already fixed?

The answer is, because that racy code, doesn't "act" wrong, in general usage patterns.  But optimizations due to how volatile works and how JIT developers exploit lack of "concurrency selected" coding, could, at any point, break that code.

This is how all of that legacy code, laying around on the internet, is going to come down.  It's going to start randomly breaking and causing completely unexplainable bugs.  People will not remember enough details about this code to actually understand that the JIT is causing them problems with reordering, loop hoisting or other "nifty" optimizations that provide a .1% improvement in speed.   It will be a giant waste of peoples time trying to reconcile what is actually going wrong, and in some cases, there will be real impact on the users lives, welfare and/or safety, potentially.

Call my position extreme, but I think it's vital for the Java community and Oracle in particular to understand that the path we are going down is absolutely a perilous disaster without something very specific being visible to developers to allow them to understand how their code is being executed so that they can see exactly what parts of the code need to be fixed.

Gregg Wonderly

On Apr 30, 2013, at 7:48 PM, "Boehm, Hans" <hans.boehm at hp.com<mailto:hans.boehm at hp.com>> wrote:


A nice simple example to consider here is a user application that declares an array, and calls a library to sort it.  The sort library uses a parallel sort that relies on a sequential sort to sort small sections of the array.  In a sequential-by-default world, how would you declare the parallel sections?  Would it be any different than what we do now?  The user application that declares the array may never know that there is any parallel code involved.  Nor should it.

Applications such as this are naturally data-race-free, thus there is no issue with the compiler "breaking" code.  And the compiler can apply nearly all sequentially valid transformations on synchronization-free code, such as the sequential sort operations.  If you accidentally introduce a data race bug, it's unlikely your code would run correctly even if the compiler guaranteed sequential consistency. Your code may be a bit easier to debug with a hypothetical compiler that ensures sequential consistency.  But so long as you avoided intentional (unannotated) data races, I think a data race detector would also make this fairly easy.

Hans

From: concurrency-interest-bounces at cs.oswego.edu<mailto:concurrency-interest-bounces at cs.oswego.edu> [mailto:concurrency-interest-bounces at cs.oswego.edu<mailto:interest-bounces at cs.oswego.edu>] On Behalf Of Martin Thompson
Sent: Tuesday, April 30, 2013 5:57 AM
To: Kirk Pepperdine
Cc: Gregg Wonderly; concurrency-interest at cs.oswego.edu<mailto:concurrency-interest at cs.oswego.edu>
Subject: Re: [concurrency-interest] JLS 17.7 Non-atomic treatment of double and long : Android

I agree with Kirk here and would take it further.  By default the vast majority of code should be single threaded and concurrent programming is only utilized in regions of data exchange.

If all code was sequentially consistent then most hardware and compiler optimizations would be defeated.  A default position that all code is concurrent is sending the industry the wrong way in my view.  It makes a more sense to explicitly define the regions of data exchange in our programs and therefore what ordering semantics are required in those regions.

Martin...
------------------------------
Message: 3
Date: Tue, 30 Apr 2013 07:38:01 +0200
From: Kirk Pepperdine <kirk at kodewerk.com<mailto:kirk at kodewerk.com>>
To: Gregg Wonderly <gergg at cox.net<mailto:gergg at cox.net>>
Cc: concurrency-interest at cs.oswego.edu
<mailto:concurrency-interest at cs.oswego.edu>Subject: Re: [concurrency-interest] JLS 17.7 Non-atomic treatment of
        double  and long : Android
Message-ID: <5D85887E-8BFE-4F09-AFA8-53FE1DFD52D3 at kodewerk.com<mailto:5D85887E-8BFE-4F09-AFA8-53FE1DFD52D3 at kodewerk.com>>
Content-Type: text/plain; charset="windows-1252"
Sorry but making thing volatile by default would be a horrible thing to do. Code wouldn't be a bit slower, it would be a lot slower and then you'd end up with the same problem in reverse!
Regards,
Kirk
On 2013-04-30, at 12:10 AM, Gregg Wonderly <gergg at cox.net<mailto:gergg at cox.net>> wrote:
> This code exists everywhere on the Java JVM now, because no one expects the loop hoist?   People are living with it, or eventually declaring the loop variable volatile after finding these discussions.
>
> Java, by default, should of used nothing but volatile variables, and developers should of needed to add non-volatile declarations via annotations, without the 'volatile' keyword being used, at all.
>
> That would of made it hard to "break" code without actually looking up what you were doing, because the added verbosity would only be tolerated when it actually accomplished a performance improvement.  Today, code is "Faster" without "volatile".   If everything was "volatile" by default, then code would be slower to start with, and proper "concurrency programming" would then make your code faster, as concurrency should.
>
> Gregg
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu<mailto:Concurrency-interest at cs.oswego.edu>
http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130501/427e996b/attachment-0001.html>

From hallerp at gmail.com  Wed May  1 19:03:35 2013
From: hallerp at gmail.com (Philipp Haller)
Date: Thu, 2 May 2013 01:03:35 +0200
Subject: [concurrency-interest] Critical fix in jsr166y for Scala 2.10.2
In-Reply-To: <518104AF.4010909@cs.oswego.edu>
References: <EAFE4F7A-BAF7-4677-83D1-F12F676ED67E@gmail.com>
	<518104AF.4010909@cs.oswego.edu>
Message-ID: <3A97D946-4B99-4CF7-AD35-1E6AC7D022FA@gmail.com>

Thanks for your prompt response, Doug.


On May 1, 2013, at 2:03 PM, Doug Lea wrote:

> On 05/01/13 04:07, Philipp Haller wrote:
>> Hi all,
>> 
>> Scala's standard library ships with a fork of jsr166y to provide a
>> high-performance thread pool on Java6.
>> 
>> We've recently discovered a performance/scalability bug in our fork which lead
>> to underutilized worker threads (see https://issues.scala-lang.org/browse/SI-7438).
>> 
>> The most recent versions of jsr166y and jsr166e do not suffer from this bug.
>> 
>> A workaround using the revision of jsr166y shipped in Scala is not to use `fork`
>> in our global "execution context". However, this seems like a drastic measure,
> 
> Right, don't do this.

OK, thanks for confirming.

> 
>> - Given that Scala 2.10.2 will be binary compatible with Scala 2.10.1, is it
>> safe to update to the latest revision of jsr166y? In other words, are later
>> revisions of jsr166y binary compatible with earlier revisions?
> 
> To the best of my knowledge, yes. The jsr166{x,y,e} builds are
> not guaranteed to be stable, but jsr166y (i.e., updates of jdk7
> functionality usable on JDK6+) hasn't changed incompatibly recently.

That's good news for us.

> 
>> - Besides the above fix, are there any other observable changes to be expected
>> when updating jsr166y to the latest revision?
> 
> Nothing I know of that would impact existing usages.

OK, perfect.
So, do you recommend upgrading to the latest revision then? Or could you point us to a specific revision which fixes the issue we're seeing (if that's feasible...)?

Thanks,
Philipp



From zhong.j.yu at gmail.com  Thu May  2 00:16:59 2013
From: zhong.j.yu at gmail.com (Zhong Yu)
Date: Wed, 1 May 2013 23:16:59 -0500
Subject: [concurrency-interest] JLS 17.7 Non-atomic treatment of double
 and long : Android
In-Reply-To: <A3E67C2071F49C4CBC4F17E6D77CDDD236BB8631@G9W0725.americas.hpqcorp.net>
References: <CAChYfd-8DO0YCu+cEHj_MGVNzyoPewD+HSwuaF_HmAr_6K-31A@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD236BB8242@G9W0725.americas.hpqcorp.net>
	<7EC0C469-F8AA-4419-94D5-592305C62293@cox.net>
	<A3E67C2071F49C4CBC4F17E6D77CDDD236BB8631@G9W0725.americas.hpqcorp.net>
Message-ID: <CACuKZqFCaDNtB1eNir-yQ8j+2kwhEVc=SQ4Cw8kVHeDH9mhngA@mail.gmail.com>

Good point Hans, however I doubt any sane programmer would intentionally
play with interleaving which is too hard to reason. Regardless of memory
model, the most important use case of shared variable is atomic publication
- a thread writes to some places that nobody else is supposed to be
reading, then writes to a shared variable to publish all previous writes.
No interleaving. Programmers intuitively assume that it should work; the
possibility of out-of-order writes is too bizzare to register in our minds.
Is it really worth it for JMM to go against this intuition? How much
performance gain are we talking about anyway? The java community has wasted
a lot of time on it.

Another problem is the syntax - volatility is declared on the variable,
instead of on read/write actions. If it's the later, programmers are more
likely to do the right thing because they do think about inter-thread
effects when they read/write shared variables.

Zhong Yu




On Wed, May 1, 2013 at 2:41 PM, Boehm, Hans <hans.boehm at hp.com> wrote:

>  I think this is the wrong programming model.  Even if the implementation
> guaranteed sequential consistency, data races would usually indicate a
> bug.  Sequential consistency helps with Thread.setName() only because the
> natural implementation of the operation is indivisible.  If it involved
> setting more than one field, the racy code would still break, even in a
> sequentially consistent implementation.****
>
> ** **
>
> I claim that in reality programmers expect not just sequential consistency
> (i.e. as if the threads were simply interleaved), but they also expect that
> code sections that appear sequential, i.e. those that contain no
> synchronization, execute sequentially, without interference from other
> threads.  I can?t read a piece of code if any field can change out from
> under me at any time.  The only way I can get that required guarantee is by
> writing data-race-free code, which requires me to annotate synchronization
> variables as volatile.  Once I do that, we know how to built tools that
> will report data races in real code, admittedly with 10x-100x slowdown (in
> the absence of hardware support).  If such a tool reports no data races,
> then I know that the execution is sequential consistency, even without
> forcing my compiler to treat everything as volatile.  And since I didn?t
> declare everything as volatile, I also get the equally important guarantee
> that synchronization-free code executes sequentially, i.e. atomically, and
> thus I don?t need to worry about possible interleaving at  every
> instruction.  I don?t need to worry about whether synchronization-free
> library calls are implemented with a single assignment or three.  I think
> this is a much more practical programming model than just sequential
> consistency.  And it?s the one that motivates various current language
> memory models, including the Java one.****
>
> ** **
>
> In this world, which requires better tools, and somewhat more disciplined
> code, but not a sequentially consistent memory model, I expect the
> Thread.setName() problem would have been fairly easy to debug.  We
> unfortunately still rely on testing with a race detector.  But if we can
> get the code to fail under those conditions, the error message would have
> clearly pointed at the problem before you ever saw a corrupt thread name.
> (The most natural fix in this case may still involve adding synchronization
> to the library, and fixing the documentation to reflect that, but that?s a
> separate issue.)****
>
> ** **
>
> (Reality is slightly more complicated, and I think we?d need a way other
> than ?volatile? to annotate races on variables when we don?t expect
> enforcement of sequential consistency, but just want the current. tricky
> and misspecified, Java variable semantics, such as they are.  But I don?t
> think that mechanism should be used by, or interfere with, the vast
> majority of programmers, whom I believe Gregg is addressing.)****
>
> ** **
>
> Hans****
>
> ** **
>
> *From:* Gregg Wonderly [mailto:gergg at cox.net]
> *Sent:* Tuesday, April 30, 2013 7:21 PM
> *To:* Boehm, Hans
> *Cc:* Martin Thompson; Kirk Pepperdine; concurrency-interest at cs.oswego.edu
>
> *Subject:* Re: [concurrency-interest] JLS 17.7 Non-atomic treatment of
> double and long : Android****
>
>  ** **
>
> The problem is that people can trivially create data races.  It's happened
> everywhere.  There is so much legacy Java code that has data races.
> That's no the large problem.  Data races are one thing.  The large problem
> is when the JIT reorders code and optimizes code to not be sequentially
> consistent.  It then lands on the developers shoulders to understand every
> possible, undocumented optimization that might be done by the compiler to
> try and understand why the code is misbehaving.  Only then, can they
> possibly formulate which changes will actually correct the behavior of
> their code, without making it "slower" than it needs to be.  To me, that is
> exactly the wrong way for a developer to test and optimize their code.
>  Visible data races that are only about "the data", are trivial to reason
> about.  ****
>
> ** **
>
> If I saw the Thread.setName/getName lack of synchronization, out in the
> wild, create a corrupted thread name, I'd be writing a bug report about the
> JVM creating memory corruption before I would consider that the array had
> been allocated and made visible before it was filled.  That's not how the
> code is written, and that's not how it should execute without the developer
> deciding that such an optimization is safe for their application to see.**
> **
>
> ** **
>
> Suggesting that denying the visibility of the optimization is selected by
> the developer not doing anything, is where I respectfully, cannot agree
> with such reasoning.  This is not a new issue.  The design of many
> functional languages is based on the simple fact that developers don't need
> to be burdened with designating that they want correctly executing code.
>  They should get that by default, and have to work at "faster" or "more
> performant" or "less latency", by explicit actions they take in code
> structure.****
>
> ** **
>
> Thread.setName/getName is a great example of "no thought given" to
> concurrency in the original design.  That code, as said before, is only
> executed in a concurrent environment.  Why didn't the original code have
> the correct concurrency design.  Why hasn't it been visible to enough
> people that a bug report written and it already fixed?****
>
> ** **
>
> The answer is, because that racy code, doesn't "act" wrong, in general
> usage patterns.  But optimizations due to how volatile works and how JIT
> developers exploit lack of "concurrency selected" coding, could, at any
> point, break that code.****
>
> ** **
>
> This is how all of that legacy code, laying around on the internet, is
> going to come down.  It's going to start randomly breaking and causing
> completely unexplainable bugs.  People will not remember enough details
> about this code to actually understand that the JIT is causing them
> problems with reordering, loop hoisting or other "nifty" optimizations that
> provide a .1% improvement in speed.   It will be a giant waste of peoples
> time trying to reconcile what is actually going wrong, and in some cases,
> there will be real impact on the users lives, welfare and/or safety,
> potentially.****
>
> ** **
>
> Call my position extreme, but I think it's vital for the Java community
> and Oracle in particular to understand that the path we are going down is
> absolutely a perilous disaster without something very specific being
> visible to developers to allow them to understand how their code is being
> executed so that they can see exactly what parts of the code need to be
> fixed.****
>
> ** **
>
> Gregg Wonderly****
>
> ** **
>
> On Apr 30, 2013, at 7:48 PM, "Boehm, Hans" <hans.boehm at hp.com> wrote:****
>
>
>
> ****
>
> A nice simple example to consider here is a user application that declares
> an array, and calls a library to sort it.  The sort library uses a parallel
> sort that relies on a sequential sort to sort small sections of the array.
> In a sequential-by-default world, how would you declare the parallel
> sections?  Would it be any different than what we do now?  The user
> application that declares the array may never know that there is any
> parallel code involved.  Nor should it.****
>
>  ****
>
> Applications such as this are naturally data-race-free, thus there is no
> issue with the compiler ?breaking? code.  And the compiler can apply nearly
> all sequentially valid transformations on synchronization-free code, such
> as the sequential sort operations.  If you accidentally introduce a data
> race bug, it?s unlikely your code would run correctly even if the compiler
> guaranteed sequential consistency. Your code may be a bit easier to debug
> with a hypothetical compiler that ensures sequential consistency.  But so
> long as you avoided intentional (unannotated) data races, I think a data
> race detector would also make this fairly easy.****
>
>  ****
>
> Hans****
>
>  ****
>
> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-
> interest-bounces at cs.oswego.edu] *On Behalf Of *Martin Thompson
> *Sent:* Tuesday, April 30, 2013 5:57 AM
> *To:* Kirk Pepperdine
> *Cc:* Gregg Wonderly; concurrency-interest at cs.oswego.edu
> *Subject:* Re: [concurrency-interest] JLS 17.7 Non-atomic treatment of
> double and long : Android****
>
>  ****
>
> I agree with Kirk here and would take it further.  By default the vast
> majority of code should be single threaded and concurrent programming is
> only utilized in regions of data exchange.****
>
>  ****
>
> If all code was sequentially consistent then most hardware and compiler
> optimizations would be defeated.  A default position that all code is
> concurrent is sending the industry the wrong way in my view.  It makes a
> more sense to explicitly define the regions of data exchange in our
> programs and therefore what ordering semantics are required in those
> regions.****
>
>  ****
>
> Martin...****
>
>  ------------------------------
> Message: 3
> Date: Tue, 30 Apr 2013 07:38:01 +0200
> From: Kirk Pepperdine <kirk at kodewerk.com>
> To: Gregg Wonderly <gergg at cox.net>
> Cc: concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] JLS 17.7 Non-atomic treatment of
>         double  and long : Android
> Message-ID: <5D85887E-8BFE-4F09-AFA8-53FE1DFD52D3 at kodewerk.com>
> Content-Type: text/plain; charset="windows-1252"
> Sorry but making thing volatile by default would be a horrible thing to
> do. Code wouldn't be a bit slower, it would be a lot slower and then you'd
> end up with the same problem in reverse!
> Regards,
> Kirk
> On 2013-04-30, at 12:10 AM, Gregg Wonderly <gergg at cox.net> wrote:
> > This code exists everywhere on the Java JVM now, because no one expects
> the loop hoist?   People are living with it, or eventually declaring the
> loop variable volatile after finding these discussions.
> >
> > Java, by default, should of used nothing but volatile variables, and
> developers should of needed to add non-volatile declarations via
> annotations, without the 'volatile' keyword being used, at all.
> >
> > That would of made it hard to "break" code without actually looking up
> what you were doing, because the added verbosity would only be tolerated
> when it actually accomplished a performance improvement.  Today, code is
> "Faster" without "volatile".   If everything was "volatile" by default,
> then code would be slower to start with, and proper "concurrency
> programming" would then make your code faster, as concurrency should.
> >
> > Gregg****
>
>  _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest****
>
> ** **
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130501/6ca1921f/attachment-0001.html>

From james.roper at typesafe.com  Thu May  2 03:36:39 2013
From: james.roper at typesafe.com (James Roper)
Date: Thu, 2 May 2013 17:36:39 +1000
Subject: [concurrency-interest] Thread local contexts and CompletableFutures
Message-ID: <CABY0rKMEBuRMCLgdu7kL-VL1A3m73shE93KOrtJa9rkunvH8Jg@mail.gmail.com>

Hi All,

This is my first post to this list, so let me first introduce myself.  I
work for Typesafe as the tech lead on Play Framework.

Play Framework is an asynchronous web framework that supports both Scala
and Java applications.  For Java, we currently have our own Promise API
which is heavily used by users of our framework.  This is backed by the
Scala Future API, and uses it's own SAM interfaces for passing functions so
it's lambda ready.  However, we would very much like to use the
CompletableFuture API instead, as this will allow seamless interoperability
with other Java libraries that use this API.  Right now we are working out
a plan for how we can move towards it (given that for a time, we still need
to support Java 6 and 7).

However there is an important feature that we would need for the
CompletableFuture API to be useful to us.  Thread locals are very commonly
used in Java frameworks.  In particular, standard Java APIs such as JPA use
them to store transaction context, they're used for storing a security
context in JEE, and almost anything that works directly with classloaders,
including using reflection or any sort of byte code generation, must use
the context classloader in most deployment environments, in particular in a
servlet container.  Play also makes heavy use of thread locals when using
the Java API, particularly for holding request context, when using JPA, and
the context classloader is also very important with many of the popular
Java third party libraries that we use.

As far as I can see, and correct me if I'm wrong, there is nothing in the
CompletableFuture API that addresses this, or allows it to be addressed.
 This will seriously inhibit the usefulness of CompletableFutures in many
typical deployment scenarios, most obviously in a JEE container, and also
where I am most concerned, in Play Framework.

In contrast, our existing Promise API can easily address this, because
Scala's ExecutionContext (roughly equivalent to the ExecutorService that
gets passed in with callbacks to the CompletableFuture API) has a prepare
method that returns a new (or the same) execution context.  APIs that
accept callbacks to be executed in a given execution context then invoke
the prepare method when the callback is submitted, which allows a thread
local aware ExecutionContext to capture the current state, and set it in
the callbacks thread when the callback is executed.  This feature I think
is far more important to Java code than it is to Scala code though, since
Java frameworks typically make much more use of thread locals and
reflection.

This allows a transparent way of ensuring that callbacks execute in the
right thread context.  The management of this context is then also managed
by the same resource manager that manages the threads themselves, and this
seems like the right place for it to be.

So has this concern been discussed or addressed before?  Any thoughts?
 Could ExecutorService have a similar method added to it?  Or would it make
sense to have another interface that sat above ExecutorService that managed
this?

One way that it could managed is by convention in user code, instead of
user code having an ExecutorService that it passed to CompletableFuture
methods, it could have a factory/builder class that produced an
ExecutorService, and the user would be required to invoke this themselves
each time they passed it, and not hold references to the produced
ExecutorService.  From a syntax point of view the overhead is not too high,
but I personally don't think it's a good idea to force a convention like
this.

Regards,

James Roper

-- 
*James Roper*
*Software Engineer*
*
*
Typesafe <http://www.typesafe.com/> - The software stack for applications
that scale
Twitter: @jroper
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130502/74f0817d/attachment.html>

From aph at redhat.com  Thu May  2 04:37:01 2013
From: aph at redhat.com (Andrew Haley)
Date: Thu, 02 May 2013 09:37:01 +0100
Subject: [concurrency-interest] JLS 17.7 Non-atomic treatment of double
 and long : Android
In-Reply-To: <7EC0C469-F8AA-4419-94D5-592305C62293@cox.net>
References: <CAChYfd-8DO0YCu+cEHj_MGVNzyoPewD+HSwuaF_HmAr_6K-31A@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD236BB8242@G9W0725.americas.hpqcorp.net>
	<7EC0C469-F8AA-4419-94D5-592305C62293@cox.net>
Message-ID: <518225AD.3050900@redhat.com>

On 05/01/2013 03:21 AM, Gregg Wonderly wrote:

> The problem is that people can trivially create data races.  It's
> happened everywhere.  There is so much legacy Java code that has
> data races.  That's no the large problem.  Data races are one thing.
> The large problem is when the JIT reorders code and optimizes code
> to not be sequentially consistent.  It then lands on the developers
> shoulders to understand every possible, undocumented optimization
> that might be done by the compiler to try and understand why the
> code is misbehaving.

Well, yes, but it's not just the compiler that's doing it, it's the
hardware.  Java developers have been spoiled by the x86, which is
more-or-less sequentially consistent, but ARM certainly isn't.
Programmers are just going to have to get used to it.

The best way to think about this is as an opportunity for upskilling.

Andrew.

From mjpt777 at gmail.com  Thu May  2 05:22:16 2013
From: mjpt777 at gmail.com (Martin Thompson)
Date: Thu, 2 May 2013 10:22:16 +0100
Subject: [concurrency-interest] JLS 17.7 Non-atomic treatment of double
 and long : Android
In-Reply-To: <518225AD.3050900@redhat.com>
References: <CAChYfd-8DO0YCu+cEHj_MGVNzyoPewD+HSwuaF_HmAr_6K-31A@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD236BB8242@G9W0725.americas.hpqcorp.net>
	<7EC0C469-F8AA-4419-94D5-592305C62293@cox.net>
	<518225AD.3050900@redhat.com>
Message-ID: <CAChYfd_Ui0LBEqu65SX4p63xjYoTzD3fGaL3dJsC6Uha-Z_6JA@mail.gmail.com>

The key to this is not that "the JIT reorders code and optimizes code to
not be sequentially consistent", it is the hardware doing this.  All modern
processors employ a store buffer whose contents are not visible to other
cores until drained, and therefore loads can appear to pass older stores.
 This is the fundamental reason x86 is not sequentially consistent.  Other
processors, with the exception of SPARC, have even weaker consistency.

Hans made a great point on how important it is to support parallelism which
is very different to concurrency but closely linked.  To make all
instructions sequentially consistent would destroy performance.

The bottom line is most developers are not capable of reasoning about
concurrent code.  The university of Brunel identified this with the
infamous double hump paper.  The summary is that the difficulty goes up a 3
step scale following structured programming, recursion, and then
concurrency.  so few can cope with concurrency.

We need programming paradigms which enable the vast majority of programmers
to avoid the subject of concurrency.  If people really need to work in this
space then they need the appropriate skills.

This discussion reminds me of similar discussions had by C programmers.
 Many C programmers do not understand undefined behaviour such as multiple
assignments between sequence points.  It does not make C wrong.  It just
illustrates that many do not understand the tools at hand.

Andrew makes the good point in that regions of data exchange are not
clearly marked in Java code.  We have to track back to the declarations to
see if a field is qualified volatile to understand ordering and visibility
semantics.  I think C++ 11 has done this better by making the ordering
semantics explicit at point of data exchange.

Let's refine the tools and techniques for concurrency for those who should
be working in this space rather than try to dumb down a subject that is
just plain hard.  Sure mistakes like Thread.getName()/setName() will be
made and we need to correct them.  Reviews and feedback is often a better
solution than red tape and legislation, which enforcing sequential
consistency feels like.

Martin...

On Thu, May 2, 2013 at 9:37 AM, Andrew Haley <aph at redhat.com> wrote:

> On 05/01/2013 03:21 AM, Gregg Wonderly wrote:
>
> > The problem is that people can trivially create data races.  It's
> > happened everywhere.  There is so much legacy Java code that has
> > data races.  That's no the large problem.  Data races are one thing.
> > The large problem is when the JIT reorders code and optimizes code
> > to not be sequentially consistent.  It then lands on the developers
> > shoulders to understand every possible, undocumented optimization
> > that might be done by the compiler to try and understand why the
> > code is misbehaving.
>
> Well, yes, but it's not just the compiler that's doing it, it's the
> hardware.  Java developers have been spoiled by the x86, which is
> more-or-less sequentially consistent, but ARM certainly isn't.
> Programmers are just going to have to get used to it.
>
> The best way to think about this is as an opportunity for upskilling.
>
> Andrew.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130502/32e3a738/attachment.html>

From oleksandr.otenko at oracle.com  Thu May  2 05:57:01 2013
From: oleksandr.otenko at oracle.com (oleksandr otenko)
Date: Thu, 02 May 2013 10:57:01 +0100
Subject: [concurrency-interest] JLS 17.7 Non-atomic treatment of double
 and long : Android
In-Reply-To: <A3E67C2071F49C4CBC4F17E6D77CDDD236BB8631@G9W0725.americas.hpqcorp.net>
References: <CAChYfd-8DO0YCu+cEHj_MGVNzyoPewD+HSwuaF_HmAr_6K-31A@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD236BB8242@G9W0725.americas.hpqcorp.net>
	<7EC0C469-F8AA-4419-94D5-592305C62293@cox.net>
	<A3E67C2071F49C4CBC4F17E6D77CDDD236BB8631@G9W0725.americas.hpqcorp.net>
Message-ID: <5182386D.7000000@oracle.com>

Out of interest, shouldn't these tools for automated race detection take 
into account the number of threads?

Or are they limited to "race detected" vs "unknown"?


Alex


On 01/05/2013 20:41, Boehm, Hans wrote:
>
> I think this is the wrong programming model.  Even if the 
> implementation guaranteed sequential consistency, data races would 
> usually indicate a bug.  Sequential consistency helps with 
> Thread.setName() only because the natural implementation of the 
> operation is indivisible.  If it involved setting more than one field, 
> the racy code would still break, even in a sequentially consistent 
> implementation.
>
> I claim that in reality programmers expect not just sequential 
> consistency (i.e. as if the threads were simply interleaved), but they 
> also expect that code sections that appear sequential, i.e. those that 
> contain no synchronization, execute sequentially, without interference 
> from other threads.  I can't read a piece of code if any field can 
> change out from under me at any time.  The only way I can get that 
> required guarantee is by writing data-race-free code, which requires 
> me to annotate synchronization variables as volatile.  Once I do that, 
> we know how to built tools that will report data races in real code, 
> admittedly with 10x-100x slowdown (in the absence of hardware 
> support).  If such a tool reports no data races, then I know that the 
> execution is sequential consistency, even without forcing my compiler 
> to treat everything as volatile.  And since I didn't declare 
> everything as volatile, I also get the equally important guarantee 
> that synchronization-free code executes sequentially, i.e. atomically, 
> and thus I don't need to worry about possible interleaving at  every 
> instruction.  I don't need to worry about whether synchronization-free 
> library calls are implemented with a single assignment or three.  I 
> think this is a much more practical programming model than just 
> sequential consistency.  And it's the one that motivates various 
> current language memory models, including the Java one.
>
> In this world, which requires better tools, and somewhat more 
> disciplined code, but not a sequentially consistent memory model, I 
> expect the Thread.setName() problem would have been fairly easy to 
> debug.  We unfortunately still rely on testing with a race detector.  
> But if we can get the code to fail under those conditions, the error 
> message would have clearly pointed at the problem before you ever saw 
> a corrupt thread name.  (The most natural fix in this case may still 
> involve adding synchronization to the library, and fixing the 
> documentation to reflect that, but that's a separate issue.)
>
> (Reality is slightly more complicated, and I think we'd need a way 
> other than "volatile" to annotate races on variables when we don't 
> expect enforcement of sequential consistency, but just want the 
> current. tricky and misspecified, Java variable semantics, such as 
> they are.  But I don't think that mechanism should be used by, or 
> interfere with, the vast majority of programmers, whom I believe Gregg 
> is addressing.)
>
> Hans
>
> *From:*Gregg Wonderly [mailto:gergg at cox.net]
> *Sent:* Tuesday, April 30, 2013 7:21 PM
> *To:* Boehm, Hans
> *Cc:* Martin Thompson; Kirk Pepperdine; concurrency-interest at cs.oswego.edu
> *Subject:* Re: [concurrency-interest] JLS 17.7 Non-atomic treatment of 
> double and long : Android
>
> The problem is that people can trivially create data races.  It's 
> happened everywhere.  There is so much legacy Java code that has data 
> races.   That's no the large problem.  Data races are one thing.  The 
> large problem is when the JIT reorders code and optimizes code to not 
> be sequentially consistent.  It then lands on the developers shoulders 
> to understand every possible, undocumented optimization that might be 
> done by the compiler to try and understand why the code is 
> misbehaving.  Only then, can they possibly formulate which changes 
> will actually correct the behavior of their code, without making it 
> "slower" than it needs to be.  To me, that is exactly the wrong way 
> for a developer to test and optimize their code.  Visible data races 
> that are only about "the data", are trivial to reason about.
>
> If I saw the Thread.setName/getName lack of synchronization, out in 
> the wild, create a corrupted thread name, I'd be writing a bug report 
> about the JVM creating memory corruption before I would consider that 
> the array had been allocated and made visible before it was filled. 
>  That's not how the code is written, and that's not how it should 
> execute without the developer deciding that such an optimization is 
> safe for their application to see.
>
> Suggesting that denying the visibility of the optimization is selected 
> by the developer not doing anything, is where I respectfully, cannot 
> agree with such reasoning.  This is not a new issue.  The design of 
> many functional languages is based on the simple fact that developers 
> don't need to be burdened with designating that they want correctly 
> executing code.  They should get that by default, and have to work at 
> "faster" or "more performant" or "less latency", by explicit actions 
> they take in code structure.
>
> Thread.setName/getName is a great example of "no thought given" to 
> concurrency in the original design.  That code, as said before, is 
> only executed in a concurrent environment.  Why didn't the original 
> code have the correct concurrency design.  Why hasn't it been visible 
> to enough people that a bug report written and it already fixed?
>
> The answer is, because that racy code, doesn't "act" wrong, in general 
> usage patterns.  But optimizations due to how volatile works and how 
> JIT developers exploit lack of "concurrency selected" coding, could, 
> at any point, break that code.
>
> This is how all of that legacy code, laying around on the internet, is 
> going to come down.  It's going to start randomly breaking and causing 
> completely unexplainable bugs.  People will not remember enough 
> details about this code to actually understand that the JIT is causing 
> them problems with reordering, loop hoisting or other "nifty" 
> optimizations that provide a .1% improvement in speed.   It will be a 
> giant waste of peoples time trying to reconcile what is actually going 
> wrong, and in some cases, there will be real impact on the users 
> lives, welfare and/or safety, potentially.
>
> Call my position extreme, but I think it's vital for the Java 
> community and Oracle in particular to understand that the path we are 
> going down is absolutely a perilous disaster without something very 
> specific being visible to developers to allow them to understand how 
> their code is being executed so that they can see exactly what parts 
> of the code need to be fixed.
>
> Gregg Wonderly
>
> On Apr 30, 2013, at 7:48 PM, "Boehm, Hans" <hans.boehm at hp.com 
> <mailto:hans.boehm at hp.com>> wrote:
>
>
>
> A nice simple example to consider here is a user application that 
> declares an array, and calls a library to sort it.  The sort library 
> uses a parallel sort that relies on a sequential sort to sort small 
> sections of the array.  In a sequential-by-default world, how would 
> you declare the parallel sections?  Would it be any different than 
> what we do now?  The user application that declares the array may 
> never know that there is any parallel code involved. Nor should it.
>
> Applications such as this are naturally data-race-free, thus there is 
> no issue with the compiler "breaking" code.  And the compiler can 
> apply nearly all sequentially valid transformations on 
> synchronization-free code, such as the sequential sort operations.  If 
> you accidentally introduce a data race bug, it's unlikely your code 
> would run correctly even if the compiler guaranteed sequential 
> consistency. Your code may be a bit easier to debug with a 
> hypothetical compiler that ensures sequential consistency. But so long 
> as you avoided intentional (unannotated) data races, I think a data 
> race detector would also make this fairly easy.
>
> Hans
>
> *From:*concurrency-interest-bounces at cs.oswego.edu 
> <mailto:concurrency-interest-bounces at cs.oswego.edu>[mailto:concurrency-interest-bounces at cs.oswego.edu 
> <mailto:interest-bounces at cs.oswego.edu>]*On Behalf Of*Martin Thompson
> *Sent:*Tuesday, April 30, 2013 5:57 AM
> *To:*Kirk Pepperdine
> *Cc:*Gregg Wonderly;concurrency-interest at cs.oswego.edu 
> <mailto:concurrency-interest at cs.oswego.edu>
> *Subject:*Re: [concurrency-interest] JLS 17.7 Non-atomic treatment of 
> double and long : Android
>
> I agree with Kirk here and would take it further.  By default the vast 
> majority of code should be single threaded and concurrent programming 
> is only utilized in regions of data exchange.
>
> If all code was sequentially consistent then most hardware and 
> compiler optimizations would be defeated.  A default position that all 
> code is concurrent is sending the industry the wrong way in my view. 
>  It makes a more sense to explicitly define the regions of data 
> exchange in our programs and therefore what ordering semantics are 
> required in those regions.
>
> Martin...
>
>     ------------------------------
>     Message: 3
>     Date: Tue, 30 Apr 2013 07:38:01 +0200
>     From: Kirk Pepperdine <kirk at kodewerk.com <mailto:kirk at kodewerk.com>>
>     To: Gregg Wonderly <gergg at cox.net <mailto:gergg at cox.net>>
>     Cc: concurrency-interest at cs.oswego.edu
>     <mailto:concurrency-interest at cs.oswego.edu>Subject: Re:
>     [concurrency-interest] JLS 17.7 Non-atomic treatment of
>             double  and long : Android
>     Message-ID: <5D85887E-8BFE-4F09-AFA8-53FE1DFD52D3 at kodewerk.com
>     <mailto:5D85887E-8BFE-4F09-AFA8-53FE1DFD52D3 at kodewerk.com>>
>     Content-Type: text/plain; charset="windows-1252"
>     Sorry but making thing volatile by default would be a horrible
>     thing to do. Code wouldn't be a bit slower, it would be a lot
>     slower and then you'd end up with the same problem in reverse!
>     Regards,
>     Kirk
>     On 2013-04-30, at 12:10 AM, Gregg Wonderly <gergg at cox.net
>     <mailto:gergg at cox.net>> wrote:
>     > This code exists everywhere on the Java JVM now, because no one
>     expects the loop hoist?   People are living with it, or eventually
>     declaring the loop variable volatile after finding these discussions.
>     >
>     > Java, by default, should of used nothing but volatile variables,
>     and developers should of needed to add non-volatile declarations
>     via annotations, without the 'volatile' keyword being used, at all.
>     >
>     > That would of made it hard to "break" code without actually
>     looking up what you were doing, because the added verbosity would
>     only be tolerated when it actually accomplished a performance
>     improvement.  Today, code is "Faster" without "volatile".   If
>     everything was "volatile" by default, then code would be slower to
>     start with, and proper "concurrency programming" would then make
>     your code faster, as concurrency should.
>     >
>     > Gregg
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu 
> <mailto:Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130502/28dacb35/attachment-0001.html>

From peter.firmstone at zeus.net.au  Thu May  2 06:03:39 2013
From: peter.firmstone at zeus.net.au (Peter Firmstone)
Date: Thu, 02 May 2013 20:03:39 +1000
Subject: [concurrency-interest] RacerAJ race detector
Message-ID: <518239FB.4050005@zeus.net.au>

Anyone have any experience with RacerAJ?

http://www.bodden.de/tools/raceraj/

The author claims it can detect all race conditions, with some false 
positives.

I plan to try it out on the weekend.

Cheers,

Peter.

From oleksandr.otenko at oracle.com  Thu May  2 06:28:17 2013
From: oleksandr.otenko at oracle.com (oleksandr otenko)
Date: Thu, 02 May 2013 11:28:17 +0100
Subject: [concurrency-interest] JLS 17.7 Non-atomic treatment of double
 and long : Android
In-Reply-To: <CACuKZqFCaDNtB1eNir-yQ8j+2kwhEVc=SQ4Cw8kVHeDH9mhngA@mail.gmail.com>
References: <CAChYfd-8DO0YCu+cEHj_MGVNzyoPewD+HSwuaF_HmAr_6K-31A@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD236BB8242@G9W0725.americas.hpqcorp.net>
	<7EC0C469-F8AA-4419-94D5-592305C62293@cox.net>
	<A3E67C2071F49C4CBC4F17E6D77CDDD236BB8631@G9W0725.americas.hpqcorp.net>
	<CACuKZqFCaDNtB1eNir-yQ8j+2kwhEVc=SQ4Cw8kVHeDH9mhngA@mail.gmail.com>
Message-ID: <51823FC1.6090004@oracle.com>

Publication is never atomic, unless you are using transactional memory. 
The availability of data then is guaranteed through structuring the data 
- can't read thread-local object, until a pointer to it is published - 
or through structuring code (the dual of data) - will traverse the list 
only between designated list head and tail, even if technically more 
data is reachable. In both cases it doesn't matter in what order the 
writes really occur, as long as they are happening before the publishing 
of the condition. So it is not against intuition.

Volatility at application site (vs declaration) is a good point. That is 
probably what the rumoured fences API can provide.


Alex

On 02/05/2013 05:16, Zhong Yu wrote:
> Good point Hans, however I doubt any sane programmer would 
> intentionally play with interleaving which is too hard to reason. 
> Regardless of memory model, the most important use case of shared 
> variable is atomic publication - a thread writes to some places that 
> nobody else is supposed to be reading, then writes to a shared 
> variable to publish all previous writes. No interleaving. Programmers 
> intuitively assume that it should work; the possibility of 
> out-of-order writes is too bizzare to register in our minds. Is it 
> really worth it for JMM to go against this intuition? How much 
> performance gain are we talking about anyway? The java community has 
> wasted a lot of time on it.
>
> Another problem is the syntax - volatility is declared on the 
> variable, instead of on read/write actions. If it's the later, 
> programmers are more likely to do the right thing because they do 
> think about inter-thread effects when they read/write shared variables.
>
> Zhong Yu
>
>
>
>
> On Wed, May 1, 2013 at 2:41 PM, Boehm, Hans <hans.boehm at hp.com 
> <mailto:hans.boehm at hp.com>> wrote:
>
>     I think this is the wrong programming model.  Even if the
>     implementation guaranteed sequential consistency, data races would
>     usually indicate a bug.  Sequential consistency helps with
>     Thread.setName() only because the natural implementation of the
>     operation is indivisible.  If it involved setting more than one
>     field, the racy code would still break, even in a sequentially
>     consistent implementation.
>
>     I claim that in reality programmers expect not just sequential
>     consistency (i.e. as if the threads were simply interleaved), but
>     they also expect that code sections that appear sequential, i.e.
>     those that contain no synchronization, execute sequentially,
>     without interference from other threads.  I can't read a piece of
>     code if any field can change out from under me at any time.  The
>     only way I can get that required guarantee is by writing
>     data-race-free code, which requires me to annotate synchronization
>     variables as volatile.  Once I do that, we know how to built tools
>     that will report data races in real code, admittedly with 10x-100x
>     slowdown (in the absence of hardware support).  If such a tool
>     reports no data races, then I know that the execution is
>     sequential consistency, even without forcing my compiler to treat
>     everything as volatile.  And since I didn't declare everything as
>     volatile, I also get the equally important guarantee that
>     synchronization-free code executes sequentially, i.e. atomically,
>     and thus I don't need to worry about possible interleaving at
>      every instruction.  I don't need to worry about whether
>     synchronization-free library calls are implemented with a single
>     assignment or three.  I think this is a much more practical
>     programming model than just sequential consistency.  And it's the
>     one that motivates various current language memory models,
>     including the Java one.
>
>     In this world, which requires better tools, and somewhat more
>     disciplined code, but not a sequentially consistent memory model,
>     I expect the Thread.setName() problem would have been fairly easy
>     to debug.  We unfortunately still rely on testing with a race
>     detector.  But if we can get the code to fail under those
>     conditions, the error message would have clearly pointed at the
>     problem before you ever saw a corrupt thread name.  (The most
>     natural fix in this case may still involve adding synchronization
>     to the library, and fixing the documentation to reflect that, but
>     that's a separate issue.)
>
>     (Reality is slightly more complicated, and I think we'd need a way
>     other than "volatile" to annotate races on variables when we don't
>     expect enforcement of sequential consistency, but just want the
>     current. tricky and misspecified, Java variable semantics, such as
>     they are.  But I don't think that mechanism should be used by, or
>     interfere with, the vast majority of programmers, whom I believe
>     Gregg is addressing.)
>
>     Hans
>
>     *From:*Gregg Wonderly [mailto:gergg at cox.net <mailto:gergg at cox.net>]
>     *Sent:* Tuesday, April 30, 2013 7:21 PM
>     *To:* Boehm, Hans
>     *Cc:* Martin Thompson; Kirk Pepperdine;
>     concurrency-interest at cs.oswego.edu
>     <mailto:concurrency-interest at cs.oswego.edu>
>
>
>     *Subject:* Re: [concurrency-interest] JLS 17.7 Non-atomic
>     treatment of double and long : Android
>
>     The problem is that people can trivially create data races.  It's
>     happened everywhere.  There is so much legacy Java code that has
>     data races.   That's no the large problem.  Data races are one
>     thing.  The large problem is when the JIT reorders code and
>     optimizes code to not be sequentially consistent.  It then lands
>     on the developers shoulders to understand every possible,
>     undocumented optimization that might be done by the compiler to
>     try and understand why the code is misbehaving.  Only then, can
>     they possibly formulate which changes will actually correct the
>     behavior of their code, without making it "slower" than it needs
>     to be.  To me, that is exactly the wrong way for a developer to
>     test and optimize their code.  Visible data races that are only
>     about "the data", are trivial to reason about.
>
>     If I saw the Thread.setName/getName lack of synchronization, out
>     in the wild, create a corrupted thread name, I'd be writing a bug
>     report about the JVM creating memory corruption before I would
>     consider that the array had been allocated and made visible before
>     it was filled.  That's not how the code is written, and that's not
>     how it should execute without the developer deciding that such an
>     optimization is safe for their application to see.
>
>     Suggesting that denying the visibility of the optimization is
>     selected by the developer not doing anything, is where I
>     respectfully, cannot agree with such reasoning.  This is not a new
>     issue.  The design of many functional languages is based on the
>     simple fact that developers don't need to be burdened with
>     designating that they want correctly executing code.  They should
>     get that by default, and have to work at "faster" or "more
>     performant" or "less latency", by explicit actions they take in
>     code structure.
>
>     Thread.setName/getName is a great example of "no thought given" to
>     concurrency in the original design.  That code, as said before, is
>     only executed in a concurrent environment.  Why didn't the
>     original code have the correct concurrency design.  Why hasn't it
>     been visible to enough people that a bug report written and it
>     already fixed?
>
>     The answer is, because that racy code, doesn't "act" wrong, in
>     general usage patterns.  But optimizations due to how volatile
>     works and how JIT developers exploit lack of "concurrency
>     selected" coding, could, at any point, break that code.
>
>     This is how all of that legacy code, laying around on the
>     internet, is going to come down.  It's going to start randomly
>     breaking and causing completely unexplainable bugs.  People will
>     not remember enough details about this code to actually understand
>     that the JIT is causing them problems with reordering, loop
>     hoisting or other "nifty" optimizations that provide a .1%
>     improvement in speed.   It will be a giant waste of peoples time
>     trying to reconcile what is actually going wrong, and in some
>     cases, there will be real impact on the users lives, welfare
>     and/or safety, potentially.
>
>     Call my position extreme, but I think it's vital for the Java
>     community and Oracle in particular to understand that the path we
>     are going down is absolutely a perilous disaster without something
>     very specific being visible to developers to allow them to
>     understand how their code is being executed so that they can see
>     exactly what parts of the code need to be fixed.
>
>     Gregg Wonderly
>
>     On Apr 30, 2013, at 7:48 PM, "Boehm, Hans" <hans.boehm at hp.com
>     <mailto:hans.boehm at hp.com>> wrote:
>
>
>
>     A nice simple example to consider here is a user application that
>     declares an array, and calls a library to sort it.  The sort
>     library uses a parallel sort that relies on a sequential sort to
>     sort small sections of the array.  In a sequential-by-default
>     world, how would you declare the parallel sections?  Would it be
>     any different than what we do now?  The user application that
>     declares the array may never know that there is any parallel code
>     involved.  Nor should it.
>
>     Applications such as this are naturally data-race-free, thus there
>     is no issue with the compiler "breaking" code.  And the compiler
>     can apply nearly all sequentially valid transformations on
>     synchronization-free code, such as the sequential sort
>     operations.  If you accidentally introduce a data race bug, it's
>     unlikely your code would run correctly even if the compiler
>     guaranteed sequential consistency. Your code may be a bit easier
>     to debug with a hypothetical compiler that ensures sequential
>     consistency.  But so long as you avoided intentional (unannotated)
>     data races, I think a data race detector would also make this
>     fairly easy.
>
>     Hans
>
>     *From:*concurrency-interest-bounces at cs.oswego.edu
>     <mailto:concurrency-interest-bounces at cs.oswego.edu>[mailto:concurrency-
>     <mailto:concurrency->interest-bounces at cs.oswego.edu
>     <mailto:interest-bounces at cs.oswego.edu>]*On Behalf Of*Martin Thompson
>     *Sent:*Tuesday, April 30, 2013 5:57 AM
>     *To:*Kirk Pepperdine
>     *Cc:*Gregg Wonderly;concurrency-interest at cs.oswego.edu
>     <mailto:concurrency-interest at cs.oswego.edu>
>     *Subject:*Re: [concurrency-interest] JLS 17.7 Non-atomic treatment
>     of double and long : Android
>
>     I agree with Kirk here and would take it further.  By default the
>     vast majority of code should be single threaded and concurrent
>     programming is only utilized in regions of data exchange.
>
>     If all code was sequentially consistent then most hardware and
>     compiler optimizations would be defeated.  A default position that
>     all code is concurrent is sending the industry the wrong way in my
>     view.  It makes a more sense to explicitly define the regions of
>     data exchange in our programs and therefore what ordering
>     semantics are required in those regions.
>
>     Martin...
>
>         ------------------------------
>         Message: 3
>         Date: Tue, 30 Apr 2013 07:38:01 +0200
>         From: Kirk Pepperdine <kirk at kodewerk.com
>         <mailto:kirk at kodewerk.com>>
>         To: Gregg Wonderly <gergg at cox.net <mailto:gergg at cox.net>>
>         Cc: concurrency-interest at cs.oswego.edu
>         <mailto:concurrency-interest at cs.oswego.edu>Subject: Re:
>         [concurrency-interest] JLS 17.7 Non-atomic treatment of
>                 double  and long : Android
>         Message-ID: <5D85887E-8BFE-4F09-AFA8-53FE1DFD52D3 at kodewerk.com
>         <mailto:5D85887E-8BFE-4F09-AFA8-53FE1DFD52D3 at kodewerk.com>>
>         Content-Type: text/plain; charset="windows-1252"
>         Sorry but making thing volatile by default would be a horrible
>         thing to do. Code wouldn't be a bit slower, it would be a lot
>         slower and then you'd end up with the same problem in reverse!
>         Regards,
>         Kirk
>         On 2013-04-30, at 12:10 AM, Gregg Wonderly <gergg at cox.net
>         <mailto:gergg at cox.net>> wrote:
>         > This code exists everywhere on the Java JVM now, because no
>         one expects the loop hoist?   People are living with it, or
>         eventually declaring the loop variable volatile after finding
>         these discussions.
>         >
>         > Java, by default, should of used nothing but volatile
>         variables, and developers should of needed to add non-volatile
>         declarations via annotations, without the 'volatile' keyword
>         being used, at all.
>         >
>         > That would of made it hard to "break" code without actually
>         looking up what you were doing, because the added verbosity
>         would only be tolerated when it actually accomplished a
>         performance improvement.  Today, code is "Faster" without
>         "volatile". If everything was "volatile" by default, then code
>         would be slower to start with, and proper "concurrency
>         programming" would then make your code faster, as concurrency
>         should.
>         >
>         > Gregg
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130502/b9c59dda/attachment-0001.html>

From dl at cs.oswego.edu  Thu May  2 06:54:57 2013
From: dl at cs.oswego.edu (Doug Lea)
Date: Thu, 02 May 2013 06:54:57 -0400
Subject: [concurrency-interest] Thread local contexts and
	CompletableFutures
In-Reply-To: <CABY0rKMEBuRMCLgdu7kL-VL1A3m73shE93KOrtJa9rkunvH8Jg@mail.gmail.com>
References: <CABY0rKMEBuRMCLgdu7kL-VL1A3m73shE93KOrtJa9rkunvH8Jg@mail.gmail.com>
Message-ID: <51824601.6080701@cs.oswego.edu>

On 05/02/13 03:36, James Roper wrote:
>
> However there is an important feature that we would need for the
> CompletableFuture API to be useful to us.  Thread locals are very commonly used
> in Java frameworks.
>
> As far as I can see, and correct me if I'm wrong, there is nothing in the
> CompletableFuture API that addresses this, or allows it to be addressed.
>
> In contrast, our existing Promise API can easily address this, because Scala's
> ExecutionContext (roughly equivalent to the ExecutorService that gets passed in
> with callbacks to the CompletableFuture API) has a prepare method that returns a
> new (or the same) execution context.

This seems possible using the current API.
All async completion methods have a form accepting an Executor,
that could be some form of your ExecutionContext.
You might want to layer on something ensuring this.
Additionally, if it is possible for direct (non-async) execution
to also use a form of ExecutionContext, then you'd need
to use the async form in these cases as well, but supply
a version of ExecutionContext that directly executes after
establishing context. (in other words, there's nothing about
the "async" methods that strictly requires asynchrony.)
It's conceivable that there is some case not handled by these
options, but I don't see any offhand.

-Doug


From dl at cs.oswego.edu  Thu May  2 07:06:33 2013
From: dl at cs.oswego.edu (Doug Lea)
Date: Thu, 02 May 2013 07:06:33 -0400
Subject: [concurrency-interest] Critical fix in jsr166y for Scala 2.10.2
In-Reply-To: <3A97D946-4B99-4CF7-AD35-1E6AC7D022FA@gmail.com>
References: <EAFE4F7A-BAF7-4677-83D1-F12F676ED67E@gmail.com>
	<518104AF.4010909@cs.oswego.edu>
	<3A97D946-4B99-4CF7-AD35-1E6AC7D022FA@gmail.com>
Message-ID: <518248B9.5080300@cs.oswego.edu>

On 05/01/13 19:03, Philipp Haller wrote:
>
>>> - Given that Scala 2.10.2 will be binary compatible with Scala 2.10.1, is it
>>> safe to update to the latest revision of jsr166y? In other words, are later
>>> revisions of jsr166y binary compatible with earlier revisions?
>>
>> To the best of my knowledge, yes. The jsr166{x,y,e} builds are
>> not guaranteed to be stable, but jsr166y (i.e., updates of jdk7
>> functionality usable on JDK6+) hasn't changed incompatibly recently.
>
> OK, perfect.
> So, do you recommend upgrading to the latest revision then? Or could you point us to a specific revision which fixes the issue we're seeing (if that's feasible...)?
>

In general, always use the latest versions.
Versions that introduce regressions or new problems
don't last long before being updated.

(Unrelatedly: This is not always the case for some ongoing JDK8
j.u.c classes. Some are still dependent on other JDK8 features that
haven't stabilized, so might not work for those not building
openjdk from same sources and tracking bugfixes.)

-Doug


From hallerp at gmail.com  Thu May  2 08:15:47 2013
From: hallerp at gmail.com (Philipp Haller)
Date: Thu, 2 May 2013 14:15:47 +0200
Subject: [concurrency-interest] Critical fix in jsr166y for Scala 2.10.2
In-Reply-To: <518248B9.5080300@cs.oswego.edu>
References: <EAFE4F7A-BAF7-4677-83D1-F12F676ED67E@gmail.com>
	<518104AF.4010909@cs.oswego.edu>
	<3A97D946-4B99-4CF7-AD35-1E6AC7D022FA@gmail.com>
	<518248B9.5080300@cs.oswego.edu>
Message-ID: <EC215C2C-5917-4955-8834-2426B5E8925B@gmail.com>

On May 2, 2013, at 1:06 PM, Doug Lea wrote:

> On 05/01/13 19:03, Philipp Haller wrote:
>> 
>>>> - Given that Scala 2.10.2 will be binary compatible with Scala 2.10.1, is it
>>>> safe to update to the latest revision of jsr166y? In other words, are later
>>>> revisions of jsr166y binary compatible with earlier revisions?
>>> 
>>> To the best of my knowledge, yes. The jsr166{x,y,e} builds are
>>> not guaranteed to be stable, but jsr166y (i.e., updates of jdk7
>>> functionality usable on JDK6+) hasn't changed incompatibly recently.
>> 
>> OK, perfect.
>> So, do you recommend upgrading to the latest revision then? Or could you point us to a specific revision which fixes the issue we're seeing (if that's feasible...)?
>> 
> 
> In general, always use the latest versions.
> Versions that introduce regressions or new problems
> don't last long before being updated.

OK, sounds good.

> 
> (Unrelatedly: This is not always the case for some ongoing JDK8
> j.u.c classes. Some are still dependent on other JDK8 features that
> haven't stabilized, so might not work for those not building
> openjdk from same sources and tracking bugfixes.)

Good to know.

Thanks again,
Philipp



From hans.boehm at hp.com  Thu May  2 12:31:59 2013
From: hans.boehm at hp.com (Boehm, Hans)
Date: Thu, 2 May 2013 16:31:59 +0000
Subject: [concurrency-interest] JLS 17.7 Non-atomic treatment of double
 and long : Android
In-Reply-To: <CACuKZqFCaDNtB1eNir-yQ8j+2kwhEVc=SQ4Cw8kVHeDH9mhngA@mail.gmail.com>
References: <CAChYfd-8DO0YCu+cEHj_MGVNzyoPewD+HSwuaF_HmAr_6K-31A@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD236BB8242@G9W0725.americas.hpqcorp.net>
	<7EC0C469-F8AA-4419-94D5-592305C62293@cox.net>
	<A3E67C2071F49C4CBC4F17E6D77CDDD236BB8631@G9W0725.americas.hpqcorp.net>
	<CACuKZqFCaDNtB1eNir-yQ8j+2kwhEVc=SQ4Cw8kVHeDH9mhngA@mail.gmail.com>
Message-ID: <A3E67C2071F49C4CBC4F17E6D77CDDD236BB8B23@G9W0725.americas.hpqcorp.net>

I wonder whether the expectation about atomic publication is really a sign that we have been miseducating people.  The problem may be that it comes too close to working in Java, since we guarantee that even ordinary pointer updates are indivisible.  In traditional C, there is no such guarantee, and such code is clearly broken, or at least not portable, without even considering memory ordering.  In modern C++ code, it will be even more obviously broken if the code is templatized with respect to the pointer type or uses reference-counted pointers.

In general, I think the right mental model for most programmers is that an assignment first changes a field to an invalid value, and only then changes it  to the specified value.  That's more or less how Java long and double actually behave.  The problem is that, in order to support security and some tricky high performance code, we provide somewhat stronger guarantees for other types.  But those guarantees are not strong enough to be useful to most people.  They just think they are.

I still slightly favor annotating declarations with volatile/atomic, rather than the accesses.  But there are clearly arguments on both sides.  The arguments in favor of the declarations are:


-          Atomic access may have implications on required alignment.  This is probably more of an issue in C than Java, and it's becoming less common.

-          It's easy to forget an annotation on an individual access.  I would rather err on the fail-safe side for individual accesses. (In an ideal world, there probably should be a way to declare individual accesses to a Java volatile as not racing. We mostly have that in C++.)

-          I don't think data structure declarations are really understandable without knowing what's used for communication.  (The same is true for the accesses, but we're used to having to understand declarations when reading code, more so than the other way around.)

This seems to be a legitimately controversial issue.

Hans

From: Zhong Yu [mailto:zhong.j.yu at gmail.com]
Sent: Wednesday, May 01, 2013 9:17 PM
To: Boehm, Hans
Cc: Gregg Wonderly; concurrency-interest at cs.oswego.edu; Martin Thompson
Subject: Re: [concurrency-interest] JLS 17.7 Non-atomic treatment of double and long : Android

Good point Hans, however I doubt any sane programmer would intentionally play with interleaving which is too hard to reason. Regardless of memory model, the most important use case of shared variable is atomic publication - a thread writes to some places that nobody else is supposed to be reading, then writes to a shared variable to publish all previous writes. No interleaving. Programmers intuitively assume that it should work; the possibility of out-of-order writes is too bizzare to register in our minds. Is it really worth it for JMM to go against this intuition? How much performance gain are we talking about anyway? The java community has wasted a lot of time on it.
Another problem is the syntax - volatility is declared on the variable, instead of on read/write actions. If it's the later, programmers are more likely to do the right thing because they do think about inter-thread effects when they read/write shared variables.

Zhong Yu



On Wed, May 1, 2013 at 2:41 PM, Boehm, Hans <hans.boehm at hp.com<mailto:hans.boehm at hp.com>> wrote:
I think this is the wrong programming model.  Even if the implementation guaranteed sequential consistency, data races would usually indicate a bug.  Sequential consistency helps with Thread.setName() only because the natural implementation of the operation is indivisible.  If it involved setting more than one field, the racy code would still break, even in a sequentially consistent implementation.

I claim that in reality programmers expect not just sequential consistency (i.e. as if the threads were simply interleaved), but they also expect that code sections that appear sequential, i.e. those that contain no synchronization, execute sequentially, without interference from other threads.  I can't read a piece of code if any field can change out from under me at any time.  The only way I can get that required guarantee is by writing data-race-free code, which requires me to annotate synchronization variables as volatile.  Once I do that, we know how to built tools that will report data races in real code, admittedly with 10x-100x slowdown (in the absence of hardware support).  If such a tool reports no data races, then I know that the execution is sequential consistency, even without forcing my compiler to treat everything as volatile.  And since I didn't declare everything as volatile, I also get the equally important guarantee that synchronization-free code executes sequentially, i.e. atomically, and thus I don't need to worry about possible interleaving at  every instruction.  I don't need to worry about whether synchronization-free library calls are implemented with a single assignment or three.  I think this is a much more practical programming model than just sequential consistency.  And it's the one that motivates various current language memory models, including the Java one.

In this world, which requires better tools, and somewhat more disciplined code, but not a sequentially consistent memory model, I expect the Thread.setName() problem would have been fairly easy to debug.  We unfortunately still rely on testing with a race detector.  But if we can get the code to fail under those conditions, the error message would have clearly pointed at the problem before you ever saw a corrupt thread name.  (The most natural fix in this case may still involve adding synchronization to the library, and fixing the documentation to reflect that, but that's a separate issue.)

(Reality is slightly more complicated, and I think we'd need a way other than "volatile" to annotate races on variables when we don't expect enforcement of sequential consistency, but just want the current. tricky and misspecified, Java variable semantics, such as they are.  But I don't think that mechanism should be used by, or interfere with, the vast majority of programmers, whom I believe Gregg is addressing.)

Hans

From: Gregg Wonderly [mailto:gergg at cox.net<mailto:gergg at cox.net>]
Sent: Tuesday, April 30, 2013 7:21 PM
To: Boehm, Hans
Cc: Martin Thompson; Kirk Pepperdine; concurrency-interest at cs.oswego.edu<mailto:concurrency-interest at cs.oswego.edu>

Subject: Re: [concurrency-interest] JLS 17.7 Non-atomic treatment of double and long : Android

The problem is that people can trivially create data races.  It's happened everywhere.  There is so much legacy Java code that has data races.   That's no the large problem.  Data races are one thing.  The large problem is when the JIT reorders code and optimizes code to not be sequentially consistent.  It then lands on the developers shoulders to understand every possible, undocumented optimization that might be done by the compiler to try and understand why the code is misbehaving.  Only then, can they possibly formulate which changes will actually correct the behavior of their code, without making it "slower" than it needs to be.  To me, that is exactly the wrong way for a developer to test and optimize their code.  Visible data races that are only about "the data", are trivial to reason about.

If I saw the Thread.setName/getName lack of synchronization, out in the wild, create a corrupted thread name, I'd be writing a bug report about the JVM creating memory corruption before I would consider that the array had been allocated and made visible before it was filled.  That's not how the code is written, and that's not how it should execute without the developer deciding that such an optimization is safe for their application to see.

Suggesting that denying the visibility of the optimization is selected by the developer not doing anything, is where I respectfully, cannot agree with such reasoning.  This is not a new issue.  The design of many functional languages is based on the simple fact that developers don't need to be burdened with designating that they want correctly executing code.  They should get that by default, and have to work at "faster" or "more performant" or "less latency", by explicit actions they take in code structure.

Thread.setName/getName is a great example of "no thought given" to concurrency in the original design.  That code, as said before, is only executed in a concurrent environment.  Why didn't the original code have the correct concurrency design.  Why hasn't it been visible to enough people that a bug report written and it already fixed?

The answer is, because that racy code, doesn't "act" wrong, in general usage patterns.  But optimizations due to how volatile works and how JIT developers exploit lack of "concurrency selected" coding, could, at any point, break that code.

This is how all of that legacy code, laying around on the internet, is going to come down.  It's going to start randomly breaking and causing completely unexplainable bugs.  People will not remember enough details about this code to actually understand that the JIT is causing them problems with reordering, loop hoisting or other "nifty" optimizations that provide a .1% improvement in speed.   It will be a giant waste of peoples time trying to reconcile what is actually going wrong, and in some cases, there will be real impact on the users lives, welfare and/or safety, potentially.

Call my position extreme, but I think it's vital for the Java community and Oracle in particular to understand that the path we are going down is absolutely a perilous disaster without something very specific being visible to developers to allow them to understand how their code is being executed so that they can see exactly what parts of the code need to be fixed.

Gregg Wonderly

On Apr 30, 2013, at 7:48 PM, "Boehm, Hans" <hans.boehm at hp.com<mailto:hans.boehm at hp.com>> wrote:

A nice simple example to consider here is a user application that declares an array, and calls a library to sort it.  The sort library uses a parallel sort that relies on a sequential sort to sort small sections of the array.  In a sequential-by-default world, how would you declare the parallel sections?  Would it be any different than what we do now?  The user application that declares the array may never know that there is any parallel code involved.  Nor should it.

Applications such as this are naturally data-race-free, thus there is no issue with the compiler "breaking" code.  And the compiler can apply nearly all sequentially valid transformations on synchronization-free code, such as the sequential sort operations.  If you accidentally introduce a data race bug, it's unlikely your code would run correctly even if the compiler guaranteed sequential consistency. Your code may be a bit easier to debug with a hypothetical compiler that ensures sequential consistency.  But so long as you avoided intentional (unannotated) data races, I think a data race detector would also make this fairly easy.

Hans

From: concurrency-interest-bounces at cs.oswego.edu<mailto:concurrency-interest-bounces at cs.oswego.edu> [mailto:concurrency-<mailto:concurrency->interest-bounces at cs.oswego.edu<mailto:interest-bounces at cs.oswego.edu>] On Behalf Of Martin Thompson
Sent: Tuesday, April 30, 2013 5:57 AM
To: Kirk Pepperdine
Cc: Gregg Wonderly; concurrency-interest at cs.oswego.edu<mailto:concurrency-interest at cs.oswego.edu>
Subject: Re: [concurrency-interest] JLS 17.7 Non-atomic treatment of double and long : Android

I agree with Kirk here and would take it further.  By default the vast majority of code should be single threaded and concurrent programming is only utilized in regions of data exchange.

If all code was sequentially consistent then most hardware and compiler optimizations would be defeated.  A default position that all code is concurrent is sending the industry the wrong way in my view.  It makes a more sense to explicitly define the regions of data exchange in our programs and therefore what ordering semantics are required in those regions.

Martin...
------------------------------
Message: 3
Date: Tue, 30 Apr 2013 07:38:01 +0200
From: Kirk Pepperdine <kirk at kodewerk.com<mailto:kirk at kodewerk.com>>
To: Gregg Wonderly <gergg at cox.net<mailto:gergg at cox.net>>
Cc: concurrency-interest at cs.oswego.edu
<mailto:concurrency-interest at cs.oswego.edu>Subject: Re: [concurrency-interest] JLS 17.7 Non-atomic treatment of
        double  and long : Android
Message-ID: <5D85887E-8BFE-4F09-AFA8-53FE1DFD52D3 at kodewerk.com<mailto:5D85887E-8BFE-4F09-AFA8-53FE1DFD52D3 at kodewerk.com>>
Content-Type: text/plain; charset="windows-1252"
Sorry but making thing volatile by default would be a horrible thing to do. Code wouldn't be a bit slower, it would be a lot slower and then you'd end up with the same problem in reverse!
Regards,
Kirk
On 2013-04-30, at 12:10 AM, Gregg Wonderly <gergg at cox.net<mailto:gergg at cox.net>> wrote:
> This code exists everywhere on the Java JVM now, because no one expects the loop hoist?   People are living with it, or eventually declaring the loop variable volatile after finding these discussions.
>
> Java, by default, should of used nothing but volatile variables, and developers should of needed to add non-volatile declarations via annotations, without the 'volatile' keyword being used, at all.
>
> That would of made it hard to "break" code without actually looking up what you were doing, because the added verbosity would only be tolerated when it actually accomplished a performance improvement.  Today, code is "Faster" without "volatile".   If everything was "volatile" by default, then code would be slower to start with, and proper "concurrency programming" would then make your code faster, as concurrency should.
>
> Gregg
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu<mailto:Concurrency-interest at cs.oswego.edu>
http://cs.oswego.edu/mailman/listinfo/concurrency-interest


_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu<mailto:Concurrency-interest at cs.oswego.edu>
http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130502/f548c0ed/attachment-0001.html>

From gergg at cox.net  Thu May  2 13:12:18 2013
From: gergg at cox.net (Gregg Wonderly)
Date: Thu, 2 May 2013 12:12:18 -0500
Subject: [concurrency-interest] JLS 17.7 Non-atomic treatment of double
	and long : Android
In-Reply-To: <A3E67C2071F49C4CBC4F17E6D77CDDD236BB8B23@G9W0725.americas.hpqcorp.net>
References: <CAChYfd-8DO0YCu+cEHj_MGVNzyoPewD+HSwuaF_HmAr_6K-31A@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD236BB8242@G9W0725.americas.hpqcorp.net>
	<7EC0C469-F8AA-4419-94D5-592305C62293@cox.net>
	<A3E67C2071F49C4CBC4F17E6D77CDDD236BB8631@G9W0725.americas.hpqcorp.net>
	<CACuKZqFCaDNtB1eNir-yQ8j+2kwhEVc=SQ4Cw8kVHeDH9mhngA@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD236BB8B23@G9W0725.americas.hpqcorp.net>
Message-ID: <23C6DD8C-A145-461E-8CEE-77721AF5DCC5@cox.net>

One the most interesting issues is the simple fact that we don't know when "another thread" might actually end up executing code that we've designed and implemented to be single threaded.  The declarations on methods today, are about "visibility", but that doesn't always designate "thread usage".  It certainly indicates the opportunity though.  We used to just rely on "synchronized" to do everything.  And because it was "slow" on older generation processors, it's been left off of many shared value usages in legacy code, because there wasn't any problems, then, with reordering or other things that could cause multi-threaded, concurrent access to suddenly create problems.

If one were writing a data flow analysis tool, which could completely validate execution paths and shared data access, one would have to know "how" threads are created, how their lifecycle transitions occur, as well as then doing the execution flow analysis.

If we had marker annotations for methods and interfaces, so that Callable, Runnable (and even Thread as a sub-class) and others could be indicated as "thread sources", and then we could also mark methods with the intended usage (perhaps even having aliases for thread instances for things like AWT/Swing), we might be able to start providing markup that tools could completely be successful at understanding "how the code is designed to be used" vs "how it is being used".

What really seems important to me, is to make sure developers are aware of when they can expect non-sequential execution to disappear, or when they've designed their code to be race free, but have overlooked an opportunity for "access" to shared data by multiple threads, when they are not intending that to happen.

Gregg Wonderly

On May 2, 2013, at 11:31 AM, "Boehm, Hans" <hans.boehm at hp.com> wrote:

> I wonder whether the expectation about atomic publication is really a sign that we have been miseducating people.  The problem may be that it comes too close to working in Java, since we guarantee that even ordinary pointer updates are indivisible.  In traditional C, there is no such guarantee, and such code is clearly broken, or at least not portable, without even considering memory ordering.  In modern C++ code, it will be even more obviously broken if the code is templatized with respect to the pointer type or uses reference-counted pointers.
>  
> In general, I think the right mental model for most programmers is that an assignment first changes a field to an invalid value, and only then changes it  to the specified value.  That?s more or less how Java long and double actually behave.  The problem is that, in order to support security and some tricky high performance code, we provide somewhat stronger guarantees for other types.  But those guarantees are not strong enough to be useful to most people.  They just think they are.
>  
> I still slightly favor annotating declarations with volatile/atomic, rather than the accesses.  But there are clearly arguments on both sides.  The arguments in favor of the declarations are:
>  
> -          Atomic access may have implications on required alignment.  This is probably more of an issue in C than Java, and it?s becoming less common.
> -          It?s easy to forget an annotation on an individual access.  I would rather err on the fail-safe side for individual accesses. (In an ideal world, there probably should be a way to declare individual accesses to a Java volatile as not racing. We mostly have that in C++.)
> -          I don?t think data structure declarations are really understandable without knowing what?s used for communication.  (The same is true for the accesses, but we?re used to having to understand declarations when reading code, more so than the other way around.)
>  
> This seems to be a legitimately controversial issue.
>  
> Hans
>  
> From: Zhong Yu [mailto:zhong.j.yu at gmail.com] 
> Sent: Wednesday, May 01, 2013 9:17 PM
> To: Boehm, Hans
> Cc: Gregg Wonderly; concurrency-interest at cs.oswego.edu; Martin Thompson
> Subject: Re: [concurrency-interest] JLS 17.7 Non-atomic treatment of double and long : Android
>  
> Good point Hans, however I doubt any sane programmer would intentionally play with interleaving which is too hard to reason. Regardless of memory model, the most important use case of shared variable is atomic publication - a thread writes to some places that nobody else is supposed to be reading, then writes to a shared variable to publish all previous writes. No interleaving. Programmers intuitively assume that it should work; the possibility of out-of-order writes is too bizzare to register in our minds. Is it really worth it for JMM to go against this intuition? How much performance gain are we talking about anyway? The java community has wasted a lot of time on it.
> 
> Another problem is the syntax - volatility is declared on the variable, instead of on read/write actions. If it's the later, programmers are more likely to do the right thing because they do think about inter-thread effects when they read/write shared variables.
>  
> Zhong Yu
>  
>  
>  
> 
> On Wed, May 1, 2013 at 2:41 PM, Boehm, Hans <hans.boehm at hp.com> wrote:
> I think this is the wrong programming model.  Even if the implementation guaranteed sequential consistency, data races would usually indicate a bug.  Sequential consistency helps with Thread.setName() only because the natural implementation of the operation is indivisible.  If it involved setting more than one field, the racy code would still break, even in a sequentially consistent implementation.
>  
> I claim that in reality programmers expect not just sequential consistency (i.e. as if the threads were simply interleaved), but they also expect that code sections that appear sequential, i.e. those that contain no synchronization, execute sequentially, without interference from other threads.  I can?t read a piece of code if any field can change out from under me at any time.  The only way I can get that required guarantee is by writing data-race-free code, which requires me to annotate synchronization variables as volatile.  Once I do that, we know how to built tools that will report data races in real code, admittedly with 10x-100x slowdown (in the absence of hardware support).  If such a tool reports no data races, then I know that the execution is sequential consistency, even without forcing my compiler to treat everything as volatile.  And since I didn?t declare everything as volatile, I also get the equally important guarantee that synchronization-free code executes sequentially, i.e. atomically, and thus I don?t need to worry about possible interleaving at  every instruction.  I don?t need to worry about whether synchronization-free library calls are implemented with a single assignment or three.  I think this is a much more practical programming model than just sequential consistency.  And it?s the one that motivates various current language memory models, including the Java one.
>  
> In this world, which requires better tools, and somewhat more disciplined code, but not a sequentially consistent memory model, I expect the Thread.setName() problem would have been fairly easy to debug.  We unfortunately still rely on testing with a race detector.  But if we can get the code to fail under those conditions, the error message would have clearly pointed at the problem before you ever saw a corrupt thread name.  (The most natural fix in this case may still involve adding synchronization to the library, and fixing the documentation to reflect that, but that?s a separate issue.)
>  
> (Reality is slightly more complicated, and I think we?d need a way other than ?volatile? to annotate races on variables when we don?t expect enforcement of sequential consistency, but just want the current. tricky and misspecified, Java variable semantics, such as they are.  But I don?t think that mechanism should be used by, or interfere with, the vast majority of programmers, whom I believe Gregg is addressing.)
>  
> Hans
>  
> From: Gregg Wonderly [mailto:gergg at cox.net] 
> Sent: Tuesday, April 30, 2013 7:21 PM
> To: Boehm, Hans
> Cc: Martin Thompson; Kirk Pepperdine; concurrency-interest at cs.oswego.edu
> 
> Subject: Re: [concurrency-interest] JLS 17.7 Non-atomic treatment of double and long : Android
>  
> The problem is that people can trivially create data races.  It's happened everywhere.  There is so much legacy Java code that has data races.   That's no the large problem.  Data races are one thing.  The large problem is when the JIT reorders code and optimizes code to not be sequentially consistent.  It then lands on the developers shoulders to understand every possible, undocumented optimization that might be done by the compiler to try and understand why the code is misbehaving.  Only then, can they possibly formulate which changes will actually correct the behavior of their code, without making it "slower" than it needs to be.  To me, that is exactly the wrong way for a developer to test and optimize their code.  Visible data races that are only about "the data", are trivial to reason about.  
>  
> If I saw the Thread.setName/getName lack of synchronization, out in the wild, create a corrupted thread name, I'd be writing a bug report about the JVM creating memory corruption before I would consider that the array had been allocated and made visible before it was filled.  That's not how the code is written, and that's not how it should execute without the developer deciding that such an optimization is safe for their application to see.
>  
> Suggesting that denying the visibility of the optimization is selected by the developer not doing anything, is where I respectfully, cannot agree with such reasoning.  This is not a new issue.  The design of many functional languages is based on the simple fact that developers don't need to be burdened with designating that they want correctly executing code.  They should get that by default, and have to work at "faster" or "more performant" or "less latency", by explicit actions they take in code structure.
>  
> Thread.setName/getName is a great example of "no thought given" to concurrency in the original design.  That code, as said before, is only executed in a concurrent environment.  Why didn't the original code have the correct concurrency design.  Why hasn't it been visible to enough people that a bug report written and it already fixed?
>  
> The answer is, because that racy code, doesn't "act" wrong, in general usage patterns.  But optimizations due to how volatile works and how JIT developers exploit lack of "concurrency selected" coding, could, at any point, break that code.
>  
> This is how all of that legacy code, laying around on the internet, is going to come down.  It's going to start randomly breaking and causing completely unexplainable bugs.  People will not remember enough details about this code to actually understand that the JIT is causing them problems with reordering, loop hoisting or other "nifty" optimizations that provide a .1% improvement in speed.   It will be a giant waste of peoples time trying to reconcile what is actually going wrong, and in some cases, there will be real impact on the users lives, welfare and/or safety, potentially.
>  
> Call my position extreme, but I think it's vital for the Java community and Oracle in particular to understand that the path we are going down is absolutely a perilous disaster without something very specific being visible to developers to allow them to understand how their code is being executed so that they can see exactly what parts of the code need to be fixed.
>  
> Gregg Wonderly
>  
> On Apr 30, 2013, at 7:48 PM, "Boehm, Hans" <hans.boehm at hp.com> wrote:
>  
> 
> A nice simple example to consider here is a user application that declares an array, and calls a library to sort it.  The sort library uses a parallel sort that relies on a sequential sort to sort small sections of the array.  In a sequential-by-default world, how would you declare the parallel sections?  Would it be any different than what we do now?  The user application that declares the array may never know that there is any parallel code involved.  Nor should it.
>  
> Applications such as this are naturally data-race-free, thus there is no issue with the compiler ?breaking? code.  And the compiler can apply nearly all sequentially valid transformations on synchronization-free code, such as the sequential sort operations.  If you accidentally introduce a data race bug, it?s unlikely your code would run correctly even if the compiler guaranteed sequential consistency. Your code may be a bit easier to debug with a hypothetical compiler that ensures sequential consistency.  But so long as you avoided intentional (unannotated) data races, I think a data race detector would also make this fairly easy.
>  
> Hans
>  
> From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Martin Thompson
> Sent: Tuesday, April 30, 2013 5:57 AM
> To: Kirk Pepperdine
> Cc: Gregg Wonderly; concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] JLS 17.7 Non-atomic treatment of double and long : Android
>  
> I agree with Kirk here and would take it further.  By default the vast majority of code should be single threaded and concurrent programming is only utilized in regions of data exchange.
>  
> If all code was sequentially consistent then most hardware and compiler optimizations would be defeated.  A default position that all code is concurrent is sending the industry the wrong way in my view.  It makes a more sense to explicitly define the regions of data exchange in our programs and therefore what ordering semantics are required in those regions.
>  
> Martin...
> ------------------------------
> Message: 3
> Date: Tue, 30 Apr 2013 07:38:01 +0200
> From: Kirk Pepperdine <kirk at kodewerk.com>
> To: Gregg Wonderly <gergg at cox.net>
> Cc: concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] JLS 17.7 Non-atomic treatment of
>         double  and long : Android
> Message-ID: <5D85887E-8BFE-4F09-AFA8-53FE1DFD52D3 at kodewerk.com>
> Content-Type: text/plain; charset="windows-1252"
> Sorry but making thing volatile by default would be a horrible thing to do. Code wouldn't be a bit slower, it would be a lot slower and then you'd end up with the same problem in reverse!
> Regards,
> Kirk
> On 2013-04-30, at 12:10 AM, Gregg Wonderly <gergg at cox.net> wrote:
> > This code exists everywhere on the Java JVM now, because no one expects the loop hoist?   People are living with it, or eventually declaring the loop variable volatile after finding these discussions.
> >
> > Java, by default, should of used nothing but volatile variables, and developers should of needed to add non-volatile declarations via annotations, without the 'volatile' keyword being used, at all.
> >
> > That would of made it hard to "break" code without actually looking up what you were doing, because the added verbosity would only be tolerated when it actually accomplished a performance improvement.  Today, code is "Faster" without "volatile".   If everything was "volatile" by default, then code would be slower to start with, and proper "concurrency programming" would then make your code faster, as concurrency should.
> >
> > Gregg
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>  
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
>  
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130502/43c4db48/attachment-0001.html>

From james.roper at typesafe.com  Thu May  2 18:34:59 2013
From: james.roper at typesafe.com (James Roper)
Date: Fri, 3 May 2013 08:34:59 +1000
Subject: [concurrency-interest] Thread local contexts and
	CompletableFutures
In-Reply-To: <51824601.6080701@cs.oswego.edu>
References: <CABY0rKMEBuRMCLgdu7kL-VL1A3m73shE93KOrtJa9rkunvH8Jg@mail.gmail.com>
	<51824601.6080701@cs.oswego.edu>
Message-ID: <CABY0rKM+Dxe3OWsFa0ztbnOGd5aEum8ywQDQXBWVQDAdCz05Gg@mail.gmail.com>

On Thu, May 2, 2013 at 8:54 PM, Doug Lea <dl at cs.oswego.edu> wrote:

> On 05/02/13 03:36, James Roper wrote:
>
>>
>> However there is an important feature that we would need for the
>> CompletableFuture API to be useful to us.  Thread locals are very
>> commonly used
>> in Java frameworks.
>>
>> As far as I can see, and correct me if I'm wrong, there is nothing in the
>> CompletableFuture API that addresses this, or allows it to be addressed.
>>
>> In contrast, our existing Promise API can easily address this, because
>> Scala's
>> ExecutionContext (roughly equivalent to the ExecutorService that gets
>> passed in
>> with callbacks to the CompletableFuture API) has a prepare method that
>> returns a
>> new (or the same) execution context.
>>
>
> This seems possible using the current API.
> All async completion methods have a form accepting an Executor,
> that could be some form of your ExecutionContext.
> You might want to layer on something ensuring this.
> Additionally, if it is possible for direct (non-async) execution
> to also use a form of ExecutionContext, then you'd need
> to use the async form in these cases as well, but supply
> a version of ExecutionContext that directly executes after
> establishing context. (in other words, there's nothing about
> the "async" methods that strictly requires asynchrony.)
> It's conceivable that there is some case not handled by these
> options, but I don't see any offhand.


Yes, the executor can definitely should hold the context, but the
difficulty is how it gets the context.  If the client code is responsible
for instantiating the context, then that's the answer, but ideally, client
code would just be able to use an executor provided by whatever container
they are in.  For example, currently this wouldn't be possible:

@Inject
public MyService(Executor executor, AsyncDao dao) {
  this.e = executor;
  this.dao = dao;
}

@TransactionAttribute(REQUIRED)
public CompletableFuture<Foo> getFoo(long id) {
  return dao.get(id).thenApplyAsync(...., executor);
}

Since nothing interacts with the executor in the original thread, and so it
can't capture the context.  On the other hand, this would be feasible:

public CompletableFuture<Foo> getFoo(long id) {
  return dao.get(id).thenApplyAsync(...., StaticContext.currentExecutor());
}

though has the disadvantage of being static.  The final possibility would
be:

@Inject
public MyService(Context context, AsyncDao dao) {
  this.context = context;
  this.dao = dao;
}

public CompletableFuture<Foo> getFoo(long id) {
  return dao.get(id).thenApplyAsync(...., context.currentExecutor());
}

This solution is not too bad, it's just not as nice since it's not
transparent, and requires the user to understand that they must never cache
the current executor.

Anyway, I've possibly answered my own questions here, but do any of the
above seem to be what you would expect to become a typical usage pattern
inside containers that use thread locals?  Eg would this become the way
things are done in a possible future asynchronous JEE spec with
asynchronous database access?  The important thing that I want to ensure
here is that whatever we do in Play framework will be idiomatic for a Java
developer, but as yet there are no idioms because this stuff is new.

Cheers,

James


>
>
> -Doug
>
>
> ______________________________**_________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>



-- 
*James Roper*
*Software Engineer*
*
*
Typesafe <http://www.typesafe.com/> - The software stack for applications
that scale
Twitter: @jroper
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130503/f9e2a848/attachment.html>

From martinrb at google.com  Thu May  2 23:58:44 2013
From: martinrb at google.com (Martin Buchholz)
Date: Thu, 2 May 2013 20:58:44 -0700
Subject: [concurrency-interest] Thread local contexts and
	CompletableFutures
In-Reply-To: <CABY0rKM+Dxe3OWsFa0ztbnOGd5aEum8ywQDQXBWVQDAdCz05Gg@mail.gmail.com>
References: <CABY0rKMEBuRMCLgdu7kL-VL1A3m73shE93KOrtJa9rkunvH8Jg@mail.gmail.com>
	<51824601.6080701@cs.oswego.edu>
	<CABY0rKM+Dxe3OWsFa0ztbnOGd5aEum8ywQDQXBWVQDAdCz05Gg@mail.gmail.com>
Message-ID: <CA+kOe0_iY_zbvX4O0V=qg1XwaPX9vhonzBL-z_84kN2UF=1P5Q@mail.gmail.com>

What concrete additions to CompletableFuture might you suggest that
couldn't be easily done in a layered library?  The current API is
conceptually rather simple.


On Thu, May 2, 2013 at 3:34 PM, James Roper <james.roper at typesafe.com>wrote:

> On Thu, May 2, 2013 at 8:54 PM, Doug Lea <dl at cs.oswego.edu> wrote:
>
>> On 05/02/13 03:36, James Roper wrote:
>>
>>>
>>> However there is an important feature that we would need for the
>>> CompletableFuture API to be useful to us.  Thread locals are very
>>> commonly used
>>> in Java frameworks.
>>>
>>> As far as I can see, and correct me if I'm wrong, there is nothing in the
>>> CompletableFuture API that addresses this, or allows it to be addressed.
>>>
>>> In contrast, our existing Promise API can easily address this, because
>>> Scala's
>>> ExecutionContext (roughly equivalent to the ExecutorService that gets
>>> passed in
>>> with callbacks to the CompletableFuture API) has a prepare method that
>>> returns a
>>> new (or the same) execution context.
>>>
>>
>> This seems possible using the current API.
>> All async completion methods have a form accepting an Executor,
>> that could be some form of your ExecutionContext.
>> You might want to layer on something ensuring this.
>> Additionally, if it is possible for direct (non-async) execution
>> to also use a form of ExecutionContext, then you'd need
>> to use the async form in these cases as well, but supply
>> a version of ExecutionContext that directly executes after
>> establishing context. (in other words, there's nothing about
>> the "async" methods that strictly requires asynchrony.)
>> It's conceivable that there is some case not handled by these
>> options, but I don't see any offhand.
>
>
> Yes, the executor can definitely should hold the context, but the
> difficulty is how it gets the context.  If the client code is responsible
> for instantiating the context, then that's the answer, but ideally, client
> code would just be able to use an executor provided by whatever container
> they are in.  For example, currently this wouldn't be possible:
>
> @Inject
> public MyService(Executor executor, AsyncDao dao) {
>   this.e = executor;
>   this.dao = dao;
> }
>
> @TransactionAttribute(REQUIRED)
> public CompletableFuture<Foo> getFoo(long id) {
>   return dao.get(id).thenApplyAsync(...., executor);
> }
>
> Since nothing interacts with the executor in the original thread, and so
> it can't capture the context.  On the other hand, this would be feasible:
>
> public CompletableFuture<Foo> getFoo(long id) {
>   return dao.get(id).thenApplyAsync(...., StaticContext.currentExecutor());
> }
>
> though has the disadvantage of being static.  The final possibility would
> be:
>
> @Inject
> public MyService(Context context, AsyncDao dao) {
>   this.context = context;
>   this.dao = dao;
> }
>
> public CompletableFuture<Foo> getFoo(long id) {
>   return dao.get(id).thenApplyAsync(...., context.currentExecutor());
> }
>
> This solution is not too bad, it's just not as nice since it's not
> transparent, and requires the user to understand that they must never cache
> the current executor.
>
> Anyway, I've possibly answered my own questions here, but do any of the
> above seem to be what you would expect to become a typical usage pattern
> inside containers that use thread locals?  Eg would this become the way
> things are done in a possible future asynchronous JEE spec with
> asynchronous database access?  The important thing that I want to ensure
> here is that whatever we do in Play framework will be idiomatic for a Java
> developer, but as yet there are no idioms because this stuff is new.
>
> Cheers,
>
> James
>
>
>>
>>
>> -Doug
>>
>>
>> ______________________________**_________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
>> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>
>
>
>
> --
> *James Roper*
> *Software Engineer*
> *
> *
> Typesafe <http://www.typesafe.com/> - The software stack for applications
> that scale
> Twitter: @jroper
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130502/9ec3c033/attachment.html>

From james.roper at typesafe.com  Fri May  3 00:24:22 2013
From: james.roper at typesafe.com (James Roper)
Date: Fri, 3 May 2013 14:24:22 +1000
Subject: [concurrency-interest] Thread local contexts and
	CompletableFutures
In-Reply-To: <CA+kOe0_iY_zbvX4O0V=qg1XwaPX9vhonzBL-z_84kN2UF=1P5Q@mail.gmail.com>
References: <CABY0rKMEBuRMCLgdu7kL-VL1A3m73shE93KOrtJa9rkunvH8Jg@mail.gmail.com>
	<51824601.6080701@cs.oswego.edu>
	<CABY0rKM+Dxe3OWsFa0ztbnOGd5aEum8ywQDQXBWVQDAdCz05Gg@mail.gmail.com>
	<CA+kOe0_iY_zbvX4O0V=qg1XwaPX9vhonzBL-z_84kN2UF=1P5Q@mail.gmail.com>
Message-ID: <CABY0rKOWECfBbN=ZhG=WYAKFo=58o+3-2pNnQqTbcuXsw+3GHA@mail.gmail.com>

On Fri, May 3, 2013 at 1:58 PM, Martin Buchholz <martinrb at google.com> wrote:

> What concrete additions to CompletableFuture might you suggest that
> couldn't be easily done in a layered library?  The current API is
> conceptually rather simple.
>

Are you suggesting that we layer something on top of CompletableFuture to
do this?  That won't do, since the API that created the CompletableFuture
won't know anything about the context that the callbacks passed to it need.

Consider this code:

HttpClient httpClient = ...
DatabaseClient databaseClient = ...

CompletableFuture<Response> httpResponse = httpClient.makeRequest("
http://foo");
httpResponse.thenComposeAsync(response ->
databaseClient.makeQuery(response.getBody));

The callback passed to thenComposeAsync needs to be run with the
transaction context setup.  But this is on a CompletableFuture returned by
the HttpClient, what does HttpClient know about transaction contexts?  It
can't wrap the future in one that will handle this, since it doesn't nor
should it know anything about the contexts of the callbacks passed to it.
 Layering on top of CompletableFuture is definitely not an option.

I wouldn't suggest changing the CompletableFuture API.  What I would
suggest is adding something like:

Excecutor captureContext();

To java.util.concurrent.Executor, and having every call that accepted an
executor in the CompletableFuture API call it.  Then layering can be
transparently done on top of the Executor API to handle this.  Though, this
would mean introducing the concept of context to executor, which currently
doesn't exist.


>
> On Thu, May 2, 2013 at 3:34 PM, James Roper <james.roper at typesafe.com>wrote:
>
>> On Thu, May 2, 2013 at 8:54 PM, Doug Lea <dl at cs.oswego.edu> wrote:
>>
>>> On 05/02/13 03:36, James Roper wrote:
>>>
>>>>
>>>> However there is an important feature that we would need for the
>>>> CompletableFuture API to be useful to us.  Thread locals are very
>>>> commonly used
>>>> in Java frameworks.
>>>>
>>>> As far as I can see, and correct me if I'm wrong, there is nothing in
>>>> the
>>>> CompletableFuture API that addresses this, or allows it to be addressed.
>>>>
>>>> In contrast, our existing Promise API can easily address this, because
>>>> Scala's
>>>> ExecutionContext (roughly equivalent to the ExecutorService that gets
>>>> passed in
>>>> with callbacks to the CompletableFuture API) has a prepare method that
>>>> returns a
>>>> new (or the same) execution context.
>>>>
>>>
>>> This seems possible using the current API.
>>> All async completion methods have a form accepting an Executor,
>>> that could be some form of your ExecutionContext.
>>> You might want to layer on something ensuring this.
>>> Additionally, if it is possible for direct (non-async) execution
>>> to also use a form of ExecutionContext, then you'd need
>>> to use the async form in these cases as well, but supply
>>> a version of ExecutionContext that directly executes after
>>> establishing context. (in other words, there's nothing about
>>> the "async" methods that strictly requires asynchrony.)
>>> It's conceivable that there is some case not handled by these
>>> options, but I don't see any offhand.
>>
>>
>> Yes, the executor can definitely should hold the context, but the
>> difficulty is how it gets the context.  If the client code is responsible
>> for instantiating the context, then that's the answer, but ideally, client
>> code would just be able to use an executor provided by whatever container
>> they are in.  For example, currently this wouldn't be possible:
>>
>> @Inject
>> public MyService(Executor executor, AsyncDao dao) {
>>   this.e = executor;
>>   this.dao = dao;
>> }
>>
>> @TransactionAttribute(REQUIRED)
>> public CompletableFuture<Foo> getFoo(long id) {
>>   return dao.get(id).thenApplyAsync(...., executor);
>> }
>>
>> Since nothing interacts with the executor in the original thread, and so
>> it can't capture the context.  On the other hand, this would be feasible:
>>
>> public CompletableFuture<Foo> getFoo(long id) {
>>   return dao.get(id).thenApplyAsync(....,
>> StaticContext.currentExecutor());
>> }
>>
>> though has the disadvantage of being static.  The final possibility would
>> be:
>>
>> @Inject
>> public MyService(Context context, AsyncDao dao) {
>>   this.context = context;
>>   this.dao = dao;
>> }
>>
>> public CompletableFuture<Foo> getFoo(long id) {
>>   return dao.get(id).thenApplyAsync(...., context.currentExecutor());
>> }
>>
>> This solution is not too bad, it's just not as nice since it's not
>> transparent, and requires the user to understand that they must never cache
>> the current executor.
>>
>> Anyway, I've possibly answered my own questions here, but do any of the
>> above seem to be what you would expect to become a typical usage pattern
>> inside containers that use thread locals?  Eg would this become the way
>> things are done in a possible future asynchronous JEE spec with
>> asynchronous database access?  The important thing that I want to ensure
>> here is that whatever we do in Play framework will be idiomatic for a Java
>> developer, but as yet there are no idioms because this stuff is new.
>>
>> Cheers,
>>
>> James
>>
>>
>>>
>>>
>>> -Doug
>>>
>>>
>>> ______________________________**_________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.**oswego.edu<Concurrency-interest at cs.oswego.edu>
>>> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>>
>>
>>
>>
>> --
>> *James Roper*
>> *Software Engineer*
>> *
>> *
>> Typesafe <http://www.typesafe.com/> - The software stack for
>> applications that scale
>> Twitter: @jroper
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>


-- 
*James Roper*
*Software Engineer*
*
*
Typesafe <http://www.typesafe.com/> - The software stack for applications
that scale
Twitter: @jroper
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130503/ae530e28/attachment-0001.html>

From hans.boehm at hp.com  Fri May  3 01:47:58 2013
From: hans.boehm at hp.com (Boehm, Hans)
Date: Fri, 3 May 2013 05:47:58 +0000
Subject: [concurrency-interest] JLS 17.7 Non-atomic treatment of double
 and long : Android
In-Reply-To: <23C6DD8C-A145-461E-8CEE-77721AF5DCC5@cox.net>
References: <CAChYfd-8DO0YCu+cEHj_MGVNzyoPewD+HSwuaF_HmAr_6K-31A@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD236BB8242@G9W0725.americas.hpqcorp.net>
	<7EC0C469-F8AA-4419-94D5-592305C62293@cox.net>
	<A3E67C2071F49C4CBC4F17E6D77CDDD236BB8631@G9W0725.americas.hpqcorp.net>
	<CACuKZqFCaDNtB1eNir-yQ8j+2kwhEVc=SQ4Cw8kVHeDH9mhngA@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD236BB8B23@G9W0725.americas.hpqcorp.net>
	<23C6DD8C-A145-461E-8CEE-77721AF5DCC5@cox.net>
Message-ID: <A3E67C2071F49C4CBC4F17E6D77CDDD236BB9048@G9W0725.americas.hpqcorp.net>

I'm not sure I fully understand the comments here.  Certainly any sort of interface should be clear about the conditions under which it's safe to invoke it from multiple threads simultaneously.  There's no way to avoid that, sequential consistency or not.

My sense is that the most practical race detectors are dynamic detectors that understand Java happens-before relationships and do not generate any false positives.  Most commonly these track happens-before relationships and when each field is last accessed by each thread, and then directly reporting conflicting accesses to the same variable that are not ordered by happens-before.  They treat volatiles as well as locks.  The technology is well understood, scales reasonably well, but involves appreciable overhead (which could in theory be dramatically reduced with hardware support, and some very mild compromises in precision).  They don't solve the test coverage problem.  If you don't exercise the race during testing, you won't find the problem.  But you won't see a violation of sequential consistency either.

Clearly such race detectors don't avoid the requirement that interfaces need to specify thread safety.  But they do help you find the cases in which you got it wrong.

Hans

From: Gregg Wonderly [mailto:gergg at cox.net]
Sent: Thursday, May 02, 2013 10:12 AM
To: Boehm, Hans
Cc: Zhong Yu; Martin Thompson; concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] JLS 17.7 Non-atomic treatment of double and long : Android

One the most interesting issues is the simple fact that we don't know when "another thread" might actually end up executing code that we've designed and implemented to be single threaded.  The declarations on methods today, are about "visibility", but that doesn't always designate "thread usage".  It certainly indicates the opportunity though.  We used to just rely on "synchronized" to do everything.  And because it was "slow" on older generation processors, it's been left off of many shared value usages in legacy code, because there wasn't any problems, then, with reordering or other things that could cause multi-threaded, concurrent access to suddenly create problems.

If one were writing a data flow analysis tool, which could completely validate execution paths and shared data access, one would have to know "how" threads are created, how their lifecycle transitions occur, as well as then doing the execution flow analysis.

If we had marker annotations for methods and interfaces, so that Callable, Runnable (and even Thread as a sub-class) and others could be indicated as "thread sources", and then we could also mark methods with the intended usage (perhaps even having aliases for thread instances for things like AWT/Swing), we might be able to start providing markup that tools could completely be successful at understanding "how the code is designed to be used" vs "how it is being used".

What really seems important to me, is to make sure developers are aware of when they can expect non-sequential execution to disappear, or when they've designed their code to be race free, but have overlooked an opportunity for "access" to shared data by multiple threads, when they are not intending that to happen.

Gregg Wonderly

On May 2, 2013, at 11:31 AM, "Boehm, Hans" <hans.boehm at hp.com<mailto:hans.boehm at hp.com>> wrote:


I wonder whether the expectation about atomic publication is really a sign that we have been miseducating people.  The problem may be that it comes too close to working in Java, since we guarantee that even ordinary pointer updates are indivisible.  In traditional C, there is no such guarantee, and such code is clearly broken, or at least not portable, without even considering memory ordering.  In modern C++ code, it will be even more obviously broken if the code is templatized with respect to the pointer type or uses reference-counted pointers.

In general, I think the right mental model for most programmers is that an assignment first changes a field to an invalid value, and only then changes it  to the specified value.  That's more or less how Java long and double actually behave.  The problem is that, in order to support security and some tricky high performance code, we provide somewhat stronger guarantees for other types.  But those guarantees are not strong enough to be useful to most people.  They just think they are.

I still slightly favor annotating declarations with volatile/atomic, rather than the accesses.  But there are clearly arguments on both sides.  The arguments in favor of the declarations are:

-          Atomic access may have implications on required alignment.  This is probably more of an issue in C than Java, and it's becoming less common.
-          It's easy to forget an annotation on an individual access.  I would rather err on the fail-safe side for individual accesses. (In an ideal world, there probably should be a way to declare individual accesses to a Java volatile as not racing. We mostly have that in C++.)
-          I don't think data structure declarations are really understandable without knowing what's used for communication.  (The same is true for the accesses, but we're used to having to understand declarations when reading code, more so than the other way around.)

This seems to be a legitimately controversial issue.

Hans

From: Zhong Yu [mailto:zhong.j.yu at gmail.com<http://gmail.com>]
Sent: Wednesday, May 01, 2013 9:17 PM
To: Boehm, Hans
Cc: Gregg Wonderly; concurrency-interest at cs.oswego.edu<mailto:concurrency-interest at cs.oswego.edu>; Martin Thompson
Subject: Re: [concurrency-interest] JLS 17.7 Non-atomic treatment of double and long : Android

Good point Hans, however I doubt any sane programmer would intentionally play with interleaving which is too hard to reason. Regardless of memory model, the most important use case of shared variable is atomic publication - a thread writes to some places that nobody else is supposed to be reading, then writes to a shared variable to publish all previous writes. No interleaving. Programmers intuitively assume that it should work; the possibility of out-of-order writes is too bizzare to register in our minds. Is it really worth it for JMM to go against this intuition? How much performance gain are we talking about anyway? The java community has wasted a lot of time on it.
Another problem is the syntax - volatility is declared on the variable, instead of on read/write actions. If it's the later, programmers are more likely to do the right thing because they do think about inter-thread effects when they read/write shared variables.

Zhong Yu



On Wed, May 1, 2013 at 2:41 PM, Boehm, Hans <hans.boehm at hp.com<mailto:hans.boehm at hp.com>> wrote:
I think this is the wrong programming model.  Even if the implementation guaranteed sequential consistency, data races would usually indicate a bug.  Sequential consistency helps with Thread.setName() only because the natural implementation of the operation is indivisible.  If it involved setting more than one field, the racy code would still break, even in a sequentially consistent implementation.

I claim that in reality programmers expect not just sequential consistency (i.e. as if the threads were simply interleaved), but they also expect that code sections that appear sequential, i.e. those that contain no synchronization, execute sequentially, without interference from other threads.  I can't read a piece of code if any field can change out from under me at any time.  The only way I can get that required guarantee is by writing data-race-free code, which requires me to annotate synchronization variables as volatile.  Once I do that, we know how to built tools that will report data races in real code, admittedly with 10x-100x slowdown (in the absence of hardware support).  If such a tool reports no data races, then I know that the execution is sequential consistency, even without forcing my compiler to treat everything as volatile.  And since I didn't declare everything as volatile, I also get the equally important guarantee that synchronization-free code executes sequentially, i.e. atomically, and thus I don't need to worry about possible interleaving at  every instruction.  I don't need to worry about whether synchronization-free library calls are implemented with a single assignment or three.  I think this is a much more practical programming model than just sequential consistency.  And it's the one that motivates various current language memory models, including the Java one.

In this world, which requires better tools, and somewhat more disciplined code, but not a sequentially consistent memory model, I expect the Thread.setName() problem would have been fairly easy to debug.  We unfortunately still rely on testing with a race detector.  But if we can get the code to fail under those conditions, the error message would have clearly pointed at the problem before you ever saw a corrupt thread name.  (The most natural fix in this case may still involve adding synchronization to the library, and fixing the documentation to reflect that, but that's a separate issue.)

(Reality is slightly more complicated, and I think we'd need a way other than "volatile" to annotate races on variables when we don't expect enforcement of sequential consistency, but just want the current. tricky and misspecified, Java variable semantics, such as they are.  But I don't think that mechanism should be used by, or interfere with, the vast majority of programmers, whom I believe Gregg is addressing.)

Hans

From: Gregg Wonderly [mailto:gergg at cox.net<mailto:gergg at cox.net>]
Sent: Tuesday, April 30, 2013 7:21 PM
To: Boehm, Hans
Cc: Martin Thompson; Kirk Pepperdine; concurrency-interest at cs.oswego.edu<mailto:concurrency-interest at cs.oswego.edu>

Subject: Re: [concurrency-interest] JLS 17.7 Non-atomic treatment of double and long : Android

The problem is that people can trivially create data races.  It's happened everywhere.  There is so much legacy Java code that has data races.   That's no the large problem.  Data races are one thing.  The large problem is when the JIT reorders code and optimizes code to not be sequentially consistent.  It then lands on the developers shoulders to understand every possible, undocumented optimization that might be done by the compiler to try and understand why the code is misbehaving.  Only then, can they possibly formulate which changes will actually correct the behavior of their code, without making it "slower" than it needs to be.  To me, that is exactly the wrong way for a developer to test and optimize their code.  Visible data races that are only about "the data", are trivial to reason about.

If I saw the Thread.setName/getName lack of synchronization, out in the wild, create a corrupted thread name, I'd be writing a bug report about the JVM creating memory corruption before I would consider that the array had been allocated and made visible before it was filled.  That's not how the code is written, and that's not how it should execute without the developer deciding that such an optimization is safe for their application to see.

Suggesting that denying the visibility of the optimization is selected by the developer not doing anything, is where I respectfully, cannot agree with such reasoning.  This is not a new issue.  The design of many functional languages is based on the simple fact that developers don't need to be burdened with designating that they want correctly executing code.  They should get that by default, and have to work at "faster" or "more performant" or "less latency", by explicit actions they take in code structure.

Thread.setName/getName is a great example of "no thought given" to concurrency in the original design.  That code, as said before, is only executed in a concurrent environment.  Why didn't the original code have the correct concurrency design.  Why hasn't it been visible to enough people that a bug report written and it already fixed?

The answer is, because that racy code, doesn't "act" wrong, in general usage patterns.  But optimizations due to how volatile works and how JIT developers exploit lack of "concurrency selected" coding, could, at any point, break that code.

This is how all of that legacy code, laying around on the internet, is going to come down.  It's going to start randomly breaking and causing completely unexplainable bugs.  People will not remember enough details about this code to actually understand that the JIT is causing them problems with reordering, loop hoisting or other "nifty" optimizations that provide a .1% improvement in speed.   It will be a giant waste of peoples time trying to reconcile what is actually going wrong, and in some cases, there will be real impact on the users lives, welfare and/or safety, potentially.

Call my position extreme, but I think it's vital for the Java community and Oracle in particular to understand that the path we are going down is absolutely a perilous disaster without something very specific being visible to developers to allow them to understand how their code is being executed so that they can see exactly what parts of the code need to be fixed.

Gregg Wonderly

On Apr 30, 2013, at 7:48 PM, "Boehm, Hans" <hans.boehm at hp.com<mailto:hans.boehm at hp.com>> wrote:

A nice simple example to consider here is a user application that declares an array, and calls a library to sort it.  The sort library uses a parallel sort that relies on a sequential sort to sort small sections of the array.  In a sequential-by-default world, how would you declare the parallel sections?  Would it be any different than what we do now?  The user application that declares the array may never know that there is any parallel code involved.  Nor should it.

Applications such as this are naturally data-race-free, thus there is no issue with the compiler "breaking" code.  And the compiler can apply nearly all sequentially valid transformations on synchronization-free code, such as the sequential sort operations.  If you accidentally introduce a data race bug, it's unlikely your code would run correctly even if the compiler guaranteed sequential consistency. Your code may be a bit easier to debug with a hypothetical compiler that ensures sequential consistency.  But so long as you avoided intentional (unannotated) data races, I think a data race detector would also make this fairly easy.

Hans

From: concurrency-interest-bounces at cs.oswego.edu<mailto:concurrency-interest-bounces at cs.oswego.edu> [mailto:concurrency-<mailto:concurrency->interest-bounces at cs.oswego.edu<mailto:interest-bounces at cs.oswego.edu>] On Behalf Of Martin Thompson
Sent: Tuesday, April 30, 2013 5:57 AM
To: Kirk Pepperdine
Cc: Gregg Wonderly; concurrency-interest at cs.oswego.edu<mailto:concurrency-interest at cs.oswego.edu>
Subject: Re: [concurrency-interest] JLS 17.7 Non-atomic treatment of double and long : Android

I agree with Kirk here and would take it further.  By default the vast majority of code should be single threaded and concurrent programming is only utilized in regions of data exchange.

If all code was sequentially consistent then most hardware and compiler optimizations would be defeated.  A default position that all code is concurrent is sending the industry the wrong way in my view.  It makes a more sense to explicitly define the regions of data exchange in our programs and therefore what ordering semantics are required in those regions.

Martin...
------------------------------
Message: 3
Date: Tue, 30 Apr 2013 07:38:01 +0200
From: Kirk Pepperdine <kirk at kodewerk.com<mailto:kirk at kodewerk.com>>
To: Gregg Wonderly <gergg at cox.net<mailto:gergg at cox.net>>
Cc: concurrency-interest at cs.oswego.edu
<mailto:concurrency-interest at cs.oswego.edu>Subject: Re: [concurrency-interest] JLS 17.7 Non-atomic treatment of
        double  and long : Android
Message-ID: <5D85887E-8BFE-4F09-AFA8-53FE1DFD52D3 at kodewerk.com<mailto:5D85887E-8BFE-4F09-AFA8-53FE1DFD52D3 at kodewerk.com>>
Content-Type: text/plain; charset="windows-1252"
Sorry but making thing volatile by default would be a horrible thing to do. Code wouldn't be a bit slower, it would be a lot slower and then you'd end up with the same problem in reverse!
Regards,
Kirk
On 2013-04-30, at 12:10 AM, Gregg Wonderly <gergg at cox.net<mailto:gergg at cox.net>> wrote:
> This code exists everywhere on the Java JVM now, because no one expects the loop hoist?   People are living with it, or eventually declaring the loop variable volatile after finding these discussions.
>
> Java, by default, should of used nothing but volatile variables, and developers should of needed to add non-volatile declarations via annotations, without the 'volatile' keyword being used, at all.
>
> That would of made it hard to "break" code without actually looking up what you were doing, because the added verbosity would only be tolerated when it actually accomplished a performance improvement.  Today, code is "Faster" without "volatile".   If everything was "volatile" by default, then code would be slower to start with, and proper "concurrency programming" would then make your code faster, as concurrency should.
>
> Gregg
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu<mailto:Concurrency-interest at cs.oswego.edu>
http://cs.oswego.edu/mailman/listinfo/concurrency-interest


_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu<mailto:Concurrency-interest at cs.oswego.edu>
http://cs.oswego.edu/mailman/listinfo/concurrency-interest

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu<mailto:Concurrency-interest at cs.oswego.edu>
http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130503/97580e08/attachment-0001.html>

From elizarov at devexperts.com  Fri May  3 03:39:07 2013
From: elizarov at devexperts.com (Roman Elizarov)
Date: Fri, 3 May 2013 07:39:07 +0000
Subject: [concurrency-interest] JLS 17.7 Non-atomic treatment of double
 and long : Android
In-Reply-To: <A3E67C2071F49C4CBC4F17E6D77CDDD236BB9048@G9W0725.americas.hpqcorp.net>
References: <CAChYfd-8DO0YCu+cEHj_MGVNzyoPewD+HSwuaF_HmAr_6K-31A@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD236BB8242@G9W0725.americas.hpqcorp.net>
	<7EC0C469-F8AA-4419-94D5-592305C62293@cox.net>
	<A3E67C2071F49C4CBC4F17E6D77CDDD236BB8631@G9W0725.americas.hpqcorp.net>
	<CACuKZqFCaDNtB1eNir-yQ8j+2kwhEVc=SQ4Cw8kVHeDH9mhngA@mail.gmail.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD236BB8B23@G9W0725.americas.hpqcorp.net>
	<23C6DD8C-A145-461E-8CEE-77721AF5DCC5@cox.net>
	<A3E67C2071F49C4CBC4F17E6D77CDDD236BB9048@G9W0725.americas.hpqcorp.net>
Message-ID: <C248BCD79E2CBC4B93C0AE3B1E77E9A824B05F13@RAVEN.office.devexperts.com>

Run-time data race detectors is mostly a solved problem (we have written one in our company and it works as expected). The million-dollar problem to solve is to design a practical language framework that will let one prove that concurrent code works as intended (or at least is data-race free) during compile-time, not during run-time. It has to involve some kind of extensions to type-system (type annotations) to decompose the proof of a modular system into local proofs of each module's adherence to its declared contracts. There's a lot of research into this area, but I've yet to see anything that really scales to big projects and supports all the various concurrency patterns that people have to use in practice. The first challenge on this path, as I see it, is that many concurrency patterns rely on immutability. However, a universal language support for immutability has not emerged yet (maybe IGJ within the checker framework for JSR 308 in Java 8 will do the trick, maybe not). Anyway, without a compile-type checking for immutability we cannot even approach compile-time checks for general concurrency problems (and even for data-race freedom checks, which is a simpler problem to solve).

From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Boehm, Hans
Sent: Friday, May 03, 2013 9:48 AM
To: Gregg Wonderly
Cc: concurrency-interest at cs.oswego.edu; Martin Thompson
Subject: Re: [concurrency-interest] JLS 17.7 Non-atomic treatment of double and long : Android

I'm not sure I fully understand the comments here.  Certainly any sort of interface should be clear about the conditions under which it's safe to invoke it from multiple threads simultaneously.  There's no way to avoid that, sequential consistency or not.

My sense is that the most practical race detectors are dynamic detectors that understand Java happens-before relationships and do not generate any false positives.  Most commonly these track happens-before relationships and when each field is last accessed by each thread, and then directly reporting conflicting accesses to the same variable that are not ordered by happens-before.  They treat volatiles as well as locks.  The technology is well understood, scales reasonably well, but involves appreciable overhead (which could in theory be dramatically reduced with hardware support, and some very mild compromises in precision).  They don't solve the test coverage problem.  If you don't exercise the race during testing, you won't find the problem.  But you won't see a violation of sequential consistency either.

Clearly such race detectors don't avoid the requirement that interfaces need to specify thread safety.  But they do help you find the cases in which you got it wrong.

Hans

From: Gregg Wonderly [mailto:gergg at cox.net]
Sent: Thursday, May 02, 2013 10:12 AM
To: Boehm, Hans
Cc: Zhong Yu; Martin Thompson; concurrency-interest at cs.oswego.edu<mailto:concurrency-interest at cs.oswego.edu>
Subject: Re: [concurrency-interest] JLS 17.7 Non-atomic treatment of double and long : Android

One the most interesting issues is the simple fact that we don't know when "another thread" might actually end up executing code that we've designed and implemented to be single threaded.  The declarations on methods today, are about "visibility", but that doesn't always designate "thread usage".  It certainly indicates the opportunity though.  We used to just rely on "synchronized" to do everything.  And because it was "slow" on older generation processors, it's been left off of many shared value usages in legacy code, because there wasn't any problems, then, with reordering or other things that could cause multi-threaded, concurrent access to suddenly create problems.

If one were writing a data flow analysis tool, which could completely validate execution paths and shared data access, one would have to know "how" threads are created, how their lifecycle transitions occur, as well as then doing the execution flow analysis.

If we had marker annotations for methods and interfaces, so that Callable, Runnable (and even Thread as a sub-class) and others could be indicated as "thread sources", and then we could also mark methods with the intended usage (perhaps even having aliases for thread instances for things like AWT/Swing), we might be able to start providing markup that tools could completely be successful at understanding "how the code is designed to be used" vs "how it is being used".

What really seems important to me, is to make sure developers are aware of when they can expect non-sequential execution to disappear, or when they've designed their code to be race free, but have overlooked an opportunity for "access" to shared data by multiple threads, when they are not intending that to happen.

Gregg Wonderly

On May 2, 2013, at 11:31 AM, "Boehm, Hans" <hans.boehm at hp.com<mailto:hans.boehm at hp.com>> wrote:

I wonder whether the expectation about atomic publication is really a sign that we have been miseducating people.  The problem may be that it comes too close to working in Java, since we guarantee that even ordinary pointer updates are indivisible.  In traditional C, there is no such guarantee, and such code is clearly broken, or at least not portable, without even considering memory ordering.  In modern C++ code, it will be even more obviously broken if the code is templatized with respect to the pointer type or uses reference-counted pointers.

In general, I think the right mental model for most programmers is that an assignment first changes a field to an invalid value, and only then changes it  to the specified value.  That's more or less how Java long and double actually behave.  The problem is that, in order to support security and some tricky high performance code, we provide somewhat stronger guarantees for other types.  But those guarantees are not strong enough to be useful to most people.  They just think they are.

I still slightly favor annotating declarations with volatile/atomic, rather than the accesses.  But there are clearly arguments on both sides.  The arguments in favor of the declarations are:

-          Atomic access may have implications on required alignment.  This is probably more of an issue in C than Java, and it's becoming less common.
-          It's easy to forget an annotation on an individual access.  I would rather err on the fail-safe side for individual accesses. (In an ideal world, there probably should be a way to declare individual accesses to a Java volatile as not racing. We mostly have that in C++.)
-          I don't think data structure declarations are really understandable without knowing what's used for communication.  (The same is true for the accesses, but we're used to having to understand declarations when reading code, more so than the other way around.)

This seems to be a legitimately controversial issue.

Hans

From: Zhong Yu [mailto:zhong.j.yu at gmail.com<http://gmail.com>]
Sent: Wednesday, May 01, 2013 9:17 PM
To: Boehm, Hans
Cc: Gregg Wonderly; concurrency-interest at cs.oswego.edu<mailto:concurrency-interest at cs.oswego.edu>; Martin Thompson
Subject: Re: [concurrency-interest] JLS 17.7 Non-atomic treatment of double and long : Android

Good point Hans, however I doubt any sane programmer would intentionally play with interleaving which is too hard to reason. Regardless of memory model, the most important use case of shared variable is atomic publication - a thread writes to some places that nobody else is supposed to be reading, then writes to a shared variable to publish all previous writes. No interleaving. Programmers intuitively assume that it should work; the possibility of out-of-order writes is too bizzare to register in our minds. Is it really worth it for JMM to go against this intuition? How much performance gain are we talking about anyway? The java community has wasted a lot of time on it.
Another problem is the syntax - volatility is declared on the variable, instead of on read/write actions. If it's the later, programmers are more likely to do the right thing because they do think about inter-thread effects when they read/write shared variables.

Zhong Yu



On Wed, May 1, 2013 at 2:41 PM, Boehm, Hans <hans.boehm at hp.com<mailto:hans.boehm at hp.com>> wrote:
I think this is the wrong programming model.  Even if the implementation guaranteed sequential consistency, data races would usually indicate a bug.  Sequential consistency helps with Thread.setName() only because the natural implementation of the operation is indivisible.  If it involved setting more than one field, the racy code would still break, even in a sequentially consistent implementation.

I claim that in reality programmers expect not just sequential consistency (i.e. as if the threads were simply interleaved), but they also expect that code sections that appear sequential, i.e. those that contain no synchronization, execute sequentially, without interference from other threads.  I can't read a piece of code if any field can change out from under me at any time.  The only way I can get that required guarantee is by writing data-race-free code, which requires me to annotate synchronization variables as volatile.  Once I do that, we know how to built tools that will report data races in real code, admittedly with 10x-100x slowdown (in the absence of hardware support).  If such a tool reports no data races, then I know that the execution is sequential consistency, even without forcing my compiler to treat everything as volatile.  And since I didn't declare everything as volatile, I also get the equally important guarantee that synchronization-free code executes sequentially, i.e. atomically, and thus I don't need to worry about possible interleaving at  every instruction.  I don't need to worry about whether synchronization-free library calls are implemented with a single assignment or three.  I think this is a much more practical programming model than just sequential consistency.  And it's the one that motivates various current language memory models, including the Java one.

In this world, which requires better tools, and somewhat more disciplined code, but not a sequentially consistent memory model, I expect the Thread.setName() problem would have been fairly easy to debug.  We unfortunately still rely on testing with a race detector.  But if we can get the code to fail under those conditions, the error message would have clearly pointed at the problem before you ever saw a corrupt thread name.  (The most natural fix in this case may still involve adding synchronization to the library, and fixing the documentation to reflect that, but that's a separate issue.)

(Reality is slightly more complicated, and I think we'd need a way other than "volatile" to annotate races on variables when we don't expect enforcement of sequential consistency, but just want the current. tricky and misspecified, Java variable semantics, such as they are.  But I don't think that mechanism should be used by, or interfere with, the vast majority of programmers, whom I believe Gregg is addressing.)

Hans

From: Gregg Wonderly [mailto:gergg at cox.net<mailto:gergg at cox.net>]
Sent: Tuesday, April 30, 2013 7:21 PM
To: Boehm, Hans
Cc: Martin Thompson; Kirk Pepperdine; concurrency-interest at cs.oswego.edu<mailto:concurrency-interest at cs.oswego.edu>

Subject: Re: [concurrency-interest] JLS 17.7 Non-atomic treatment of double and long : Android

The problem is that people can trivially create data races.  It's happened everywhere.  There is so much legacy Java code that has data races.   That's no the large problem.  Data races are one thing.  The large problem is when the JIT reorders code and optimizes code to not be sequentially consistent.  It then lands on the developers shoulders to understand every possible, undocumented optimization that might be done by the compiler to try and understand why the code is misbehaving.  Only then, can they possibly formulate which changes will actually correct the behavior of their code, without making it "slower" than it needs to be.  To me, that is exactly the wrong way for a developer to test and optimize their code.  Visible data races that are only about "the data", are trivial to reason about.

If I saw the Thread.setName/getName lack of synchronization, out in the wild, create a corrupted thread name, I'd be writing a bug report about the JVM creating memory corruption before I would consider that the array had been allocated and made visible before it was filled.  That's not how the code is written, and that's not how it should execute without the developer deciding that such an optimization is safe for their application to see.

Suggesting that denying the visibility of the optimization is selected by the developer not doing anything, is where I respectfully, cannot agree with such reasoning.  This is not a new issue.  The design of many functional languages is based on the simple fact that developers don't need to be burdened with designating that they want correctly executing code.  They should get that by default, and have to work at "faster" or "more performant" or "less latency", by explicit actions they take in code structure.

Thread.setName/getName is a great example of "no thought given" to concurrency in the original design.  That code, as said before, is only executed in a concurrent environment.  Why didn't the original code have the correct concurrency design.  Why hasn't it been visible to enough people that a bug report written and it already fixed?

The answer is, because that racy code, doesn't "act" wrong, in general usage patterns.  But optimizations due to how volatile works and how JIT developers exploit lack of "concurrency selected" coding, could, at any point, break that code.

This is how all of that legacy code, laying around on the internet, is going to come down.  It's going to start randomly breaking and causing completely unexplainable bugs.  People will not remember enough details about this code to actually understand that the JIT is causing them problems with reordering, loop hoisting or other "nifty" optimizations that provide a .1% improvement in speed.   It will be a giant waste of peoples time trying to reconcile what is actually going wrong, and in some cases, there will be real impact on the users lives, welfare and/or safety, potentially.

Call my position extreme, but I think it's vital for the Java community and Oracle in particular to understand that the path we are going down is absolutely a perilous disaster without something very specific being visible to developers to allow them to understand how their code is being executed so that they can see exactly what parts of the code need to be fixed.

Gregg Wonderly

On Apr 30, 2013, at 7:48 PM, "Boehm, Hans" <hans.boehm at hp.com<mailto:hans.boehm at hp.com>> wrote:

A nice simple example to consider here is a user application that declares an array, and calls a library to sort it.  The sort library uses a parallel sort that relies on a sequential sort to sort small sections of the array.  In a sequential-by-default world, how would you declare the parallel sections?  Would it be any different than what we do now?  The user application that declares the array may never know that there is any parallel code involved.  Nor should it.

Applications such as this are naturally data-race-free, thus there is no issue with the compiler "breaking" code.  And the compiler can apply nearly all sequentially valid transformations on synchronization-free code, such as the sequential sort operations.  If you accidentally introduce a data race bug, it's unlikely your code would run correctly even if the compiler guaranteed sequential consistency. Your code may be a bit easier to debug with a hypothetical compiler that ensures sequential consistency.  But so long as you avoided intentional (unannotated) data races, I think a data race detector would also make this fairly easy.

Hans

From: concurrency-interest-bounces at cs.oswego.edu<mailto:concurrency-interest-bounces at cs.oswego.edu> [mailto:concurrency-<mailto:concurrency->interest-bounces at cs.oswego.edu<mailto:interest-bounces at cs.oswego.edu>] On Behalf Of Martin Thompson
Sent: Tuesday, April 30, 2013 5:57 AM
To: Kirk Pepperdine
Cc: Gregg Wonderly; concurrency-interest at cs.oswego.edu<mailto:concurrency-interest at cs.oswego.edu>
Subject: Re: [concurrency-interest] JLS 17.7 Non-atomic treatment of double and long : Android

I agree with Kirk here and would take it further.  By default the vast majority of code should be single threaded and concurrent programming is only utilized in regions of data exchange.

If all code was sequentially consistent then most hardware and compiler optimizations would be defeated.  A default position that all code is concurrent is sending the industry the wrong way in my view.  It makes a more sense to explicitly define the regions of data exchange in our programs and therefore what ordering semantics are required in those regions.

Martin...
------------------------------
Message: 3
Date: Tue, 30 Apr 2013 07:38:01 +0200
From: Kirk Pepperdine <kirk at kodewerk.com<mailto:kirk at kodewerk.com>>
To: Gregg Wonderly <gergg at cox.net<mailto:gergg at cox.net>>
Cc: concurrency-interest at cs.oswego.edu
<mailto:concurrency-interest at cs.oswego.edu>Subject: Re: [concurrency-interest] JLS 17.7 Non-atomic treatment of
        double  and long : Android
Message-ID: <5D85887E-8BFE-4F09-AFA8-53FE1DFD52D3 at kodewerk.com<mailto:5D85887E-8BFE-4F09-AFA8-53FE1DFD52D3 at kodewerk.com>>
Content-Type: text/plain; charset="windows-1252"
Sorry but making thing volatile by default would be a horrible thing to do. Code wouldn't be a bit slower, it would be a lot slower and then you'd end up with the same problem in reverse!
Regards,
Kirk
On 2013-04-30, at 12:10 AM, Gregg Wonderly <gergg at cox.net<mailto:gergg at cox.net>> wrote:
> This code exists everywhere on the Java JVM now, because no one expects the loop hoist?   People are living with it, or eventually declaring the loop variable volatile after finding these discussions.
>
> Java, by default, should of used nothing but volatile variables, and developers should of needed to add non-volatile declarations via annotations, without the 'volatile' keyword being used, at all.
>
> That would of made it hard to "break" code without actually looking up what you were doing, because the added verbosity would only be tolerated when it actually accomplished a performance improvement.  Today, code is "Faster" without "volatile".   If everything was "volatile" by default, then code would be slower to start with, and proper "concurrency programming" would then make your code faster, as concurrency should.
>
> Gregg
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu<mailto:Concurrency-interest at cs.oswego.edu>
http://cs.oswego.edu/mailman/listinfo/concurrency-interest


_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu<mailto:Concurrency-interest at cs.oswego.edu>
http://cs.oswego.edu/mailman/listinfo/concurrency-interest

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu<mailto:Concurrency-interest at cs.oswego.edu>
http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130503/bbe2caed/attachment-0001.html>

From lonnie_mkhize at yahoo.com  Tue May  7 18:43:38 2013
From: lonnie_mkhize at yahoo.com (Lonnie Mkhize)
Date: Tue, 7 May 2013 15:43:38 -0700 (PDT)
Subject: [concurrency-interest] We are confirmed
Message-ID: <1367966618.22913.YahooMailNeo@web162906.mail.bf1.yahoo.com>



Thanks

Lonnie Mkhize
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130507/f7272250/attachment.html>

From peter.firmstone at zeus.net.au  Wed May  8 05:27:36 2013
From: peter.firmstone at zeus.net.au (Peter Firmstone)
Date: Wed, 08 May 2013 19:27:36 +1000
Subject: [concurrency-interest] safe construction
Message-ID: <518A1A88.10007@zeus.net.au>

I'm attempting to fix an object (public api) that uses unsafe 
construction (lets "this" escape to other threads via inner classes 
during construction).

To safely construct the object I need to modify the object to delay 
publication of the "this" reference until after construction.

I was considering a start() or init() method to do so.

However legacy code won't call that method, so I need a way to invoke 
start() or init() without relying on other methods.

Having every method check the state either isn't practical, since legacy 
code doesn't necessarily invoke any method calls immediately and the 
constructor invokes background threads to perform work that client code 
eventually uses.

The solution I'm thinking of is to move the implementation into a 
package private abstract superclass, so the object becomes stateless 
(the superclass is responsible for state).  Then from the constructor, 
call the superclass constructor, then call the start() method.

Is it reasonable to assume that after the superclass constructor 
returns, all superclass final fields are safely published?  So the 
stateless child class can call the start() method from within the 
constructor safely publishing the "this" reference after all final 
fields and the objects they reference are fully initialized.

Thanks in advance,

Peter.



From peter.levart at gmail.com  Wed May  8 08:16:26 2013
From: peter.levart at gmail.com (Peter Levart)
Date: Wed, 08 May 2013 14:16:26 +0200
Subject: [concurrency-interest] safe construction
In-Reply-To: <518A1A88.10007@zeus.net.au>
References: <518A1A88.10007@zeus.net.au>
Message-ID: <518A421A.7060003@gmail.com>


On 05/08/2013 11:27 AM, Peter Firmstone wrote:
> I'm attempting to fix an object (public api) that uses unsafe 
> construction (lets "this" escape to other threads via inner classes 
> during construction).
>
> To safely construct the object I need to modify the object to delay 
> publication of the "this" reference until after construction.
>
> I was considering a start() or init() method to do so.
>
> However legacy code won't call that method, so I need a way to invoke 
> start() or init() without relying on other methods.
>
> Having every method check the state either isn't practical, since 
> legacy code doesn't necessarily invoke any method calls immediately 
> and the constructor invokes background threads to perform work that 
> client code eventually uses.
>
> The solution I'm thinking of is to move the implementation into a 
> package private abstract superclass, so the object becomes stateless 
> (the superclass is responsible for state).  Then from the constructor, 
> call the superclass constructor, then call the start() method.
>
> Is it reasonable to assume that after the superclass constructor 
> returns, all superclass final fields are safely published?  So the 
> stateless child class can call the start() method from within the 
> constructor safely publishing the "this" reference after all final 
> fields and the objects they reference are fully initialized.
>
> Thanks in advance,
>
> Peter.

Hi,

A related question that I have asked myself already but don't have the 
answer yet. If constructor of a class calls another constructor (via 
this(...)), is it, upon return from this(...) call, safe to publish 
'this' to other threads?

class X {
     final int value;
     X() {
         this(123);
         // is it safe to publish 'this' here?
     }

     X(int value) {
         this.value = value;
     }
}

Regards, Peter

>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130508/c4d5a59f/attachment.html>

From peter.levart at gmail.com  Wed May  8 08:36:15 2013
From: peter.levart at gmail.com (Peter Levart)
Date: Wed, 08 May 2013 14:36:15 +0200
Subject: [concurrency-interest] safe construction
In-Reply-To: <518A421A.7060003@gmail.com>
References: <518A1A88.10007@zeus.net.au> <518A421A.7060003@gmail.com>
Message-ID: <518A46BF.80809@gmail.com>

Hi Peter,

If neither of invoking super or sibling constructor works, and I have a 
feeling that they don't, you could try delegation...

before:

public class A {
     ... state ...
     public A() {
         // initialization
         ...
         start();
     }

     void start() {
         // inner classes starting threads, etc...
     }
}

after:

public class A {
     final B delegate;
     public A() {
         delegate = new B();
         delegate.start();
     }

     // delegate all methods to 'delegate'
     ...
}

class B {
     ... state ...
     B() {
         // initialization
         ...
     }

     void start() {
         // inner classes starting threads, etc...
     }
}


Regards, Peter


On 05/08/2013 02:16 PM, Peter Levart wrote:
>
> On 05/08/2013 11:27 AM, Peter Firmstone wrote:
>> I'm attempting to fix an object (public api) that uses unsafe 
>> construction (lets "this" escape to other threads via inner classes 
>> during construction).
>>
>> To safely construct the object I need to modify the object to delay 
>> publication of the "this" reference until after construction.
>>
>> I was considering a start() or init() method to do so.
>>
>> However legacy code won't call that method, so I need a way to invoke 
>> start() or init() without relying on other methods.
>>
>> Having every method check the state either isn't practical, since 
>> legacy code doesn't necessarily invoke any method calls immediately 
>> and the constructor invokes background threads to perform work that 
>> client code eventually uses.
>>
>> The solution I'm thinking of is to move the implementation into a 
>> package private abstract superclass, so the object becomes stateless 
>> (the superclass is responsible for state).  Then from the 
>> constructor, call the superclass constructor, then call the start() 
>> method.
>>
>> Is it reasonable to assume that after the superclass constructor 
>> returns, all superclass final fields are safely published?  So the 
>> stateless child class can call the start() method from within the 
>> constructor safely publishing the "this" reference after all final 
>> fields and the objects they reference are fully initialized.
>>
>> Thanks in advance,
>>
>> Peter.
>
> Hi,
>
> A related question that I have asked myself already but don't have the 
> answer yet. If constructor of a class calls another constructor (via 
> this(...)), is it, upon return from this(...) call, safe to publish 
> 'this' to other threads?
>
> class X {
>     final int value;
>     X() {
>         this(123);
>         // is it safe to publish 'this' here?
>     }
>
>     X(int value) {
>         this.value = value;
>     }
> }
>
> Regards, Peter
>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130508/c008a876/attachment.html>

From vitalyd at gmail.com  Wed May  8 08:43:23 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 8 May 2013 08:43:23 -0400
Subject: [concurrency-interest] safe construction
In-Reply-To: <518A421A.7060003@gmail.com>
References: <518A1A88.10007@zeus.net.au>
	<518A421A.7060003@gmail.com>
Message-ID: <CAHjP37GhQFd4aKM9ca+HgrYufwvkKwnktTFQuDeEJKfzuRrDGQ@mail.gmail.com>

I wouldn't think so because the "freeze" on the final fields is done before
the constructed object is assigned, but I don't think any non-dependent
order within constructor is guaranteed.  On archs where final stores are
not plain stores this would get expensive if every final store was emitting
a fence.  Basically, leaking "this" from constructor is unsafe.

Could be wrong though, but leaking "this" from constructor is bad code
usually anyway.

Sent from my phone
On May 8, 2013 8:22 AM, "Peter Levart" <peter.levart at gmail.com> wrote:

>
> On 05/08/2013 11:27 AM, Peter Firmstone wrote:
>
> I'm attempting to fix an object (public api) that uses unsafe construction
> (lets "this" escape to other threads via inner classes during
> construction).
>
> To safely construct the object I need to modify the object to delay
> publication of the "this" reference until after construction.
>
> I was considering a start() or init() method to do so.
>
> However legacy code won't call that method, so I need a way to invoke
> start() or init() without relying on other methods.
>
> Having every method check the state either isn't practical, since legacy
> code doesn't necessarily invoke any method calls immediately and the
> constructor invokes background threads to perform work that client code
> eventually uses.
>
> The solution I'm thinking of is to move the implementation into a package
> private abstract superclass, so the object becomes stateless (the
> superclass is responsible for state).  Then from the constructor, call the
> superclass constructor, then call the start() method.
>
> Is it reasonable to assume that after the superclass constructor returns,
> all superclass final fields are safely published?  So the stateless child
> class can call the start() method from within the constructor safely
> publishing the "this" reference after all final fields and the objects they
> reference are fully initialized.
>
> Thanks in advance,
>
> Peter.
>
>
> Hi,
>
> A related question that I have asked myself already but don't have the
> answer yet. If constructor of a class calls another constructor (via
> this(...)), is it, upon return from this(...) call, safe to publish 'this'
> to other threads?
>
> class X {
>     final int value;
>     X() {
>         this(123);
>         // is it safe to publish 'this' here?
>     }
>
>     X(int value) {
>         this.value = value;
>     }
> }
>
> Regards, Peter
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130508/cd5241dd/attachment-0001.html>

From vitalyd at gmail.com  Wed May  8 08:56:04 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 8 May 2013 08:56:04 -0400
Subject: [concurrency-interest] safe construction
In-Reply-To: <518A46BF.80809@gmail.com>
References: <518A1A88.10007@zeus.net.au> <518A421A.7060003@gmail.com>
	<518A46BF.80809@gmail.com>
Message-ID: <CAHjP37E8uxGX9-F_OE3S29f_+UBHbnN4BetAUqYwpXZSQL6-WA@mail.gmail.com>

If inner class starts a thread and that thread reads state from outer class
then it should work fine since starting a thread creates an HB edge.  The
problem would arise if, e.g., "this" was assigned to a static field read by
other threads.

Sent from my phone
On May 8, 2013 8:44 AM, "Peter Levart" <peter.levart at gmail.com> wrote:

>  Hi Peter,
>
> If neither of invoking super or sibling constructor works, and I have a
> feeling that they don't, you could try delegation...
>
> before:
>
> public class A {
>     ... state ...
>     public A() {
>         // initialization
>         ...
>         start();
>     }
>
>     void start() {
>         // inner classes starting threads, etc...
>     }
> }
>
> after:
>
> public class A {
>     final B delegate;
>     public A() {
>         delegate = new B();
>         delegate.start();
>     }
>
>     // delegate all methods to 'delegate'
>     ...
> }
>
> class B {
>     ... state ...
>     B() {
>          // initialization
>          ...
>      }
>
>      void start() {
>          // inner classes starting threads, etc...
>      }
>  }
>
>
> Regards, Peter
>
>
> On 05/08/2013 02:16 PM, Peter Levart wrote:
>
>
> On 05/08/2013 11:27 AM, Peter Firmstone wrote:
>
> I'm attempting to fix an object (public api) that uses unsafe construction
> (lets "this" escape to other threads via inner classes during
> construction).
>
> To safely construct the object I need to modify the object to delay
> publication of the "this" reference until after construction.
>
> I was considering a start() or init() method to do so.
>
> However legacy code won't call that method, so I need a way to invoke
> start() or init() without relying on other methods.
>
> Having every method check the state either isn't practical, since legacy
> code doesn't necessarily invoke any method calls immediately and the
> constructor invokes background threads to perform work that client code
> eventually uses.
>
> The solution I'm thinking of is to move the implementation into a package
> private abstract superclass, so the object becomes stateless (the
> superclass is responsible for state).  Then from the constructor, call the
> superclass constructor, then call the start() method.
>
> Is it reasonable to assume that after the superclass constructor returns,
> all superclass final fields are safely published?  So the stateless child
> class can call the start() method from within the constructor safely
> publishing the "this" reference after all final fields and the objects they
> reference are fully initialized.
>
> Thanks in advance,
>
> Peter.
>
>
> Hi,
>
> A related question that I have asked myself already but don't have the
> answer yet. If constructor of a class calls another constructor (via
> this(...)), is it, upon return from this(...) call, safe to publish 'this'
> to other threads?
>
> class X {
>     final int value;
>     X() {
>         this(123);
>         // is it safe to publish 'this' here?
>     }
>
>     X(int value) {
>         this.value = value;
>     }
> }
>
> Regards, Peter
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130508/ef37909d/attachment.html>

From peter.firmstone at zeus.net.au  Wed May  8 09:28:35 2013
From: peter.firmstone at zeus.net.au (Peter Firmstone)
Date: Wed, 08 May 2013 23:28:35 +1000
Subject: [concurrency-interest] safe construction
In-Reply-To: <CAHjP37GhQFd4aKM9ca+HgrYufwvkKwnktTFQuDeEJKfzuRrDGQ@mail.gmail.com>
References: <518A1A88.10007@zeus.net.au>	<518A421A.7060003@gmail.com>
	<CAHjP37GhQFd4aKM9ca+HgrYufwvkKwnktTFQuDeEJKfzuRrDGQ@mail.gmail.com>
Message-ID: <518A5303.3060306@zeus.net.au>

http://www.cs.umd.edu/~pugh/java/memoryModel/semantics.pdf

This explains it very clearly.

8.7.2 Freezing final fields
When a constructor terminates normally, the thread
performs freeze actions on all final fields defined in
that class. If a constructor A1 for A chains to another
constructor A2 for A, the fields are only frozen at the
completion of A1. If a constructor B1 for B chains to
a constructor A1 for A (a superclass of B), then upon
completion of A1, final fields declared in A are frozen,
and upon completion of B1, final fields declared in B
are frozen.

So the answers are:

If called from within a constructor:

super() - final fields are frozen when it completes.
this() - final fields aren't frozen yet.

Peter.

On 8/05/2013 10:43 PM, Vitaly Davidovich wrote:
>
> I wouldn't think so because the "freeze" on the final fields is done 
> before the constructed object is assigned, but I don't think any 
> non-dependent order within constructor is guaranteed.  On archs where 
> final stores are not plain stores this would get expensive if every 
> final store was emitting a fence.  Basically, leaking "this" from 
> constructor is unsafe.
>
> Could be wrong though, but leaking "this" from constructor is bad code 
> usually anyway.
>
> Sent from my phone
>
> On May 8, 2013 8:22 AM, "Peter Levart" <peter.levart at gmail.com 
> <mailto:peter.levart at gmail.com>> wrote:
>
>
>     On 05/08/2013 11:27 AM, Peter Firmstone wrote:
>>     I'm attempting to fix an object (public api) that uses unsafe
>>     construction (lets "this" escape to other threads via inner
>>     classes during construction).
>>
>>     To safely construct the object I need to modify the object to
>>     delay publication of the "this" reference until after construction.
>>
>>     I was considering a start() or init() method to do so.
>>
>>     However legacy code won't call that method, so I need a way to
>>     invoke start() or init() without relying on other methods.
>>
>>     Having every method check the state either isn't practical, since
>>     legacy code doesn't necessarily invoke any method calls
>>     immediately and the constructor invokes background threads to
>>     perform work that client code eventually uses.
>>
>>     The solution I'm thinking of is to move the implementation into a
>>     package private abstract superclass, so the object becomes
>>     stateless (the superclass is responsible for state).  Then from
>>     the constructor, call the superclass constructor, then call the
>>     start() method.
>>
>>     Is it reasonable to assume that after the superclass constructor
>>     returns, all superclass final fields are safely published?  So
>>     the stateless child class can call the start() method from within
>>     the constructor safely publishing the "this" reference after all
>>     final fields and the objects they reference are fully initialized.
>>
>>     Thanks in advance,
>>
>>     Peter.
>
>     Hi,
>
>     A related question that I have asked myself already but don't have
>     the answer yet. If constructor of a class calls another
>     constructor (via this(...)), is it, upon return from this(...)
>     call, safe to publish 'this' to other threads?
>
>     class X {
>         final int value;
>         X() {
>             this(123);
>             // is it safe to publish 'this' here?
>         }
>
>         X(int value) {
>             this.value = value;
>         }
>     }
>
>     Regards, Peter
>
>>
>>
>>     _______________________________________________
>>     Concurrency-interest mailing list
>>     Concurrency-interest at cs.oswego.edu
>>     <mailto:Concurrency-interest at cs.oswego.edu>
>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From peter.firmstone at zeus.net.au  Wed May  8 09:40:06 2013
From: peter.firmstone at zeus.net.au (Peter Firmstone)
Date: Wed, 08 May 2013 23:40:06 +1000
Subject: [concurrency-interest] safe construction
In-Reply-To: <CAHjP37E8uxGX9-F_OE3S29f_+UBHbnN4BetAUqYwpXZSQL6-WA@mail.gmail.com>
References: <518A1A88.10007@zeus.net.au>	<518A421A.7060003@gmail.com>	<518A46BF.80809@gmail.com>
	<CAHjP37E8uxGX9-F_OE3S29f_+UBHbnN4BetAUqYwpXZSQL6-WA@mail.gmail.com>
Message-ID: <518A55B6.8050407@zeus.net.au>

I was under the impression it would still allow "this" to escape prior 
to construction completing?

Interesting stuff.

Presently I'm working around this by delaying starting of any threads, 
especially those in inner classes until after construction completes.

Cheers,

Peter.

On 8/05/2013 10:56 PM, Vitaly Davidovich wrote:
>
> If inner class starts a thread and that thread reads state from outer 
> class then it should work fine since starting a thread creates an HB 
> edge.  The problem would arise if, e.g., "this" was assigned to a 
> static field read by other threads.
>
> Sent from my phone
>
> On May 8, 2013 8:44 AM, "Peter Levart" <peter.levart at gmail.com 
> <mailto:peter.levart at gmail.com>> wrote:
>
>     Hi Peter,
>
>     If neither of invoking super or sibling constructor works, and I
>     have a feeling that they don't, you could try delegation...
>
>     before:
>
>     public class A {
>         ... state ...
>         public A() {
>             // initialization
>             ...
>             start();
>         }
>
>         void start() {
>             // inner classes starting threads, etc...
>         }
>     }
>
>     after:
>
>     public class A {
>         final B delegate;
>         public A() {
>             delegate = new B();
>             delegate.start();
>         }
>
>         // delegate all methods to 'delegate'
>         ...
>     }
>
>     class B {
>         ... state ...
>         B() {
>             // initialization
>             ...
>         }
>
>         void start() {
>             // inner classes starting threads, etc...
>         }
>     }
>
>
>     Regards, Peter
>
>
>     On 05/08/2013 02:16 PM, Peter Levart wrote:
>>
>>     On 05/08/2013 11:27 AM, Peter Firmstone wrote:
>>>     I'm attempting to fix an object (public api) that uses unsafe
>>>     construction (lets "this" escape to other threads via inner
>>>     classes during construction).
>>>
>>>     To safely construct the object I need to modify the object to
>>>     delay publication of the "this" reference until after construction.
>>>
>>>     I was considering a start() or init() method to do so.
>>>
>>>     However legacy code won't call that method, so I need a way to
>>>     invoke start() or init() without relying on other methods.
>>>
>>>     Having every method check the state either isn't practical,
>>>     since legacy code doesn't necessarily invoke any method calls
>>>     immediately and the constructor invokes background threads to
>>>     perform work that client code eventually uses.
>>>
>>>     The solution I'm thinking of is to move the implementation into
>>>     a package private abstract superclass, so the object becomes
>>>     stateless (the superclass is responsible for state).  Then from
>>>     the constructor, call the superclass constructor, then call the
>>>     start() method.
>>>
>>>     Is it reasonable to assume that after the superclass constructor
>>>     returns, all superclass final fields are safely published?  So
>>>     the stateless child class can call the start() method from
>>>     within the constructor safely publishing the "this" reference
>>>     after all final fields and the objects they reference are fully
>>>     initialized.
>>>
>>>     Thanks in advance,
>>>
>>>     Peter.
>>
>>     Hi,
>>
>>     A related question that I have asked myself already but don't
>>     have the answer yet. If constructor of a class calls another
>>     constructor (via this(...)), is it, upon return from this(...)
>>     call, safe to publish 'this' to other threads?
>>
>>     class X {
>>         final int value;
>>         X() {
>>             this(123);
>>             // is it safe to publish 'this' here?
>>         }
>>
>>         X(int value) {
>>             this.value = value;
>>         }
>>     }
>>
>>     Regards, Peter
>>
>>>
>>>
>>>     _______________________________________________
>>>     Concurrency-interest mailing list
>>>     Concurrency-interest at cs.oswego.edu
>>>     <mailto:Concurrency-interest at cs.oswego.edu>
>>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From vitalyd at gmail.com  Wed May  8 09:52:13 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 8 May 2013 09:52:13 -0400
Subject: [concurrency-interest] safe construction
In-Reply-To: <518A5303.3060306@zeus.net.au>
References: <518A1A88.10007@zeus.net.au> <518A421A.7060003@gmail.com>
	<CAHjP37GhQFd4aKM9ca+HgrYufwvkKwnktTFQuDeEJKfzuRrDGQ@mail.gmail.com>
	<518A5303.3060306@zeus.net.au>
Message-ID: <CAHjP37FSRq=GkmWdfXTWN1OX2SDgFfMqH9JbmQH5f28LFrm8kw@mail.gmail.com>

Good find, thanks.  Makes sense that super () freezes as otherwise "rogue"
subclasses could subdue the integrity of super.

Sent from my phone
On May 8, 2013 9:28 AM, "Peter Firmstone" <peter.firmstone at zeus.net.au>
wrote:

> http://www.cs.umd.edu/~pugh/**java/memoryModel/semantics.pdf<http://www.cs.umd.edu/~pugh/java/memoryModel/semantics.pdf>
>
> This explains it very clearly.
>
> 8.7.2 Freezing final fields
> When a constructor terminates normally, the thread
> performs freeze actions on all final fields defi ned in
> that class. If a constructor A1 for A chains to another
> constructor A2 for A, the fi elds are only frozen at the
> completion of A1. If a constructor B1 for B chains to
> a constructor A1 for A (a superclass of B), then upon
> completion of A1, final fi elds declared in A are frozen,
> and upon completion of B1, final fields declared in B
> are frozen.
>
> So the answers are:
>
> If called from within a constructor:
>
> super() - final fields are frozen when it completes.
> this() - final fields aren't frozen yet.
>
> Peter.
>
> On 8/05/2013 10:43 PM, Vitaly Davidovich wrote:
>
>>
>> I wouldn't think so because the "freeze" on the final fields is done
>> before the constructed object is assigned, but I don't think any
>> non-dependent order within constructor is guaranteed.  On archs where final
>> stores are not plain stores this would get expensive if every final store
>> was emitting a fence.  Basically, leaking "this" from constructor is unsafe.
>>
>> Could be wrong though, but leaking "this" from constructor is bad code
>> usually anyway.
>>
>> Sent from my phone
>>
>> On May 8, 2013 8:22 AM, "Peter Levart" <peter.levart at gmail.com <mailto:
>> peter.levart at gmail.com**>> wrote:
>>
>>
>>     On 05/08/2013 11:27 AM, Peter Firmstone wrote:
>>
>>>     I'm attempting to fix an object (public api) that uses unsafe
>>>     construction (lets "this" escape to other threads via inner
>>>     classes during construction).
>>>
>>>     To safely construct the object I need to modify the object to
>>>     delay publication of the "this" reference until after construction.
>>>
>>>     I was considering a start() or init() method to do so.
>>>
>>>     However legacy code won't call that method, so I need a way to
>>>     invoke start() or init() without relying on other methods.
>>>
>>>     Having every method check the state either isn't practical, since
>>>     legacy code doesn't necessarily invoke any method calls
>>>     immediately and the constructor invokes background threads to
>>>     perform work that client code eventually uses.
>>>
>>>     The solution I'm thinking of is to move the implementation into a
>>>     package private abstract superclass, so the object becomes
>>>     stateless (the superclass is responsible for state).  Then from
>>>     the constructor, call the superclass constructor, then call the
>>>     start() method.
>>>
>>>     Is it reasonable to assume that after the superclass constructor
>>>     returns, all superclass final fields are safely published?  So
>>>     the stateless child class can call the start() method from within
>>>     the constructor safely publishing the "this" reference after all
>>>     final fields and the objects they reference are fully initialized.
>>>
>>>     Thanks in advance,
>>>
>>>     Peter.
>>>
>>
>>     Hi,
>>
>>     A related question that I have asked myself already but don't have
>>     the answer yet. If constructor of a class calls another
>>     constructor (via this(...)), is it, upon return from this(...)
>>     call, safe to publish 'this' to other threads?
>>
>>     class X {
>>         final int value;
>>         X() {
>>             this(123);
>>             // is it safe to publish 'this' here?
>>         }
>>
>>         X(int value) {
>>             this.value = value;
>>         }
>>     }
>>
>>     Regards, Peter
>>
>>
>>>
>>>     ______________________________**_________________
>>>     Concurrency-interest mailing list
>>>     Concurrency-interest at cs.**oswego.edu<Concurrency-interest at cs.oswego.edu>
>>>     <mailto:Concurrency-interest@**cs.oswego.edu<Concurrency-interest at cs.oswego.edu>
>>> >
>>>     http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>>
>>
>>
>>     ______________________________**_________________
>>     Concurrency-interest mailing list
>>     Concurrency-interest at cs.**oswego.edu<Concurrency-interest at cs.oswego.edu>
>>     <mailto:Concurrency-interest@**cs.oswego.edu<Concurrency-interest at cs.oswego.edu>
>> >
>>     http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130508/64b8af79/attachment-0001.html>

From vitalyd at gmail.com  Wed May  8 09:57:12 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 8 May 2013 09:57:12 -0400
Subject: [concurrency-interest] safe construction
In-Reply-To: <518A55B6.8050407@zeus.net.au>
References: <518A1A88.10007@zeus.net.au> <518A421A.7060003@gmail.com>
	<518A46BF.80809@gmail.com>
	<CAHjP37E8uxGX9-F_OE3S29f_+UBHbnN4BetAUqYwpXZSQL6-WA@mail.gmail.com>
	<518A55B6.8050407@zeus.net.au>
Message-ID: <CAHjP37FNcEmG0_dYb_ijSQucdW3uS0GPE4Oa3Mr1xTwkrpoDnA@mail.gmail.com>

"this" does escape but all stores done prior to Thread.start() will be
visible in that new thread.  That's standard JMM and I don't think
constructors are treated any differently for this scenario.

Your workaround sounds like the right way to go (starting threads in
constructor is bad design anyway).

Sent from my phone
On May 8, 2013 9:40 AM, "Peter Firmstone" <peter.firmstone at zeus.net.au>
wrote:

> I was under the impression it would still allow "this" to escape prior to
> construction completing?
>
> Interesting stuff.
>
> Presently I'm working around this by delaying starting of any threads,
> especially those in inner classes until after construction completes.
>
> Cheers,
>
> Peter.
>
> On 8/05/2013 10:56 PM, Vitaly Davidovich wrote:
>
>>
>> If inner class starts a thread and that thread reads state from outer
>> class then it should work fine since starting a thread creates an HB edge.
>>  The problem would arise if, e.g., "this" was assigned to a static field
>> read by other threads.
>>
>> Sent from my phone
>>
>> On May 8, 2013 8:44 AM, "Peter Levart" <peter.levart at gmail.com <mailto:
>> peter.levart at gmail.com**>> wrote:
>>
>>     Hi Peter,
>>
>>     If neither of invoking super or sibling constructor works, and I
>>     have a feeling that they don't, you could try delegation...
>>
>>     before:
>>
>>     public class A {
>>         ... state ...
>>         public A() {
>>             // initialization
>>             ...
>>             start();
>>         }
>>
>>         void start() {
>>             // inner classes starting threads, etc...
>>         }
>>     }
>>
>>     after:
>>
>>     public class A {
>>         final B delegate;
>>         public A() {
>>             delegate = new B();
>>             delegate.start();
>>         }
>>
>>         // delegate all methods to 'delegate'
>>         ...
>>     }
>>
>>     class B {
>>         ... state ...
>>         B() {
>>             // initialization
>>             ...
>>         }
>>
>>         void start() {
>>             // inner classes starting threads, etc...
>>         }
>>     }
>>
>>
>>     Regards, Peter
>>
>>
>>     On 05/08/2013 02:16 PM, Peter Levart wrote:
>>
>>>
>>>     On 05/08/2013 11:27 AM, Peter Firmstone wrote:
>>>
>>>>     I'm attempting to fix an object (public api) that uses unsafe
>>>>     construction (lets "this" escape to other threads via inner
>>>>     classes during construction).
>>>>
>>>>     To safely construct the object I need to modify the object to
>>>>     delay publication of the "this" reference until after construction.
>>>>
>>>>     I was considering a start() or init() method to do so.
>>>>
>>>>     However legacy code won't call that method, so I need a way to
>>>>     invoke start() or init() without relying on other methods.
>>>>
>>>>     Having every method check the state either isn't practical,
>>>>     since legacy code doesn't necessarily invoke any method calls
>>>>     immediately and the constructor invokes background threads to
>>>>     perform work that client code eventually uses.
>>>>
>>>>     The solution I'm thinking of is to move the implementation into
>>>>     a package private abstract superclass, so the object becomes
>>>>     stateless (the superclass is responsible for state).  Then from
>>>>     the constructor, call the superclass constructor, then call the
>>>>     start() method.
>>>>
>>>>     Is it reasonable to assume that after the superclass constructor
>>>>     returns, all superclass final fields are safely published?  So
>>>>     the stateless child class can call the start() method from
>>>>     within the constructor safely publishing the "this" reference
>>>>     after all final fields and the objects they reference are fully
>>>>     initialized.
>>>>
>>>>     Thanks in advance,
>>>>
>>>>     Peter.
>>>>
>>>
>>>     Hi,
>>>
>>>     A related question that I have asked myself already but don't
>>>     have the answer yet. If constructor of a class calls another
>>>     constructor (via this(...)), is it, upon return from this(...)
>>>     call, safe to publish 'this' to other threads?
>>>
>>>     class X {
>>>         final int value;
>>>         X() {
>>>             this(123);
>>>             // is it safe to publish 'this' here?
>>>         }
>>>
>>>         X(int value) {
>>>             this.value = value;
>>>         }
>>>     }
>>>
>>>     Regards, Peter
>>>
>>>
>>>>
>>>>     ______________________________**_________________
>>>>     Concurrency-interest mailing list
>>>>     Concurrency-interest at cs.**oswego.edu<Concurrency-interest at cs.oswego.edu>
>>>>     <mailto:Concurrency-interest@**cs.oswego.edu<Concurrency-interest at cs.oswego.edu>
>>>> >
>>>>     http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>>>
>>>
>>>
>>
>>     ______________________________**_________________
>>     Concurrency-interest mailing list
>>     Concurrency-interest at cs.**oswego.edu<Concurrency-interest at cs.oswego.edu>
>>     <mailto:Concurrency-interest@**cs.oswego.edu<Concurrency-interest at cs.oswego.edu>
>> >
>>     http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130508/1f3d47a7/attachment.html>

From zhong.j.yu at gmail.com  Wed May  8 10:49:28 2013
From: zhong.j.yu at gmail.com (Zhong Yu)
Date: Wed, 8 May 2013 09:49:28 -0500
Subject: [concurrency-interest] safe construction
In-Reply-To: <518A5303.3060306@zeus.net.au>
References: <518A1A88.10007@zeus.net.au> <518A421A.7060003@gmail.com>
	<CAHjP37GhQFd4aKM9ca+HgrYufwvkKwnktTFQuDeEJKfzuRrDGQ@mail.gmail.com>
	<518A5303.3060306@zeus.net.au>
Message-ID: <CACuKZqHj==3Arm_E_2tLpnsWniWBvQAD41c+Y4wy2_ocd-Y25g@mail.gmail.com>

On Wed, May 8, 2013 at 8:28 AM, Peter Firmstone <peter.firmstone at zeus.net.au
> wrote:

> http://www.cs.umd.edu/~pugh/**java/memoryModel/semantics.pdf<http://www.cs.umd.edu/~pugh/java/memoryModel/semantics.pdf>
>
> This explains it very clearly.
>
> 8.7.2 Freezing final fields
> When a constructor terminates normally, the thread
> performs freeze actions on all final fields defi ned in
> that class. If a constructor A1 for A chains to another
> constructor A2 for A, the fi elds are only frozen at the
> completion of A1.


JLS says differently:

http://docs.oracle.com/javase/specs/jls/se5.0/html/memory.html#17.5.1

   Note that if one constructor invokes another constructor, and the
invoked constructor sets a final field, the freeze for the final field
takes place at the end of the invoked constructor.



> If a constructor B1 for B chains to
> a constructor A1 for A (a superclass of B), then upon
> completion of A1, final fi elds declared in A are frozen,
> and upon completion of B1, final fields declared in B
> are frozen.
>
> So the answers are:
>
> If called from within a constructor:
>
> super() - final fields are frozen when it completes.
> this() - final fields aren't frozen yet.
>
> Peter.
>
>
> On 8/05/2013 10:43 PM, Vitaly Davidovich wrote:
>
>>
>> I wouldn't think so because the "freeze" on the final fields is done
>> before the constructed object is assigned, but I don't think any
>> non-dependent order within constructor is guaranteed.  On archs where final
>> stores are not plain stores this would get expensive if every final store
>> was emitting a fence.  Basically, leaking "this" from constructor is unsafe.
>>
>> Could be wrong though, but leaking "this" from constructor is bad code
>> usually anyway.
>>
>> Sent from my phone
>>
>> On May 8, 2013 8:22 AM, "Peter Levart" <peter.levart at gmail.com <mailto:
>> peter.levart at gmail.com**>> wrote:
>>
>>
>>     On 05/08/2013 11:27 AM, Peter Firmstone wrote:
>>
>>>     I'm attempting to fix an object (public api) that uses unsafe
>>>     construction (lets "this" escape to other threads via inner
>>>     classes during construction).
>>>
>>>     To safely construct the object I need to modify the object to
>>>     delay publication of the "this" reference until after construction.
>>>
>>>     I was considering a start() or init() method to do so.
>>>
>>>     However legacy code won't call that method, so I need a way to
>>>     invoke start() or init() without relying on other methods.
>>>
>>>     Having every method check the state either isn't practical, since
>>>     legacy code doesn't necessarily invoke any method calls
>>>     immediately and the constructor invokes background threads to
>>>     perform work that client code eventually uses.
>>>
>>>     The solution I'm thinking of is to move the implementation into a
>>>     package private abstract superclass, so the object becomes
>>>     stateless (the superclass is responsible for state).  Then from
>>>     the constructor, call the superclass constructor, then call the
>>>     start() method.
>>>
>>>     Is it reasonable to assume that after the superclass constructor
>>>     returns, all superclass final fields are safely published?  So
>>>     the stateless child class can call the start() method from within
>>>     the constructor safely publishing the "this" reference after all
>>>     final fields and the objects they reference are fully initialized.
>>>
>>>     Thanks in advance,
>>>
>>>     Peter.
>>>
>>
>>     Hi,
>>
>>     A related question that I have asked myself already but don't have
>>     the answer yet. If constructor of a class calls another
>>     constructor (via this(...)), is it, upon return from this(...)
>>     call, safe to publish 'this' to other threads?
>>
>>     class X {
>>         final int value;
>>         X() {
>>             this(123);
>>             // is it safe to publish 'this' here?
>>         }
>>
>>         X(int value) {
>>             this.value = value;
>>         }
>>     }
>>
>>     Regards, Peter
>>
>>
>>>
>>>     ______________________________**_________________
>>>     Concurrency-interest mailing list
>>>     Concurrency-interest at cs.**oswego.edu<Concurrency-interest at cs.oswego.edu>
>>>     <mailto:Concurrency-interest@**cs.oswego.edu<Concurrency-interest at cs.oswego.edu>
>>> >
>>>     http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>>
>>
>>
>>     ______________________________**_________________
>>     Concurrency-interest mailing list
>>     Concurrency-interest at cs.**oswego.edu<Concurrency-interest at cs.oswego.edu>
>>     <mailto:Concurrency-interest@**cs.oswego.edu<Concurrency-interest at cs.oswego.edu>
>> >
>>     http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>
>>
> ______________________________**_________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130508/4d844279/attachment-0001.html>

From zhong.j.yu at gmail.com  Wed May  8 11:02:55 2013
From: zhong.j.yu at gmail.com (Zhong Yu)
Date: Wed, 8 May 2013 10:02:55 -0500
Subject: [concurrency-interest] safe construction
In-Reply-To: <518A1A88.10007@zeus.net.au>
References: <518A1A88.10007@zeus.net.au>
Message-ID: <CACuKZqGjr2qLbPqYes47ZkyCTKuZm=i4k0V+y+DSk_-+C+BCYQ@mail.gmail.com>

If the publication is safe, there is nothing to worry about, as long as
`this` is leaked after fields are assigned.

If the publication is unsafe, a trick I would use it to wrap the object
reference as a final field, and publish the wrapper

    class FinalReference<T>
        final T referent;
        FinalReference(T referent){ this.referent=referent; }

    static FinalReference<Foo> unsafePubVar;

    Foo foo = ...;
    unsafePubVar = new FinalReference(foo);

forcing the subscription side to go though a final field to access `foo`
and its fields.

Zhong Yu



On Wed, May 8, 2013 at 4:27 AM, Peter Firmstone <peter.firmstone at zeus.net.au
> wrote:

> I'm attempting to fix an object (public api) that uses unsafe construction
> (lets "this" escape to other threads via inner classes during construction).
>
> To safely construct the object I need to modify the object to delay
> publication of the "this" reference until after construction.
>
> I was considering a start() or init() method to do so.
>
> However legacy code won't call that method, so I need a way to invoke
> start() or init() without relying on other methods.
>
> Having every method check the state either isn't practical, since legacy
> code doesn't necessarily invoke any method calls immediately and the
> constructor invokes background threads to perform work that client code
> eventually uses.
>
> The solution I'm thinking of is to move the implementation into a package
> private abstract superclass, so the object becomes stateless (the
> superclass is responsible for state).  Then from the constructor, call the
> superclass constructor, then call the start() method.
>
> Is it reasonable to assume that after the superclass constructor returns,
> all superclass final fields are safely published?  So the stateless child
> class can call the start() method from within the constructor safely
> publishing the "this" reference after all final fields and the objects they
> reference are fully initialized.
>
> Thanks in advance,
>
> Peter.
>
>
> ______________________________**_________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130508/3752b193/attachment.html>

From nathan.reynolds at oracle.com  Wed May  8 14:14:51 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Wed, 08 May 2013 11:14:51 -0700
Subject: [concurrency-interest] safe construction
In-Reply-To: <CACuKZqGjr2qLbPqYes47ZkyCTKuZm=i4k0V+y+DSk_-+C+BCYQ@mail.gmail.com>
References: <518A1A88.10007@zeus.net.au>
	<CACuKZqGjr2qLbPqYes47ZkyCTKuZm=i4k0V+y+DSk_-+C+BCYQ@mail.gmail.com>
Message-ID: <518A961B.8030509@oracle.com>

A much heavier handed solution would be...

class MyClass
{
     static volatile boolean s_fence;

     public MyClass()
     {
         // initialize all fields
         s_fence = false;
         // leak "this"
     }
}

Writing to s_fence guarantees that all of the fields will be globally 
visible before "this" is leaked.  This is more expensive since it adds a 
Store-Load memory fence rather than just a Store-Store fence.  The 
Store-Store fence will turn into a noop on x86.

The other problem with writing to s_fence is that the cache line holding 
it is going to be contended.  If enough MyClass objects are created 
concurrently on different cores, cache performance is going to be the 
bottleneck.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Architect | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 5/8/2013 8:02 AM, Zhong Yu wrote:
> If the publication is safe, there is nothing to worry about, as long 
> as `this` is leaked after fields are assigned.
>
> If the publication is unsafe, a trick I would use it to wrap the 
> object reference as a final field, and publish the wrapper
>
>     class FinalReference<T>
>         final T referent;
>         FinalReference(T referent){ this.referent=referent; }
>
>     static FinalReference<Foo> unsafePubVar;
>
>     Foo foo = ...;
>     unsafePubVar = new FinalReference(foo);
>
> forcing the subscription side to go though a final field to access 
> `foo` and its fields.
>
> Zhong Yu
>
>
>
> On Wed, May 8, 2013 at 4:27 AM, Peter Firmstone 
> <peter.firmstone at zeus.net.au <mailto:peter.firmstone at zeus.net.au>> wrote:
>
>     I'm attempting to fix an object (public api) that uses unsafe
>     construction (lets "this" escape to other threads via inner
>     classes during construction).
>
>     To safely construct the object I need to modify the object to
>     delay publication of the "this" reference until after construction.
>
>     I was considering a start() or init() method to do so.
>
>     However legacy code won't call that method, so I need a way to
>     invoke start() or init() without relying on other methods.
>
>     Having every method check the state either isn't practical, since
>     legacy code doesn't necessarily invoke any method calls
>     immediately and the constructor invokes background threads to
>     perform work that client code eventually uses.
>
>     The solution I'm thinking of is to move the implementation into a
>     package private abstract superclass, so the object becomes
>     stateless (the superclass is responsible for state).  Then from
>     the constructor, call the superclass constructor, then call the
>     start() method.
>
>     Is it reasonable to assume that after the superclass constructor
>     returns, all superclass final fields are safely published?  So the
>     stateless child class can call the start() method from within the
>     constructor safely publishing the "this" reference after all final
>     fields and the objects they reference are fully initialized.
>
>     Thanks in advance,
>
>     Peter.
>
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130508/0f1a6f27/attachment.html>

From yankee.sierra at gmail.com  Wed May  8 14:42:00 2013
From: yankee.sierra at gmail.com (Yuval Shavit)
Date: Wed, 8 May 2013 14:42:00 -0400
Subject: [concurrency-interest] safe construction
In-Reply-To: <518A961B.8030509@oracle.com>
References: <518A1A88.10007@zeus.net.au>
	<CACuKZqGjr2qLbPqYes47ZkyCTKuZm=i4k0V+y+DSk_-+C+BCYQ@mail.gmail.com>
	<518A961B.8030509@oracle.com>
Message-ID: <CAE+h5-A1rbtHBWUtiJOy1aJyH5HdKAZ+mfAZyo-eqbWoLop44w@mail.gmail.com>

Is that an artifact of the common/global implementation, or the JLS itself?
I thought that in order for it to work according to the JLS, you would have
to read s_fence as the first thing within each of MyClass's methods:

    public void foo() {
        if (s_fence) throw new AssertionError();
        doMyWhatever();
    }

This would establish a HB before the read and the post-initialization
write. There's no formal HB without the read, right?


On Wed, May 8, 2013 at 2:14 PM, Nathan Reynolds
<nathan.reynolds at oracle.com>wrote:

>  A much heavier handed solution would be...
>
> class MyClass
> {
>     static volatile boolean s_fence;
>
>     public MyClass()
>     {
>         // initialize all fields
>         s_fence = false;
>         // leak "this"
>     }
> }
>
> Writing to s_fence guarantees that all of the fields will be globally
> visible before "this" is leaked.  This is more expensive since it adds a
> Store-Load memory fence rather than just a Store-Store fence.  The
> Store-Store fence will turn into a noop on x86.
>
> The other problem with writing to s_fence is that the cache line holding
> it is going to be contended.  If enough MyClass objects are created
> concurrently on different cores, cache performance is going to be the
> bottleneck.
>
> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Architect |
> 602.333.9091
> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>  On 5/8/2013 8:02 AM, Zhong Yu wrote:
>
>  If the publication is safe, there is nothing to worry about, as long as
> `this` is leaked after fields are assigned.
>
>  If the publication is unsafe, a trick I would use it to wrap the object
> reference as a final field, and publish the wrapper
>
>      class FinalReference<T>
>          final T referent;
>         FinalReference(T referent){ this.referent=referent; }
>
>     static FinalReference<Foo> unsafePubVar;
>
>      Foo foo = ...;
>      unsafePubVar = new FinalReference(foo);
>
>  forcing the subscription side to go though a final field to access `foo`
> and its fields.
>
>  Zhong Yu
>
>
>
> On Wed, May 8, 2013 at 4:27 AM, Peter Firmstone <
> peter.firmstone at zeus.net.au> wrote:
>
>> I'm attempting to fix an object (public api) that uses unsafe
>> construction (lets "this" escape to other threads via inner classes during
>> construction).
>>
>> To safely construct the object I need to modify the object to delay
>> publication of the "this" reference until after construction.
>>
>> I was considering a start() or init() method to do so.
>>
>> However legacy code won't call that method, so I need a way to invoke
>> start() or init() without relying on other methods.
>>
>> Having every method check the state either isn't practical, since legacy
>> code doesn't necessarily invoke any method calls immediately and the
>> constructor invokes background threads to perform work that client code
>> eventually uses.
>>
>> The solution I'm thinking of is to move the implementation into a package
>> private abstract superclass, so the object becomes stateless (the
>> superclass is responsible for state).  Then from the constructor, call the
>> superclass constructor, then call the start() method.
>>
>> Is it reasonable to assume that after the superclass constructor returns,
>> all superclass final fields are safely published?  So the stateless child
>> class can call the start() method from within the constructor safely
>> publishing the "this" reference after all final fields and the objects they
>> reference are fully initialized.
>>
>> Thanks in advance,
>>
>> Peter.
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
>
> _______________________________________________
> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130508/4606e53b/attachment.html>

From nathan.reynolds at oracle.com  Wed May  8 16:00:03 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Wed, 08 May 2013 13:00:03 -0700
Subject: [concurrency-interest] safe construction
In-Reply-To: <CAE+h5-A1rbtHBWUtiJOy1aJyH5HdKAZ+mfAZyo-eqbWoLop44w@mail.gmail.com>
References: <518A1A88.10007@zeus.net.au>
	<CACuKZqGjr2qLbPqYes47ZkyCTKuZm=i4k0V+y+DSk_-+C+BCYQ@mail.gmail.com>
	<518A961B.8030509@oracle.com>
	<CAE+h5-A1rbtHBWUtiJOy1aJyH5HdKAZ+mfAZyo-eqbWoLop44w@mail.gmail.com>
Message-ID: <518AAEC3.9040000@oracle.com>

There is no formal happens before without the read.  However, HotSpot 
JIT will insert the fence after writing to s_fence.  But, this code will 
break with an advanced JIT which can eliminate the fence through code 
analysis.  The alternative is to use the non-existent Fences API.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Architect | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 5/8/2013 11:42 AM, Yuval Shavit wrote:
> Is that an artifact of the common/global implementation, or the JLS 
> itself? I thought that in order for it to work according to the JLS, 
> you would have to read s_fence as the first thing within each of 
> MyClass's methods:
>
>     public void foo() {
>         if (s_fence) throw new AssertionError();
>         doMyWhatever();
>     }
>
> This would establish a HB before the read and the post-initialization 
> write. There's no formal HB without the read, right?
>
>
> On Wed, May 8, 2013 at 2:14 PM, Nathan Reynolds 
> <nathan.reynolds at oracle.com <mailto:nathan.reynolds at oracle.com>> wrote:
>
>     A much heavier handed solution would be...
>
>     class MyClass
>     {
>         static volatile boolean s_fence;
>
>         public MyClass()
>         {
>             // initialize all fields
>             s_fence = false;
>             // leak "this"
>         }
>     }
>
>     Writing to s_fence guarantees that all of the fields will be
>     globally visible before "this" is leaked.  This is more expensive
>     since it adds a Store-Load memory fence rather than just a
>     Store-Store fence.  The Store-Store fence will turn into a noop on
>     x86.
>
>     The other problem with writing to s_fence is that the cache line
>     holding it is going to be contended.  If enough MyClass objects
>     are created concurrently on different cores, cache performance is
>     going to be the bottleneck.
>
>     Nathan Reynolds
>     <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> |
>     Architect | 602.333.9091 <tel:602.333.9091>
>     Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>     On 5/8/2013 8:02 AM, Zhong Yu wrote:
>>     If the publication is safe, there is nothing to worry about, as
>>     long as `this` is leaked after fields are assigned.
>>
>>     If the publication is unsafe, a trick I would use it to wrap the
>>     object reference as a final field, and publish the wrapper
>>
>>         class FinalReference<T>
>>             final T referent;
>>             FinalReference(T referent){ this.referent=referent; }
>>
>>         static FinalReference<Foo> unsafePubVar;
>>
>>         Foo foo = ...;
>>         unsafePubVar = new FinalReference(foo);
>>
>>     forcing the subscription side to go though a final field to
>>     access `foo` and its fields.
>>
>>     Zhong Yu
>>
>>
>>
>>     On Wed, May 8, 2013 at 4:27 AM, Peter Firmstone
>>     <peter.firmstone at zeus.net.au
>>     <mailto:peter.firmstone at zeus.net.au>> wrote:
>>
>>         I'm attempting to fix an object (public api) that uses unsafe
>>         construction (lets "this" escape to other threads via inner
>>         classes during construction).
>>
>>         To safely construct the object I need to modify the object to
>>         delay publication of the "this" reference until after
>>         construction.
>>
>>         I was considering a start() or init() method to do so.
>>
>>         However legacy code won't call that method, so I need a way
>>         to invoke start() or init() without relying on other methods.
>>
>>         Having every method check the state either isn't practical,
>>         since legacy code doesn't necessarily invoke any method calls
>>         immediately and the constructor invokes background threads to
>>         perform work that client code eventually uses.
>>
>>         The solution I'm thinking of is to move the implementation
>>         into a package private abstract superclass, so the object
>>         becomes stateless (the superclass is responsible for state).
>>          Then from the constructor, call the superclass constructor,
>>         then call the start() method.
>>
>>         Is it reasonable to assume that after the superclass
>>         constructor returns, all superclass final fields are safely
>>         published?  So the stateless child class can call the start()
>>         method from within the constructor safely publishing the
>>         "this" reference after all final fields and the objects they
>>         reference are fully initialized.
>>
>>         Thanks in advance,
>>
>>         Peter.
>>
>>
>>         _______________________________________________
>>         Concurrency-interest mailing list
>>         Concurrency-interest at cs.oswego.edu
>>         <mailto:Concurrency-interest at cs.oswego.edu>
>>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>>
>>     _______________________________________________
>>     Concurrency-interest mailing list
>>     Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130508/249d7441/attachment-0001.html>

From oleksandr.otenko at oracle.com  Wed May  8 16:49:28 2013
From: oleksandr.otenko at oracle.com (oleksandr otenko)
Date: Wed, 08 May 2013 21:49:28 +0100
Subject: [concurrency-interest] safe construction
In-Reply-To: <CAE+h5-A1rbtHBWUtiJOy1aJyH5HdKAZ+mfAZyo-eqbWoLop44w@mail.gmail.com>
References: <518A1A88.10007@zeus.net.au>
	<CACuKZqGjr2qLbPqYes47ZkyCTKuZm=i4k0V+y+DSk_-+C+BCYQ@mail.gmail.com>
	<518A961B.8030509@oracle.com>
	<CAE+h5-A1rbtHBWUtiJOy1aJyH5HdKAZ+mfAZyo-eqbWoLop44w@mail.gmail.com>
Message-ID: <518ABA58.3060505@oracle.com>

No, just a volatile read immediately after the write.

Alex

On 08/05/2013 19:42, Yuval Shavit wrote:
> Is that an artifact of the common/global implementation, or the JLS 
> itself? I thought that in order for it to work according to the JLS, 
> you would have to read s_fence as the first thing within each of 
> MyClass's methods:
>
>     public void foo() {
>         if (s_fence) throw new AssertionError();
>         doMyWhatever();
>     }
>
> This would establish a HB before the read and the post-initialization 
> write. There's no formal HB without the read, right?
>
>
> On Wed, May 8, 2013 at 2:14 PM, Nathan Reynolds 
> <nathan.reynolds at oracle.com <mailto:nathan.reynolds at oracle.com>> wrote:
>
>     A much heavier handed solution would be...
>
>     class MyClass
>     {
>         static volatile boolean s_fence;
>
>         public MyClass()
>         {
>             // initialize all fields
>             s_fence = false;
>             // leak "this"
>         }
>     }
>
>     Writing to s_fence guarantees that all of the fields will be
>     globally visible before "this" is leaked.  This is more expensive
>     since it adds a Store-Load memory fence rather than just a
>     Store-Store fence.  The Store-Store fence will turn into a noop on
>     x86.
>
>     The other problem with writing to s_fence is that the cache line
>     holding it is going to be contended.  If enough MyClass objects
>     are created concurrently on different cores, cache performance is
>     going to be the bottleneck.
>
>     Nathan Reynolds
>     <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> |
>     Architect | 602.333.9091 <tel:602.333.9091>
>     Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>     On 5/8/2013 8:02 AM, Zhong Yu wrote:
>>     If the publication is safe, there is nothing to worry about, as
>>     long as `this` is leaked after fields are assigned.
>>
>>     If the publication is unsafe, a trick I would use it to wrap the
>>     object reference as a final field, and publish the wrapper
>>
>>         class FinalReference<T>
>>             final T referent;
>>             FinalReference(T referent){ this.referent=referent; }
>>
>>         static FinalReference<Foo> unsafePubVar;
>>
>>         Foo foo = ...;
>>         unsafePubVar = new FinalReference(foo);
>>
>>     forcing the subscription side to go though a final field to
>>     access `foo` and its fields.
>>
>>     Zhong Yu
>>
>>
>>
>>     On Wed, May 8, 2013 at 4:27 AM, Peter Firmstone
>>     <peter.firmstone at zeus.net.au
>>     <mailto:peter.firmstone at zeus.net.au>> wrote:
>>
>>         I'm attempting to fix an object (public api) that uses unsafe
>>         construction (lets "this" escape to other threads via inner
>>         classes during construction).
>>
>>         To safely construct the object I need to modify the object to
>>         delay publication of the "this" reference until after
>>         construction.
>>
>>         I was considering a start() or init() method to do so.
>>
>>         However legacy code won't call that method, so I need a way
>>         to invoke start() or init() without relying on other methods.
>>
>>         Having every method check the state either isn't practical,
>>         since legacy code doesn't necessarily invoke any method calls
>>         immediately and the constructor invokes background threads to
>>         perform work that client code eventually uses.
>>
>>         The solution I'm thinking of is to move the implementation
>>         into a package private abstract superclass, so the object
>>         becomes stateless (the superclass is responsible for state).
>>          Then from the constructor, call the superclass constructor,
>>         then call the start() method.
>>
>>         Is it reasonable to assume that after the superclass
>>         constructor returns, all superclass final fields are safely
>>         published?  So the stateless child class can call the start()
>>         method from within the constructor safely publishing the
>>         "this" reference after all final fields and the objects they
>>         reference are fully initialized.
>>
>>         Thanks in advance,
>>
>>         Peter.
>>
>>
>>         _______________________________________________
>>         Concurrency-interest mailing list
>>         Concurrency-interest at cs.oswego.edu
>>         <mailto:Concurrency-interest at cs.oswego.edu>
>>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>>
>>     _______________________________________________
>>     Concurrency-interest mailing list
>>     Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130508/c8a2a97a/attachment.html>

From vitalyd at gmail.com  Wed May  8 19:33:38 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Wed, 8 May 2013 19:33:38 -0400
Subject: [concurrency-interest] safe construction
In-Reply-To: <518ABA58.3060505@oracle.com>
References: <518A1A88.10007@zeus.net.au>
	<CACuKZqGjr2qLbPqYes47ZkyCTKuZm=i4k0V+y+DSk_-+C+BCYQ@mail.gmail.com>
	<518A961B.8030509@oracle.com>
	<CAE+h5-A1rbtHBWUtiJOy1aJyH5HdKAZ+mfAZyo-eqbWoLop44w@mail.gmail.com>
	<518ABA58.3060505@oracle.com>
Message-ID: <CAHjP37Ee=jAR-5rmtFrzZzb475TMSHWsyeOEweE8UKEnPRsgkw@mail.gmail.com>

For JMM, I believe it has to be a read of s_fence everywhere where you want
to then read the normal writes preceding the volatile store, as Yuval
said.  That's how HB is defined.

Sent from my phone
On May 8, 2013 4:57 PM, "oleksandr otenko" <oleksandr.otenko at oracle.com>
wrote:

>  No, just a volatile read immediately after the write.
>
> Alex
>
>  On 08/05/2013 19:42, Yuval Shavit wrote:
>
> Is that an artifact of the common/global implementation, or the JLS
> itself? I thought that in order for it to work according to the JLS, you
> would have to read s_fence as the first thing within each of MyClass's
> methods:
>
>      public void foo() {
>         if (s_fence) throw new AssertionError();
>         doMyWhatever();
>     }
>
>  This would establish a HB before the read and the post-initialization
> write. There's no formal HB without the read, right?
>
>
> On Wed, May 8, 2013 at 2:14 PM, Nathan Reynolds <
> nathan.reynolds at oracle.com> wrote:
>
>>  A much heavier handed solution would be...
>>
>> class MyClass
>> {
>>     static volatile boolean s_fence;
>>
>>     public MyClass()
>>     {
>>         // initialize all fields
>>         s_fence = false;
>>         // leak "this"
>>     }
>> }
>>
>> Writing to s_fence guarantees that all of the fields will be globally
>> visible before "this" is leaked.  This is more expensive since it adds a
>> Store-Load memory fence rather than just a Store-Store fence.  The
>> Store-Store fence will turn into a noop on x86.
>>
>> The other problem with writing to s_fence is that the cache line holding
>> it is going to be contended.  If enough MyClass objects are created
>> concurrently on different cores, cache performance is going to be the
>> bottleneck.
>>
>> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Architect |
>> 602.333.9091
>> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>>   On 5/8/2013 8:02 AM, Zhong Yu wrote:
>>
>>  If the publication is safe, there is nothing to worry about, as long as
>> `this` is leaked after fields are assigned.
>>
>>  If the publication is unsafe, a trick I would use it to wrap the object
>> reference as a final field, and publish the wrapper
>>
>>      class FinalReference<T>
>>          final T referent;
>>         FinalReference(T referent){ this.referent=referent; }
>>
>>     static FinalReference<Foo> unsafePubVar;
>>
>>      Foo foo = ...;
>>      unsafePubVar = new FinalReference(foo);
>>
>>  forcing the subscription side to go though a final field to access
>> `foo` and its fields.
>>
>>  Zhong Yu
>>
>>
>>
>> On Wed, May 8, 2013 at 4:27 AM, Peter Firmstone <
>> peter.firmstone at zeus.net.au> wrote:
>>
>>> I'm attempting to fix an object (public api) that uses unsafe
>>> construction (lets "this" escape to other threads via inner classes during
>>> construction).
>>>
>>> To safely construct the object I need to modify the object to delay
>>> publication of the "this" reference until after construction.
>>>
>>> I was considering a start() or init() method to do so.
>>>
>>> However legacy code won't call that method, so I need a way to invoke
>>> start() or init() without relying on other methods.
>>>
>>> Having every method check the state either isn't practical, since legacy
>>> code doesn't necessarily invoke any method calls immediately and the
>>> constructor invokes background threads to perform work that client code
>>> eventually uses.
>>>
>>> The solution I'm thinking of is to move the implementation into a
>>> package private abstract superclass, so the object becomes stateless (the
>>> superclass is responsible for state).  Then from the constructor, call the
>>> superclass constructor, then call the start() method.
>>>
>>> Is it reasonable to assume that after the superclass constructor
>>> returns, all superclass final fields are safely published?  So the
>>> stateless child class can call the start() method from within the
>>> constructor safely publishing the "this" reference after all final fields
>>> and the objects they reference are fully initialized.
>>>
>>> Thanks in advance,
>>>
>>> Peter.
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
>
> _______________________________________________
> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130508/780b5b1f/attachment-0001.html>

From oleksandr.otenko at oracle.com  Thu May  9 09:44:19 2013
From: oleksandr.otenko at oracle.com (oleksandr otenko)
Date: Thu, 09 May 2013 14:44:19 +0100
Subject: [concurrency-interest] safe construction
In-Reply-To: <CAHjP37Ee=jAR-5rmtFrzZzb475TMSHWsyeOEweE8UKEnPRsgkw@mail.gmail.com>
References: <518A1A88.10007@zeus.net.au>
	<CACuKZqGjr2qLbPqYes47ZkyCTKuZm=i4k0V+y+DSk_-+C+BCYQ@mail.gmail.com>
	<518A961B.8030509@oracle.com>
	<CAE+h5-A1rbtHBWUtiJOy1aJyH5HdKAZ+mfAZyo-eqbWoLop44w@mail.gmail.com>
	<518ABA58.3060505@oracle.com>
	<CAHjP37Ee=jAR-5rmtFrzZzb475TMSHWsyeOEweE8UKEnPRsgkw@mail.gmail.com>
Message-ID: <518BA833.10609@oracle.com>

No, not in this case. In this case "this" is published, and you cannot 
call public void foo without observing shared==this.


JMM only defines thread-local orderings. The processors don't vote which 
instructions to reorder.

"Thread 1 storing a state happens-before Thread 2 reads shared" is wrong 
kind of reasoning. As if we can tell Thread 2 /when/ to read. Don't try 
to bend the spoon, it is impossible.

Same, but put in different words:

"shared == this /*may*/ be observed" -> "super class state /*is*/ 
consistent" for some definition of consistency.

Here arrow "->" is implication, and it turns out to be exactly opposite 
of happens-before.

"may" deliberately states that we cannot tell what anyone else is doing. 
"shared == this" deliberately states what condition we observe, not just 
"any subsequent load of the same variable". (what is "subsequent"? 
remember, processors don't vote on this. It is a true statement about 
"any load /that happens to be subsequent/", but that's tautological)

"is" deliberately states what we guarantee thread-locally.


It is clear that shared==this can not be observed, if we don't store to 
shared. Now we have everything that is needed to reason about validity 
of code using only thread-local ordering.

"shared == this may be observed" -> "shared=this was executed" // obviously
"shared=this was executed" -> "super class state is consistent" // 
/this/ is happens-before, and it is thread-local

That's it. Add store-store barrier between shared=this and end of 
super-class constructor.


Nathan's example is not complete, but not because volatile loads are 
missing in other methods, but because of three simple rules in JMM:

1. Volatile load can go ahead of anything non-volatile
2. Anything non-volatile can go ahead of volatile store
3. Anything non-volatile can go ahead of anything non-volatile

So, shared=this may be allowed to go ahead of s_fence=false, if shared 
is not volatile, and then can be reordered with "super class state is 
consistent". In order to stop this from happening, you need a volatile 
load after a volatile store and before shared=this. This way shared=this 
cannot go ahead of s_fence=false, because volatile load cannot go ahead 
of s_fence=false, and shared=this cannot go ahead of a volatile load.

shared.x is always issued as Load shared; Load shared.x, no additional 
barriers are needed in threads that access superclass fields initialized 
prior to publishing shared=this.


There is one thin case. If, and this is a big if, Load shared.x is 
allowed to go ahead of Load shared (eg if CPU already loaded that cache 
line and cache coherency is weak), then there must be a Load-Load 
barrier between Load shared and Load shared.x, but even this cannot be 
achieved through a volatile load at the beginning of the method body - 
because of rule (1) volatile load can go ahead of non-volatile Load 
shared, and we are back to square one.

So, a volatile load at the beginning of a method is wrong, and I'd check 
what JMM says about reordering of dependent normal field x loads and a 
stronger barrier may be needed after the first load of shared - there 
certainly is a guarantee about final x. I don't think the data 
dependency reordering is possible (how can you eliminate Load shared.x 
before you know what shared refers to), but let's check.


Alex


On 09/05/2013 00:33, Vitaly Davidovich wrote:
>
> For JMM, I believe it has to be a read of s_fence everywhere where you 
> want to then read the normal writes preceding the volatile store, as 
> Yuval said.  That's how HB is defined.
>
> Sent from my phone
>
> On May 8, 2013 4:57 PM, "oleksandr otenko" 
> <oleksandr.otenko at oracle.com <mailto:oleksandr.otenko at oracle.com>> wrote:
>
>     No, just a volatile read immediately after the write.
>
>     Alex
>
>     On 08/05/2013 19:42, Yuval Shavit wrote:
>>     Is that an artifact of the common/global implementation, or the
>>     JLS itself? I thought that in order for it to work according to
>>     the JLS, you would have to read s_fence as the first thing within
>>     each of MyClass's methods:
>>
>>         public void foo() {
>>             if (s_fence) throw new AssertionError();
>>             doMyWhatever();
>>         }
>>
>>     This would establish a HB before the read and the
>>     post-initialization write. There's no formal HB without the read,
>>     right?
>>
>>
>>     On Wed, May 8, 2013 at 2:14 PM, Nathan Reynolds
>>     <nathan.reynolds at oracle.com <mailto:nathan.reynolds at oracle.com>>
>>     wrote:
>>
>>         A much heavier handed solution would be...
>>
>>         class MyClass
>>         {
>>             static volatile boolean s_fence;
>>
>>             public MyClass()
>>             {
>>                 // initialize all fields
>>                 s_fence = false;
>>                 // leak "this"
>>             }
>>         }
>>
>>         Writing to s_fence guarantees that all of the fields will be
>>         globally visible before "this" is leaked.  This is more
>>         expensive since it adds a Store-Load memory fence rather than
>>         just a Store-Store fence.  The Store-Store fence will turn
>>         into a noop on x86.
>>
>>         The other problem with writing to s_fence is that the cache
>>         line holding it is going to be contended.  If enough MyClass
>>         objects are created concurrently on different cores, cache
>>         performance is going to be the bottleneck.
>>
>>         Nathan Reynolds
>>         <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> |
>>         Architect | 602.333.9091 <tel:602.333.9091>
>>         Oracle PSR Engineering <http://psr.us.oracle.com/> | Server
>>         Technology
>>         On 5/8/2013 8:02 AM, Zhong Yu wrote:
>>>         If the publication is safe, there is nothing to worry about,
>>>         as long as `this` is leaked after fields are assigned.
>>>
>>>         If the publication is unsafe, a trick I would use it to wrap
>>>         the object reference as a final field, and publish the wrapper
>>>
>>>             class FinalReference<T>
>>>                 final T referent;
>>>                 FinalReference(T referent){ this.referent=referent; }
>>>
>>>             static FinalReference<Foo> unsafePubVar;
>>>
>>>             Foo foo = ...;
>>>             unsafePubVar = new FinalReference(foo);
>>>
>>>         forcing the subscription side to go though a final field to
>>>         access `foo` and its fields.
>>>
>>>         Zhong Yu
>>>
>>>
>>>
>>>         On Wed, May 8, 2013 at 4:27 AM, Peter Firmstone
>>>         <peter.firmstone at zeus.net.au
>>>         <mailto:peter.firmstone at zeus.net.au>> wrote:
>>>
>>>             I'm attempting to fix an object (public api) that uses
>>>             unsafe construction (lets "this" escape to other threads
>>>             via inner classes during construction).
>>>
>>>             To safely construct the object I need to modify the
>>>             object to delay publication of the "this" reference
>>>             until after construction.
>>>
>>>             I was considering a start() or init() method to do so.
>>>
>>>             However legacy code won't call that method, so I need a
>>>             way to invoke start() or init() without relying on other
>>>             methods.
>>>
>>>             Having every method check the state either isn't
>>>             practical, since legacy code doesn't necessarily invoke
>>>             any method calls immediately and the constructor invokes
>>>             background threads to perform work that client code
>>>             eventually uses.
>>>
>>>             The solution I'm thinking of is to move the
>>>             implementation into a package private abstract
>>>             superclass, so the object becomes stateless (the
>>>             superclass is responsible for state).  Then from the
>>>             constructor, call the superclass constructor, then call
>>>             the start() method.
>>>
>>>             Is it reasonable to assume that after the superclass
>>>             constructor returns, all superclass final fields are
>>>             safely published?  So the stateless child class can call
>>>             the start() method from within the constructor safely
>>>             publishing the "this" reference after all final fields
>>>             and the objects they reference are fully initialized.
>>>
>>>             Thanks in advance,
>>>
>>>             Peter.
>>>
>>>
>>>             _______________________________________________
>>>             Concurrency-interest mailing list
>>>             Concurrency-interest at cs.oswego.edu
>>>             <mailto:Concurrency-interest at cs.oswego.edu>
>>>             http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>>
>>>
>>>         _______________________________________________
>>>         Concurrency-interest mailing list
>>>         Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>         _______________________________________________
>>         Concurrency-interest mailing list
>>         Concurrency-interest at cs.oswego.edu
>>         <mailto:Concurrency-interest at cs.oswego.edu>
>>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>>
>>     _______________________________________________
>>     Concurrency-interest mailing list
>>     Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130509/cf6af1b5/attachment-0001.html>

From yankee.sierra at gmail.com  Thu May  9 10:42:18 2013
From: yankee.sierra at gmail.com (Yuval Shavit)
Date: Thu, 9 May 2013 10:42:18 -0400
Subject: [concurrency-interest] safe construction
In-Reply-To: <518BA833.10609@oracle.com>
References: <518A1A88.10007@zeus.net.au>
	<CACuKZqGjr2qLbPqYes47ZkyCTKuZm=i4k0V+y+DSk_-+C+BCYQ@mail.gmail.com>
	<518A961B.8030509@oracle.com>
	<CAE+h5-A1rbtHBWUtiJOy1aJyH5HdKAZ+mfAZyo-eqbWoLop44w@mail.gmail.com>
	<518ABA58.3060505@oracle.com>
	<CAHjP37Ee=jAR-5rmtFrzZzb475TMSHWsyeOEweE8UKEnPRsgkw@mail.gmail.com>
	<518BA833.10609@oracle.com>
Message-ID: <CAE+h5-AVw_Xtd62gVnafTXYTdVpr==sqptkpAXHUmHpDxfdaYQ@mail.gmail.com>

I *think* I get what you're saying. It sounds like the problem is that the
if the volatile read happens after s_fence is written (at the end of the
ctor), things will be okay; but otherwise, all bets are off, since there
may not have been a volatile write to establish a HB edge. So, we could get
an ordering of:

1) thread A: object init, start setting fields
2) threadA: shared = this // reordered to happen before s_fence = false,
via rule 2 in your JMM rules
3) thread B: shared.foo() reads s_fence, but so what
4) thread B: shared.foo() uses partially-visible state
5) thread A: finish setting fields
6) thread A: <init> s_fence = false

Is that the gist of it? If so, what if we slightly adjust Nathan's example:

class MyClass
{
   private volatile boolean s_fence; // not static anymore

    public MyClass()
    {
        // initialize all fields
        s_fence = true; // not false
        // leak "this"
    }

    public void foo() {
        while (!s_fence) { }
        // use fields
    }
}

This causes each invocation of foo() to spin until it sees s_fence==true,
at which point it must also see all of the activity that happened before
s_fence = true. The above sequence turns into:

1) thread A: object init, start setting fields
2) threadA: shared = this
3) thread B: shared.foo() reads s_fence == false, starts spinning
4) thread A: finish setting fields
5) thread A: <init> s_fence = true
6) thread B: reads s_fence == true, stops spinning
7) thread B: shared.foo() continues and sees all fields as set in ctor


On Thu, May 9, 2013 at 9:44 AM, oleksandr otenko <
oleksandr.otenko at oracle.com> wrote:

>  No, not in this case. In this case "this" is published, and you cannot
> call public void foo without observing shared==this.
>
>
> JMM only defines thread-local orderings. The processors don't vote which
> instructions to reorder.
>
> "Thread 1 storing a state happens-before Thread 2 reads shared" is wrong
> kind of reasoning. As if we can tell Thread 2 *when* to read. Don't try
> to bend the spoon, it is impossible.
>
> Same, but put in different words:
>
> "shared == this *may* be observed" -> "super class state *is* consistent"
> for some definition of consistency.
>
> Here arrow "->" is implication, and it turns out to be exactly opposite of
> happens-before.
>
> "may" deliberately states that we cannot tell what anyone else is doing.
> "shared == this" deliberately states what condition we observe, not just
> "any subsequent load of the same variable". (what is "subsequent"?
> remember, processors don't vote on this. It is a true statement about "any
> load *that happens to be subsequent*", but that's tautological)
>
> "is" deliberately states what we guarantee thread-locally.
>
>
> It is clear that shared==this can not be observed, if we don't store to
> shared. Now we have everything that is needed to reason about validity of
> code using only thread-local ordering.
>
> "shared == this may be observed" -> "shared=this was executed"  //
> obviously
> "shared=this was executed" -> "super class state is consistent" // *this*is happens-before, and it is thread-local
>
> That's it. Add store-store barrier between shared=this and end of
> super-class constructor.
>
>
> Nathan's example is not complete, but not because volatile loads are
> missing in other methods, but because of three simple rules in JMM:
>
> 1. Volatile load can go ahead of anything non-volatile
> 2. Anything non-volatile can go ahead of volatile store
> 3. Anything non-volatile can go ahead of anything non-volatile
>
> So, shared=this may be allowed to go ahead of s_fence=false, if shared is
> not volatile, and then can be reordered with "super class state is
> consistent". In order to stop this from happening, you need a volatile load
> after a volatile store and before shared=this. This way shared=this cannot
> go ahead of s_fence=false, because volatile load cannot go ahead of
> s_fence=false, and shared=this cannot go ahead of a volatile load.
>
> shared.x is always issued as Load shared; Load shared.x, no additional
> barriers are needed in threads that access superclass fields initialized
> prior to publishing shared=this.
>
>
> There is one thin case. If, and this is a big if, Load shared.x is allowed
> to go ahead of Load shared (eg if CPU already loaded that cache line and
> cache coherency is weak), then there must be a Load-Load barrier between
> Load shared and Load shared.x, but even this cannot be achieved through a
> volatile load at the beginning of the method body - because of rule (1)
> volatile load can go ahead of non-volatile Load shared, and we are back to
> square one.
>
> So, a volatile load at the beginning of a method is wrong, and I'd check
> what JMM says about reordering of dependent normal field x loads and a
> stronger barrier may be needed after the first load of shared - there
> certainly is a guarantee about final x. I don't think the data dependency
> reordering is possible (how can you eliminate Load shared.x before you know
> what shared refers to), but let's check.
>
>
> Alex
>
>
>
> On 09/05/2013 00:33, Vitaly Davidovich wrote:
>
> For JMM, I believe it has to be a read of s_fence everywhere where you
> want to then read the normal writes preceding the volatile store, as Yuval
> said.  That's how HB is defined.
>
> Sent from my phone
> On May 8, 2013 4:57 PM, "oleksandr otenko" <oleksandr.otenko at oracle.com>
> wrote:
>
>>  No, just a volatile read immediately after the write.
>>
>> Alex
>>
>>  On 08/05/2013 19:42, Yuval Shavit wrote:
>>
>> Is that an artifact of the common/global implementation, or the JLS
>> itself? I thought that in order for it to work according to the JLS, you
>> would have to read s_fence as the first thing within each of MyClass's
>> methods:
>>
>>      public void foo() {
>>         if (s_fence) throw new AssertionError();
>>         doMyWhatever();
>>     }
>>
>>  This would establish a HB before the read and the post-initialization
>> write. There's no formal HB without the read, right?
>>
>>
>> On Wed, May 8, 2013 at 2:14 PM, Nathan Reynolds <
>> nathan.reynolds at oracle.com> wrote:
>>
>>>  A much heavier handed solution would be...
>>>
>>> class MyClass
>>> {
>>>     static volatile boolean s_fence;
>>>
>>>     public MyClass()
>>>     {
>>>         // initialize all fields
>>>         s_fence = false;
>>>         // leak "this"
>>>     }
>>> }
>>>
>>> Writing to s_fence guarantees that all of the fields will be globally
>>> visible before "this" is leaked.  This is more expensive since it adds a
>>> Store-Load memory fence rather than just a Store-Store fence.  The
>>> Store-Store fence will turn into a noop on x86.
>>>
>>> The other problem with writing to s_fence is that the cache line holding
>>> it is going to be contended.  If enough MyClass objects are created
>>> concurrently on different cores, cache performance is going to be the
>>> bottleneck.
>>>
>>> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Architect |
>>> 602.333.9091
>>> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>>>   On 5/8/2013 8:02 AM, Zhong Yu wrote:
>>>
>>>  If the publication is safe, there is nothing to worry about, as long
>>> as `this` is leaked after fields are assigned.
>>>
>>>  If the publication is unsafe, a trick I would use it to wrap the object
>>> reference as a final field, and publish the wrapper
>>>
>>>      class FinalReference<T>
>>>          final T referent;
>>>         FinalReference(T referent){ this.referent=referent; }
>>>
>>>     static FinalReference<Foo> unsafePubVar;
>>>
>>>      Foo foo = ...;
>>>      unsafePubVar = new FinalReference(foo);
>>>
>>>  forcing the subscription side to go though a final field to access
>>> `foo` and its fields.
>>>
>>>  Zhong Yu
>>>
>>>
>>>
>>> On Wed, May 8, 2013 at 4:27 AM, Peter Firmstone <
>>> peter.firmstone at zeus.net.au> wrote:
>>>
>>>> I'm attempting to fix an object (public api) that uses unsafe
>>>> construction (lets "this" escape to other threads via inner classes during
>>>> construction).
>>>>
>>>> To safely construct the object I need to modify the object to delay
>>>> publication of the "this" reference until after construction.
>>>>
>>>> I was considering a start() or init() method to do so.
>>>>
>>>> However legacy code won't call that method, so I need a way to invoke
>>>> start() or init() without relying on other methods.
>>>>
>>>> Having every method check the state either isn't practical, since
>>>> legacy code doesn't necessarily invoke any method calls immediately and the
>>>> constructor invokes background threads to perform work that client code
>>>> eventually uses.
>>>>
>>>> The solution I'm thinking of is to move the implementation into a
>>>> package private abstract superclass, so the object becomes stateless (the
>>>> superclass is responsible for state).  Then from the constructor, call the
>>>> superclass constructor, then call the start() method.
>>>>
>>>> Is it reasonable to assume that after the superclass constructor
>>>> returns, all superclass final fields are safely published?  So the
>>>> stateless child class can call the start() method from within the
>>>> constructor safely publishing the "this" reference after all final fields
>>>> and the objects they reference are fully initialized.
>>>>
>>>> Thanks in advance,
>>>>
>>>> Peter.
>>>>
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130509/fd995311/attachment-0001.html>

From zhong.j.yu at gmail.com  Thu May  9 10:50:13 2013
From: zhong.j.yu at gmail.com (Zhong Yu)
Date: Thu, 9 May 2013 09:50:13 -0500
Subject: [concurrency-interest] safe construction
In-Reply-To: <518BA833.10609@oracle.com>
References: <518A1A88.10007@zeus.net.au>
	<CACuKZqGjr2qLbPqYes47ZkyCTKuZm=i4k0V+y+DSk_-+C+BCYQ@mail.gmail.com>
	<518A961B.8030509@oracle.com>
	<CAE+h5-A1rbtHBWUtiJOy1aJyH5HdKAZ+mfAZyo-eqbWoLop44w@mail.gmail.com>
	<518ABA58.3060505@oracle.com>
	<CAHjP37Ee=jAR-5rmtFrzZzb475TMSHWsyeOEweE8UKEnPRsgkw@mail.gmail.com>
	<518BA833.10609@oracle.com>
Message-ID: <CACuKZqGW8E+gX++gVapoisC+FOqxaM_20TfQGPWGwvrve-1KvQ@mail.gmail.com>

On Thu, May 9, 2013 at 8:44 AM, oleksandr otenko <
oleksandr.otenko at oracle.com> wrote:

>  No, not in this case. In this case "this" is published, and you cannot
> call public void foo without observing shared==this.
>
>
> JMM only defines thread-local orderings. The processors don't vote which
> instructions to reorder.
>
> "Thread 1 storing a state happens-before Thread 2 reads shared" is wrong
> kind of reasoning. As if we can tell Thread 2 *when* to read. Don't try
> to bend the spoon, it is impossible.
>
> Same, but put in different words:
>
> "shared == this *may* be observed" -> "super class state *is* consistent"
> for some definition of consistency.
>
> Here arrow "->" is implication, and it turns out to be exactly opposite of
> happens-before.
>
> "may" deliberately states that we cannot tell what anyone else is doing.
> "shared == this" deliberately states what condition we observe, not just
> "any subsequent load of the same variable". (what is "subsequent"?
> remember, processors don't vote on this. It is a true statement about "any
> load *that happens to be subsequent*", but that's tautological)
>
> "is" deliberately states what we guarantee thread-locally.
>
>
> It is clear that shared==this can not be observed, if we don't store to
> shared. Now we have everything that is needed to reason about validity of
> code using only thread-local ordering.
>
> "shared == this may be observed" -> "shared=this was executed"  //
> obviously
> "shared=this was executed" -> "super class state is consistent" // *this*is happens-before, and it is thread-local
>
> That's it. Add store-store barrier between shared=this and end of
> super-class constructor.
>
>
> Nathan's example is not complete, but not because volatile loads are
> missing in other methods, but because of three simple rules in JMM:
>
> 1. Volatile load can go ahead of anything non-volatile
> 2. Anything non-volatile can go ahead of volatile store
> 3. Anything non-volatile can go ahead of anything non-volatile
>

Alex, I don't think these rules are in JMM. JMM itself is a little weaker.
For example,

    var = 1;   // write#1
    synchronized(new Object()){}
    var = 2;  // write#2

the 2nd line can be elided, and the two writes can be reordered, as far as
JMM is concerned.

Zhong Yu



>
> So, shared=this may be allowed to go ahead of s_fence=false, if shared is
> not volatile, and then can be reordered with "super class state is
> consistent". In order to stop this from happening, you need a volatile load
> after a volatile store and before shared=this. This way shared=this cannot
> go ahead of s_fence=false, because volatile load cannot go ahead of
> s_fence=false, and shared=this cannot go ahead of a volatile load.
>
> shared.x is always issued as Load shared; Load shared.x, no additional
> barriers are needed in threads that access superclass fields initialized
> prior to publishing shared=this.
>
>
> There is one thin case. If, and this is a big if, Load shared.x is allowed
> to go ahead of Load shared (eg if CPU already loaded that cache line and
> cache coherency is weak), then there must be a Load-Load barrier between
> Load shared and Load shared.x, but even this cannot be achieved through a
> volatile load at the beginning of the method body - because of rule (1)
> volatile load can go ahead of non-volatile Load shared, and we are back to
> square one.
>
> So, a volatile load at the beginning of a method is wrong, and I'd check
> what JMM says about reordering of dependent normal field x loads and a
> stronger barrier may be needed after the first load of shared - there
> certainly is a guarantee about final x. I don't think the data dependency
> reordering is possible (how can you eliminate Load shared.x before you know
> what shared refers to), but let's check.
>
>
> Alex
>
>
>
> On 09/05/2013 00:33, Vitaly Davidovich wrote:
>
> For JMM, I believe it has to be a read of s_fence everywhere where you
> want to then read the normal writes preceding the volatile store, as Yuval
> said.  That's how HB is defined.
>
> Sent from my phone
> On May 8, 2013 4:57 PM, "oleksandr otenko" <oleksandr.otenko at oracle.com>
> wrote:
>
>>  No, just a volatile read immediately after the write.
>>
>> Alex
>>
>>  On 08/05/2013 19:42, Yuval Shavit wrote:
>>
>> Is that an artifact of the common/global implementation, or the JLS
>> itself? I thought that in order for it to work according to the JLS, you
>> would have to read s_fence as the first thing within each of MyClass's
>> methods:
>>
>>      public void foo() {
>>         if (s_fence) throw new AssertionError();
>>         doMyWhatever();
>>     }
>>
>>  This would establish a HB before the read and the post-initialization
>> write. There's no formal HB without the read, right?
>>
>>
>> On Wed, May 8, 2013 at 2:14 PM, Nathan Reynolds <
>> nathan.reynolds at oracle.com> wrote:
>>
>>>  A much heavier handed solution would be...
>>>
>>> class MyClass
>>> {
>>>     static volatile boolean s_fence;
>>>
>>>     public MyClass()
>>>     {
>>>         // initialize all fields
>>>         s_fence = false;
>>>         // leak "this"
>>>     }
>>> }
>>>
>>> Writing to s_fence guarantees that all of the fields will be globally
>>> visible before "this" is leaked.  This is more expensive since it adds a
>>> Store-Load memory fence rather than just a Store-Store fence.  The
>>> Store-Store fence will turn into a noop on x86.
>>>
>>> The other problem with writing to s_fence is that the cache line holding
>>> it is going to be contended.  If enough MyClass objects are created
>>> concurrently on different cores, cache performance is going to be the
>>> bottleneck.
>>>
>>> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Architect |
>>> 602.333.9091
>>> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>>>   On 5/8/2013 8:02 AM, Zhong Yu wrote:
>>>
>>>  If the publication is safe, there is nothing to worry about, as long
>>> as `this` is leaked after fields are assigned.
>>>
>>>  If the publication is unsafe, a trick I would use it to wrap the object
>>> reference as a final field, and publish the wrapper
>>>
>>>      class FinalReference<T>
>>>          final T referent;
>>>         FinalReference(T referent){ this.referent=referent; }
>>>
>>>     static FinalReference<Foo> unsafePubVar;
>>>
>>>      Foo foo = ...;
>>>      unsafePubVar = new FinalReference(foo);
>>>
>>>  forcing the subscription side to go though a final field to access
>>> `foo` and its fields.
>>>
>>>  Zhong Yu
>>>
>>>
>>>
>>> On Wed, May 8, 2013 at 4:27 AM, Peter Firmstone <
>>> peter.firmstone at zeus.net.au> wrote:
>>>
>>>> I'm attempting to fix an object (public api) that uses unsafe
>>>> construction (lets "this" escape to other threads via inner classes during
>>>> construction).
>>>>
>>>> To safely construct the object I need to modify the object to delay
>>>> publication of the "this" reference until after construction.
>>>>
>>>> I was considering a start() or init() method to do so.
>>>>
>>>> However legacy code won't call that method, so I need a way to invoke
>>>> start() or init() without relying on other methods.
>>>>
>>>> Having every method check the state either isn't practical, since
>>>> legacy code doesn't necessarily invoke any method calls immediately and the
>>>> constructor invokes background threads to perform work that client code
>>>> eventually uses.
>>>>
>>>> The solution I'm thinking of is to move the implementation into a
>>>> package private abstract superclass, so the object becomes stateless (the
>>>> superclass is responsible for state).  Then from the constructor, call the
>>>> superclass constructor, then call the start() method.
>>>>
>>>> Is it reasonable to assume that after the superclass constructor
>>>> returns, all superclass final fields are safely published?  So the
>>>> stateless child class can call the start() method from within the
>>>> constructor safely publishing the "this" reference after all final fields
>>>> and the objects they reference are fully initialized.
>>>>
>>>> Thanks in advance,
>>>>
>>>> Peter.
>>>>
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130509/a8c5392a/attachment-0001.html>

From oleksandr.otenko at oracle.com  Thu May  9 11:24:06 2013
From: oleksandr.otenko at oracle.com (oleksandr otenko)
Date: Thu, 09 May 2013 16:24:06 +0100
Subject: [concurrency-interest] safe construction
In-Reply-To: <CAE+h5-AVw_Xtd62gVnafTXYTdVpr==sqptkpAXHUmHpDxfdaYQ@mail.gmail.com>
References: <518A1A88.10007@zeus.net.au>
	<CACuKZqGjr2qLbPqYes47ZkyCTKuZm=i4k0V+y+DSk_-+C+BCYQ@mail.gmail.com>
	<518A961B.8030509@oracle.com>
	<CAE+h5-A1rbtHBWUtiJOy1aJyH5HdKAZ+mfAZyo-eqbWoLop44w@mail.gmail.com>
	<518ABA58.3060505@oracle.com>
	<CAHjP37Ee=jAR-5rmtFrzZzb475TMSHWsyeOEweE8UKEnPRsgkw@mail.gmail.com>
	<518BA833.10609@oracle.com>
	<CAE+h5-AVw_Xtd62gVnafTXYTdVpr==sqptkpAXHUmHpDxfdaYQ@mail.gmail.com>
Message-ID: <518BBF96.6000007@oracle.com>

I am trying to get across that:

1. A concurrent algorithm is a network of propositions, and the program 
is a constructive proof of it.

2. The common explanation of happens-before is not entirely clear, since 
it leads to misconceptions like "stores become visible atomically after 
a volatile store" and "let's create a happens-before edge". 
Happens-before edge to what? Often people talk about HB edge between 
events in two threads. Maybe there is a way to put it like a edge 
between events in two threads, but it seems to me it is misinterpreted, 
like in this thread.

3. 'roach motel' analogy is visually appealing, but it's like having 
implications with arrows going left and right. Reasoning is possible, 
but much easier with arrows going in one direction only. I choose the 
arrows pointing in the direction opposite to happens-before, because 
they mean implication, so I can use deduction.


Once we understand what sort of proofs can be constructed, the whole 
barriers and reordering thing becomes very clear - both what is said and 
implied.


So, the publisher needs to order shared=this with respect to 
shared.x=something. That is achieved through a volatile store followed 
by a volatile load.

Then the consumer needs to order:
Load shared;
Load shared.foo;
call shared.foo;
Load shared.x

Trying to insert a volatile load between call shared.foo and Load 
shared.x is creating a happens-before edge to what? What is the 
proposition we prove with it?

I think a proposition that makes sense is this: "we are reading the 
field of the object whose reference we just loaded".

It makes perfect sense that a compiler proves this: "Load shared.x is 
issued" -> "Load shared is issued", meaning that if the compiler has to 
load the reference to the object, it means it will assume the field of 
the object also needs loading - cannot reuse the previous value 
available at the same address, since it can belong to a different object 
that was there at the time. Then which loads issued by compiler can the 
platform reorder? I don't think the hardware platforms can guess the 
address Load shared.x is accessing without first loading shared, so 
cannot reorder it.


In this particular case, the fact that you called foo() is the proof 
that Load shared occurred. Then at least one Load shared.x must have 
occurred after that - there is no way to eliminate it, only reorder and 
fuse with the other loads of the same address. I would expect that the 
compiler takes care of two accesses to the same address that possibly 
mean two different objects.

Then from "Load shared.x" -> "Load shared" and from "shared == this may 
be observed" -> "super-class state is consistent" we conclude "Load 
shared.x" -> "super-class state is consistent"


So the volatile loads, loops and other tricks are completely unnecessary 
on the side consuming the value of shared, unless we can hear of a 
platform that can guess shared.x before loading shared.


Alex


On 09/05/2013 15:42, Yuval Shavit wrote:
> I *think* I get what you're saying. It sounds like the problem is that 
> the if the volatile read happens after s_fence is written (at the end 
> of the ctor), things will be okay; but otherwise, all bets are off, 
> since there may not have been a volatile write to establish a HB edge. 
> So, we could get an ordering of:
>
> 1) thread A: object init, start setting fields
> 2) threadA: shared = this // reordered to happen before s_fence = 
> false, via rule 2 in your JMM rules
> 3) thread B: shared.foo() reads s_fence, but so what
> 4) thread B: shared.foo() uses partially-visible state
> 5) thread A: finish setting fields
> 6) thread A: <init> s_fence = false
>
> Is that the gist of it? If so, what if we slightly adjust Nathan's 
> example:
>
> class MyClass
> {
>  private volatile boolean s_fence; // not static anymore
>
> public MyClass()
> {
>     // initialize all fields
>     s_fence = true; // not false
>     // leak "this"
> }
>
>     public void foo() {
>         while (!s_fence) { }
>         // use fields
>     }
> }
>
> This causes each invocation of foo() to spin until it sees 
> s_fence==true, at which point it must also see all of the activity 
> that happened before s_fence = true. The above sequence turns into:
>
> 1) thread A: object init, start setting fields
> 2) threadA: shared = this
> 3) thread B: shared.foo() reads s_fence == false, starts spinning
> 4) thread A: finish setting fields
> 5) thread A: <init> s_fence = true
> 6) thread B: reads s_fence == true, stops spinning
> 7) thread B: shared.foo() continues and sees all fields as set in ctor
>
>
> On Thu, May 9, 2013 at 9:44 AM, oleksandr otenko 
> <oleksandr.otenko at oracle.com <mailto:oleksandr.otenko at oracle.com>> wrote:
>
>     No, not in this case. In this case "this" is published, and you
>     cannot call public void foo without observing shared==this.
>
>
>     JMM only defines thread-local orderings. The processors don't vote
>     which instructions to reorder.
>
>     "Thread 1 storing a state happens-before Thread 2 reads shared" is
>     wrong kind of reasoning. As if we can tell Thread 2 /when/ to
>     read. Don't try to bend the spoon, it is impossible.
>
>     Same, but put in different words:
>
>     "shared == this /*may*/ be observed" -> "super class state /*is*/
>     consistent" for some definition of consistency.
>
>     Here arrow "->" is implication, and it turns out to be exactly
>     opposite of happens-before.
>
>     "may" deliberately states that we cannot tell what anyone else is
>     doing. "shared == this" deliberately states what condition we
>     observe, not just "any subsequent load of the same variable".
>     (what is "subsequent"? remember, processors don't vote on this. It
>     is a true statement about "any load /that happens to be
>     subsequent/", but that's tautological)
>
>     "is" deliberately states what we guarantee thread-locally.
>
>
>     It is clear that shared==this can not be observed, if we don't
>     store to shared. Now we have everything that is needed to reason
>     about validity of code using only thread-local ordering.
>
>     "shared == this may be observed" -> "shared=this was executed"  //
>     obviously
>     "shared=this was executed" -> "super class state is consistent" //
>     /this/ is happens-before, and it is thread-local
>
>     That's it. Add store-store barrier between shared=this and end of
>     super-class constructor.
>
>
>     Nathan's example is not complete, but not because volatile loads
>     are missing in other methods, but because of three simple rules in
>     JMM:
>
>     1. Volatile load can go ahead of anything non-volatile
>     2. Anything non-volatile can go ahead of volatile store
>     3. Anything non-volatile can go ahead of anything non-volatile
>
>     So, shared=this may be allowed to go ahead of s_fence=false, if
>     shared is not volatile, and then can be reordered with "super
>     class state is consistent". In order to stop this from happening,
>     you need a volatile load after a volatile store and before
>     shared=this. This way shared=this cannot go ahead of
>     s_fence=false, because volatile load cannot go ahead of
>     s_fence=false, and shared=this cannot go ahead of a volatile load.
>
>     shared.x is always issued as Load shared; Load shared.x, no
>     additional barriers are needed in threads that access superclass
>     fields initialized prior to publishing shared=this.
>
>
>     There is one thin case. If, and this is a big if, Load shared.x is
>     allowed to go ahead of Load shared (eg if CPU already loaded that
>     cache line and cache coherency is weak), then there must be a
>     Load-Load barrier between Load shared and Load shared.x, but even
>     this cannot be achieved through a volatile load at the beginning
>     of the method body - because of rule (1) volatile load can go
>     ahead of non-volatile Load shared, and we are back to square one.
>
>     So, a volatile load at the beginning of a method is wrong, and I'd
>     check what JMM says about reordering of dependent normal field x
>     loads and a stronger barrier may be needed after the first load of
>     shared - there certainly is a guarantee about final x. I don't
>     think the data dependency reordering is possible (how can you
>     eliminate Load shared.x before you know what shared refers to),
>     but let's check.
>
>
>     Alex
>
>
>
>     On 09/05/2013 00:33, Vitaly Davidovich wrote:
>>
>>     For JMM, I believe it has to be a read of s_fence everywhere
>>     where you want to then read the normal writes preceding the
>>     volatile store, as Yuval said.  That's how HB is defined.
>>
>>     Sent from my phone
>>
>>     On May 8, 2013 4:57 PM, "oleksandr otenko"
>>     <oleksandr.otenko at oracle.com
>>     <mailto:oleksandr.otenko at oracle.com>> wrote:
>>
>>         No, just a volatile read immediately after the write.
>>
>>         Alex
>>
>>         On 08/05/2013 19:42, Yuval Shavit wrote:
>>>         Is that an artifact of the common/global implementation, or
>>>         the JLS itself? I thought that in order for it to work
>>>         according to the JLS, you would have to read s_fence as the
>>>         first thing within each of MyClass's methods:
>>>
>>>             public void foo() {
>>>                 if (s_fence) throw new AssertionError();
>>>                 doMyWhatever();
>>>             }
>>>
>>>         This would establish a HB before the read and the
>>>         post-initialization write. There's no formal HB without the
>>>         read, right?
>>>
>>>
>>>         On Wed, May 8, 2013 at 2:14 PM, Nathan Reynolds
>>>         <nathan.reynolds at oracle.com
>>>         <mailto:nathan.reynolds at oracle.com>> wrote:
>>>
>>>             A much heavier handed solution would be...
>>>
>>>             class MyClass
>>>             {
>>>                 static volatile boolean s_fence;
>>>
>>>                 public MyClass()
>>>                 {
>>>                     // initialize all fields
>>>                     s_fence = false;
>>>                     // leak "this"
>>>                 }
>>>             }
>>>
>>>             Writing to s_fence guarantees that all of the fields
>>>             will be globally visible before "this" is leaked. This
>>>             is more expensive since it adds a Store-Load memory
>>>             fence rather than just a Store-Store fence.  The
>>>             Store-Store fence will turn into a noop on x86.
>>>
>>>             The other problem with writing to s_fence is that the
>>>             cache line holding it is going to be contended.  If
>>>             enough MyClass objects are created concurrently on
>>>             different cores, cache performance is going to be the
>>>             bottleneck.
>>>
>>>             Nathan Reynolds
>>>             <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>
>>>             | Architect | 602.333.9091 <tel:602.333.9091>
>>>             Oracle PSR Engineering <http://psr.us.oracle.com/> |
>>>             Server Technology
>>>             On 5/8/2013 8:02 AM, Zhong Yu wrote:
>>>>             If the publication is safe, there is nothing to worry
>>>>             about, as long as `this` is leaked after fields are
>>>>             assigned.
>>>>
>>>>             If the publication is unsafe, a trick I would use it to
>>>>             wrap the object reference as a final field, and publish
>>>>             the wrapper
>>>>
>>>>                 class FinalReference<T>
>>>>                     final T referent;
>>>>                     FinalReference(T referent){
>>>>             this.referent=referent; }
>>>>
>>>>                 static FinalReference<Foo> unsafePubVar;
>>>>
>>>>                 Foo foo = ...;
>>>>                 unsafePubVar = new FinalReference(foo);
>>>>
>>>>             forcing the subscription side to go though a final
>>>>             field to access `foo` and its fields.
>>>>
>>>>             Zhong Yu
>>>>
>>>>
>>>>
>>>>             On Wed, May 8, 2013 at 4:27 AM, Peter Firmstone
>>>>             <peter.firmstone at zeus.net.au
>>>>             <mailto:peter.firmstone at zeus.net.au>> wrote:
>>>>
>>>>                 I'm attempting to fix an object (public api) that
>>>>                 uses unsafe construction (lets "this" escape to
>>>>                 other threads via inner classes during construction).
>>>>
>>>>                 To safely construct the object I need to modify the
>>>>                 object to delay publication of the "this" reference
>>>>                 until after construction.
>>>>
>>>>                 I was considering a start() or init() method to do so.
>>>>
>>>>                 However legacy code won't call that method, so I
>>>>                 need a way to invoke start() or init() without
>>>>                 relying on other methods.
>>>>
>>>>                 Having every method check the state either isn't
>>>>                 practical, since legacy code doesn't necessarily
>>>>                 invoke any method calls immediately and the
>>>>                 constructor invokes background threads to perform
>>>>                 work that client code eventually uses.
>>>>
>>>>                 The solution I'm thinking of is to move the
>>>>                 implementation into a package private abstract
>>>>                 superclass, so the object becomes stateless (the
>>>>                 superclass is responsible for state).  Then from
>>>>                 the constructor, call the superclass constructor,
>>>>                 then call the start() method.
>>>>
>>>>                 Is it reasonable to assume that after the
>>>>                 superclass constructor returns, all superclass
>>>>                 final fields are safely published?  So the
>>>>                 stateless child class can call the start() method
>>>>                 from within the constructor safely publishing the
>>>>                 "this" reference after all final fields and the
>>>>                 objects they reference are fully initialized.
>>>>
>>>>                 Thanks in advance,
>>>>
>>>>                 Peter.
>>>>
>>>>
>>>>                 _______________________________________________
>>>>                 Concurrency-interest mailing list
>>>>                 Concurrency-interest at cs.oswego.edu
>>>>                 <mailto:Concurrency-interest at cs.oswego.edu>
>>>>                 http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>>
>>>>
>>>>             _______________________________________________
>>>>             Concurrency-interest mailing list
>>>>             Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>>>             http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>>             _______________________________________________
>>>             Concurrency-interest mailing list
>>>             Concurrency-interest at cs.oswego.edu
>>>             <mailto:Concurrency-interest at cs.oswego.edu>
>>>             http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>>
>>>
>>>         _______________________________________________
>>>         Concurrency-interest mailing list
>>>         Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>         _______________________________________________
>>         Concurrency-interest mailing list
>>         Concurrency-interest at cs.oswego.edu
>>         <mailto:Concurrency-interest at cs.oswego.edu>
>>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130509/5351cb63/attachment-0001.html>

From oleksandr.otenko at oracle.com  Thu May  9 11:30:33 2013
From: oleksandr.otenko at oracle.com (oleksandr otenko)
Date: Thu, 09 May 2013 16:30:33 +0100
Subject: [concurrency-interest] safe construction
In-Reply-To: <CACuKZqGW8E+gX++gVapoisC+FOqxaM_20TfQGPWGwvrve-1KvQ@mail.gmail.com>
References: <518A1A88.10007@zeus.net.au>
	<CACuKZqGjr2qLbPqYes47ZkyCTKuZm=i4k0V+y+DSk_-+C+BCYQ@mail.gmail.com>
	<518A961B.8030509@oracle.com>
	<CAE+h5-A1rbtHBWUtiJOy1aJyH5HdKAZ+mfAZyo-eqbWoLop44w@mail.gmail.com>
	<518ABA58.3060505@oracle.com>
	<CAHjP37Ee=jAR-5rmtFrzZzb475TMSHWsyeOEweE8UKEnPRsgkw@mail.gmail.com>
	<518BA833.10609@oracle.com>
	<CACuKZqGW8E+gX++gVapoisC+FOqxaM_20TfQGPWGwvrve-1KvQ@mail.gmail.com>
Message-ID: <518BC119.4050003@oracle.com>

I deliberately do not include locks here.

Your statement does not violate my world. synchronized has lock acquire 
and lock release. They can be elided for the reason that they can also 
be reordered - the lock acquire may be reordered anywhere into negative 
infinity, because there is no lock release preceding it possible, and 
lock release can be overtaken by any instruction following it - the same 
as moving the lock release into positive infinity because no lock 
acquire on this lock is possible, but I like thinking about motion 
happening in one direction.

Alex

On 09/05/2013 15:50, Zhong Yu wrote:
>
>
>
> On Thu, May 9, 2013 at 8:44 AM, oleksandr otenko 
> <oleksandr.otenko at oracle.com <mailto:oleksandr.otenko at oracle.com>> wrote:
>
>     No, not in this case. In this case "this" is published, and you
>     cannot call public void foo without observing shared==this.
>
>
>     JMM only defines thread-local orderings. The processors don't vote
>     which instructions to reorder.
>
>     "Thread 1 storing a state happens-before Thread 2 reads shared" is
>     wrong kind of reasoning. As if we can tell Thread 2 /when/ to
>     read. Don't try to bend the spoon, it is impossible.
>
>     Same, but put in different words:
>
>     "shared == this /*may*/ be observed" -> "super class state /*is*/
>     consistent" for some definition of consistency.
>
>     Here arrow "->" is implication, and it turns out to be exactly
>     opposite of happens-before.
>
>     "may" deliberately states that we cannot tell what anyone else is
>     doing. "shared == this" deliberately states what condition we
>     observe, not just "any subsequent load of the same variable".
>     (what is "subsequent"? remember, processors don't vote on this. It
>     is a true statement about "any load /that happens to be
>     subsequent/", but that's tautological)
>
>     "is" deliberately states what we guarantee thread-locally.
>
>
>     It is clear that shared==this can not be observed, if we don't
>     store to shared. Now we have everything that is needed to reason
>     about validity of code using only thread-local ordering.
>
>     "shared == this may be observed" -> "shared=this was executed"  //
>     obviously
>     "shared=this was executed" -> "super class state is consistent" //
>     /this/ is happens-before, and it is thread-local
>
>     That's it. Add store-store barrier between shared=this and end of
>     super-class constructor.
>
>
>     Nathan's example is not complete, but not because volatile loads
>     are missing in other methods, but because of three simple rules in
>     JMM:
>
>     1. Volatile load can go ahead of anything non-volatile
>     2. Anything non-volatile can go ahead of volatile store
>     3. Anything non-volatile can go ahead of anything non-volatile
>
>
> Alex, I don't think these rules are in JMM. JMM itself is a little 
> weaker. For example,
>
>     var = 1;   // write#1
>     synchronized(new Object()){}
>     var = 2;  // write#2
>
> the 2nd line can be elided, and the two writes can be reordered, as 
> far as JMM is concerned.
>
> Zhong Yu
>
>
>     So, shared=this may be allowed to go ahead of s_fence=false, if
>     shared is not volatile, and then can be reordered with "super
>     class state is consistent". In order to stop this from happening,
>     you need a volatile load after a volatile store and before
>     shared=this. This way shared=this cannot go ahead of
>     s_fence=false, because volatile load cannot go ahead of
>     s_fence=false, and shared=this cannot go ahead of a volatile load.
>
>     shared.x is always issued as Load shared; Load shared.x, no
>     additional barriers are needed in threads that access superclass
>     fields initialized prior to publishing shared=this.
>
>
>     There is one thin case. If, and this is a big if, Load shared.x is
>     allowed to go ahead of Load shared (eg if CPU already loaded that
>     cache line and cache coherency is weak), then there must be a
>     Load-Load barrier between Load shared and Load shared.x, but even
>     this cannot be achieved through a volatile load at the beginning
>     of the method body - because of rule (1) volatile load can go
>     ahead of non-volatile Load shared, and we are back to square one.
>
>     So, a volatile load at the beginning of a method is wrong, and I'd
>     check what JMM says about reordering of dependent normal field x
>     loads and a stronger barrier may be needed after the first load of
>     shared - there certainly is a guarantee about final x. I don't
>     think the data dependency reordering is possible (how can you
>     eliminate Load shared.x before you know what shared refers to),
>     but let's check.
>
>
>     Alex
>
>
>
>     On 09/05/2013 00:33, Vitaly Davidovich wrote:
>>
>>     For JMM, I believe it has to be a read of s_fence everywhere
>>     where you want to then read the normal writes preceding the
>>     volatile store, as Yuval said.  That's how HB is defined.
>>
>>     Sent from my phone
>>
>>     On May 8, 2013 4:57 PM, "oleksandr otenko"
>>     <oleksandr.otenko at oracle.com
>>     <mailto:oleksandr.otenko at oracle.com>> wrote:
>>
>>         No, just a volatile read immediately after the write.
>>
>>         Alex
>>
>>         On 08/05/2013 19:42, Yuval Shavit wrote:
>>>         Is that an artifact of the common/global implementation, or
>>>         the JLS itself? I thought that in order for it to work
>>>         according to the JLS, you would have to read s_fence as the
>>>         first thing within each of MyClass's methods:
>>>
>>>             public void foo() {
>>>                 if (s_fence) throw new AssertionError();
>>>                 doMyWhatever();
>>>             }
>>>
>>>         This would establish a HB before the read and the
>>>         post-initialization write. There's no formal HB without the
>>>         read, right?
>>>
>>>
>>>         On Wed, May 8, 2013 at 2:14 PM, Nathan Reynolds
>>>         <nathan.reynolds at oracle.com
>>>         <mailto:nathan.reynolds at oracle.com>> wrote:
>>>
>>>             A much heavier handed solution would be...
>>>
>>>             class MyClass
>>>             {
>>>                 static volatile boolean s_fence;
>>>
>>>                 public MyClass()
>>>                 {
>>>                     // initialize all fields
>>>                     s_fence = false;
>>>                     // leak "this"
>>>                 }
>>>             }
>>>
>>>             Writing to s_fence guarantees that all of the fields
>>>             will be globally visible before "this" is leaked.  This
>>>             is more expensive since it adds a Store-Load memory
>>>             fence rather than just a Store-Store fence. The
>>>             Store-Store fence will turn into a noop on x86.
>>>
>>>             The other problem with writing to s_fence is that the
>>>             cache line holding it is going to be contended.  If
>>>             enough MyClass objects are created concurrently on
>>>             different cores, cache performance is going to be the
>>>             bottleneck.
>>>
>>>             Nathan Reynolds
>>>             <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>
>>>             | Architect | 602.333.9091 <tel:602.333.9091>
>>>             Oracle PSR Engineering <http://psr.us.oracle.com/> |
>>>             Server Technology
>>>             On 5/8/2013 8:02 AM, Zhong Yu wrote:
>>>>             If the publication is safe, there is nothing to worry
>>>>             about, as long as `this` is leaked after fields are
>>>>             assigned.
>>>>
>>>>             If the publication is unsafe, a trick I would use it to
>>>>             wrap the object reference as a final field, and publish
>>>>             the wrapper
>>>>
>>>>                 class FinalReference<T>
>>>>                     final T referent;
>>>>                     FinalReference(T referent){
>>>>             this.referent=referent; }
>>>>
>>>>                 static FinalReference<Foo> unsafePubVar;
>>>>
>>>>                 Foo foo = ...;
>>>>                 unsafePubVar = new FinalReference(foo);
>>>>
>>>>             forcing the subscription side to go though a final
>>>>             field to access `foo` and its fields.
>>>>
>>>>             Zhong Yu
>>>>
>>>>
>>>>
>>>>             On Wed, May 8, 2013 at 4:27 AM, Peter Firmstone
>>>>             <peter.firmstone at zeus.net.au
>>>>             <mailto:peter.firmstone at zeus.net.au>> wrote:
>>>>
>>>>                 I'm attempting to fix an object (public api) that
>>>>                 uses unsafe construction (lets "this" escape to
>>>>                 other threads via inner classes during construction).
>>>>
>>>>                 To safely construct the object I need to modify the
>>>>                 object to delay publication of the "this" reference
>>>>                 until after construction.
>>>>
>>>>                 I was considering a start() or init() method to do so.
>>>>
>>>>                 However legacy code won't call that method, so I
>>>>                 need a way to invoke start() or init() without
>>>>                 relying on other methods.
>>>>
>>>>                 Having every method check the state either isn't
>>>>                 practical, since legacy code doesn't necessarily
>>>>                 invoke any method calls immediately and the
>>>>                 constructor invokes background threads to perform
>>>>                 work that client code eventually uses.
>>>>
>>>>                 The solution I'm thinking of is to move the
>>>>                 implementation into a package private abstract
>>>>                 superclass, so the object becomes stateless (the
>>>>                 superclass is responsible for state).  Then from
>>>>                 the constructor, call the superclass constructor,
>>>>                 then call the start() method.
>>>>
>>>>                 Is it reasonable to assume that after the
>>>>                 superclass constructor returns, all superclass
>>>>                 final fields are safely published?  So the
>>>>                 stateless child class can call the start() method
>>>>                 from within the constructor safely publishing the
>>>>                 "this" reference after all final fields and the
>>>>                 objects they reference are fully initialized.
>>>>
>>>>                 Thanks in advance,
>>>>
>>>>                 Peter.
>>>>
>>>>
>>>>                 _______________________________________________
>>>>                 Concurrency-interest mailing list
>>>>                 Concurrency-interest at cs.oswego.edu
>>>>                 <mailto:Concurrency-interest at cs.oswego.edu>
>>>>                 http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>>
>>>>
>>>>             _______________________________________________
>>>>             Concurrency-interest mailing list
>>>>             Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>>>             http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>>             _______________________________________________
>>>             Concurrency-interest mailing list
>>>             Concurrency-interest at cs.oswego.edu
>>>             <mailto:Concurrency-interest at cs.oswego.edu>
>>>             http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>>
>>>
>>>         _______________________________________________
>>>         Concurrency-interest mailing list
>>>         Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>         _______________________________________________
>>         Concurrency-interest mailing list
>>         Concurrency-interest at cs.oswego.edu
>>         <mailto:Concurrency-interest at cs.oswego.edu>
>>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130509/9e9aeafe/attachment-0001.html>

From oleksandr.otenko at oracle.com  Thu May  9 12:11:27 2013
From: oleksandr.otenko at oracle.com (oleksandr otenko)
Date: Thu, 09 May 2013 17:11:27 +0100
Subject: [concurrency-interest] safe construction
In-Reply-To: <518BC119.4050003@oracle.com>
References: <518A1A88.10007@zeus.net.au>
	<CACuKZqGjr2qLbPqYes47ZkyCTKuZm=i4k0V+y+DSk_-+C+BCYQ@mail.gmail.com>
	<518A961B.8030509@oracle.com>
	<CAE+h5-A1rbtHBWUtiJOy1aJyH5HdKAZ+mfAZyo-eqbWoLop44w@mail.gmail.com>
	<518ABA58.3060505@oracle.com>
	<CAHjP37Ee=jAR-5rmtFrzZzb475TMSHWsyeOEweE8UKEnPRsgkw@mail.gmail.com>
	<518BA833.10609@oracle.com>
	<CACuKZqGW8E+gX++gVapoisC+FOqxaM_20TfQGPWGwvrve-1KvQ@mail.gmail.com>
	<518BC119.4050003@oracle.com>
Message-ID: <518BCAAF.4070808@oracle.com>

Plus the barriers and serialization points can be reordered only to some 
extent - until other lock acquires or volatile accesses. But that 
doesn't matter for this example.

Alex

On 09/05/2013 16:30, oleksandr otenko wrote:
> I deliberately do not include locks here.
>
> Your statement does not violate my world. synchronized has lock 
> acquire and lock release. They can be elided for the reason that they 
> can also be reordered - the lock acquire may be reordered anywhere 
> into negative infinity, because there is no lock release preceding it 
> possible, and lock release can be overtaken by any instruction 
> following it - the same as moving the lock release into positive 
> infinity because no lock acquire on this lock is possible, but I like 
> thinking about motion happening in one direction.
>
> Alex
>
> On 09/05/2013 15:50, Zhong Yu wrote:
>>
>>
>>
>> On Thu, May 9, 2013 at 8:44 AM, oleksandr otenko 
>> <oleksandr.otenko at oracle.com <mailto:oleksandr.otenko at oracle.com>> wrote:
>>
>>     No, not in this case. In this case "this" is published, and you
>>     cannot call public void foo without observing shared==this.
>>
>>
>>     JMM only defines thread-local orderings. The processors don't
>>     vote which instructions to reorder.
>>
>>     "Thread 1 storing a state happens-before Thread 2 reads shared"
>>     is wrong kind of reasoning. As if we can tell Thread 2 /when/ to
>>     read. Don't try to bend the spoon, it is impossible.
>>
>>     Same, but put in different words:
>>
>>     "shared == this /*may*/ be observed" -> "super class state /*is*/
>>     consistent" for some definition of consistency.
>>
>>     Here arrow "->" is implication, and it turns out to be exactly
>>     opposite of happens-before.
>>
>>     "may" deliberately states that we cannot tell what anyone else is
>>     doing. "shared == this" deliberately states what condition we
>>     observe, not just "any subsequent load of the same variable".
>>     (what is "subsequent"? remember, processors don't vote on this.
>>     It is a true statement about "any load /that happens to be
>>     subsequent/", but that's tautological)
>>
>>     "is" deliberately states what we guarantee thread-locally.
>>
>>
>>     It is clear that shared==this can not be observed, if we don't
>>     store to shared. Now we have everything that is needed to reason
>>     about validity of code using only thread-local ordering.
>>
>>     "shared == this may be observed" -> "shared=this was executed" 
>>     // obviously
>>     "shared=this was executed" -> "super class state is consistent"
>>     // /this/ is happens-before, and it is thread-local
>>
>>     That's it. Add store-store barrier between shared=this and end of
>>     super-class constructor.
>>
>>
>>     Nathan's example is not complete, but not because volatile loads
>>     are missing in other methods, but because of three simple rules
>>     in JMM:
>>
>>     1. Volatile load can go ahead of anything non-volatile
>>     2. Anything non-volatile can go ahead of volatile store
>>     3. Anything non-volatile can go ahead of anything non-volatile
>>
>>
>> Alex, I don't think these rules are in JMM. JMM itself is a little 
>> weaker. For example,
>>
>>     var = 1;   // write#1
>>     synchronized(new Object()){}
>>     var = 2;  // write#2
>>
>> the 2nd line can be elided, and the two writes can be reordered, as 
>> far as JMM is concerned.
>>
>> Zhong Yu
>>
>>
>>     So, shared=this may be allowed to go ahead of s_fence=false, if
>>     shared is not volatile, and then can be reordered with "super
>>     class state is consistent". In order to stop this from happening,
>>     you need a volatile load after a volatile store and before
>>     shared=this. This way shared=this cannot go ahead of
>>     s_fence=false, because volatile load cannot go ahead of
>>     s_fence=false, and shared=this cannot go ahead of a volatile load.
>>
>>     shared.x is always issued as Load shared; Load shared.x, no
>>     additional barriers are needed in threads that access superclass
>>     fields initialized prior to publishing shared=this.
>>
>>
>>     There is one thin case. If, and this is a big if, Load shared.x
>>     is allowed to go ahead of Load shared (eg if CPU already loaded
>>     that cache line and cache coherency is weak), then there must be
>>     a Load-Load barrier between Load shared and Load shared.x, but
>>     even this cannot be achieved through a volatile load at the
>>     beginning of the method body - because of rule (1) volatile load
>>     can go ahead of non-volatile Load shared, and we are back to
>>     square one.
>>
>>     So, a volatile load at the beginning of a method is wrong, and
>>     I'd check what JMM says about reordering of dependent normal
>>     field x loads and a stronger barrier may be needed after the
>>     first load of shared - there certainly is a guarantee about final
>>     x. I don't think the data dependency reordering is possible (how
>>     can you eliminate Load shared.x before you know what shared
>>     refers to), but let's check.
>>
>>
>>     Alex
>>
>>
>>
>>     On 09/05/2013 00:33, Vitaly Davidovich wrote:
>>>
>>>     For JMM, I believe it has to be a read of s_fence everywhere
>>>     where you want to then read the normal writes preceding the
>>>     volatile store, as Yuval said.  That's how HB is defined.
>>>
>>>     Sent from my phone
>>>
>>>     On May 8, 2013 4:57 PM, "oleksandr otenko"
>>>     <oleksandr.otenko at oracle.com
>>>     <mailto:oleksandr.otenko at oracle.com>> wrote:
>>>
>>>         No, just a volatile read immediately after the write.
>>>
>>>         Alex
>>>
>>>         On 08/05/2013 19:42, Yuval Shavit wrote:
>>>>         Is that an artifact of the common/global implementation, or
>>>>         the JLS itself? I thought that in order for it to work
>>>>         according to the JLS, you would have to read s_fence as the
>>>>         first thing within each of MyClass's methods:
>>>>
>>>>             public void foo() {
>>>>                 if (s_fence) throw new AssertionError();
>>>>                 doMyWhatever();
>>>>             }
>>>>
>>>>         This would establish a HB before the read and the
>>>>         post-initialization write. There's no formal HB without the
>>>>         read, right?
>>>>
>>>>
>>>>         On Wed, May 8, 2013 at 2:14 PM, Nathan Reynolds
>>>>         <nathan.reynolds at oracle.com
>>>>         <mailto:nathan.reynolds at oracle.com>> wrote:
>>>>
>>>>             A much heavier handed solution would be...
>>>>
>>>>             class MyClass
>>>>             {
>>>>                 static volatile boolean s_fence;
>>>>
>>>>                 public MyClass()
>>>>                 {
>>>>                     // initialize all fields
>>>>                     s_fence = false;
>>>>                     // leak "this"
>>>>                 }
>>>>             }
>>>>
>>>>             Writing to s_fence guarantees that all of the fields
>>>>             will be globally visible before "this" is leaked.  This
>>>>             is more expensive since it adds a Store-Load memory
>>>>             fence rather than just a Store-Store fence.  The
>>>>             Store-Store fence will turn into a noop on x86.
>>>>
>>>>             The other problem with writing to s_fence is that the
>>>>             cache line holding it is going to be contended.  If
>>>>             enough MyClass objects are created concurrently on
>>>>             different cores, cache performance is going to be the
>>>>             bottleneck.
>>>>
>>>>             Nathan Reynolds
>>>>             <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>
>>>>             | Architect | 602.333.9091 <tel:602.333.9091>
>>>>             Oracle PSR Engineering <http://psr.us.oracle.com/> |
>>>>             Server Technology
>>>>             On 5/8/2013 8:02 AM, Zhong Yu wrote:
>>>>>             If the publication is safe, there is nothing to worry
>>>>>             about, as long as `this` is leaked after fields are
>>>>>             assigned.
>>>>>
>>>>>             If the publication is unsafe, a trick I would use it
>>>>>             to wrap the object reference as a final field, and
>>>>>             publish the wrapper
>>>>>
>>>>>                 class FinalReference<T>
>>>>>                     final T referent;
>>>>>             FinalReference(T referent){ this.referent=referent; }
>>>>>
>>>>>                 static FinalReference<Foo> unsafePubVar;
>>>>>
>>>>>                 Foo foo = ...;
>>>>>                 unsafePubVar = new FinalReference(foo);
>>>>>
>>>>>             forcing the subscription side to go though a final
>>>>>             field to access `foo` and its fields.
>>>>>
>>>>>             Zhong Yu
>>>>>
>>>>>
>>>>>
>>>>>             On Wed, May 8, 2013 at 4:27 AM, Peter Firmstone
>>>>>             <peter.firmstone at zeus.net.au
>>>>>             <mailto:peter.firmstone at zeus.net.au>> wrote:
>>>>>
>>>>>                 I'm attempting to fix an object (public api) that
>>>>>                 uses unsafe construction (lets "this" escape to
>>>>>                 other threads via inner classes during construction).
>>>>>
>>>>>                 To safely construct the object I need to modify
>>>>>                 the object to delay publication of the "this"
>>>>>                 reference until after construction.
>>>>>
>>>>>                 I was considering a start() or init() method to do so.
>>>>>
>>>>>                 However legacy code won't call that method, so I
>>>>>                 need a way to invoke start() or init() without
>>>>>                 relying on other methods.
>>>>>
>>>>>                 Having every method check the state either isn't
>>>>>                 practical, since legacy code doesn't necessarily
>>>>>                 invoke any method calls immediately and the
>>>>>                 constructor invokes background threads to perform
>>>>>                 work that client code eventually uses.
>>>>>
>>>>>                 The solution I'm thinking of is to move the
>>>>>                 implementation into a package private abstract
>>>>>                 superclass, so the object becomes stateless (the
>>>>>                 superclass is responsible for state).  Then from
>>>>>                 the constructor, call the superclass constructor,
>>>>>                 then call the start() method.
>>>>>
>>>>>                 Is it reasonable to assume that after the
>>>>>                 superclass constructor returns, all superclass
>>>>>                 final fields are safely published?  So the
>>>>>                 stateless child class can call the start() method
>>>>>                 from within the constructor safely publishing the
>>>>>                 "this" reference after all final fields and the
>>>>>                 objects they reference are fully initialized.
>>>>>
>>>>>                 Thanks in advance,
>>>>>
>>>>>                 Peter.
>>>>>
>>>>>
>>>>>                 _______________________________________________
>>>>>                 Concurrency-interest mailing list
>>>>>                 Concurrency-interest at cs.oswego.edu
>>>>>                 <mailto:Concurrency-interest at cs.oswego.edu>
>>>>>                 http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>             _______________________________________________
>>>>>             Concurrency-interest mailing list
>>>>>             Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>>>>             http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>>             _______________________________________________
>>>>             Concurrency-interest mailing list
>>>>             Concurrency-interest at cs.oswego.edu
>>>>             <mailto:Concurrency-interest at cs.oswego.edu>
>>>>             http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>>
>>>>
>>>>         _______________________________________________
>>>>         Concurrency-interest mailing list
>>>>         Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>>>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>>         _______________________________________________
>>>         Concurrency-interest mailing list
>>>         Concurrency-interest at cs.oswego.edu
>>>         <mailto:Concurrency-interest at cs.oswego.edu>
>>>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>
>>
>>     _______________________________________________
>>     Concurrency-interest mailing list
>>     Concurrency-interest at cs.oswego.edu
>>     <mailto:Concurrency-interest at cs.oswego.edu>
>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130509/84b9e1eb/attachment-0001.html>

From yankee.sierra at gmail.com  Thu May  9 12:23:59 2013
From: yankee.sierra at gmail.com (Yuval Shavit)
Date: Thu, 9 May 2013 12:23:59 -0400
Subject: [concurrency-interest] safe construction
In-Reply-To: <518BBF96.6000007@oracle.com>
References: <518A1A88.10007@zeus.net.au>
	<CACuKZqGjr2qLbPqYes47ZkyCTKuZm=i4k0V+y+DSk_-+C+BCYQ@mail.gmail.com>
	<518A961B.8030509@oracle.com>
	<CAE+h5-A1rbtHBWUtiJOy1aJyH5HdKAZ+mfAZyo-eqbWoLop44w@mail.gmail.com>
	<518ABA58.3060505@oracle.com>
	<CAHjP37Ee=jAR-5rmtFrzZzb475TMSHWsyeOEweE8UKEnPRsgkw@mail.gmail.com>
	<518BA833.10609@oracle.com>
	<CAE+h5-AVw_Xtd62gVnafTXYTdVpr==sqptkpAXHUmHpDxfdaYQ@mail.gmail.com>
	<518BBF96.6000007@oracle.com>
Message-ID: <CAE+h5-C8E-BdRDWtJKZOSM3XySmmt=_a+Cw9qzsOtj6F-QX3Ew@mail.gmail.com>

On Thu, May 9, 2013 at 11:24 AM, oleksandr otenko <
oleksandr.otenko at oracle.com> wrote:

>
> So the volatile loads, loops and other tricks are completely unnecessary
> on the side consuming the value of shared, unless we can hear of a platform
> that can guess shared.x before loading shared.
>

I thought the issue is that, having loaded shared, we need some way of
establishing which value of shared.x thread B sees -- the default value or
the one set in the constructor. Final field semantics don't apply, since
they're negated by the fact that "this" is shared. So, the question is, how
can we ensure that thread B sees a value of shared.x that was assigned to
it in the constructor, and not just its default value. And if shared.x is a
reference variable, how can we also ensure that the object it points to
will also be seen fully-formed (as it was at shared's constructor).
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130509/c0c7c9f0/attachment.html>

From oleksandr.otenko at oracle.com  Thu May  9 12:43:06 2013
From: oleksandr.otenko at oracle.com (oleksandr otenko)
Date: Thu, 09 May 2013 17:43:06 +0100
Subject: [concurrency-interest] safe construction
In-Reply-To: <CAE+h5-C8E-BdRDWtJKZOSM3XySmmt=_a+Cw9qzsOtj6F-QX3Ew@mail.gmail.com>
References: <518A1A88.10007@zeus.net.au>
	<CACuKZqGjr2qLbPqYes47ZkyCTKuZm=i4k0V+y+DSk_-+C+BCYQ@mail.gmail.com>
	<518A961B.8030509@oracle.com>
	<CAE+h5-A1rbtHBWUtiJOy1aJyH5HdKAZ+mfAZyo-eqbWoLop44w@mail.gmail.com>
	<518ABA58.3060505@oracle.com>
	<CAHjP37Ee=jAR-5rmtFrzZzb475TMSHWsyeOEweE8UKEnPRsgkw@mail.gmail.com>
	<518BA833.10609@oracle.com>
	<CAE+h5-AVw_Xtd62gVnafTXYTdVpr==sqptkpAXHUmHpDxfdaYQ@mail.gmail.com>
	<518BBF96.6000007@oracle.com>
	<CAE+h5-C8E-BdRDWtJKZOSM3XySmmt=_a+Cw9qzsOtj6F-QX3Ew@mail.gmail.com>
Message-ID: <518BD21A.9000400@oracle.com>

If Load shared.x is issued, it will read the value of shared.x. So the 
question is, will Load shared.x be issued. And the answer is that at 
least one Load shared.x is issued after Load shared, because that'd be 
the single-threaded semantics of it.

Then the second question is, can Load shared.x be reordered with Load 
shared. And the answer to that is we must find a platform where the 
processor can guess the address of shared.x before loading shared.

Whatever the answer to any of the above, volatile load at the start of 
the method is not a valid barrier, because a volatile load can go ahead 
of plain loads - that is, it can go ahead of Load shared.

Alex

On 09/05/2013 17:23, Yuval Shavit wrote:
> On Thu, May 9, 2013 at 11:24 AM, oleksandr otenko 
> <oleksandr.otenko at oracle.com <mailto:oleksandr.otenko at oracle.com>> wrote:
>
>
>     So the volatile loads, loops and other tricks are completely
>     unnecessary on the side consuming the value of shared, unless we
>     can hear of a platform that can guess shared.x before loading shared.
>
>
> I thought the issue is that, having loaded shared, we need some way of 
> establishing which value of shared.x thread B sees -- the default 
> value or the one set in the constructor. Final field semantics don't 
> apply, since they're negated by the fact that "this" is shared. So, 
> the question is, how can we ensure that thread B sees a value of 
> shared.x that was assigned to it in the constructor, and not just its 
> default value. And if shared.x is a reference variable, how can we 
> also ensure that the object it points to will also be seen 
> fully-formed (as it was at shared's constructor).

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130509/edc39175/attachment.html>

From yankee.sierra at gmail.com  Thu May  9 12:51:17 2013
From: yankee.sierra at gmail.com (Yuval Shavit)
Date: Thu, 9 May 2013 12:51:17 -0400
Subject: [concurrency-interest] safe construction
In-Reply-To: <518BD21A.9000400@oracle.com>
References: <518A1A88.10007@zeus.net.au>
	<CACuKZqGjr2qLbPqYes47ZkyCTKuZm=i4k0V+y+DSk_-+C+BCYQ@mail.gmail.com>
	<518A961B.8030509@oracle.com>
	<CAE+h5-A1rbtHBWUtiJOy1aJyH5HdKAZ+mfAZyo-eqbWoLop44w@mail.gmail.com>
	<518ABA58.3060505@oracle.com>
	<CAHjP37Ee=jAR-5rmtFrzZzb475TMSHWsyeOEweE8UKEnPRsgkw@mail.gmail.com>
	<518BA833.10609@oracle.com>
	<CAE+h5-AVw_Xtd62gVnafTXYTdVpr==sqptkpAXHUmHpDxfdaYQ@mail.gmail.com>
	<518BBF96.6000007@oracle.com>
	<CAE+h5-C8E-BdRDWtJKZOSM3XySmmt=_a+Cw9qzsOtj6F-QX3Ew@mail.gmail.com>
	<518BD21A.9000400@oracle.com>
Message-ID: <CAE+h5-CXBtT6gFhmL+08SYvh=ZTvA4hsimVK9qXzC_4E1pgJGA@mail.gmail.com>

Well but again, you seem to be speaking in terms of the JVM implementation,
not the JMM as defined in the JLS. I don't think the latter talks about
Load or Store at all, just about observable behavior. Apologies if I've
misunderstood.


On Thu, May 9, 2013 at 12:43 PM, oleksandr otenko <
oleksandr.otenko at oracle.com> wrote:

>  If Load shared.x is issued, it will read the value of shared.x. So the
> question is, will Load shared.x be issued. And the answer is that at least
> one Load shared.x is issued after Load shared, because that'd be the
> single-threaded semantics of it.
>
> Then the second question is, can Load shared.x be reordered with Load
> shared. And the answer to that is we must find a platform where the
> processor can guess the address of shared.x before loading shared.
>
> Whatever the answer to any of the above, volatile load at the start of the
> method is not a valid barrier, because a volatile load can go ahead of
> plain loads - that is, it can go ahead of Load shared.
>
> Alex
>
>
> On 09/05/2013 17:23, Yuval Shavit wrote:
>
> On Thu, May 9, 2013 at 11:24 AM, oleksandr otenko <
> oleksandr.otenko at oracle.com> wrote:
>
>>
>> So the volatile loads, loops and other tricks are completely unnecessary
>> on the side consuming the value of shared, unless we can hear of a platform
>> that can guess shared.x before loading shared.
>>
>
>  I thought the issue is that, having loaded shared, we need some way of
> establishing which value of shared.x thread B sees -- the default value or
> the one set in the constructor. Final field semantics don't apply, since
> they're negated by the fact that "this" is shared. So, the question is, how
> can we ensure that thread B sees a value of shared.x that was assigned to
> it in the constructor, and not just its default value. And if shared.x is a
> reference variable, how can we also ensure that the object it points to
> will also be seen fully-formed (as it was at shared's constructor).
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130509/3af32d86/attachment.html>

From nathan.reynolds at oracle.com  Thu May  9 13:54:22 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Thu, 09 May 2013 10:54:22 -0700
Subject: [concurrency-interest] safe construction
In-Reply-To: <518BD21A.9000400@oracle.com>
References: <518A1A88.10007@zeus.net.au>
	<CACuKZqGjr2qLbPqYes47ZkyCTKuZm=i4k0V+y+DSk_-+C+BCYQ@mail.gmail.com>
	<518A961B.8030509@oracle.com>
	<CAE+h5-A1rbtHBWUtiJOy1aJyH5HdKAZ+mfAZyo-eqbWoLop44w@mail.gmail.com>
	<518ABA58.3060505@oracle.com>
	<CAHjP37Ee=jAR-5rmtFrzZzb475TMSHWsyeOEweE8UKEnPRsgkw@mail.gmail.com>
	<518BA833.10609@oracle.com>
	<CAE+h5-AVw_Xtd62gVnafTXYTdVpr==sqptkpAXHUmHpDxfdaYQ@mail.gmail.com>
	<518BBF96.6000007@oracle.com>
	<CAE+h5-C8E-BdRDWtJKZOSM3XySmmt=_a+Cw9qzsOtj6F-QX3Ew@mail.gmail.com>
	<518BD21A.9000400@oracle.com>
Message-ID: <518BE2CE.5080704@oracle.com>

 > Then the second question is, can Load shared.x be reordered with Load 
shared. And the answer to that is we must find a platform where the 
processor can guess the address of shared.x before loading shared.

The only platform where I have heard this is possible is the DEC Alpha 
processor.  Implementing the JMM on such a processor would be a lot of 
fun since very few barriers would turn into noops.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Architect | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 5/9/2013 9:43 AM, oleksandr otenko wrote:
> If Load shared.x is issued, it will read the value of shared.x. So the 
> question is, will Load shared.x be issued. And the answer is that at 
> least one Load shared.x is issued after Load shared, because that'd be 
> the single-threaded semantics of it.
>
> Then the second question is, can Load shared.x be reordered with Load 
> shared. And the answer to that is we must find a platform where the 
> processor can guess the address of shared.x before loading shared.
>
> Whatever the answer to any of the above, volatile load at the start of 
> the method is not a valid barrier, because a volatile load can go 
> ahead of plain loads - that is, it can go ahead of Load shared.
>
> Alex
>
> On 09/05/2013 17:23, Yuval Shavit wrote:
>> On Thu, May 9, 2013 at 11:24 AM, oleksandr otenko 
>> <oleksandr.otenko at oracle.com <mailto:oleksandr.otenko at oracle.com>> 
>> wrote:
>>
>>
>>     So the volatile loads, loops and other tricks are completely
>>     unnecessary on the side consuming the value of shared, unless we
>>     can hear of a platform that can guess shared.x before loading shared.
>>
>>
>> I thought the issue is that, having loaded shared, we need some way 
>> of establishing which value of shared.x thread B sees -- the default 
>> value or the one set in the constructor. Final field semantics don't 
>> apply, since they're negated by the fact that "this" is shared. So, 
>> the question is, how can we ensure that thread B sees a value of 
>> shared.x that was assigned to it in the constructor, and not just its 
>> default value. And if shared.x is a reference variable, how can we 
>> also ensure that the object it points to will also be seen 
>> fully-formed (as it was at shared's constructor).
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130509/97428d29/attachment-0001.html>

From oleksandr.otenko at oracle.com  Thu May  9 14:31:02 2013
From: oleksandr.otenko at oracle.com (oleksandr otenko)
Date: Thu, 09 May 2013 19:31:02 +0100
Subject: [concurrency-interest] safe construction
In-Reply-To: <518BE2CE.5080704@oracle.com>
References: <518A1A88.10007@zeus.net.au>
	<CACuKZqGjr2qLbPqYes47ZkyCTKuZm=i4k0V+y+DSk_-+C+BCYQ@mail.gmail.com>
	<518A961B.8030509@oracle.com>
	<CAE+h5-A1rbtHBWUtiJOy1aJyH5HdKAZ+mfAZyo-eqbWoLop44w@mail.gmail.com>
	<518ABA58.3060505@oracle.com>
	<CAHjP37Ee=jAR-5rmtFrzZzb475TMSHWsyeOEweE8UKEnPRsgkw@mail.gmail.com>
	<518BA833.10609@oracle.com>
	<CAE+h5-AVw_Xtd62gVnafTXYTdVpr==sqptkpAXHUmHpDxfdaYQ@mail.gmail.com>
	<518BBF96.6000007@oracle.com>
	<CAE+h5-C8E-BdRDWtJKZOSM3XySmmt=_a+Cw9qzsOtj6F-QX3Ew@mail.gmail.com>
	<518BD21A.9000400@oracle.com> <518BE2CE.5080704@oracle.com>
Message-ID: <518BEB66.40003@oracle.com>

Excellent case for a stronger barrier. :)

Alex

On 09/05/2013 18:54, Nathan Reynolds wrote:
> > Then the second question is, can Load shared.x be reordered with 
> Load shared. And the answer to that is we must find a platform where 
> the processor can guess the address of shared.x before loading shared.
>
> The only platform where I have heard this is possible is the DEC Alpha 
> processor.  Implementing the JMM on such a processor would be a lot of 
> fun since very few barriers would turn into noops.
>
> Nathan Reynolds 
> <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
> Architect | 602.333.9091
> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
> On 5/9/2013 9:43 AM, oleksandr otenko wrote:
>> If Load shared.x is issued, it will read the value of shared.x. So 
>> the question is, will Load shared.x be issued. And the answer is that 
>> at least one Load shared.x is issued after Load shared, because 
>> that'd be the single-threaded semantics of it.
>>
>> Then the second question is, can Load shared.x be reordered with Load 
>> shared. And the answer to that is we must find a platform where the 
>> processor can guess the address of shared.x before loading shared.
>>
>> Whatever the answer to any of the above, volatile load at the start 
>> of the method is not a valid barrier, because a volatile load can go 
>> ahead of plain loads - that is, it can go ahead of Load shared.
>>
>> Alex
>>
>> On 09/05/2013 17:23, Yuval Shavit wrote:
>>> On Thu, May 9, 2013 at 11:24 AM, oleksandr otenko 
>>> <oleksandr.otenko at oracle.com <mailto:oleksandr.otenko at oracle.com>> 
>>> wrote:
>>>
>>>
>>>     So the volatile loads, loops and other tricks are completely
>>>     unnecessary on the side consuming the value of shared, unless we
>>>     can hear of a platform that can guess shared.x before loading
>>>     shared.
>>>
>>>
>>> I thought the issue is that, having loaded shared, we need some way 
>>> of establishing which value of shared.x thread B sees -- the default 
>>> value or the one set in the constructor. Final field semantics don't 
>>> apply, since they're negated by the fact that "this" is shared. So, 
>>> the question is, how can we ensure that thread B sees a value of 
>>> shared.x that was assigned to it in the constructor, and not just 
>>> its default value. And if shared.x is a reference variable, how can 
>>> we also ensure that the object it points to will also be seen 
>>> fully-formed (as it was at shared's constructor).
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130509/a3d4568a/attachment.html>

From davidcholmes at aapt.net.au  Thu May  9 18:43:44 2013
From: davidcholmes at aapt.net.au (David Holmes)
Date: Fri, 10 May 2013 08:43:44 +1000
Subject: [concurrency-interest] safe construction
In-Reply-To: <CACuKZqGW8E+gX++gVapoisC+FOqxaM_20TfQGPWGwvrve-1KvQ@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCEEOMJMAA.davidcholmes@aapt.net.au>

You can elide the monitor acquisition but you can't elide the happens-before
edges.

David
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Zhong Yu
  Sent: Friday, 10 May 2013 12:50 AM
  To: oleksandr otenko
  Cc: Concurrency-interest at cs.oswego.edu
  Subject: Re: [concurrency-interest] safe construction







  On Thu, May 9, 2013 at 8:44 AM, oleksandr otenko
<oleksandr.otenko at oracle.com> wrote:

    No, not in this case. In this case "this" is published, and you cannot
call public void foo without observing shared==this.


    JMM only defines thread-local orderings. The processors don't vote which
instructions to reorder.

    "Thread 1 storing a state happens-before Thread 2 reads shared" is wrong
kind of reasoning. As if we can tell Thread 2 when to read. Don't try to
bend the spoon, it is impossible.

    Same, but put in different words:

    "shared == this may be observed" -> "super class state is consistent"
for some definition of consistency.

    Here arrow "->" is implication, and it turns out to be exactly opposite
of happens-before.

    "may" deliberately states that we cannot tell what anyone else is doing.
"shared == this" deliberately states what condition we observe, not just
"any subsequent load of the same variable". (what is "subsequent"? remember,
processors don't vote on this. It is a true statement about "any load that
happens to be subsequent", but that's tautological)

    "is" deliberately states what we guarantee thread-locally.


    It is clear that shared==this can not be observed, if we don't store to
shared. Now we have everything that is needed to reason about validity of
code using only thread-local ordering.

    "shared == this may be observed" -> "shared=this was executed"  //
obviously
    "shared=this was executed" -> "super class state is consistent" // this
is happens-before, and it is thread-local

    That's it. Add store-store barrier between shared=this and end of
super-class constructor.


    Nathan's example is not complete, but not because volatile loads are
missing in other methods, but because of three simple rules in JMM:

    1. Volatile load can go ahead of anything non-volatile
    2. Anything non-volatile can go ahead of volatile store
    3. Anything non-volatile can go ahead of anything non-volatile



  Alex, I don't think these rules are in JMM. JMM itself is a little weaker.
For example,


      var = 1;   // write#1

      synchronized(new Object()){}

      var = 2;  // write#2


  the 2nd line can be elided, and the two writes can be reordered, as far as
JMM is concerned.



  Zhong Yu





    So, shared=this may be allowed to go ahead of s_fence=false, if shared
is not volatile, and then can be reordered with "super class state is
consistent". In order to stop this from happening, you need a volatile load
after a volatile store and before shared=this. This way shared=this cannot
go ahead of s_fence=false, because volatile load cannot go ahead of
s_fence=false, and shared=this cannot go ahead of a volatile load.

    shared.x is always issued as Load shared; Load shared.x, no additional
barriers are needed in threads that access superclass fields initialized
prior to publishing shared=this.


    There is one thin case. If, and this is a big if, Load shared.x is
allowed to go ahead of Load shared (eg if CPU already loaded that cache line
and cache coherency is weak), then there must be a Load-Load barrier between
Load shared and Load shared.x, but even this cannot be achieved through a
volatile load at the beginning of the method body - because of rule (1)
volatile load can go ahead of non-volatile Load shared, and we are back to
square one.

    So, a volatile load at the beginning of a method is wrong, and I'd check
what JMM says about reordering of dependent normal field x loads and a
stronger barrier may be needed after the first load of shared - there
certainly is a guarantee about final x. I don't think the data dependency
reordering is possible (how can you eliminate Load shared.x before you know
what shared refers to), but let's check.


    Alex




    On 09/05/2013 00:33, Vitaly Davidovich wrote:

      For JMM, I believe it has to be a read of s_fence everywhere where you
want to then read the normal writes preceding the volatile store, as Yuval
said.  That's how HB is defined.

      Sent from my phone

      On May 8, 2013 4:57 PM, "oleksandr otenko"
<oleksandr.otenko at oracle.com> wrote:

        No, just a volatile read immediately after the write.

        Alex


        On 08/05/2013 19:42, Yuval Shavit wrote:

          Is that an artifact of the common/global implementation, or the
JLS itself? I thought that in order for it to work according to the JLS, you
would have to read s_fence as the first thing within each of MyClass's
methods:


              public void foo() {
                  if (s_fence) throw new AssertionError();
                  doMyWhatever();
              }


          This would establish a HB before the read and the
post-initialization write. There's no formal HB without the read, right?



          On Wed, May 8, 2013 at 2:14 PM, Nathan Reynolds
<nathan.reynolds at oracle.com> wrote:

            A much heavier handed solution would be...

            class MyClass
            {
                static volatile boolean s_fence;

                public MyClass()
                {
                    // initialize all fields
                    s_fence = false;
                    // leak "this"
                }
            }

            Writing to s_fence guarantees that all of the fields will be
globally visible before "this" is leaked.  This is more expensive since it
adds a Store-Load memory fence rather than just a Store-Store fence.  The
Store-Store fence will turn into a noop on x86.

            The other problem with writing to s_fence is that the cache line
holding it is going to be contended.  If enough MyClass objects are created
concurrently on different cores, cache performance is going to be the
bottleneck.


            Nathan Reynolds | Architect | 602.333.9091
            Oracle PSR Engineering | Server Technology

            On 5/8/2013 8:02 AM, Zhong Yu wrote:

              If the publication is safe, there is nothing to worry about,
as long as `this` is leaked after fields are assigned.


              If the publication is unsafe, a trick I would use it to wrap
the object reference as a final field, and publish the wrapper


                  class FinalReference<T>

                      final T referent;
                      FinalReference(T referent){ this.referent=referent; }

                  static FinalReference<Foo> unsafePubVar;


                  Foo foo = ...;

                  unsafePubVar = new FinalReference(foo);


              forcing the subscription side to go though a final field to
access `foo` and its fields.



              Zhong Yu






              On Wed, May 8, 2013 at 4:27 AM, Peter Firmstone
<peter.firmstone at zeus.net.au> wrote:

                I'm attempting to fix an object (public api) that uses
unsafe construction (lets "this" escape to other threads via inner classes
during construction).

                To safely construct the object I need to modify the object
to delay publication of the "this" reference until after construction.

                I was considering a start() or init() method to do so.

                However legacy code won't call that method, so I need a way
to invoke start() or init() without relying on other methods.

                Having every method check the state either isn't practical,
since legacy code doesn't necessarily invoke any method calls immediately
and the constructor invokes background threads to perform work that client
code eventually uses.

                The solution I'm thinking of is to move the implementation
into a package private abstract superclass, so the object becomes stateless
(the superclass is responsible for state).  Then from the constructor, call
the superclass constructor, then call the start() method.

                Is it reasonable to assume that after the superclass
constructor returns, all superclass final fields are safely published?  So
the stateless child class can call the start() method from within the
constructor safely publishing the "this" reference after all final fields
and the objects they reference are fully initialized.

                Thanks in advance,

                Peter.


                _______________________________________________
                Concurrency-interest mailing list
                Concurrency-interest at cs.oswego.edu
                http://cs.oswego.edu/mailman/listinfo/concurrency-interest






_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest



            _______________________________________________
            Concurrency-interest mailing list
            Concurrency-interest at cs.oswego.edu
            http://cs.oswego.edu/mailman/listinfo/concurrency-interest







_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest



        _______________________________________________
        Concurrency-interest mailing list
        Concurrency-interest at cs.oswego.edu
        http://cs.oswego.edu/mailman/listinfo/concurrency-interest





    _______________________________________________
    Concurrency-interest mailing list
    Concurrency-interest at cs.oswego.edu
    http://cs.oswego.edu/mailman/listinfo/concurrency-interest



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130510/a521683a/attachment-0001.html>

From davidcholmes at aapt.net.au  Thu May  9 18:50:04 2013
From: davidcholmes at aapt.net.au (David Holmes)
Date: Fri, 10 May 2013 08:50:04 +1000
Subject: [concurrency-interest] safe construction
In-Reply-To: <NFBBKALFDCPFIDBNKAPCEEOMJMAA.davidcholmes@aapt.net.au>
Message-ID: <NFBBKALFDCPFIDBNKAPCMEOMJMAA.davidcholmes@aapt.net.au>

Ignore this - it is meaningless in this context. Both writes can be moved
into the sync block anyway and then reordered within it.

David
  -----Original Message-----
  From: David Holmes [mailto:davidcholmes at aapt.net.au]
  Sent: Friday, 10 May 2013 8:44 AM
  To: Zhong Yu; oleksandr otenko
  Cc: Concurrency-interest at cs.oswego.edu
  Subject: RE: [concurrency-interest] safe construction


  You can elide the monitor acquisition but you can't elide the
happens-before edges.

  David
    -----Original Message-----
    From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Zhong Yu
    Sent: Friday, 10 May 2013 12:50 AM
    To: oleksandr otenko
    Cc: Concurrency-interest at cs.oswego.edu
    Subject: Re: [concurrency-interest] safe construction







    On Thu, May 9, 2013 at 8:44 AM, oleksandr otenko
<oleksandr.otenko at oracle.com> wrote:

      No, not in this case. In this case "this" is published, and you cannot
call public void foo without observing shared==this.


      JMM only defines thread-local orderings. The processors don't vote
which instructions to reorder.

      "Thread 1 storing a state happens-before Thread 2 reads shared" is
wrong kind of reasoning. As if we can tell Thread 2 when to read. Don't try
to bend the spoon, it is impossible.

      Same, but put in different words:

      "shared == this may be observed" -> "super class state is consistent"
for some definition of consistency.

      Here arrow "->" is implication, and it turns out to be exactly
opposite of happens-before.

      "may" deliberately states that we cannot tell what anyone else is
doing. "shared == this" deliberately states what condition we observe, not
just "any subsequent load of the same variable". (what is "subsequent"?
remember, processors don't vote on this. It is a true statement about "any
load that happens to be subsequent", but that's tautological)

      "is" deliberately states what we guarantee thread-locally.


      It is clear that shared==this can not be observed, if we don't store
to shared. Now we have everything that is needed to reason about validity of
code using only thread-local ordering.

      "shared == this may be observed" -> "shared=this was executed"  //
obviously
      "shared=this was executed" -> "super class state is consistent" //
this is happens-before, and it is thread-local

      That's it. Add store-store barrier between shared=this and end of
super-class constructor.


      Nathan's example is not complete, but not because volatile loads are
missing in other methods, but because of three simple rules in JMM:

      1. Volatile load can go ahead of anything non-volatile
      2. Anything non-volatile can go ahead of volatile store
      3. Anything non-volatile can go ahead of anything non-volatile



    Alex, I don't think these rules are in JMM. JMM itself is a little
weaker. For example,


        var = 1;   // write#1

        synchronized(new Object()){}

        var = 2;  // write#2


    the 2nd line can be elided, and the two writes can be reordered, as far
as JMM is concerned.



    Zhong Yu





      So, shared=this may be allowed to go ahead of s_fence=false, if shared
is not volatile, and then can be reordered with "super class state is
consistent". In order to stop this from happening, you need a volatile load
after a volatile store and before shared=this. This way shared=this cannot
go ahead of s_fence=false, because volatile load cannot go ahead of
s_fence=false, and shared=this cannot go ahead of a volatile load.

      shared.x is always issued as Load shared; Load shared.x, no additional
barriers are needed in threads that access superclass fields initialized
prior to publishing shared=this.


      There is one thin case. If, and this is a big if, Load shared.x is
allowed to go ahead of Load shared (eg if CPU already loaded that cache line
and cache coherency is weak), then there must be a Load-Load barrier between
Load shared and Load shared.x, but even this cannot be achieved through a
volatile load at the beginning of the method body - because of rule (1)
volatile load can go ahead of non-volatile Load shared, and we are back to
square one.

      So, a volatile load at the beginning of a method is wrong, and I'd
check what JMM says about reordering of dependent normal field x loads and a
stronger barrier may be needed after the first load of shared - there
certainly is a guarantee about final x. I don't think the data dependency
reordering is possible (how can you eliminate Load shared.x before you know
what shared refers to), but let's check.


      Alex




      On 09/05/2013 00:33, Vitaly Davidovich wrote:

        For JMM, I believe it has to be a read of s_fence everywhere where
you want to then read the normal writes preceding the volatile store, as
Yuval said.  That's how HB is defined.

        Sent from my phone

        On May 8, 2013 4:57 PM, "oleksandr otenko"
<oleksandr.otenko at oracle.com> wrote:

          No, just a volatile read immediately after the write.

          Alex


          On 08/05/2013 19:42, Yuval Shavit wrote:

            Is that an artifact of the common/global implementation, or the
JLS itself? I thought that in order for it to work according to the JLS, you
would have to read s_fence as the first thing within each of MyClass's
methods:


                public void foo() {
                    if (s_fence) throw new AssertionError();
                    doMyWhatever();
                }


            This would establish a HB before the read and the
post-initialization write. There's no formal HB without the read, right?



            On Wed, May 8, 2013 at 2:14 PM, Nathan Reynolds
<nathan.reynolds at oracle.com> wrote:

              A much heavier handed solution would be...

              class MyClass
              {
                  static volatile boolean s_fence;

                  public MyClass()
                  {
                      // initialize all fields
                      s_fence = false;
                      // leak "this"
                  }
              }

              Writing to s_fence guarantees that all of the fields will be
globally visible before "this" is leaked.  This is more expensive since it
adds a Store-Load memory fence rather than just a Store-Store fence.  The
Store-Store fence will turn into a noop on x86.

              The other problem with writing to s_fence is that the cache
line holding it is going to be contended.  If enough MyClass objects are
created concurrently on different cores, cache performance is going to be
the bottleneck.


              Nathan Reynolds | Architect | 602.333.9091
              Oracle PSR Engineering | Server Technology

              On 5/8/2013 8:02 AM, Zhong Yu wrote:

                If the publication is safe, there is nothing to worry about,
as long as `this` is leaked after fields are assigned.


                If the publication is unsafe, a trick I would use it to wrap
the object reference as a final field, and publish the wrapper


                    class FinalReference<T>

                        final T referent;
                        FinalReference(T referent){
this.referent=referent; }

                    static FinalReference<Foo> unsafePubVar;


                    Foo foo = ...;

                    unsafePubVar = new FinalReference(foo);


                forcing the subscription side to go though a final field to
access `foo` and its fields.



                Zhong Yu






                On Wed, May 8, 2013 at 4:27 AM, Peter Firmstone
<peter.firmstone at zeus.net.au> wrote:

                  I'm attempting to fix an object (public api) that uses
unsafe construction (lets "this" escape to other threads via inner classes
during construction).

                  To safely construct the object I need to modify the object
to delay publication of the "this" reference until after construction.

                  I was considering a start() or init() method to do so.

                  However legacy code won't call that method, so I need a
way to invoke start() or init() without relying on other methods.

                  Having every method check the state either isn't
practical, since legacy code doesn't necessarily invoke any method calls
immediately and the constructor invokes background threads to perform work
that client code eventually uses.

                  The solution I'm thinking of is to move the implementation
into a package private abstract superclass, so the object becomes stateless
(the superclass is responsible for state).  Then from the constructor, call
the superclass constructor, then call the start() method.

                  Is it reasonable to assume that after the superclass
constructor returns, all superclass final fields are safely published?  So
the stateless child class can call the start() method from within the
constructor safely publishing the "this" reference after all final fields
and the objects they reference are fully initialized.

                  Thanks in advance,

                  Peter.


                  _______________________________________________
                  Concurrency-interest mailing list
                  Concurrency-interest at cs.oswego.edu
                  http://cs.oswego.edu/mailman/listinfo/concurrency-interest






_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest



              _______________________________________________
              Concurrency-interest mailing list
              Concurrency-interest at cs.oswego.edu
              http://cs.oswego.edu/mailman/listinfo/concurrency-interest







_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest



          _______________________________________________
          Concurrency-interest mailing list
          Concurrency-interest at cs.oswego.edu
          http://cs.oswego.edu/mailman/listinfo/concurrency-interest





      _______________________________________________
      Concurrency-interest mailing list
      Concurrency-interest at cs.oswego.edu
      http://cs.oswego.edu/mailman/listinfo/concurrency-interest



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130510/52b7f096/attachment-0001.html>

From zhong.j.yu at gmail.com  Thu May  9 19:14:14 2013
From: zhong.j.yu at gmail.com (Zhong Yu)
Date: Thu, 9 May 2013 18:14:14 -0500
Subject: [concurrency-interest] safe construction
In-Reply-To: <NFBBKALFDCPFIDBNKAPCMEOMJMAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCEEOMJMAA.davidcholmes@aapt.net.au>
	<NFBBKALFDCPFIDBNKAPCMEOMJMAA.davidcholmes@aapt.net.au>
Message-ID: <CACuKZqE+CxunTi3KPtRTG9Q1bmjVYkrdpboJnD9rE099btFA_A@mail.gmail.com>

Right, [lock, unlock] is not analogous to Alex's solution of [volatile
write, volatile read]. How about this example

    v1 = 1;
    synchronized(new Object()){}
    synchronized(new Object()){}
    v2 = 2;

JMM allows reordering of the two writes, but roach motel doesn't.

Zhong Yu

On Thu, May 9, 2013 at 5:50 PM, David Holmes <davidcholmes at aapt.net.au>wrote:

> **
> Ignore this - it is meaningless in this context. Both writes can be moved
> into the sync block anyway and then reordered within it.
>
> David
>
> -----Original Message-----
> *From:* David Holmes [mailto:davidcholmes at aapt.net.au]
> *Sent:* Friday, 10 May 2013 8:44 AM
> *To:* Zhong Yu; oleksandr otenko
> *Cc:* Concurrency-interest at cs.oswego.edu
> *Subject:* RE: [concurrency-interest] safe construction
>
> You can elide the monitor acquisition but you can't elide the
> happens-before edges.
>
> David
>
> -----Original Message-----
> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
> concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of *Zhong Yu
> *Sent:* Friday, 10 May 2013 12:50 AM
> *To:* oleksandr otenko
> *Cc:* Concurrency-interest at cs.oswego.edu
> *Subject:* Re: [concurrency-interest] safe construction
>
>
>
>
> On Thu, May 9, 2013 at 8:44 AM, oleksandr otenko <
> oleksandr.otenko at oracle.com> wrote:
>
>> No, not in this case. In this case "this" is published, and you cannot
>> call public void foo without observing shared==this.
>>
>>
>> JMM only defines thread-local orderings. The processors don't vote which
>> instructions to reorder.
>>
>> "Thread 1 storing a state happens-before Thread 2 reads shared" is wrong
>> kind of reasoning. As if we can tell Thread 2 *when* to read. Don't try
>> to bend the spoon, it is impossible.
>>
>> Same, but put in different words:
>>
>> "shared == this *may* be observed" -> "super class state *is*consistent" for some definition of consistency.
>>
>> Here arrow "->" is implication, and it turns out to be exactly opposite
>> of happens-before.
>>
>> "may" deliberately states that we cannot tell what anyone else is doing.
>> "shared == this" deliberately states what condition we observe, not just
>> "any subsequent load of the same variable". (what is "subsequent"?
>> remember, processors don't vote on this. It is a true statement about "any
>> load *that happens to be subsequent*", but that's tautological)
>>
>> "is" deliberately states what we guarantee thread-locally.
>>
>>
>> It is clear that shared==this can not be observed, if we don't store to
>> shared. Now we have everything that is needed to reason about validity of
>> code using only thread-local ordering.
>>
>> "shared == this may be observed" -> "shared=this was executed"  //
>> obviously
>> "shared=this was executed" -> "super class state is consistent" // *this*is happens-before, and it is thread-local
>>
>> That's it. Add store-store barrier between shared=this and end of
>> super-class constructor.
>>
>>
>> Nathan's example is not complete, but not because volatile loads are
>> missing in other methods, but because of three simple rules in JMM:
>>
>> 1. Volatile load can go ahead of anything non-volatile
>> 2. Anything non-volatile can go ahead of volatile store
>> 3. Anything non-volatile can go ahead of anything non-volatile
>>
>
> Alex, I don't think these rules are in JMM. JMM itself is a little weaker.
> For example,
>
>     var = 1;   // write#1
>     synchronized(new Object()){}
>     var = 2;  // write#2
>
> the 2nd line can be elided, and the two writes can be reordered, as far as
> JMM is concerned.
>
> Zhong Yu
>
>
>
>>
>> So, shared=this may be allowed to go ahead of s_fence=false, if shared is
>> not volatile, and then can be reordered with "super class state is
>> consistent". In order to stop this from happening, you need a volatile load
>> after a volatile store and before shared=this. This way shared=this cannot
>> go ahead of s_fence=false, because volatile load cannot go ahead of
>> s_fence=false, and shared=this cannot go ahead of a volatile load.
>>
>> shared.x is always issued as Load shared; Load shared.x, no additional
>> barriers are needed in threads that access superclass fields initialized
>> prior to publishing shared=this.
>>
>>
>> There is one thin case. If, and this is a big if, Load shared.x is
>> allowed to go ahead of Load shared (eg if CPU already loaded that cache
>> line and cache coherency is weak), then there must be a Load-Load barrier
>> between Load shared and Load shared.x, but even this cannot be achieved
>> through a volatile load at the beginning of the method body - because of
>> rule (1) volatile load can go ahead of non-volatile Load shared, and we are
>> back to square one.
>>
>> So, a volatile load at the beginning of a method is wrong, and I'd check
>> what JMM says about reordering of dependent normal field x loads and a
>> stronger barrier may be needed after the first load of shared - there
>> certainly is a guarantee about final x. I don't think the data dependency
>> reordering is possible (how can you eliminate Load shared.x before you know
>> what shared refers to), but let's check.
>>
>>
>> Alex
>>
>>
>>
>> On 09/05/2013 00:33, Vitaly Davidovich wrote:
>>
>> For JMM, I believe it has to be a read of s_fence everywhere where you
>> want to then read the normal writes preceding the volatile store, as Yuval
>> said.  That's how HB is defined.
>>
>> Sent from my phone
>> On May 8, 2013 4:57 PM, "oleksandr otenko" <oleksandr.otenko at oracle.com>
>> wrote:
>>
>>> No, just a volatile read immediately after the write.
>>>
>>> Alex
>>>
>>> On 08/05/2013 19:42, Yuval Shavit wrote:
>>>
>>> Is that an artifact of the common/global implementation, or the JLS
>>> itself? I thought that in order for it to work according to the JLS, you
>>> would have to read s_fence as the first thing within each of MyClass's
>>> methods:
>>>
>>>     public void foo() {
>>>         if (s_fence) throw new AssertionError();
>>>         doMyWhatever();
>>>     }
>>>
>>> This would establish a HB before the read and the post-initialization
>>> write. There's no formal HB without the read, right?
>>>
>>>
>>> On Wed, May 8, 2013 at 2:14 PM, Nathan Reynolds <
>>> nathan.reynolds at oracle.com> wrote:
>>>
>>>>  A much heavier handed solution would be...
>>>>
>>>> class MyClass
>>>> {
>>>>     static volatile boolean s_fence;
>>>>
>>>>     public MyClass()
>>>>     {
>>>>         // initialize all fields
>>>>         s_fence = false;
>>>>         // leak "this"
>>>>     }
>>>> }
>>>>
>>>> Writing to s_fence guarantees that all of the fields will be globally
>>>> visible before "this" is leaked.  This is more expensive since it adds a
>>>> Store-Load memory fence rather than just a Store-Store fence.  The
>>>> Store-Store fence will turn into a noop on x86.
>>>>
>>>> The other problem with writing to s_fence is that the cache line
>>>> holding it is going to be contended.  If enough MyClass objects are created
>>>> concurrently on different cores, cache performance is going to be the
>>>> bottleneck.
>>>>
>>>> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Architect |
>>>> 602.333.9091
>>>> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>>>>  On 5/8/2013 8:02 AM, Zhong Yu wrote:
>>>>
>>>>  If the publication is safe, there is nothing to worry about, as long
>>>> as `this` is leaked after fields are assigned.
>>>>
>>>> If the publication is unsafe, a trick I would use it to wrap the object
>>>> reference as a final field, and publish the wrapper
>>>>
>>>>     class FinalReference<T>
>>>>         final T referent;
>>>>         FinalReference(T referent){ this.referent=referent; }
>>>>
>>>>     static FinalReference<Foo> unsafePubVar;
>>>>
>>>>     Foo foo = ...;
>>>>     unsafePubVar = new FinalReference(foo);
>>>>
>>>> forcing the subscription side to go though a final field to access
>>>> `foo` and its fields.
>>>>
>>>> Zhong Yu
>>>>
>>>>
>>>>
>>>> On Wed, May 8, 2013 at 4:27 AM, Peter Firmstone <
>>>> peter.firmstone at zeus.net.au> wrote:
>>>>
>>>>> I'm attempting to fix an object (public api) that uses unsafe
>>>>> construction (lets "this" escape to other threads via inner classes during
>>>>> construction).
>>>>>
>>>>> To safely construct the object I need to modify the object to delay
>>>>> publication of the "this" reference until after construction.
>>>>>
>>>>> I was considering a start() or init() method to do so.
>>>>>
>>>>> However legacy code won't call that method, so I need a way to invoke
>>>>> start() or init() without relying on other methods.
>>>>>
>>>>> Having every method check the state either isn't practical, since
>>>>> legacy code doesn't necessarily invoke any method calls immediately and the
>>>>> constructor invokes background threads to perform work that client code
>>>>> eventually uses.
>>>>>
>>>>> The solution I'm thinking of is to move the implementation into a
>>>>> package private abstract superclass, so the object becomes stateless (the
>>>>> superclass is responsible for state).  Then from the constructor, call the
>>>>> superclass constructor, then call the start() method.
>>>>>
>>>>> Is it reasonable to assume that after the superclass constructor
>>>>> returns, all superclass final fields are safely published?  So the
>>>>> stateless child class can call the start() method from within the
>>>>> constructor safely publishing the "this" reference after all final fields
>>>>> and the objects they reference are fully initialized.
>>>>>
>>>>> Thanks in advance,
>>>>>
>>>>> Peter.
>>>>>
>>>>>
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at cs.oswego.edu
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>
>>>>
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130509/fdc06cd7/attachment-0001.html>

From oleksandr.otenko at oracle.com  Fri May 10 05:26:37 2013
From: oleksandr.otenko at oracle.com (oleksandr otenko)
Date: Fri, 10 May 2013 10:26:37 +0100
Subject: [concurrency-interest] safe construction
In-Reply-To: <CACuKZqE+CxunTi3KPtRTG9Q1bmjVYkrdpboJnD9rE099btFA_A@mail.gmail.com>
References: <NFBBKALFDCPFIDBNKAPCEEOMJMAA.davidcholmes@aapt.net.au>
	<NFBBKALFDCPFIDBNKAPCMEOMJMAA.davidcholmes@aapt.net.au>
	<CACuKZqE+CxunTi3KPtRTG9Q1bmjVYkrdpboJnD9rE099btFA_A@mail.gmail.com>
Message-ID: <518CBD4D.6080604@oracle.com>

Which bit of JMM allows to reorder these two writes?

Even if the lock acquire/release instructions are not issued (because we 
can prove the locks are thread-local), you'd still need to heed the 
barriers and a serialization point between the first monitor exit and 
the second monitor enter.

Alex

On 10/05/2013 00:14, Zhong Yu wrote:
> Right, [lock, unlock] is not analogous to Alex's solution of [volatile 
> write, volatile read]. How about this example
>
>     v1 = 1;
>     synchronized(new Object()){}
>     synchronized(new Object()){}
>     v2 = 2;
>
> JMM allows reordering of the two writes, but roach motel doesn't.
>
> Zhong Yu
>
> On Thu, May 9, 2013 at 5:50 PM, David Holmes <davidcholmes at aapt.net.au 
> <mailto:davidcholmes at aapt.net.au>> wrote:
>
>     Ignore this - it is meaningless in this context. Both writes can
>     be moved into the sync block anyway and then reordered within it.
>     David
>
>         -----Original Message-----
>         *From:* David Holmes [mailto:davidcholmes at aapt.net.au
>         <mailto:davidcholmes at aapt.net.au>]
>         *Sent:* Friday, 10 May 2013 8:44 AM
>         *To:* Zhong Yu; oleksandr otenko
>         *Cc:* Concurrency-interest at cs.oswego.edu
>         <mailto:Concurrency-interest at cs.oswego.edu>
>         *Subject:* RE: [concurrency-interest] safe construction
>
>         You can elide the monitor acquisition but you can't elide the
>         happens-before edges.
>         David
>
>             -----Original Message-----
>             *From:* concurrency-interest-bounces at cs.oswego.edu
>             <mailto:concurrency-interest-bounces at cs.oswego.edu>
>             [mailto:concurrency-interest-bounces at cs.oswego.edu
>             <mailto:concurrency-interest-bounces at cs.oswego.edu>]*On
>             Behalf Of *Zhong Yu
>             *Sent:* Friday, 10 May 2013 12:50 AM
>             *To:* oleksandr otenko
>             *Cc:* Concurrency-interest at cs.oswego.edu
>             <mailto:Concurrency-interest at cs.oswego.edu>
>             *Subject:* Re: [concurrency-interest] safe construction
>
>
>
>
>             On Thu, May 9, 2013 at 8:44 AM, oleksandr otenko
>             <oleksandr.otenko at oracle.com
>             <mailto:oleksandr.otenko at oracle.com>> wrote:
>
>                 No, not in this case. In this case "this" is
>                 published, and you cannot call public void foo without
>                 observing shared==this.
>
>
>                 JMM only defines thread-local orderings. The
>                 processors don't vote which instructions to reorder.
>
>                 "Thread 1 storing a state happens-before Thread 2
>                 reads shared" is wrong kind of reasoning. As if we can
>                 tell Thread 2 /when/ to read. Don't try to bend the
>                 spoon, it is impossible.
>
>                 Same, but put in different words:
>
>                 "shared == this /*may*/ be observed" -> "super class
>                 state /*is*/ consistent" for some definition of
>                 consistency.
>
>                 Here arrow "->" is implication, and it turns out to be
>                 exactly opposite of happens-before.
>
>                 "may" deliberately states that we cannot tell what
>                 anyone else is doing. "shared == this" deliberately
>                 states what condition we observe, not just "any
>                 subsequent load of the same variable". (what is
>                 "subsequent"? remember, processors don't vote on this.
>                 It is a true statement about "any load /that happens
>                 to be subsequent/", but that's tautological)
>
>                 "is" deliberately states what we guarantee thread-locally.
>
>
>                 It is clear that shared==this can not be observed, if
>                 we don't store to shared. Now we have everything that
>                 is needed to reason about validity of code using only
>                 thread-local ordering.
>
>                 "shared == this may be observed" -> "shared=this was
>                 executed"  // obviously
>                 "shared=this was executed" -> "super class state is
>                 consistent" // /this/ is happens-before, and it is
>                 thread-local
>
>                 That's it. Add store-store barrier between shared=this
>                 and end of super-class constructor.
>
>
>                 Nathan's example is not complete, but not because
>                 volatile loads are missing in other methods, but
>                 because of three simple rules in JMM:
>
>                 1. Volatile load can go ahead of anything non-volatile
>                 2. Anything non-volatile can go ahead of volatile store
>                 3. Anything non-volatile can go ahead of anything
>                 non-volatile
>
>
>             Alex, I don't think these rules are in JMM. JMM itself is
>             a little weaker. For example,
>
>                 var = 1;   // write#1
>                 synchronized(new Object()){}
>                 var = 2;  // write#2
>
>             the 2nd line can be elided, and the two writes can be
>             reordered, as far as JMM is concerned.
>
>             Zhong Yu
>
>
>                 So, shared=this may be allowed to go ahead of
>                 s_fence=false, if shared is not volatile, and then can
>                 be reordered with "super class state is consistent".
>                 In order to stop this from happening, you need a
>                 volatile load after a volatile store and before
>                 shared=this. This way shared=this cannot go ahead of
>                 s_fence=false, because volatile load cannot go ahead
>                 of s_fence=false, and shared=this cannot go ahead of a
>                 volatile load.
>
>                 shared.x is always issued as Load shared; Load
>                 shared.x, no additional barriers are needed in threads
>                 that access superclass fields initialized prior to
>                 publishing shared=this.
>
>
>                 There is one thin case. If, and this is a big if, Load
>                 shared.x is allowed to go ahead of Load shared (eg if
>                 CPU already loaded that cache line and cache coherency
>                 is weak), then there must be a Load-Load barrier
>                 between Load shared and Load shared.x, but even this
>                 cannot be achieved through a volatile load at the
>                 beginning of the method body - because of rule (1)
>                 volatile load can go ahead of non-volatile Load
>                 shared, and we are back to square one.
>
>                 So, a volatile load at the beginning of a method is
>                 wrong, and I'd check what JMM says about reordering of
>                 dependent normal field x loads and a stronger barrier
>                 may be needed after the first load of shared - there
>                 certainly is a guarantee about final x. I don't think
>                 the data dependency reordering is possible (how can
>                 you eliminate Load shared.x before you know what
>                 shared refers to), but let's check.
>
>
>                 Alex
>
>
>
>                 On 09/05/2013 00:33, Vitaly Davidovich wrote:
>>
>>                 For JMM, I believe it has to be a read of s_fence
>>                 everywhere where you want to then read the normal
>>                 writes preceding the volatile store, as Yuval said. 
>>                 That's how HB is defined.
>>
>>                 Sent from my phone
>>
>>                 On May 8, 2013 4:57 PM, "oleksandr otenko"
>>                 <oleksandr.otenko at oracle.com
>>                 <mailto:oleksandr.otenko at oracle.com>> wrote:
>>
>>                     No, just a volatile read immediately after the write.
>>
>>                     Alex
>>
>>                     On 08/05/2013 19:42, Yuval Shavit wrote:
>>>                     Is that an artifact of the common/global
>>>                     implementation, or the JLS itself? I thought
>>>                     that in order for it to work according to the
>>>                     JLS, you would have to read s_fence as the first
>>>                     thing within each of MyClass's methods:
>>>
>>>                         public void foo() {
>>>                             if (s_fence) throw new AssertionError();
>>>                     doMyWhatever();
>>>                         }
>>>
>>>                     This would establish a HB before the read and
>>>                     the post-initialization write. There's no formal
>>>                     HB without the read, right?
>>>
>>>
>>>                     On Wed, May 8, 2013 at 2:14 PM, Nathan Reynolds
>>>                     <nathan.reynolds at oracle.com
>>>                     <mailto:nathan.reynolds at oracle.com>> wrote:
>>>
>>>                         A much heavier handed solution would be...
>>>
>>>                         class MyClass
>>>                         {
>>>                             static volatile boolean s_fence;
>>>
>>>                             public MyClass()
>>>                             {
>>>                                 // initialize all fields
>>>                         s_fence = false;
>>>                                 // leak "this"
>>>                             }
>>>                         }
>>>
>>>                         Writing to s_fence guarantees that all of
>>>                         the fields will be globally visible before
>>>                         "this" is leaked.  This is more expensive
>>>                         since it adds a Store-Load memory fence
>>>                         rather than just a Store-Store fence.  The
>>>                         Store-Store fence will turn into a noop on x86.
>>>
>>>                         The other problem with writing to s_fence is
>>>                         that the cache line holding it is going to
>>>                         be contended. If enough MyClass objects are
>>>                         created concurrently on different cores,
>>>                         cache performance is going to be the bottleneck.
>>>
>>>                         Nathan Reynolds
>>>                         <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>
>>>                         | Architect | 602.333.9091 <tel:602.333.9091>
>>>                         Oracle PSR Engineering
>>>                         <http://psr.us.oracle.com/> | Server Technology
>>>                         On 5/8/2013 8:02 AM, Zhong Yu wrote:
>>>>                         If the publication is safe, there is
>>>>                         nothing to worry about, as long as `this`
>>>>                         is leaked after fields are assigned.
>>>>
>>>>                         If the publication is unsafe, a trick I
>>>>                         would use it to wrap the object reference
>>>>                         as a final field, and publish the wrapper
>>>>
>>>>                             class FinalReference<T>
>>>>                         final T referent;
>>>>                         FinalReference(T referent){
>>>>                         this.referent=referent; }
>>>>
>>>>                             static FinalReference<Foo> unsafePubVar;
>>>>
>>>>                             Foo foo = ...;
>>>>                         unsafePubVar = new FinalReference(foo);
>>>>
>>>>                         forcing the subscription side to go though
>>>>                         a final field to access `foo` and its fields.
>>>>
>>>>                         Zhong Yu
>>>>
>>>>
>>>>
>>>>                         On Wed, May 8, 2013 at 4:27 AM, Peter
>>>>                         Firmstone <peter.firmstone at zeus.net.au
>>>>                         <mailto:peter.firmstone at zeus.net.au>> wrote:
>>>>
>>>>                             I'm attempting to fix an object (public
>>>>                             api) that uses unsafe construction
>>>>                             (lets "this" escape to other threads
>>>>                             via inner classes during construction).
>>>>
>>>>                             To safely construct the object I need
>>>>                             to modify the object to delay
>>>>                             publication of the "this" reference
>>>>                             until after construction.
>>>>
>>>>                             I was considering a start() or init()
>>>>                             method to do so.
>>>>
>>>>                             However legacy code won't call that
>>>>                             method, so I need a way to invoke
>>>>                             start() or init() without relying on
>>>>                             other methods.
>>>>
>>>>                             Having every method check the state
>>>>                             either isn't practical, since legacy
>>>>                             code doesn't necessarily invoke any
>>>>                             method calls immediately and the
>>>>                             constructor invokes background threads
>>>>                             to perform work that client code
>>>>                             eventually uses.
>>>>
>>>>                             The solution I'm thinking of is to move
>>>>                             the implementation into a package
>>>>                             private abstract superclass, so the
>>>>                             object becomes stateless (the
>>>>                             superclass is responsible for state).
>>>>                              Then from the constructor, call the
>>>>                             superclass constructor, then call the
>>>>                             start() method.
>>>>
>>>>                             Is it reasonable to assume that after
>>>>                             the superclass constructor returns, all
>>>>                             superclass final fields are safely
>>>>                             published?  So the stateless child
>>>>                             class can call the start() method from
>>>>                             within the constructor safely
>>>>                             publishing the "this" reference after
>>>>                             all final fields and the objects they
>>>>                             reference are fully initialized.
>>>>
>>>>                             Thanks in advance,
>>>>
>>>>                             Peter.
>>>>
>>>>
>>>>                             _______________________________________________
>>>>                             Concurrency-interest mailing list
>>>>                             Concurrency-interest at cs.oswego.edu
>>>>                             <mailto:Concurrency-interest at cs.oswego.edu>
>>>>                             http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>>
>>>>
>>>>                         _______________________________________________
>>>>                         Concurrency-interest mailing list
>>>>                         Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>>>                         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>>                         _______________________________________________
>>>                         Concurrency-interest mailing list
>>>                         Concurrency-interest at cs.oswego.edu
>>>                         <mailto:Concurrency-interest at cs.oswego.edu>
>>>                         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>>
>>>
>>>                     _______________________________________________
>>>                     Concurrency-interest mailing list
>>>                     Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>>                     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>                     _______________________________________________
>>                     Concurrency-interest mailing list
>>                     Concurrency-interest at cs.oswego.edu
>>                     <mailto:Concurrency-interest at cs.oswego.edu>
>>                     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
>                 _______________________________________________
>                 Concurrency-interest mailing list
>                 Concurrency-interest at cs.oswego.edu
>                 <mailto:Concurrency-interest at cs.oswego.edu>
>                 http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130510/c468c2bc/attachment-0001.html>

From yankee.sierra at gmail.com  Fri May 10 09:21:10 2013
From: yankee.sierra at gmail.com (Yuval Shavit)
Date: Fri, 10 May 2013 09:21:10 -0400
Subject: [concurrency-interest] safe construction
In-Reply-To: <518CBD4D.6080604@oracle.com>
References: <NFBBKALFDCPFIDBNKAPCEEOMJMAA.davidcholmes@aapt.net.au>
	<NFBBKALFDCPFIDBNKAPCMEOMJMAA.davidcholmes@aapt.net.au>
	<CACuKZqE+CxunTi3KPtRTG9Q1bmjVYkrdpboJnD9rE099btFA_A@mail.gmail.com>
	<518CBD4D.6080604@oracle.com>
Message-ID: <CAE+h5-DrwEhrRCjG-rgiG2_zkDoGADQbX+PVTL8dBpYA+dzgWw@mail.gmail.com>

It seems to me that the two synchronized(new Object()) blocks are
meaningless. You could have 50 of them, and they'd still be meaningless.

The JMM within each thread allows any reordering as long as the reorderings
can't be noticed within that thread. Reordering "v1 = 1; ; ; v2 = 2" to ";
; v2 = 2; v1 = 1" certainly fits the bill. Within the thread,
synchronized(new Object()) has no observable affect, so it can slide around
the order at will, or even be removed -- or replaced with a no-op ";" as in
my previous sentence. Once you start factoring in other threads, there is
no HB edge established by locking these thread-local monitors, so *still*
the synchronized blocks are meaningless.

Put another way, there's no second monitor enter. There are two first
monitor enters. A synchronized block doesn't mean anything until a second
(or subsequent) attempt to acquire a given monitor, and since that can't
happen, these synchronized blocks mean nothing according to the JMM.

Part of my perspective may be tainted -- for better or worse -- by my
unfamiliarity with the JVM spec. I don't think in terms of LoadLoad or
StoreStore or any of those; just in terms of what the JLS says. And as far
as I can tell, the JLS says nothing interesting about a monitor which is
acquired only once.


On Fri, May 10, 2013 at 5:26 AM, oleksandr otenko <
oleksandr.otenko at oracle.com> wrote:

>  Which bit of JMM allows to reorder these two writes?
>
> Even if the lock acquire/release instructions are not issued (because we
> can prove the locks are thread-local), you'd still need to heed the
> barriers and a serialization point between the first monitor exit and the
> second monitor enter.
>
> Alex
>
>  On 10/05/2013 00:14, Zhong Yu wrote:
>
> Right, [lock, unlock] is not analogous to Alex's solution of [volatile
> write, volatile read]. How about this example
>
>     v1 = 1;
>     synchronized(new Object()){}
>      synchronized(new Object()){}
>      v2 = 2;
>
>  JMM allows reordering of the two writes, but roach motel doesn't.
>
> Zhong Yu
>
> On Thu, May 9, 2013 at 5:50 PM, David Holmes <davidcholmes at aapt.net.au>wrote:
>
>>  Ignore this - it is meaningless in this context. Both writes can be
>> moved into the sync block anyway and then reordered within it.
>>
>> David
>>
>>  -----Original Message-----
>> *From:* David Holmes [mailto:davidcholmes at aapt.net.au]
>> *Sent:* Friday, 10 May 2013 8:44 AM
>> *To:* Zhong Yu; oleksandr otenko
>> *Cc:* Concurrency-interest at cs.oswego.edu
>>  *Subject:* RE: [concurrency-interest] safe construction
>>
>>    You can elide the monitor acquisition but you can't elide the
>> happens-before edges.
>>
>> David
>>
>> -----Original Message-----
>> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
>> concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of *Zhong Yu
>> *Sent:* Friday, 10 May 2013 12:50 AM
>> *To:* oleksandr otenko
>> *Cc:* Concurrency-interest at cs.oswego.edu
>> *Subject:* Re: [concurrency-interest] safe construction
>>
>>
>>
>>
>> On Thu, May 9, 2013 at 8:44 AM, oleksandr otenko <
>> oleksandr.otenko at oracle.com> wrote:
>>
>>> No, not in this case. In this case "this" is published, and you cannot
>>> call public void foo without observing shared==this.
>>>
>>>
>>> JMM only defines thread-local orderings. The processors don't vote
>>> which instructions to reorder.
>>>
>>> "Thread 1 storing a state happens-before Thread 2 reads shared" is wrong
>>> kind of reasoning. As if we can tell Thread 2 *when* to read. Don't try
>>> to bend the spoon, it is impossible.
>>>
>>> Same, but put in different words:
>>>
>>> "shared == this *may* be observed" -> "super class state *is*consistent" for some definition of consistency.
>>>
>>> Here arrow "->" is implication, and it turns out to be exactly opposite
>>> of happens-before.
>>>
>>> "may" deliberately states that we cannot tell what anyone else is doing.
>>> "shared == this" deliberately states what condition we observe, not just
>>> "any subsequent load of the same variable". (what is "subsequent"?
>>> remember, processors don't vote on this. It is a true statement about "any
>>> load *that happens to be subsequent*", but that's tautological)
>>>
>>> "is" deliberately states what we guarantee thread-locally.
>>>
>>>
>>> It is clear that shared==this can not be observed, if we don't store to
>>> shared. Now we have everything that is needed to reason about validity of
>>> code using only thread-local ordering.
>>>
>>> "shared == this may be observed" -> "shared=this was executed"  //
>>> obviously
>>> "shared=this was executed" -> "super class state is consistent" // *this
>>> * is happens-before, and it is thread-local
>>>
>>> That's it. Add store-store barrier between shared=this and end of
>>> super-class constructor.
>>>
>>>
>>> Nathan's example is not complete, but not because volatile loads are
>>> missing in other methods, but because of three simple rules in JMM:
>>>
>>> 1. Volatile load can go ahead of anything non-volatile
>>> 2. Anything non-volatile can go ahead of volatile store
>>> 3. Anything non-volatile can go ahead of anything non-volatile
>>>
>>
>>  Alex, I don't think these rules are in JMM. JMM itself is a little
>> weaker. For example,
>>
>>      var = 1;   // write#1
>>      synchronized(new Object()){}
>>      var = 2;  // write#2
>>
>>  the 2nd line can be elided, and the two writes can be reordered, as far
>> as JMM is concerned.
>>
>>  Zhong Yu
>>
>>
>>
>>>
>>> So, shared=this may be allowed to go ahead of s_fence=false, if shared
>>> is not volatile, and then can be reordered with "super class state is
>>> consistent". In order to stop this from happening, you need a volatile load
>>> after a volatile store and before shared=this. This way shared=this cannot
>>> go ahead of s_fence=false, because volatile load cannot go ahead of
>>> s_fence=false, and shared=this cannot go ahead of a volatile load.
>>>
>>> shared.x is always issued as Load shared; Load shared.x, no additional
>>> barriers are needed in threads that access superclass fields initialized
>>> prior to publishing shared=this.
>>>
>>>
>>> There is one thin case. If, and this is a big if, Load shared.x is
>>> allowed to go ahead of Load shared (eg if CPU already loaded that cache
>>> line and cache coherency is weak), then there must be a Load-Load barrier
>>> between Load shared and Load shared.x, but even this cannot be achieved
>>> through a volatile load at the beginning of the method body - because of
>>> rule (1) volatile load can go ahead of non-volatile Load shared, and we are
>>> back to square one.
>>>
>>> So, a volatile load at the beginning of a method is wrong, and I'd check
>>> what JMM says about reordering of dependent normal field x loads and a
>>> stronger barrier may be needed after the first load of shared - there
>>> certainly is a guarantee about final x. I don't think the data dependency
>>> reordering is possible (how can you eliminate Load shared.x before you know
>>> what shared refers to), but let's check.
>>>
>>>
>>> Alex
>>>
>>>
>>>
>>> On 09/05/2013 00:33, Vitaly Davidovich wrote:
>>>
>>> For JMM, I believe it has to be a read of s_fence everywhere where you
>>> want to then read the normal writes preceding the volatile store, as Yuval
>>> said.  That's how HB is defined.
>>>
>>> Sent from my phone
>>> On May 8, 2013 4:57 PM, "oleksandr otenko" <oleksandr.otenko at oracle.com>
>>> wrote:
>>>
>>>> No, just a volatile read immediately after the write.
>>>>
>>>> Alex
>>>>
>>>>  On 08/05/2013 19:42, Yuval Shavit wrote:
>>>>
>>>> Is that an artifact of the common/global implementation, or the JLS
>>>> itself? I thought that in order for it to work according to the JLS, you
>>>> would have to read s_fence as the first thing within each of MyClass's
>>>> methods:
>>>>
>>>>      public void foo() {
>>>>         if (s_fence) throw new AssertionError();
>>>>         doMyWhatever();
>>>>     }
>>>>
>>>>  This would establish a HB before the read and the post-initialization
>>>> write. There's no formal HB without the read, right?
>>>>
>>>>
>>>> On Wed, May 8, 2013 at 2:14 PM, Nathan Reynolds <
>>>> nathan.reynolds at oracle.com> wrote:
>>>>
>>>>>  A much heavier handed solution would be...
>>>>>
>>>>> class MyClass
>>>>> {
>>>>>     static volatile boolean s_fence;
>>>>>
>>>>>     public MyClass()
>>>>>     {
>>>>>         // initialize all fields
>>>>>         s_fence = false;
>>>>>         // leak "this"
>>>>>     }
>>>>> }
>>>>>
>>>>> Writing to s_fence guarantees that all of the fields will be globally
>>>>> visible before "this" is leaked.  This is more expensive since it adds a
>>>>> Store-Load memory fence rather than just a Store-Store fence.  The
>>>>> Store-Store fence will turn into a noop on x86.
>>>>>
>>>>> The other problem with writing to s_fence is that the cache line
>>>>> holding it is going to be contended.  If enough MyClass objects are created
>>>>> concurrently on different cores, cache performance is going to be the
>>>>> bottleneck.
>>>>>
>>>>> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Architect |
>>>>> 602.333.9091
>>>>> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>>>>>   On 5/8/2013 8:02 AM, Zhong Yu wrote:
>>>>>
>>>>>  If the publication is safe, there is nothing to worry about, as long
>>>>> as `this` is leaked after fields are assigned.
>>>>>
>>>>>  If the publication is unsafe, a trick I would use it to wrap the
>>>>> object reference as a final field, and publish the wrapper
>>>>>
>>>>>      class FinalReference<T>
>>>>>          final T referent;
>>>>>         FinalReference(T referent){ this.referent=referent; }
>>>>>
>>>>>     static FinalReference<Foo> unsafePubVar;
>>>>>
>>>>>      Foo foo = ...;
>>>>>      unsafePubVar = new FinalReference(foo);
>>>>>
>>>>>  forcing the subscription side to go though a final field to access
>>>>> `foo` and its fields.
>>>>>
>>>>>  Zhong Yu
>>>>>
>>>>>
>>>>>
>>>>> On Wed, May 8, 2013 at 4:27 AM, Peter Firmstone <
>>>>> peter.firmstone at zeus.net.au> wrote:
>>>>>
>>>>>> I'm attempting to fix an object (public api) that uses unsafe
>>>>>> construction (lets "this" escape to other threads via inner classes during
>>>>>> construction).
>>>>>>
>>>>>> To safely construct the object I need to modify the object to delay
>>>>>> publication of the "this" reference until after construction.
>>>>>>
>>>>>> I was considering a start() or init() method to do so.
>>>>>>
>>>>>> However legacy code won't call that method, so I need a way to invoke
>>>>>> start() or init() without relying on other methods.
>>>>>>
>>>>>> Having every method check the state either isn't practical, since
>>>>>> legacy code doesn't necessarily invoke any method calls immediately and the
>>>>>> constructor invokes background threads to perform work that client code
>>>>>> eventually uses.
>>>>>>
>>>>>> The solution I'm thinking of is to move the implementation into a
>>>>>> package private abstract superclass, so the object becomes stateless (the
>>>>>> superclass is responsible for state).  Then from the constructor, call the
>>>>>> superclass constructor, then call the start() method.
>>>>>>
>>>>>> Is it reasonable to assume that after the superclass constructor
>>>>>> returns, all superclass final fields are safely published?  So the
>>>>>> stateless child class can call the start() method from within the
>>>>>> constructor safely publishing the "this" reference after all final fields
>>>>>> and the objects they reference are fully initialized.
>>>>>>
>>>>>> Thanks in advance,
>>>>>>
>>>>>> Peter.
>>>>>>
>>>>>>
>>>>>> _______________________________________________
>>>>>> Concurrency-interest mailing list
>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>
>>>>>
>>>>>
>>>>>
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>>
>>>>>
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at cs.oswego.edu
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>>
>>>>
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130510/e35d2944/attachment-0001.html>

From cheremin at gmail.com  Fri May 10 09:44:53 2013
From: cheremin at gmail.com (Ruslan Cheremin)
Date: Fri, 10 May 2013 17:44:53 +0400
Subject: [concurrency-interest] safe construction
In-Reply-To: <518CBD4D.6080604@oracle.com>
References: <NFBBKALFDCPFIDBNKAPCEEOMJMAA.davidcholmes@aapt.net.au>
	<NFBBKALFDCPFIDBNKAPCMEOMJMAA.davidcholmes@aapt.net.au>
	<CACuKZqE+CxunTi3KPtRTG9Q1bmjVYkrdpboJnD9rE099btFA_A@mail.gmail.com>
	<518CBD4D.6080604@oracle.com>
Message-ID: <CAOwENiJ9eBOrnrEcCLhK-RGaRFzV0nLVYaQrjBXUuNU0o1CjJA@mail.gmail.com>

But which bit of JMM _prohibit_ the reordering?

Why do you think we'll need serialization point between monitors? The
synchronization order (if you talking about it), which lock/unlock are part
of, is not a part of observable behavior. It is JMM-internal artefact, part
of reasoning process. You can add/remove items from SO as long as no new
observable behavior appears.

2013/5/10 oleksandr otenko <oleksandr.otenko at oracle.com>

>  Which bit of JMM allows to reorder these two writes?
>
> Even if the lock acquire/release instructions are not issued (because we
> can prove the locks are thread-local), you'd still need to heed the
> barriers and a serialization point between the first monitor exit and the
> second monitor enter.
>
> Alex
>
>  On 10/05/2013 00:14, Zhong Yu wrote:
>
> Right, [lock, unlock] is not analogous to Alex's solution of [volatile
> write, volatile read]. How about this example
>
>     v1 = 1;
>     synchronized(new Object()){}
>      synchronized(new Object()){}
>      v2 = 2;
>
>  JMM allows reordering of the two writes, but roach motel doesn't.
>
> Zhong Yu
>
> On Thu, May 9, 2013 at 5:50 PM, David Holmes <davidcholmes at aapt.net.au>wrote:
>
>>  Ignore this - it is meaningless in this context. Both writes can be
>> moved into the sync block anyway and then reordered within it.
>>
>> David
>>
>>  -----Original Message-----
>> *From:* David Holmes [mailto:davidcholmes at aapt.net.au]
>> *Sent:* Friday, 10 May 2013 8:44 AM
>> *To:* Zhong Yu; oleksandr otenko
>> *Cc:* Concurrency-interest at cs.oswego.edu
>>  *Subject:* RE: [concurrency-interest] safe construction
>>
>>    You can elide the monitor acquisition but you can't elide the
>> happens-before edges.
>>
>> David
>>
>> -----Original Message-----
>> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
>> concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of *Zhong Yu
>> *Sent:* Friday, 10 May 2013 12:50 AM
>> *To:* oleksandr otenko
>> *Cc:* Concurrency-interest at cs.oswego.edu
>> *Subject:* Re: [concurrency-interest] safe construction
>>
>>
>>
>>
>> On Thu, May 9, 2013 at 8:44 AM, oleksandr otenko <
>> oleksandr.otenko at oracle.com> wrote:
>>
>>> No, not in this case. In this case "this" is published, and you cannot
>>> call public void foo without observing shared==this.
>>>
>>>
>>> JMM only defines thread-local orderings. The processors don't vote
>>> which instructions to reorder.
>>>
>>> "Thread 1 storing a state happens-before Thread 2 reads shared" is wrong
>>> kind of reasoning. As if we can tell Thread 2 *when* to read. Don't try
>>> to bend the spoon, it is impossible.
>>>
>>> Same, but put in different words:
>>>
>>> "shared == this *may* be observed" -> "super class state *is*consistent" for some definition of consistency.
>>>
>>> Here arrow "->" is implication, and it turns out to be exactly opposite
>>> of happens-before.
>>>
>>> "may" deliberately states that we cannot tell what anyone else is doing.
>>> "shared == this" deliberately states what condition we observe, not just
>>> "any subsequent load of the same variable". (what is "subsequent"?
>>> remember, processors don't vote on this. It is a true statement about "any
>>> load *that happens to be subsequent*", but that's tautological)
>>>
>>> "is" deliberately states what we guarantee thread-locally.
>>>
>>>
>>> It is clear that shared==this can not be observed, if we don't store to
>>> shared. Now we have everything that is needed to reason about validity of
>>> code using only thread-local ordering.
>>>
>>> "shared == this may be observed" -> "shared=this was executed"  //
>>> obviously
>>> "shared=this was executed" -> "super class state is consistent" // *this
>>> * is happens-before, and it is thread-local
>>>
>>> That's it. Add store-store barrier between shared=this and end of
>>> super-class constructor.
>>>
>>>
>>> Nathan's example is not complete, but not because volatile loads are
>>> missing in other methods, but because of three simple rules in JMM:
>>>
>>> 1. Volatile load can go ahead of anything non-volatile
>>> 2. Anything non-volatile can go ahead of volatile store
>>> 3. Anything non-volatile can go ahead of anything non-volatile
>>>
>>
>>  Alex, I don't think these rules are in JMM. JMM itself is a little
>> weaker. For example,
>>
>>      var = 1;   // write#1
>>      synchronized(new Object()){}
>>      var = 2;  // write#2
>>
>>  the 2nd line can be elided, and the two writes can be reordered, as far
>> as JMM is concerned.
>>
>>  Zhong Yu
>>
>>
>>
>>>
>>> So, shared=this may be allowed to go ahead of s_fence=false, if shared
>>> is not volatile, and then can be reordered with "super class state is
>>> consistent". In order to stop this from happening, you need a volatile load
>>> after a volatile store and before shared=this. This way shared=this cannot
>>> go ahead of s_fence=false, because volatile load cannot go ahead of
>>> s_fence=false, and shared=this cannot go ahead of a volatile load.
>>>
>>> shared.x is always issued as Load shared; Load shared.x, no additional
>>> barriers are needed in threads that access superclass fields initialized
>>> prior to publishing shared=this.
>>>
>>>
>>> There is one thin case. If, and this is a big if, Load shared.x is
>>> allowed to go ahead of Load shared (eg if CPU already loaded that cache
>>> line and cache coherency is weak), then there must be a Load-Load barrier
>>> between Load shared and Load shared.x, but even this cannot be achieved
>>> through a volatile load at the beginning of the method body - because of
>>> rule (1) volatile load can go ahead of non-volatile Load shared, and we are
>>> back to square one.
>>>
>>> So, a volatile load at the beginning of a method is wrong, and I'd check
>>> what JMM says about reordering of dependent normal field x loads and a
>>> stronger barrier may be needed after the first load of shared - there
>>> certainly is a guarantee about final x. I don't think the data dependency
>>> reordering is possible (how can you eliminate Load shared.x before you know
>>> what shared refers to), but let's check.
>>>
>>>
>>> Alex
>>>
>>>
>>>
>>> On 09/05/2013 00:33, Vitaly Davidovich wrote:
>>>
>>> For JMM, I believe it has to be a read of s_fence everywhere where you
>>> want to then read the normal writes preceding the volatile store, as Yuval
>>> said.  That's how HB is defined.
>>>
>>> Sent from my phone
>>> On May 8, 2013 4:57 PM, "oleksandr otenko" <oleksandr.otenko at oracle.com>
>>> wrote:
>>>
>>>> No, just a volatile read immediately after the write.
>>>>
>>>> Alex
>>>>
>>>>  On 08/05/2013 19:42, Yuval Shavit wrote:
>>>>
>>>> Is that an artifact of the common/global implementation, or the JLS
>>>> itself? I thought that in order for it to work according to the JLS, you
>>>> would have to read s_fence as the first thing within each of MyClass's
>>>> methods:
>>>>
>>>>      public void foo() {
>>>>         if (s_fence) throw new AssertionError();
>>>>         doMyWhatever();
>>>>     }
>>>>
>>>>  This would establish a HB before the read and the post-initialization
>>>> write. There's no formal HB without the read, right?
>>>>
>>>>
>>>> On Wed, May 8, 2013 at 2:14 PM, Nathan Reynolds <
>>>> nathan.reynolds at oracle.com> wrote:
>>>>
>>>>>  A much heavier handed solution would be...
>>>>>
>>>>> class MyClass
>>>>> {
>>>>>     static volatile boolean s_fence;
>>>>>
>>>>>     public MyClass()
>>>>>     {
>>>>>         // initialize all fields
>>>>>         s_fence = false;
>>>>>         // leak "this"
>>>>>     }
>>>>> }
>>>>>
>>>>> Writing to s_fence guarantees that all of the fields will be globally
>>>>> visible before "this" is leaked.  This is more expensive since it adds a
>>>>> Store-Load memory fence rather than just a Store-Store fence.  The
>>>>> Store-Store fence will turn into a noop on x86.
>>>>>
>>>>> The other problem with writing to s_fence is that the cache line
>>>>> holding it is going to be contended.  If enough MyClass objects are created
>>>>> concurrently on different cores, cache performance is going to be the
>>>>> bottleneck.
>>>>>
>>>>> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Architect |
>>>>> 602.333.9091
>>>>> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>>>>>   On 5/8/2013 8:02 AM, Zhong Yu wrote:
>>>>>
>>>>>  If the publication is safe, there is nothing to worry about, as long
>>>>> as `this` is leaked after fields are assigned.
>>>>>
>>>>>  If the publication is unsafe, a trick I would use it to wrap the
>>>>> object reference as a final field, and publish the wrapper
>>>>>
>>>>>      class FinalReference<T>
>>>>>          final T referent;
>>>>>         FinalReference(T referent){ this.referent=referent; }
>>>>>
>>>>>     static FinalReference<Foo> unsafePubVar;
>>>>>
>>>>>      Foo foo = ...;
>>>>>      unsafePubVar = new FinalReference(foo);
>>>>>
>>>>>  forcing the subscription side to go though a final field to access
>>>>> `foo` and its fields.
>>>>>
>>>>>  Zhong Yu
>>>>>
>>>>>
>>>>>
>>>>> On Wed, May 8, 2013 at 4:27 AM, Peter Firmstone <
>>>>> peter.firmstone at zeus.net.au> wrote:
>>>>>
>>>>>> I'm attempting to fix an object (public api) that uses unsafe
>>>>>> construction (lets "this" escape to other threads via inner classes during
>>>>>> construction).
>>>>>>
>>>>>> To safely construct the object I need to modify the object to delay
>>>>>> publication of the "this" reference until after construction.
>>>>>>
>>>>>> I was considering a start() or init() method to do so.
>>>>>>
>>>>>> However legacy code won't call that method, so I need a way to invoke
>>>>>> start() or init() without relying on other methods.
>>>>>>
>>>>>> Having every method check the state either isn't practical, since
>>>>>> legacy code doesn't necessarily invoke any method calls immediately and the
>>>>>> constructor invokes background threads to perform work that client code
>>>>>> eventually uses.
>>>>>>
>>>>>> The solution I'm thinking of is to move the implementation into a
>>>>>> package private abstract superclass, so the object becomes stateless (the
>>>>>> superclass is responsible for state).  Then from the constructor, call the
>>>>>> superclass constructor, then call the start() method.
>>>>>>
>>>>>> Is it reasonable to assume that after the superclass constructor
>>>>>> returns, all superclass final fields are safely published?  So the
>>>>>> stateless child class can call the start() method from within the
>>>>>> constructor safely publishing the "this" reference after all final fields
>>>>>> and the objects they reference are fully initialized.
>>>>>>
>>>>>> Thanks in advance,
>>>>>>
>>>>>> Peter.
>>>>>>
>>>>>>
>>>>>> _______________________________________________
>>>>>> Concurrency-interest mailing list
>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>
>>>>>
>>>>>
>>>>>
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>>
>>>>>
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at cs.oswego.edu
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>>
>>>>
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130510/84c5cb21/attachment-0001.html>

From zhong.j.yu at gmail.com  Fri May 10 11:27:53 2013
From: zhong.j.yu at gmail.com (Zhong Yu)
Date: Fri, 10 May 2013 10:27:53 -0500
Subject: [concurrency-interest] safe construction
In-Reply-To: <518CBD4D.6080604@oracle.com>
References: <NFBBKALFDCPFIDBNKAPCEEOMJMAA.davidcholmes@aapt.net.au>
	<NFBBKALFDCPFIDBNKAPCMEOMJMAA.davidcholmes@aapt.net.au>
	<CACuKZqE+CxunTi3KPtRTG9Q1bmjVYkrdpboJnD9rE099btFA_A@mail.gmail.com>
	<518CBD4D.6080604@oracle.com>
Message-ID: <CACuKZqGneK3kc9ed5VrxogWWmhtJtFWnQuKDGKdSZfs4Ou-wHQ@mail.gmail.com>

I guess the difference is that, JMM establishes an edge from a release to a
subsequent acquire of the *same* variable, while roach motel establishes an
edge from a release to a subsequent acquire of *any* variable., Therefore
JMM is considerably weaker than roach motel. Maybe they needed JMM to be
just strong enough to prove their main theorem (data race free ->
sequentially consistent), not stronger.

Zhong Yu



On Fri, May 10, 2013 at 4:26 AM, oleksandr otenko <
oleksandr.otenko at oracle.com> wrote:

>  Which bit of JMM allows to reorder these two writes?
>
> Even if the lock acquire/release instructions are not issued (because we
> can prove the locks are thread-local), you'd still need to heed the
> barriers and a serialization point between the first monitor exit and the
> second monitor enter.
>
> Alex
>
>  On 10/05/2013 00:14, Zhong Yu wrote:
>
> Right, [lock, unlock] is not analogous to Alex's solution of [volatile
> write, volatile read]. How about this example
>
>     v1 = 1;
>     synchronized(new Object()){}
>      synchronized(new Object()){}
>      v2 = 2;
>
>  JMM allows reordering of the two writes, but roach motel doesn't.
>
> Zhong Yu
>
> On Thu, May 9, 2013 at 5:50 PM, David Holmes <davidcholmes at aapt.net.au>wrote:
>
>>  Ignore this - it is meaningless in this context. Both writes can be
>> moved into the sync block anyway and then reordered within it.
>>
>> David
>>
>>  -----Original Message-----
>> *From:* David Holmes [mailto:davidcholmes at aapt.net.au]
>> *Sent:* Friday, 10 May 2013 8:44 AM
>> *To:* Zhong Yu; oleksandr otenko
>> *Cc:* Concurrency-interest at cs.oswego.edu
>>  *Subject:* RE: [concurrency-interest] safe construction
>>
>>    You can elide the monitor acquisition but you can't elide the
>> happens-before edges.
>>
>> David
>>
>> -----Original Message-----
>> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
>> concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of *Zhong Yu
>> *Sent:* Friday, 10 May 2013 12:50 AM
>> *To:* oleksandr otenko
>> *Cc:* Concurrency-interest at cs.oswego.edu
>> *Subject:* Re: [concurrency-interest] safe construction
>>
>>
>>
>>
>> On Thu, May 9, 2013 at 8:44 AM, oleksandr otenko <
>> oleksandr.otenko at oracle.com> wrote:
>>
>>> No, not in this case. In this case "this" is published, and you cannot
>>> call public void foo without observing shared==this.
>>>
>>>
>>> JMM only defines thread-local orderings. The processors don't vote
>>> which instructions to reorder.
>>>
>>> "Thread 1 storing a state happens-before Thread 2 reads shared" is wrong
>>> kind of reasoning. As if we can tell Thread 2 *when* to read. Don't try
>>> to bend the spoon, it is impossible.
>>>
>>> Same, but put in different words:
>>>
>>> "shared == this *may* be observed" -> "super class state *is*consistent" for some definition of consistency.
>>>
>>> Here arrow "->" is implication, and it turns out to be exactly opposite
>>> of happens-before.
>>>
>>> "may" deliberately states that we cannot tell what anyone else is doing.
>>> "shared == this" deliberately states what condition we observe, not just
>>> "any subsequent load of the same variable". (what is "subsequent"?
>>> remember, processors don't vote on this. It is a true statement about "any
>>> load *that happens to be subsequent*", but that's tautological)
>>>
>>> "is" deliberately states what we guarantee thread-locally.
>>>
>>>
>>> It is clear that shared==this can not be observed, if we don't store to
>>> shared. Now we have everything that is needed to reason about validity of
>>> code using only thread-local ordering.
>>>
>>> "shared == this may be observed" -> "shared=this was executed"  //
>>> obviously
>>> "shared=this was executed" -> "super class state is consistent" // *this
>>> * is happens-before, and it is thread-local
>>>
>>> That's it. Add store-store barrier between shared=this and end of
>>> super-class constructor.
>>>
>>>
>>> Nathan's example is not complete, but not because volatile loads are
>>> missing in other methods, but because of three simple rules in JMM:
>>>
>>> 1. Volatile load can go ahead of anything non-volatile
>>> 2. Anything non-volatile can go ahead of volatile store
>>> 3. Anything non-volatile can go ahead of anything non-volatile
>>>
>>
>>  Alex, I don't think these rules are in JMM. JMM itself is a little
>> weaker. For example,
>>
>>      var = 1;   // write#1
>>      synchronized(new Object()){}
>>      var = 2;  // write#2
>>
>>  the 2nd line can be elided, and the two writes can be reordered, as far
>> as JMM is concerned.
>>
>>  Zhong Yu
>>
>>
>>
>>>
>>> So, shared=this may be allowed to go ahead of s_fence=false, if shared
>>> is not volatile, and then can be reordered with "super class state is
>>> consistent". In order to stop this from happening, you need a volatile load
>>> after a volatile store and before shared=this. This way shared=this cannot
>>> go ahead of s_fence=false, because volatile load cannot go ahead of
>>> s_fence=false, and shared=this cannot go ahead of a volatile load.
>>>
>>> shared.x is always issued as Load shared; Load shared.x, no additional
>>> barriers are needed in threads that access superclass fields initialized
>>> prior to publishing shared=this.
>>>
>>>
>>> There is one thin case. If, and this is a big if, Load shared.x is
>>> allowed to go ahead of Load shared (eg if CPU already loaded that cache
>>> line and cache coherency is weak), then there must be a Load-Load barrier
>>> between Load shared and Load shared.x, but even this cannot be achieved
>>> through a volatile load at the beginning of the method body - because of
>>> rule (1) volatile load can go ahead of non-volatile Load shared, and we are
>>> back to square one.
>>>
>>> So, a volatile load at the beginning of a method is wrong, and I'd check
>>> what JMM says about reordering of dependent normal field x loads and a
>>> stronger barrier may be needed after the first load of shared - there
>>> certainly is a guarantee about final x. I don't think the data dependency
>>> reordering is possible (how can you eliminate Load shared.x before you know
>>> what shared refers to), but let's check.
>>>
>>>
>>> Alex
>>>
>>>
>>>
>>> On 09/05/2013 00:33, Vitaly Davidovich wrote:
>>>
>>> For JMM, I believe it has to be a read of s_fence everywhere where you
>>> want to then read the normal writes preceding the volatile store, as Yuval
>>> said.  That's how HB is defined.
>>>
>>> Sent from my phone
>>> On May 8, 2013 4:57 PM, "oleksandr otenko" <oleksandr.otenko at oracle.com>
>>> wrote:
>>>
>>>> No, just a volatile read immediately after the write.
>>>>
>>>> Alex
>>>>
>>>>  On 08/05/2013 19:42, Yuval Shavit wrote:
>>>>
>>>> Is that an artifact of the common/global implementation, or the JLS
>>>> itself? I thought that in order for it to work according to the JLS, you
>>>> would have to read s_fence as the first thing within each of MyClass's
>>>> methods:
>>>>
>>>>      public void foo() {
>>>>         if (s_fence) throw new AssertionError();
>>>>         doMyWhatever();
>>>>     }
>>>>
>>>>  This would establish a HB before the read and the post-initialization
>>>> write. There's no formal HB without the read, right?
>>>>
>>>>
>>>> On Wed, May 8, 2013 at 2:14 PM, Nathan Reynolds <
>>>> nathan.reynolds at oracle.com> wrote:
>>>>
>>>>>  A much heavier handed solution would be...
>>>>>
>>>>> class MyClass
>>>>> {
>>>>>     static volatile boolean s_fence;
>>>>>
>>>>>     public MyClass()
>>>>>     {
>>>>>         // initialize all fields
>>>>>         s_fence = false;
>>>>>         // leak "this"
>>>>>     }
>>>>> }
>>>>>
>>>>> Writing to s_fence guarantees that all of the fields will be globally
>>>>> visible before "this" is leaked.  This is more expensive since it adds a
>>>>> Store-Load memory fence rather than just a Store-Store fence.  The
>>>>> Store-Store fence will turn into a noop on x86.
>>>>>
>>>>> The other problem with writing to s_fence is that the cache line
>>>>> holding it is going to be contended.  If enough MyClass objects are created
>>>>> concurrently on different cores, cache performance is going to be the
>>>>> bottleneck.
>>>>>
>>>>> Nathan Reynolds<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>| Architect |
>>>>> 602.333.9091
>>>>> Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
>>>>>   On 5/8/2013 8:02 AM, Zhong Yu wrote:
>>>>>
>>>>>  If the publication is safe, there is nothing to worry about, as long
>>>>> as `this` is leaked after fields are assigned.
>>>>>
>>>>>  If the publication is unsafe, a trick I would use it to wrap the
>>>>> object reference as a final field, and publish the wrapper
>>>>>
>>>>>      class FinalReference<T>
>>>>>          final T referent;
>>>>>         FinalReference(T referent){ this.referent=referent; }
>>>>>
>>>>>     static FinalReference<Foo> unsafePubVar;
>>>>>
>>>>>      Foo foo = ...;
>>>>>      unsafePubVar = new FinalReference(foo);
>>>>>
>>>>>  forcing the subscription side to go though a final field to access
>>>>> `foo` and its fields.
>>>>>
>>>>>  Zhong Yu
>>>>>
>>>>>
>>>>>
>>>>> On Wed, May 8, 2013 at 4:27 AM, Peter Firmstone <
>>>>> peter.firmstone at zeus.net.au> wrote:
>>>>>
>>>>>> I'm attempting to fix an object (public api) that uses unsafe
>>>>>> construction (lets "this" escape to other threads via inner classes during
>>>>>> construction).
>>>>>>
>>>>>> To safely construct the object I need to modify the object to delay
>>>>>> publication of the "this" reference until after construction.
>>>>>>
>>>>>> I was considering a start() or init() method to do so.
>>>>>>
>>>>>> However legacy code won't call that method, so I need a way to invoke
>>>>>> start() or init() without relying on other methods.
>>>>>>
>>>>>> Having every method check the state either isn't practical, since
>>>>>> legacy code doesn't necessarily invoke any method calls immediately and the
>>>>>> constructor invokes background threads to perform work that client code
>>>>>> eventually uses.
>>>>>>
>>>>>> The solution I'm thinking of is to move the implementation into a
>>>>>> package private abstract superclass, so the object becomes stateless (the
>>>>>> superclass is responsible for state).  Then from the constructor, call the
>>>>>> superclass constructor, then call the start() method.
>>>>>>
>>>>>> Is it reasonable to assume that after the superclass constructor
>>>>>> returns, all superclass final fields are safely published?  So the
>>>>>> stateless child class can call the start() method from within the
>>>>>> constructor safely publishing the "this" reference after all final fields
>>>>>> and the objects they reference are fully initialized.
>>>>>>
>>>>>> Thanks in advance,
>>>>>>
>>>>>> Peter.
>>>>>>
>>>>>>
>>>>>> _______________________________________________
>>>>>> Concurrency-interest mailing list
>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>
>>>>>
>>>>>
>>>>>
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>>
>>>>>
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at cs.oswego.edu
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>>
>>>>
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130510/103e8e22/attachment-0001.html>

From oleksandr.otenko at oracle.com  Fri May 10 11:41:19 2013
From: oleksandr.otenko at oracle.com (oleksandr otenko)
Date: Fri, 10 May 2013 16:41:19 +0100
Subject: [concurrency-interest] safe construction
In-Reply-To: <CAOwENiJ9eBOrnrEcCLhK-RGaRFzV0nLVYaQrjBXUuNU0o1CjJA@mail.gmail.com>
References: <NFBBKALFDCPFIDBNKAPCEEOMJMAA.davidcholmes@aapt.net.au>
	<NFBBKALFDCPFIDBNKAPCMEOMJMAA.davidcholmes@aapt.net.au>
	<CACuKZqE+CxunTi3KPtRTG9Q1bmjVYkrdpboJnD9rE099btFA_A@mail.gmail.com>
	<518CBD4D.6080604@oracle.com>
	<CAOwENiJ9eBOrnrEcCLhK-RGaRFzV0nLVYaQrjBXUuNU0o1CjJA@mail.gmail.com>
Message-ID: <518D151F.7000200@oracle.com>

I don't know which bit of JMM prohibits reordering of that kind, but you 
will find it difficult to reason about lock striping.

Maybe that reordering is allowed by JMM, but I don't see a good reason 
for this. I would require correctly synchronized events bound by program 
order to be observable in that order. If two threads synchronized that 
e1 is before e2, then any thread synchronizing e3 after e2 must see e1 
was before e2.



assume global x and y.

producers do:

boolean xwasnull;
synchronized(m1) {
   xwasnull = x == null;
   if (xwasnull) x = new Object(); // e1
}

if (!xwasnull) {
   synchronized(m2) {
     y = new Object(); // e2
     m2.notify();
   }
}

consumers do:

synchronized(m2) {
   while (y == null) m2.wait(); // e3
   assert x != null: "you can't reason about consistent view of the 
universe by producers and consumers";
}

In essence, program order of producers guarantees ordered arrival of x 
and y. Consumers never observe monitor m1. Monitor release of m1 and 
monitor acquire of m2 should not be reordered (I am not talking about 
lock ownership, just barriers and serialization), otherwise consumers 
cannot reason about the order of x and y, whereas producers can.


Serialization is needed for the same reason as you need serialization of 
all stores preceding last volatile store, and make them visible globally 
before the next volatile load executes.


Alex

On 10/05/2013 14:44, Ruslan Cheremin wrote:
> But which bit of JMM _prohibit_ the reordering?
>
> Why do you think we'll need serialization point between monitors? The 
> synchronization order (if you talking about it), which lock/unlock are 
> part of, is not a part of observable behavior. It is JMM-internal 
> artefact, part of reasoning process. You can add/remove items from SO 
> as long as no new observable behavior appears.
>
> 2013/5/10 oleksandr otenko <oleksandr.otenko at oracle.com 
> <mailto:oleksandr.otenko at oracle.com>>
>
>     Which bit of JMM allows to reorder these two writes?
>
>     Even if the lock acquire/release instructions are not issued
>     (because we can prove the locks are thread-local), you'd still
>     need to heed the barriers and a serialization point between the
>     first monitor exit and the second monitor enter.
>
>     Alex
>
>     On 10/05/2013 00:14, Zhong Yu wrote:
>>     Right, [lock, unlock] is not analogous to Alex's solution of
>>     [volatile write, volatile read]. How about this example
>>
>>         v1 = 1;
>>         synchronized(new Object()){}
>>         synchronized(new Object()){}
>>         v2 = 2;
>>
>>     JMM allows reordering of the two writes, but roach motel doesn't.
>>
>>     Zhong Yu
>>
>>     On Thu, May 9, 2013 at 5:50 PM, David Holmes
>>     <davidcholmes at aapt.net.au <mailto:davidcholmes at aapt.net.au>> wrote:
>>
>>         Ignore this - it is meaningless in this context. Both writes
>>         can be moved into the sync block anyway and then reordered
>>         within it.
>>         David
>>
>>             -----Original Message-----
>>             *From:* David Holmes [mailto:davidcholmes at aapt.net.au
>>             <mailto:davidcholmes at aapt.net.au>]
>>             *Sent:* Friday, 10 May 2013 8:44 AM
>>             *To:* Zhong Yu; oleksandr otenko
>>             *Cc:* Concurrency-interest at cs.oswego.edu
>>             <mailto:Concurrency-interest at cs.oswego.edu>
>>             *Subject:* RE: [concurrency-interest] safe construction
>>
>>             You can elide the monitor acquisition but you can't elide
>>             the happens-before edges.
>>             David
>>
>>                 -----Original Message-----
>>                 *From:* concurrency-interest-bounces at cs.oswego.edu
>>                 <mailto:concurrency-interest-bounces at cs.oswego.edu>
>>                 [mailto:concurrency-interest-bounces at cs.oswego.edu
>>                 <mailto:concurrency-interest-bounces at cs.oswego.edu>]*On
>>                 Behalf Of *Zhong Yu
>>                 *Sent:* Friday, 10 May 2013 12:50 AM
>>                 *To:* oleksandr otenko
>>                 *Cc:* Concurrency-interest at cs.oswego.edu
>>                 <mailto:Concurrency-interest at cs.oswego.edu>
>>                 *Subject:* Re: [concurrency-interest] safe construction
>>
>>
>>
>>
>>                 On Thu, May 9, 2013 at 8:44 AM, oleksandr otenko
>>                 <oleksandr.otenko at oracle.com
>>                 <mailto:oleksandr.otenko at oracle.com>> wrote:
>>
>>                     No, not in this case. In this case "this" is
>>                     published, and you cannot call public void foo
>>                     without observing shared==this.
>>
>>
>>                     JMM only defines thread-local orderings. The
>>                     processors don't vote which instructions to reorder.
>>
>>                     "Thread 1 storing a state happens-before Thread 2
>>                     reads shared" is wrong kind of reasoning. As if
>>                     we can tell Thread 2 /when/ to read. Don't try to
>>                     bend the spoon, it is impossible.
>>
>>                     Same, but put in different words:
>>
>>                     "shared == this /*may*/ be observed" -> "super
>>                     class state /*is*/ consistent" for some
>>                     definition of consistency.
>>
>>                     Here arrow "->" is implication, and it turns out
>>                     to be exactly opposite of happens-before.
>>
>>                     "may" deliberately states that we cannot tell
>>                     what anyone else is doing. "shared == this"
>>                     deliberately states what condition we observe,
>>                     not just "any subsequent load of the same
>>                     variable". (what is "subsequent"? remember,
>>                     processors don't vote on this. It is a true
>>                     statement about "any load /that happens to be
>>                     subsequent/", but that's tautological)
>>
>>                     "is" deliberately states what we guarantee
>>                     thread-locally.
>>
>>
>>                     It is clear that shared==this can not be
>>                     observed, if we don't store to shared. Now we
>>                     have everything that is needed to reason about
>>                     validity of code using only thread-local ordering.
>>
>>                     "shared == this may be observed" -> "shared=this
>>                     was executed"  // obviously
>>                     "shared=this was executed" -> "super class state
>>                     is consistent" // /this/ is happens-before, and
>>                     it is thread-local
>>
>>                     That's it. Add store-store barrier between
>>                     shared=this and end of super-class constructor.
>>
>>
>>                     Nathan's example is not complete, but not because
>>                     volatile loads are missing in other methods, but
>>                     because of three simple rules in JMM:
>>
>>                     1. Volatile load can go ahead of anything
>>                     non-volatile
>>                     2. Anything non-volatile can go ahead of volatile
>>                     store
>>                     3. Anything non-volatile can go ahead of anything
>>                     non-volatile
>>
>>
>>                 Alex, I don't think these rules are in JMM. JMM
>>                 itself is a little weaker. For example,
>>
>>                     var = 1; // write#1
>>                 synchronized(new Object()){}
>>                     var = 2; // write#2
>>
>>                 the 2nd line can be elided, and the two writes can be
>>                 reordered, as far as JMM is concerned.
>>
>>                 Zhong Yu
>>
>>
>>                     So, shared=this may be allowed to go ahead of
>>                     s_fence=false, if shared is not volatile, and
>>                     then can be reordered with "super class state is
>>                     consistent". In order to stop this from
>>                     happening, you need a volatile load after a
>>                     volatile store and before shared=this. This way
>>                     shared=this cannot go ahead of s_fence=false,
>>                     because volatile load cannot go ahead of
>>                     s_fence=false, and shared=this cannot go ahead of
>>                     a volatile load.
>>
>>                     shared.x is always issued as Load shared; Load
>>                     shared.x, no additional barriers are needed in
>>                     threads that access superclass fields initialized
>>                     prior to publishing shared=this.
>>
>>
>>                     There is one thin case. If, and this is a big if,
>>                     Load shared.x is allowed to go ahead of Load
>>                     shared (eg if CPU already loaded that cache line
>>                     and cache coherency is weak), then there must be
>>                     a Load-Load barrier between Load shared and Load
>>                     shared.x, but even this cannot be achieved
>>                     through a volatile load at the beginning of the
>>                     method body - because of rule (1) volatile load
>>                     can go ahead of non-volatile Load shared, and we
>>                     are back to square one.
>>
>>                     So, a volatile load at the beginning of a method
>>                     is wrong, and I'd check what JMM says about
>>                     reordering of dependent normal field x loads and
>>                     a stronger barrier may be needed after the first
>>                     load of shared - there certainly is a guarantee
>>                     about final x. I don't think the data dependency
>>                     reordering is possible (how can you eliminate
>>                     Load shared.x before you know what shared refers
>>                     to), but let's check.
>>
>>
>>                     Alex
>>
>>
>>
>>                     On 09/05/2013 00:33, Vitaly Davidovich wrote:
>>>
>>>                     For JMM, I believe it has to be a read of
>>>                     s_fence everywhere where you want to then read
>>>                     the normal writes preceding the volatile store,
>>>                     as Yuval said. That's how HB is defined.
>>>
>>>                     Sent from my phone
>>>
>>>                     On May 8, 2013 4:57 PM, "oleksandr otenko"
>>>                     <oleksandr.otenko at oracle.com
>>>                     <mailto:oleksandr.otenko at oracle.com>> wrote:
>>>
>>>                         No, just a volatile read immediately after
>>>                         the write.
>>>
>>>                         Alex
>>>
>>>                         On 08/05/2013 19:42, Yuval Shavit wrote:
>>>>                         Is that an artifact of the common/global
>>>>                         implementation, or the JLS itself? I
>>>>                         thought that in order for it to work
>>>>                         according to the JLS, you would have to
>>>>                         read s_fence as the first thing within each
>>>>                         of MyClass's methods:
>>>>
>>>>                         public void foo() {
>>>>                         if (s_fence) throw new AssertionError();
>>>>                         doMyWhatever();
>>>>                             }
>>>>
>>>>                         This would establish a HB before the read
>>>>                         and the post-initialization write. There's
>>>>                         no formal HB without the read, right?
>>>>
>>>>
>>>>                         On Wed, May 8, 2013 at 2:14 PM, Nathan
>>>>                         Reynolds <nathan.reynolds at oracle.com
>>>>                         <mailto:nathan.reynolds at oracle.com>> wrote:
>>>>
>>>>                             A much heavier handed solution would be...
>>>>
>>>>                             class MyClass
>>>>                             {
>>>>                                 static volatile boolean s_fence;
>>>>
>>>>                                 public MyClass()
>>>>                                 {
>>>>                                     // initialize all fields
>>>>                             s_fence = false;
>>>>                                     // leak "this"
>>>>                                 }
>>>>                             }
>>>>
>>>>                             Writing to s_fence guarantees that all
>>>>                             of the fields will be globally visible
>>>>                             before "this" is leaked.  This is more
>>>>                             expensive since it adds a Store-Load
>>>>                             memory fence rather than just a
>>>>                             Store-Store fence.  The Store-Store
>>>>                             fence will turn into a noop on x86.
>>>>
>>>>                             The other problem with writing to
>>>>                             s_fence is that the cache line holding
>>>>                             it is going to be contended. If enough
>>>>                             MyClass objects are created
>>>>                             concurrently on different cores, cache
>>>>                             performance is going to be the bottleneck.
>>>>
>>>>                             Nathan Reynolds
>>>>                             <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>
>>>>                             | Architect | 602.333.9091
>>>>                             <tel:602.333.9091>
>>>>                             Oracle PSR Engineering
>>>>                             <http://psr.us.oracle.com/> | Server
>>>>                             Technology
>>>>                             On 5/8/2013 8:02 AM, Zhong Yu wrote:
>>>>>                             If the publication is safe, there is
>>>>>                             nothing to worry about, as long as
>>>>>                             `this` is leaked after fields are
>>>>>                             assigned.
>>>>>
>>>>>                             If the publication is unsafe, a trick
>>>>>                             I would use it to wrap the object
>>>>>                             reference as a final field, and
>>>>>                             publish the wrapper
>>>>>
>>>>>                                 class FinalReference<T>
>>>>>                             final T referent;
>>>>>                             FinalReference(T referent){
>>>>>                             this.referent=referent; }
>>>>>
>>>>>                                 static FinalReference<Foo>
>>>>>                             unsafePubVar;
>>>>>
>>>>>                                 Foo foo = ...;
>>>>>                             unsafePubVar = new FinalReference(foo);
>>>>>
>>>>>                             forcing the subscription side to go
>>>>>                             though a final field to access `foo`
>>>>>                             and its fields.
>>>>>
>>>>>                             Zhong Yu
>>>>>
>>>>>
>>>>>
>>>>>                             On Wed, May 8, 2013 at 4:27 AM, Peter
>>>>>                             Firmstone <peter.firmstone at zeus.net.au
>>>>>                             <mailto:peter.firmstone at zeus.net.au>>
>>>>>                             wrote:
>>>>>
>>>>>                                 I'm attempting to fix an object
>>>>>                                 (public api) that uses unsafe
>>>>>                                 construction (lets "this" escape
>>>>>                                 to other threads via inner classes
>>>>>                                 during construction).
>>>>>
>>>>>                                 To safely construct the object I
>>>>>                                 need to modify the object to delay
>>>>>                                 publication of the "this"
>>>>>                                 reference until after construction.
>>>>>
>>>>>                                 I was considering a start() or
>>>>>                                 init() method to do so.
>>>>>
>>>>>                                 However legacy code won't call
>>>>>                                 that method, so I need a way to
>>>>>                                 invoke start() or init() without
>>>>>                                 relying on other methods.
>>>>>
>>>>>                                 Having every method check the
>>>>>                                 state either isn't practical,
>>>>>                                 since legacy code doesn't
>>>>>                                 necessarily invoke any method
>>>>>                                 calls immediately and the
>>>>>                                 constructor invokes background
>>>>>                                 threads to perform work that
>>>>>                                 client code eventually uses.
>>>>>
>>>>>                                 The solution I'm thinking of is to
>>>>>                                 move the implementation into a
>>>>>                                 package private abstract
>>>>>                                 superclass, so the object becomes
>>>>>                                 stateless (the superclass is
>>>>>                                 responsible for state).  Then from
>>>>>                                 the constructor, call the
>>>>>                                 superclass constructor, then call
>>>>>                                 the start() method.
>>>>>
>>>>>                                 Is it reasonable to assume that
>>>>>                                 after the superclass constructor
>>>>>                                 returns, all superclass final
>>>>>                                 fields are safely published?  So
>>>>>                                 the stateless child class can call
>>>>>                                 the start() method from within the
>>>>>                                 constructor safely publishing the
>>>>>                                 "this" reference after all final
>>>>>                                 fields and the objects they
>>>>>                                 reference are fully initialized.
>>>>>
>>>>>                                 Thanks in advance,
>>>>>
>>>>>                                 Peter.
>>>>>
>>>>>
>>>>>                                 _______________________________________________
>>>>>                                 Concurrency-interest mailing list
>>>>>                                 Concurrency-interest at cs.oswego.edu
>>>>>                                 <mailto:Concurrency-interest at cs.oswego.edu>
>>>>>                                 http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>                             _______________________________________________
>>>>>                             Concurrency-interest mailing list
>>>>>                             Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>>>>                             http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>>                             _______________________________________________
>>>>                             Concurrency-interest mailing list
>>>>                             Concurrency-interest at cs.oswego.edu
>>>>                             <mailto:Concurrency-interest at cs.oswego.edu>
>>>>                             http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>>
>>>>
>>>>                         _______________________________________________
>>>>                         Concurrency-interest mailing list
>>>>                         Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>>>                         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>>                         _______________________________________________
>>>                         Concurrency-interest mailing list
>>>                         Concurrency-interest at cs.oswego.edu
>>>                         <mailto:Concurrency-interest at cs.oswego.edu>
>>>                         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>
>>
>>                     _______________________________________________
>>                     Concurrency-interest mailing list
>>                     Concurrency-interest at cs.oswego.edu
>>                     <mailto:Concurrency-interest at cs.oswego.edu>
>>                     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130510/14fbb054/attachment-0001.html>

From oleksandr.otenko at oracle.com  Fri May 10 13:28:48 2013
From: oleksandr.otenko at oracle.com (oleksandr otenko)
Date: Fri, 10 May 2013 18:28:48 +0100
Subject: [concurrency-interest] safe construction
In-Reply-To: <CACuKZqGneK3kc9ed5VrxogWWmhtJtFWnQuKDGKdSZfs4Ou-wHQ@mail.gmail.com>
References: <NFBBKALFDCPFIDBNKAPCEEOMJMAA.davidcholmes@aapt.net.au>
	<NFBBKALFDCPFIDBNKAPCMEOMJMAA.davidcholmes@aapt.net.au>
	<CACuKZqE+CxunTi3KPtRTG9Q1bmjVYkrdpboJnD9rE099btFA_A@mail.gmail.com>
	<518CBD4D.6080604@oracle.com>
	<CACuKZqGneK3kc9ed5VrxogWWmhtJtFWnQuKDGKdSZfs4Ou-wHQ@mail.gmail.com>
Message-ID: <518D2E50.1050402@oracle.com>

There must be a distinction between lock ownership (which requires the 
acquire of the same lock) and barriers (which must have stronger 
guarantees to enable synchronization that appears out-of-band for some 
observers - see the other email for the example).


Alex


On 10/05/2013 16:27, Zhong Yu wrote:
> I guess the difference is that, JMM establishes an edge from a release 
> to a subsequent acquire of the *same* variable, while roach motel 
> establishes an edge from a release to a subsequent acquire of *any* 
> variable., Therefore JMM is considerably weaker than roach motel. 
> Maybe they needed JMM to be just strong enough to prove their main 
> theorem (data race free -> sequentially consistent), not stronger.
>
> Zhong Yu
>
>
>
> On Fri, May 10, 2013 at 4:26 AM, oleksandr otenko 
> <oleksandr.otenko at oracle.com <mailto:oleksandr.otenko at oracle.com>> wrote:
>
>     Which bit of JMM allows to reorder these two writes?
>
>     Even if the lock acquire/release instructions are not issued
>     (because we can prove the locks are thread-local), you'd still
>     need to heed the barriers and a serialization point between the
>     first monitor exit and the second monitor enter.
>
>     Alex
>
>     On 10/05/2013 00:14, Zhong Yu wrote:
>>     Right, [lock, unlock] is not analogous to Alex's solution of
>>     [volatile write, volatile read]. How about this example
>>
>>         v1 = 1;
>>         synchronized(new Object()){}
>>         synchronized(new Object()){}
>>         v2 = 2;
>>
>>     JMM allows reordering of the two writes, but roach motel doesn't.
>>
>>     Zhong Yu
>>
>>     On Thu, May 9, 2013 at 5:50 PM, David Holmes
>>     <davidcholmes at aapt.net.au <mailto:davidcholmes at aapt.net.au>> wrote:
>>
>>         Ignore this - it is meaningless in this context. Both writes
>>         can be moved into the sync block anyway and then reordered
>>         within it.
>>         David
>>
>>             -----Original Message-----
>>             *From:* David Holmes [mailto:davidcholmes at aapt.net.au
>>             <mailto:davidcholmes at aapt.net.au>]
>>             *Sent:* Friday, 10 May 2013 8:44 AM
>>             *To:* Zhong Yu; oleksandr otenko
>>             *Cc:* Concurrency-interest at cs.oswego.edu
>>             <mailto:Concurrency-interest at cs.oswego.edu>
>>             *Subject:* RE: [concurrency-interest] safe construction
>>
>>             You can elide the monitor acquisition but you can't elide
>>             the happens-before edges.
>>             David
>>
>>                 -----Original Message-----
>>                 *From:* concurrency-interest-bounces at cs.oswego.edu
>>                 <mailto:concurrency-interest-bounces at cs.oswego.edu>
>>                 [mailto:concurrency-interest-bounces at cs.oswego.edu
>>                 <mailto:concurrency-interest-bounces at cs.oswego.edu>]*On
>>                 Behalf Of *Zhong Yu
>>                 *Sent:* Friday, 10 May 2013 12:50 AM
>>                 *To:* oleksandr otenko
>>                 *Cc:* Concurrency-interest at cs.oswego.edu
>>                 <mailto:Concurrency-interest at cs.oswego.edu>
>>                 *Subject:* Re: [concurrency-interest] safe construction
>>
>>
>>
>>
>>                 On Thu, May 9, 2013 at 8:44 AM, oleksandr otenko
>>                 <oleksandr.otenko at oracle.com
>>                 <mailto:oleksandr.otenko at oracle.com>> wrote:
>>
>>                     No, not in this case. In this case "this" is
>>                     published, and you cannot call public void foo
>>                     without observing shared==this.
>>
>>
>>                     JMM only defines thread-local orderings. The
>>                     processors don't vote which instructions to reorder.
>>
>>                     "Thread 1 storing a state happens-before Thread 2
>>                     reads shared" is wrong kind of reasoning. As if
>>                     we can tell Thread 2 /when/ to read. Don't try to
>>                     bend the spoon, it is impossible.
>>
>>                     Same, but put in different words:
>>
>>                     "shared == this /*may*/ be observed" -> "super
>>                     class state /*is*/ consistent" for some
>>                     definition of consistency.
>>
>>                     Here arrow "->" is implication, and it turns out
>>                     to be exactly opposite of happens-before.
>>
>>                     "may" deliberately states that we cannot tell
>>                     what anyone else is doing. "shared == this"
>>                     deliberately states what condition we observe,
>>                     not just "any subsequent load of the same
>>                     variable". (what is "subsequent"? remember,
>>                     processors don't vote on this. It is a true
>>                     statement about "any load /that happens to be
>>                     subsequent/", but that's tautological)
>>
>>                     "is" deliberately states what we guarantee
>>                     thread-locally.
>>
>>
>>                     It is clear that shared==this can not be
>>                     observed, if we don't store to shared. Now we
>>                     have everything that is needed to reason about
>>                     validity of code using only thread-local ordering.
>>
>>                     "shared == this may be observed" -> "shared=this
>>                     was executed"  // obviously
>>                     "shared=this was executed" -> "super class state
>>                     is consistent" // /this/ is happens-before, and
>>                     it is thread-local
>>
>>                     That's it. Add store-store barrier between
>>                     shared=this and end of super-class constructor.
>>
>>
>>                     Nathan's example is not complete, but not because
>>                     volatile loads are missing in other methods, but
>>                     because of three simple rules in JMM:
>>
>>                     1. Volatile load can go ahead of anything
>>                     non-volatile
>>                     2. Anything non-volatile can go ahead of volatile
>>                     store
>>                     3. Anything non-volatile can go ahead of anything
>>                     non-volatile
>>
>>
>>                 Alex, I don't think these rules are in JMM. JMM
>>                 itself is a little weaker. For example,
>>
>>                     var = 1;   // write#1
>>                 synchronized(new Object()){}
>>                     var = 2;  // write#2
>>
>>                 the 2nd line can be elided, and the two writes can be
>>                 reordered, as far as JMM is concerned.
>>
>>                 Zhong Yu
>>
>>
>>                     So, shared=this may be allowed to go ahead of
>>                     s_fence=false, if shared is not volatile, and
>>                     then can be reordered with "super class state is
>>                     consistent". In order to stop this from
>>                     happening, you need a volatile load after a
>>                     volatile store and before shared=this. This way
>>                     shared=this cannot go ahead of s_fence=false,
>>                     because volatile load cannot go ahead of
>>                     s_fence=false, and shared=this cannot go ahead of
>>                     a volatile load.
>>
>>                     shared.x is always issued as Load shared; Load
>>                     shared.x, no additional barriers are needed in
>>                     threads that access superclass fields initialized
>>                     prior to publishing shared=this.
>>
>>
>>                     There is one thin case. If, and this is a big if,
>>                     Load shared.x is allowed to go ahead of Load
>>                     shared (eg if CPU already loaded that cache line
>>                     and cache coherency is weak), then there must be
>>                     a Load-Load barrier between Load shared and Load
>>                     shared.x, but even this cannot be achieved
>>                     through a volatile load at the beginning of the
>>                     method body - because of rule (1) volatile load
>>                     can go ahead of non-volatile Load shared, and we
>>                     are back to square one.
>>
>>                     So, a volatile load at the beginning of a method
>>                     is wrong, and I'd check what JMM says about
>>                     reordering of dependent normal field x loads and
>>                     a stronger barrier may be needed after the first
>>                     load of shared - there certainly is a guarantee
>>                     about final x. I don't think the data dependency
>>                     reordering is possible (how can you eliminate
>>                     Load shared.x before you know what shared refers
>>                     to), but let's check.
>>
>>
>>                     Alex
>>
>>
>>
>>                     On 09/05/2013 00:33, Vitaly Davidovich wrote:
>>>
>>>                     For JMM, I believe it has to be a read of
>>>                     s_fence everywhere where you want to then read
>>>                     the normal writes preceding the volatile store,
>>>                     as Yuval said. That's how HB is defined.
>>>
>>>                     Sent from my phone
>>>
>>>                     On May 8, 2013 4:57 PM, "oleksandr otenko"
>>>                     <oleksandr.otenko at oracle.com
>>>                     <mailto:oleksandr.otenko at oracle.com>> wrote:
>>>
>>>                         No, just a volatile read immediately after
>>>                         the write.
>>>
>>>                         Alex
>>>
>>>                         On 08/05/2013 19:42, Yuval Shavit wrote:
>>>>                         Is that an artifact of the common/global
>>>>                         implementation, or the JLS itself? I
>>>>                         thought that in order for it to work
>>>>                         according to the JLS, you would have to
>>>>                         read s_fence as the first thing within each
>>>>                         of MyClass's methods:
>>>>
>>>>                         public void foo() {
>>>>                         if (s_fence) throw new AssertionError();
>>>>                         doMyWhatever();
>>>>                             }
>>>>
>>>>                         This would establish a HB before the read
>>>>                         and the post-initialization write. There's
>>>>                         no formal HB without the read, right?
>>>>
>>>>
>>>>                         On Wed, May 8, 2013 at 2:14 PM, Nathan
>>>>                         Reynolds <nathan.reynolds at oracle.com
>>>>                         <mailto:nathan.reynolds at oracle.com>> wrote:
>>>>
>>>>                             A much heavier handed solution would be...
>>>>
>>>>                             class MyClass
>>>>                             {
>>>>                                 static volatile boolean s_fence;
>>>>
>>>>                                 public MyClass()
>>>>                                 {
>>>>                                     // initialize all fields
>>>>                             s_fence = false;
>>>>                                     // leak "this"
>>>>                                 }
>>>>                             }
>>>>
>>>>                             Writing to s_fence guarantees that all
>>>>                             of the fields will be globally visible
>>>>                             before "this" is leaked.  This is more
>>>>                             expensive since it adds a Store-Load
>>>>                             memory fence rather than just a
>>>>                             Store-Store fence.  The Store-Store
>>>>                             fence will turn into a noop on x86.
>>>>
>>>>                             The other problem with writing to
>>>>                             s_fence is that the cache line holding
>>>>                             it is going to be contended. If enough
>>>>                             MyClass objects are created
>>>>                             concurrently on different cores, cache
>>>>                             performance is going to be the bottleneck.
>>>>
>>>>                             Nathan Reynolds
>>>>                             <http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds>
>>>>                             | Architect | 602.333.9091
>>>>                             <tel:602.333.9091>
>>>>                             Oracle PSR Engineering
>>>>                             <http://psr.us.oracle.com/> | Server
>>>>                             Technology
>>>>                             On 5/8/2013 8:02 AM, Zhong Yu wrote:
>>>>>                             If the publication is safe, there is
>>>>>                             nothing to worry about, as long as
>>>>>                             `this` is leaked after fields are
>>>>>                             assigned.
>>>>>
>>>>>                             If the publication is unsafe, a trick
>>>>>                             I would use it to wrap the object
>>>>>                             reference as a final field, and
>>>>>                             publish the wrapper
>>>>>
>>>>>                                 class FinalReference<T>
>>>>>                             final T referent;
>>>>>                             FinalReference(T referent){
>>>>>                             this.referent=referent; }
>>>>>
>>>>>                                 static FinalReference<Foo>
>>>>>                             unsafePubVar;
>>>>>
>>>>>                                 Foo foo = ...;
>>>>>                             unsafePubVar = new FinalReference(foo);
>>>>>
>>>>>                             forcing the subscription side to go
>>>>>                             though a final field to access `foo`
>>>>>                             and its fields.
>>>>>
>>>>>                             Zhong Yu
>>>>>
>>>>>
>>>>>
>>>>>                             On Wed, May 8, 2013 at 4:27 AM, Peter
>>>>>                             Firmstone <peter.firmstone at zeus.net.au
>>>>>                             <mailto:peter.firmstone at zeus.net.au>>
>>>>>                             wrote:
>>>>>
>>>>>                                 I'm attempting to fix an object
>>>>>                                 (public api) that uses unsafe
>>>>>                                 construction (lets "this" escape
>>>>>                                 to other threads via inner classes
>>>>>                                 during construction).
>>>>>
>>>>>                                 To safely construct the object I
>>>>>                                 need to modify the object to delay
>>>>>                                 publication of the "this"
>>>>>                                 reference until after construction.
>>>>>
>>>>>                                 I was considering a start() or
>>>>>                                 init() method to do so.
>>>>>
>>>>>                                 However legacy code won't call
>>>>>                                 that method, so I need a way to
>>>>>                                 invoke start() or init() without
>>>>>                                 relying on other methods.
>>>>>
>>>>>                                 Having every method check the
>>>>>                                 state either isn't practical,
>>>>>                                 since legacy code doesn't
>>>>>                                 necessarily invoke any method
>>>>>                                 calls immediately and the
>>>>>                                 constructor invokes background
>>>>>                                 threads to perform work that
>>>>>                                 client code eventually uses.
>>>>>
>>>>>                                 The solution I'm thinking of is to
>>>>>                                 move the implementation into a
>>>>>                                 package private abstract
>>>>>                                 superclass, so the object becomes
>>>>>                                 stateless (the superclass is
>>>>>                                 responsible for state).  Then from
>>>>>                                 the constructor, call the
>>>>>                                 superclass constructor, then call
>>>>>                                 the start() method.
>>>>>
>>>>>                                 Is it reasonable to assume that
>>>>>                                 after the superclass constructor
>>>>>                                 returns, all superclass final
>>>>>                                 fields are safely published?  So
>>>>>                                 the stateless child class can call
>>>>>                                 the start() method from within the
>>>>>                                 constructor safely publishing the
>>>>>                                 "this" reference after all final
>>>>>                                 fields and the objects they
>>>>>                                 reference are fully initialized.
>>>>>
>>>>>                                 Thanks in advance,
>>>>>
>>>>>                                 Peter.
>>>>>
>>>>>
>>>>>                                 _______________________________________________
>>>>>                                 Concurrency-interest mailing list
>>>>>                                 Concurrency-interest at cs.oswego.edu
>>>>>                                 <mailto:Concurrency-interest at cs.oswego.edu>
>>>>>                                 http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>                             _______________________________________________
>>>>>                             Concurrency-interest mailing list
>>>>>                             Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>>>>                             http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>>                             _______________________________________________
>>>>                             Concurrency-interest mailing list
>>>>                             Concurrency-interest at cs.oswego.edu
>>>>                             <mailto:Concurrency-interest at cs.oswego.edu>
>>>>                             http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>>
>>>>
>>>>                         _______________________________________________
>>>>                         Concurrency-interest mailing list
>>>>                         Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>>>                         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>>                         _______________________________________________
>>>                         Concurrency-interest mailing list
>>>                         Concurrency-interest at cs.oswego.edu
>>>                         <mailto:Concurrency-interest at cs.oswego.edu>
>>>                         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>
>>
>>                     _______________________________________________
>>                     Concurrency-interest mailing list
>>                     Concurrency-interest at cs.oswego.edu
>>                     <mailto:Concurrency-interest at cs.oswego.edu>
>>                     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130510/06d63088/attachment-0001.html>

From robert.nicholson at gmail.com  Tue May 14 20:00:17 2013
From: robert.nicholson at gmail.com (Robert Nicholson)
Date: Tue, 14 May 2013 19:00:17 -0500
Subject: [concurrency-interest] Controlling the order execution of Runnables
Message-ID: <DA983678-B5A2-4BC0-BA9C-DE867406BD68@gmail.com>

Imagine you have an executor and you wish to submit "runnables" to it to benefit by parallelizing their execution by some key state of the runnable and for each key you want to ensure that the "runnables" run start to finish in order of submission.

We have a home rolled framework that accomplishes this which essentially use a blocking queue in each runnable and the runnable is stored in  a map so that the next time a record matching the same key is submitted it submits it to the same runnable with the same queue thus guaranteeing the order of execution.

The framework has grown unnecessarily complex over time and I'm looking to see if there's anything in the open source community that accomplishes the same thing before I try to re-simply our existing framework.

Records are submitted to a processor and a key defines how these records can be parallelized and the guarantee of executing them in their order of submission exists. Order of submission only matters where the records share a common key.





From vitalyd at gmail.com  Tue May 14 20:43:36 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Tue, 14 May 2013 20:43:36 -0400
Subject: [concurrency-interest] Controlling the order execution of
	Runnables
In-Reply-To: <DA983678-B5A2-4BC0-BA9C-DE867406BD68@gmail.com>
References: <DA983678-B5A2-4BC0-BA9C-DE867406BD68@gmail.com>
Message-ID: <CAHjP37HyPto=wu78HBGpathddaibK==EjUZAjMnmZ5YmQVgKDg@mail.gmail.com>

So one common and simple design for this is to have N single-thread
executors and assign runnables to a given one based on their key (e.g.
hash(key) % N).

Sent from my phone
On May 14, 2013 8:05 PM, "Robert Nicholson" <robert.nicholson at gmail.com>
wrote:

> Imagine you have an executor and you wish to submit "runnables" to it to
> benefit by parallelizing their execution by some key state of the runnable
> and for each key you want to ensure that the "runnables" run start to
> finish in order of submission.
>
> We have a home rolled framework that accomplishes this which essentially
> use a blocking queue in each runnable and the runnable is stored in  a map
> so that the next time a record matching the same key is submitted it
> submits it to the same runnable with the same queue thus guaranteeing the
> order of execution.
>
> The framework has grown unnecessarily complex over time and I'm looking to
> see if there's anything in the open source community that accomplishes the
> same thing before I try to re-simply our existing framework.
>
> Records are submitted to a processor and a key defines how these records
> can be parallelized and the guarantee of executing them in their order of
> submission exists. Order of submission only matters where the records share
> a common key.
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130514/09a46fc1/attachment.html>

From nishant.monu51 at gmail.com  Wed May 15 00:06:58 2013
From: nishant.monu51 at gmail.com (Nishant Bangarwa)
Date: Wed, 15 May 2013 09:36:58 +0530
Subject: [concurrency-interest] Controlling the order execution of
	Runnables
In-Reply-To: <CAHjP37HyPto=wu78HBGpathddaibK==EjUZAjMnmZ5YmQVgKDg@mail.gmail.com>
References: <DA983678-B5A2-4BC0-BA9C-DE867406BD68@gmail.com>
	<CAHjP37HyPto=wu78HBGpathddaibK==EjUZAjMnmZ5YmQVgKDg@mail.gmail.com>
Message-ID: <CABs1680nKvcEU-rtqYmpjcm173Y9Kz1UsTPZQR+H0EJgpQtT0Q@mail.gmail.com>

Also have a look at http://www.javaspecialists.eu/archive/Issue206.html
This also solves similar problem.

Nishant
Terracotta
Ph.- +91-9468267500
       +91-9729200044


On Wed, May 15, 2013 at 6:13 AM, Vitaly Davidovich <vitalyd at gmail.com>wrote:

> So one common and simple design for this is to have N single-thread
> executors and assign runnables to a given one based on their key (e.g.
> hash(key) % N).
>
> Sent from my phone
> On May 14, 2013 8:05 PM, "Robert Nicholson" <robert.nicholson at gmail.com>
> wrote:
>
>> Imagine you have an executor and you wish to submit "runnables" to it to
>> benefit by parallelizing their execution by some key state of the runnable
>> and for each key you want to ensure that the "runnables" run start to
>> finish in order of submission.
>>
>> We have a home rolled framework that accomplishes this which essentially
>> use a blocking queue in each runnable and the runnable is stored in  a map
>> so that the next time a record matching the same key is submitted it
>> submits it to the same runnable with the same queue thus guaranteeing the
>> order of execution.
>>
>> The framework has grown unnecessarily complex over time and I'm looking
>> to see if there's anything in the open source community that accomplishes
>> the same thing before I try to re-simply our existing framework.
>>
>> Records are submitted to a processor and a key defines how these records
>> can be parallelized and the guarantee of executing them in their order of
>> submission exists. Order of submission only matters where the records share
>> a common key.
>>
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130515/72b5c548/attachment.html>

From jcchittoda at gmail.com  Wed May 15 00:17:25 2013
From: jcchittoda at gmail.com (Jitendra Chittoda)
Date: Wed, 15 May 2013 05:17:25 +0100
Subject: [concurrency-interest] Controlling the order execution of
	Runnables
In-Reply-To: <CAHjP37HyPto=wu78HBGpathddaibK==EjUZAjMnmZ5YmQVgKDg@mail.gmail.com>
References: <DA983678-B5A2-4BC0-BA9C-DE867406BD68@gmail.com>
	<CAHjP37HyPto=wu78HBGpathddaibK==EjUZAjMnmZ5YmQVgKDg@mail.gmail.com>
Message-ID: <CAPPZNmZZadJ2_V9+DKLbw2Zvdp-cr0P=_itqjbS+ktUHospm8w@mail.gmail.com>

Hi Robert,

You can have a look at the code present @
https://github.com/jchittoda/striped-executor-service
Also follow the Dr. Heinz Kabutz newletter
http://www.javaspecialists.eu/archive/Issue206.html

On GitHub you would find Heinz implementation which is correct(as per the
test cases) but slow. My implementation is also there in "chittoda" package
which is fast but having issues(not passing test cases).

Please let me know if this helps.


Thanks & Regards,
Jitendra Chittoda
http://chittoda.com
Skype/Twitter: JChittoda




On Wed, May 15, 2013 at 1:43 AM, Vitaly Davidovich <vitalyd at gmail.com>wrote:

> So one common and simple design for this is to have N single-thread
> executors and assign runnables to a given one based on their key (e.g.
> hash(key) % N).
>
> Sent from my phone
> On May 14, 2013 8:05 PM, "Robert Nicholson" <robert.nicholson at gmail.com>
> wrote:
>
>> Imagine you have an executor and you wish to submit "runnables" to it to
>> benefit by parallelizing their execution by some key state of the runnable
>> and for each key you want to ensure that the "runnables" run start to
>> finish in order of submission.
>>
>> We have a home rolled framework that accomplishes this which essentially
>> use a blocking queue in each runnable and the runnable is stored in  a map
>> so that the next time a record matching the same key is submitted it
>> submits it to the same runnable with the same queue thus guaranteeing the
>> order of execution.
>>
>> The framework has grown unnecessarily complex over time and I'm looking
>> to see if there's anything in the open source community that accomplishes
>> the same thing before I try to re-simply our existing framework.
>>
>> Records are submitted to a processor and a key defines how these records
>> can be parallelized and the guarantee of executing them in their order of
>> submission exists. Order of submission only matters where the records share
>> a common key.
>>
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130515/d753f3f2/attachment.html>

From jcchittoda at gmail.com  Wed May 15 00:23:26 2013
From: jcchittoda at gmail.com (Jitendra Chittoda)
Date: Wed, 15 May 2013 05:23:26 +0100
Subject: [concurrency-interest] Controlling the order execution of
	Runnables
In-Reply-To: <CAPPZNmZZadJ2_V9+DKLbw2Zvdp-cr0P=_itqjbS+ktUHospm8w@mail.gmail.com>
References: <DA983678-B5A2-4BC0-BA9C-DE867406BD68@gmail.com>
	<CAHjP37HyPto=wu78HBGpathddaibK==EjUZAjMnmZ5YmQVgKDg@mail.gmail.com>
	<CAPPZNmZZadJ2_V9+DKLbw2Zvdp-cr0P=_itqjbS+ktUHospm8w@mail.gmail.com>
Message-ID: <CAPPZNmaKTFC2ob83vGz0NS9rwjuZxMwuAzDWfgdi=v35D9x8vQ@mail.gmail.com>

Also to understand how StripedExecutorService works with animation.
Download the ppt http://tinyurl.com/bh2n7v4
Download that and open in PowerPoint to see the animated slide.

Thanks & Regards,
Jitendra Chittoda
http://chittoda.com
Skype/Twitter: JChittoda




On Wed, May 15, 2013 at 5:17 AM, Jitendra Chittoda <jcchittoda at gmail.com>wrote:

> Hi Robert,
>
> You can have a look at the code present @
> https://github.com/jchittoda/striped-executor-service
> Also follow the Dr. Heinz Kabutz newletter
> http://www.javaspecialists.eu/archive/Issue206.html
>
> On GitHub you would find Heinz implementation which is correct(as per the
> test cases) but slow. My implementation is also there in "chittoda" package
> which is fast but having issues(not passing test cases).
>
> Please let me know if this helps.
>
>
> Thanks & Regards,
> Jitendra Chittoda
> http://chittoda.com
> Skype/Twitter: JChittoda
>
>
>
>
> On Wed, May 15, 2013 at 1:43 AM, Vitaly Davidovich <vitalyd at gmail.com>wrote:
>
>> So one common and simple design for this is to have N single-thread
>> executors and assign runnables to a given one based on their key (e.g.
>> hash(key) % N).
>>
>> Sent from my phone
>> On May 14, 2013 8:05 PM, "Robert Nicholson" <robert.nicholson at gmail.com>
>> wrote:
>>
>>> Imagine you have an executor and you wish to submit "runnables" to it to
>>> benefit by parallelizing their execution by some key state of the runnable
>>> and for each key you want to ensure that the "runnables" run start to
>>> finish in order of submission.
>>>
>>> We have a home rolled framework that accomplishes this which essentially
>>> use a blocking queue in each runnable and the runnable is stored in  a map
>>> so that the next time a record matching the same key is submitted it
>>> submits it to the same runnable with the same queue thus guaranteeing the
>>> order of execution.
>>>
>>> The framework has grown unnecessarily complex over time and I'm looking
>>> to see if there's anything in the open source community that accomplishes
>>> the same thing before I try to re-simply our existing framework.
>>>
>>> Records are submitted to a processor and a key defines how these records
>>> can be parallelized and the guarantee of executing them in their order of
>>> submission exists. Order of submission only matters where the records share
>>> a common key.
>>>
>>>
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130515/e88e8744/attachment-0001.html>

From pavel.rappo at gmail.com  Wed May 15 06:14:40 2013
From: pavel.rappo at gmail.com (Pavel Rappo)
Date: Wed, 15 May 2013 11:14:40 +0100
Subject: [concurrency-interest] Controlling the order execution of
	Runnables
In-Reply-To: <CAHjP37HyPto=wu78HBGpathddaibK==EjUZAjMnmZ5YmQVgKDg@mail.gmail.com>
References: <DA983678-B5A2-4BC0-BA9C-DE867406BD68@gmail.com>
	<CAHjP37HyPto=wu78HBGpathddaibK==EjUZAjMnmZ5YmQVgKDg@mail.gmail.com>
Message-ID: <CAChcVukVNGbiWSXiTKU+=h4RHumD7QVuNRsV2nSYy_mdVJkuEg@mail.gmail.com>

What about load balancing in such a scheme? :)


On Wed, May 15, 2013 at 1:43 AM, Vitaly Davidovich <vitalyd at gmail.com>wrote:

> So one common and simple design for this is to have N single-thread
> executors and assign runnables to a given one based on their key (e.g.
> hash(key) % N).
>
> Sent from my phone
> On May 14, 2013 8:05 PM, "Robert Nicholson" <robert.nicholson at gmail.com>
> wrote:
>
>> Imagine you have an executor and you wish to submit "runnables" to it to
>> benefit by parallelizing their execution by some key state of the runnable
>> and for each key you want to ensure that the "runnables" run start to
>> finish in order of submission.
>>
>> We have a home rolled framework that accomplishes this which essentially
>> use a blocking queue in each runnable and the runnable is stored in  a map
>> so that the next time a record matching the same key is submitted it
>> submits it to the same runnable with the same queue thus guaranteeing the
>> order of execution.
>>
>> The framework has grown unnecessarily complex over time and I'm looking
>> to see if there's anything in the open source community that accomplishes
>> the same thing before I try to re-simply our existing framework.
>>
>> Records are submitted to a processor and a key defines how these records
>> can be parallelized and the guarantee of executing them in their order of
>> submission exists. Order of submission only matters where the records share
>> a common key.
>>
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


-- 
Sincerely yours, Pavel Rappo.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130515/0dde4edc/attachment.html>

From jcchittoda at gmail.com  Wed May 15 07:41:04 2013
From: jcchittoda at gmail.com (Jitendra Chittoda)
Date: Wed, 15 May 2013 12:41:04 +0100
Subject: [concurrency-interest] Controlling the order execution of
	Runnables
In-Reply-To: <CAChcVukpDUC8NyrNpifWCcCpyHBX+ieDpzWazEPEWis0=+udQA@mail.gmail.com>
References: <DA983678-B5A2-4BC0-BA9C-DE867406BD68@gmail.com>
	<CAHjP37HyPto=wu78HBGpathddaibK==EjUZAjMnmZ5YmQVgKDg@mail.gmail.com>
	<CAChcVukVNGbiWSXiTKU+=h4RHumD7QVuNRsV2nSYy_mdVJkuEg@mail.gmail.com>
	<CAPPZNmYKCgue7Y0McFApu-AMBE3dMrYNh7ZMLiBEtmdVC2rFtQ@mail.gmail.com>
	<CAChcVukpDUC8NyrNpifWCcCpyHBX+ieDpzWazEPEWis0=+udQA@mail.gmail.com>
Message-ID: <CAPPZNmZOoCveoTa0q=v2KyxynMqutZ=T+DOTWprkCE0n2Eio2Q@mail.gmail.com>

you can see my implementation @
https://github.com/jchittoda/striped-executor-service

Which works on the fixed number of queues. Instead of using hashing I am
use array indexes and maintaining the cache (map of key and index assinged)
of those indexes, so that based on the key we can identify in which queue
that tasks should be submitted.

Thanks & Regards,
Jitendra Chittoda
http://chittoda.com
Skype/Twitter: JChittoda




On Wed, May 15, 2013 at 12:34 PM, Pavel Rappo <pavel.rappo at gmail.com> wrote:

> No, I mean with hashing scheme.
>
>
> On Wed, May 15, 2013 at 12:30 PM, Jitendra Chittoda <jcchittoda at gmail.com>wrote:
>
>> Load balancing of tasks cannot be done here, as the tasks those are going
>> in one queue means those tasks are to be processed in sequence. so we
>> cannot let the tasks present in one queue processed by two threads
>> concurrently.
>>
>>
>> Thanks & Regards,
>> Jitendra Chittoda
>> http://chittoda.com
>> Skype/Twitter: JChittoda
>>
>>
>>
>>
>> On Wed, May 15, 2013 at 11:14 AM, Pavel Rappo <pavel.rappo at gmail.com>wrote:
>>
>>> What about load balancing in such a scheme? :)
>>>
>>>
>>> On Wed, May 15, 2013 at 1:43 AM, Vitaly Davidovich <vitalyd at gmail.com>wrote:
>>>
>>>> So one common and simple design for this is to have N single-thread
>>>> executors and assign runnables to a given one based on their key (e.g.
>>>> hash(key) % N).
>>>>
>>>> Sent from my phone
>>>> On May 14, 2013 8:05 PM, "Robert Nicholson" <robert.nicholson at gmail.com>
>>>> wrote:
>>>>
>>>>> Imagine you have an executor and you wish to submit "runnables" to it
>>>>> to benefit by parallelizing their execution by some key state of the
>>>>> runnable and for each key you want to ensure that the "runnables" run start
>>>>> to finish in order of submission.
>>>>>
>>>>> We have a home rolled framework that accomplishes this which
>>>>> essentially use a blocking queue in each runnable and the runnable is
>>>>> stored in  a map so that the next time a record matching the same key is
>>>>> submitted it submits it to the same runnable with the same queue thus
>>>>> guaranteeing the order of execution.
>>>>>
>>>>> The framework has grown unnecessarily complex over time and I'm
>>>>> looking to see if there's anything in the open source community that
>>>>> accomplishes the same thing before I try to re-simply our existing
>>>>> framework.
>>>>>
>>>>> Records are submitted to a processor and a key defines how these
>>>>> records can be parallelized and the guarantee of executing them in their
>>>>> order of submission exists. Order of submission only matters where the
>>>>> records share a common key.
>>>>>
>>>>>
>>>>>
>>>>>
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at cs.oswego.edu
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>
>>>
>>> --
>>> Sincerely yours, Pavel Rappo.
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>
>
>
> --
> Sincerely yours, Pavel Rappo.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130515/54f2d40d/attachment.html>

From bryan at systap.com  Wed May 15 13:37:29 2013
From: bryan at systap.com (Bryan Thompson)
Date: Wed, 15 May 2013 12:37:29 -0500
Subject: [concurrency-interest] CyclicBarrier.await(timeout,
 unit) breaks barrier if timeout is exceeded (contrary to javadoc)?
In-Reply-To: <4D346E63.4010906@optrak.co.uk>
Message-ID: <CDB93DA1.5D3B7%bryan@systap.com>

Hello,

The javadoc for CyclicBarrier.await(long timeout,TimeUnit unit) states that a TimeoutException will be thrown if the timeout is exceeded, but does not suggest that the barrier will break.

>From the javadoc:


     * <p>If the specified waiting time elapses then {@link TimeoutException}

     * is thrown. If the time is less than or equal to zero, the

     * method will not wait at all.

This await(timeout,unit) method is implemented as:

public int await(long timeout, TimeUnit unit) throws InterruptedException, BrokenBarrierException, TimeoutException {
return dowait(true, unit.toNanos(timeout));
}

However, I see the following code snip at the bottom of CyclicBarrier.doWait().  The "timed" variable is the first argument to doWait() and is true as invoked by await(timeout,unit).

if (timed && nanos <= 0L) {
breakBarrier(); // <======= breaks the barrier if there is a timeout for a timed await()
throw new TimeoutException();
}

Thus is seems pretty clear to me that the CyclicBarrier will break if the timeout is exceeded.  This is causing problems for me because I am trying to spin over barrier.await() while checking some other conditions that could require that the barrier is reset().

I'd just like to confirm if anyone else has run into this and whether my interpretation is correct.  It looks to me like the javadoc should be updated on await().

A trivial unit test for this is below.

Thanks,
Bryan

package com.bigdata.journal.jini.ha;

import java.util.concurrent.BrokenBarrierException;
import java.util.concurrent.CyclicBarrier;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.TimeoutException;

import junit.framework.TestCase;

public class TestCyclicBarrier extends TestCase {

public TestCyclicBarrier() {
}
public TestCyclicBarrier(String name) {
super(name);
}

public void test_cyclicBarrier_awaitTimeout() throws InterruptedException,
BrokenBarrierException, TimeoutException {

final CyclicBarrier b = new CyclicBarrier(2);

assertFalse(b.isBroken());

try {
b.await(1000, TimeUnit.MILLISECONDS);
fail("Barrier should not be broken");
} catch (TimeoutException ex) {

// The barrier should not be broken.
assertFalse("barrier broke with timeout.", b.isBroken());

}

}

}


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130515/69b7f1e0/attachment.html>

From martinrb at google.com  Wed May 15 14:28:11 2013
From: martinrb at google.com (Martin Buchholz)
Date: Wed, 15 May 2013 11:28:11 -0700
Subject: [concurrency-interest] CyclicBarrier.await(timeout,
 unit) breaks barrier if timeout is exceeded (contrary to javadoc)?
In-Reply-To: <CDB93DA1.5D3B7%bryan@systap.com>
References: <4D346E63.4010906@optrak.co.uk> <CDB93DA1.5D3B7%bryan@systap.com>
Message-ID: <CA+kOe08+9D_4cGt_iA9VSX88UwBpFPd4SfCMSsULsAjM_kr_PQ@mail.gmail.com>

As always, spec can be improved, but:


The CyclicBarrier uses an all-or-none breakage model for failed
synchronization attempts: If a thread leaves a barrier point prematurely
because of interruption, failure, or timeout, all other threads waiting at
that barrier point will also leave abnormally via
BrokenBarrierException<http://download.java.net/lambda/b78/docs/api/java/util/concurrent/BrokenBarrierException.html>
 (or InterruptedException<http://download.java.net/lambda/b78/docs/api/java/lang/InterruptedException.html>
if
they too were interrupted at about the same time).



On Wed, May 15, 2013 at 10:37 AM, Bryan Thompson <bryan at systap.com> wrote:

> Hello,
>
> The javadoc for CyclicBarrier.await(long timeout,TimeUnit unit) states
> that a TimeoutException will be thrown if the timeout is exceeded, but does
> not suggest that the barrier will break.
>
> From the javadoc:
>
>      * <p>If the specified waiting time elapses then {@link
> TimeoutException}
>
>      * is thrown. If the time is less than or equal to zero, the
>
>      * method will not wait at all.
>
> This await(timeout,unit) method is implemented as:
>
> public int await(long timeout, TimeUnit unit) throws
> InterruptedException, BrokenBarrierException, TimeoutException {
> return dowait(true, unit.toNanos(timeout));
> }
>
> However, I see the following code snip at the bottom of
> CyclicBarrier.doWait().  The "timed" variable is the first argument to
> doWait() and is true as invoked by await(timeout,unit).
>
> if (timed && nanos <= 0L) {
> breakBarrier(); // <======= breaks the barrier if there is a timeout for a
> timed await()
> throw new TimeoutException();
> }
>
> Thus is seems pretty clear to me that the CyclicBarrier will break if the
> timeout is exceeded.  This is causing problems for me because I am trying
> to spin over barrier.await() while checking some other conditions that
> could require that the barrier is reset().
>
> I'd just like to confirm if anyone else has run into this and whether my
> interpretation is correct.  It looks to me like the javadoc should be
> updated on await().
>
> A trivial unit test for this is below.
>
> Thanks,
> Bryan
>
> package com.bigdata.journal.jini.ha;
>
> import java.util.concurrent.BrokenBarrierException;
> import java.util.concurrent.CyclicBarrier;
> import java.util.concurrent.TimeUnit;
> import java.util.concurrent.TimeoutException;
>
> import junit.framework.TestCase;
>
> public class TestCyclicBarrier extends TestCase {
>
> public TestCyclicBarrier() {
>  }
>  public TestCyclicBarrier(String name) {
> super(name);
> }
>
> public void test_cyclicBarrier_awaitTimeout() throws InterruptedException,
> BrokenBarrierException, TimeoutException {
>
> final CyclicBarrier b = new CyclicBarrier(2);
>
> assertFalse(b.isBroken());
>
> try {
>  b.await(1000, TimeUnit.MILLISECONDS);
>  fail("Barrier should not be broken");
>  } catch (TimeoutException ex) {
>
> // The barrier should not be broken.
> assertFalse("barrier broke with timeout.", b.isBroken());
>
> }
>
> }
>
> }
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130515/f08cf0f4/attachment.html>

From bryan at systap.com  Wed May 15 14:33:10 2013
From: bryan at systap.com (Bryan Thompson)
Date: Wed, 15 May 2013 13:33:10 -0500
Subject: [concurrency-interest] CyclicBarrier.await(timeout,
 unit) breaks barrier if timeout is exceeded (contrary to javadoc)?
In-Reply-To: <CA+kOe08+9D_4cGt_iA9VSX88UwBpFPd4SfCMSsULsAjM_kr_PQ@mail.gmail.com>
Message-ID: <CDB94C4E.5D3E8%bryan@systap.com>

Yes, I saw that.  My interpretation was that it applied to await() (when you are blocked) rather than to the TimeoutException thrown out of await(timeout,unit).  A number of interfaces and classes have an "await(timeout)" pattern, but this is the only one that I can think of where the timeout has a side effect on the barrier if it expires.

I've worked around this issue by monitoring the necessary external state in another thread and then invoking barrier.reset() if necessary to break the barrier.

It would be helpful to clarify the semantics of the timeout on that method in the javadoc.

Thanks,
Bryan

From: Martin Buchholz <martinrb at google.com<mailto:martinrb at google.com>>
Date: Wednesday, May 15, 2013 2:28 PM
To: Bryan Thompson <bryan at systap.com<mailto:bryan at systap.com>>
Cc: "concurrency-interest at cs.oswego.edu<mailto:concurrency-interest at cs.oswego.edu>" <concurrency-interest at cs.oswego.edu<mailto:concurrency-interest at cs.oswego.edu>>
Subject: Re: [concurrency-interest] CyclicBarrier.await(timeout, unit) breaks barrier if timeout is exceeded (contrary to javadoc)?


As always, spec can be improved, but:


The CyclicBarrier uses an all-or-none breakage model for failed synchronization attempts: If a thread leaves a barrier point prematurely because of interruption, failure, or timeout, all other threads waiting at that barrier point will also leave abnormally via BrokenBarrierException<http://download.java.net/lambda/b78/docs/api/java/util/concurrent/BrokenBarrierException.html> (or InterruptedException<http://download.java.net/lambda/b78/docs/api/java/lang/InterruptedException.html> if they too were interrupted at about the same time).



On Wed, May 15, 2013 at 10:37 AM, Bryan Thompson <bryan at systap.com<mailto:bryan at systap.com>> wrote:
Hello,

The javadoc for CyclicBarrier.await(long timeout,TimeUnit unit) states that a TimeoutException will be thrown if the timeout is exceeded, but does not suggest that the barrier will break.

>From the javadoc:


     * <p>If the specified waiting time elapses then {@link TimeoutException}

     * is thrown. If the time is less than or equal to zero, the

     * method will not wait at all.

This await(timeout,unit) method is implemented as:

public int await(long timeout, TimeUnit unit) throws InterruptedException, BrokenBarrierException, TimeoutException {
return dowait(true, unit.toNanos(timeout));
}

However, I see the following code snip at the bottom of CyclicBarrier.doWait().  The "timed" variable is the first argument to doWait() and is true as invoked by await(timeout,unit).

if (timed && nanos <= 0L) {
breakBarrier(); // <======= breaks the barrier if there is a timeout for a timed await()
throw new TimeoutException();
}

Thus is seems pretty clear to me that the CyclicBarrier will break if the timeout is exceeded.  This is causing problems for me because I am trying to spin over barrier.await() while checking some other conditions that could require that the barrier is reset().

I'd just like to confirm if anyone else has run into this and whether my interpretation is correct.  It looks to me like the javadoc should be updated on await().

A trivial unit test for this is below.

Thanks,
Bryan

package com.bigdata.journal.jini.ha;

import java.util.concurrent.BrokenBarrierException;
import java.util.concurrent.CyclicBarrier;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.TimeoutException;

import junit.framework.TestCase;

public class TestCyclicBarrier extends TestCase {

public TestCyclicBarrier() {
}
public TestCyclicBarrier(String name) {
super(name);
}

public void test_cyclicBarrier_awaitTimeout() throws InterruptedException,
BrokenBarrierException, TimeoutException {

final CyclicBarrier b = new CyclicBarrier(2);

assertFalse(b.isBroken());

try {
b.await(1000, TimeUnit.MILLISECONDS);
fail("Barrier should not be broken");
} catch (TimeoutException ex) {

// The barrier should not be broken.
assertFalse("barrier broke with timeout.", b.isBroken());

}

}

}



_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu<mailto:Concurrency-interest at cs.oswego.edu>
http://cs.oswego.edu/mailman/listinfo/concurrency-interest


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130515/02eae4a9/attachment-0001.html>

From martinrb at google.com  Wed May 15 15:27:28 2013
From: martinrb at google.com (Martin Buchholz)
Date: Wed, 15 May 2013 12:27:28 -0700
Subject: [concurrency-interest] CyclicBarrier.await(timeout,
 unit) breaks barrier if timeout is exceeded (contrary to javadoc)?
In-Reply-To: <CDB94C4E.5D3E8%bryan@systap.com>
References: <CA+kOe08+9D_4cGt_iA9VSX88UwBpFPd4SfCMSsULsAjM_kr_PQ@mail.gmail.com>
	<CDB94C4E.5D3E8%bryan@systap.com>
Message-ID: <CA+kOe0-cdCyHerh1X8N-UtOja0H_6LMo+81+TDCkJCP9TVzwaA@mail.gmail.com>

It's confusing, in that methods named await are not normally
synchronization actions.

It's generally believed that Phaser is a more flexible replacement for
CyclicBarrier.

How about this clarification:

Index: ./main/java/util/concurrent/CyclicBarrier.java
===================================================================
RCS file:
/export/home/jsr166/jsr166/jsr166/src/main/java/util/concurrent/CyclicBarrier.java,v
retrieving revision 1.49
diff -u -r1.49 CyclicBarrier.java
--- ./main/java/util/concurrent/CyclicBarrier.java 9 Mar 2013 01:51:15 -0000
1.49
+++ ./main/java/util/concurrent/CyclicBarrier.java 15 May 2013 19:25:13
-0000
@@ -391,7 +391,8 @@
      *         to arrive and zero indicates the last to arrive
      * @throws InterruptedException if the current thread was interrupted
      *         while waiting
-     * @throws TimeoutException if the specified timeout elapses
+     * @throws TimeoutException if the specified timeout elapses.
+     *         In this case the barrier will be broken.
      * @throws BrokenBarrierException if <em>another</em> thread was
      *         interrupted or timed out while the current thread was
      *         waiting, or the barrier was reset, or the barrier was broken



On Wed, May 15, 2013 at 11:33 AM, Bryan Thompson <bryan at systap.com> wrote:

> Yes, I saw that.  My interpretation was that it applied to await() (when
> you are blocked) rather than to the TimeoutException thrown out of
> await(timeout,unit).  A number of interfaces and classes have an
> "await(timeout)" pattern, but this is the only one that I can think of
> where the timeout has a side effect on the barrier if it expires.
>
> I've worked around this issue by monitoring the necessary external state
> in another thread and then invoking barrier.reset() if necessary to break
> the barrier.
>
> It would be helpful to clarify the semantics of the timeout on that method
> in the javadoc.
>
> Thanks,
> Bryan
>
> From: Martin Buchholz <martinrb at google.com>
> Date: Wednesday, May 15, 2013 2:28 PM
> To: Bryan Thompson <bryan at systap.com>
> Cc: "concurrency-interest at cs.oswego.edu" <
> concurrency-interest at cs.oswego.edu>
> Subject: Re: [concurrency-interest] CyclicBarrier.await(timeout, unit)
> breaks barrier if timeout is exceeded (contrary to javadoc)?
>
> As always, spec can be improved, but:
>
>
> The CyclicBarrier uses an all-or-none breakage model for failed
> synchronization attempts: If a thread leaves a barrier point prematurely
> because of interruption, failure, or timeout, all other threads waiting at
> that barrier point will also leave abnormally via BrokenBarrierException<http://download.java.net/lambda/b78/docs/api/java/util/concurrent/BrokenBarrierException.html>
>  (or InterruptedException<http://download.java.net/lambda/b78/docs/api/java/lang/InterruptedException.html> if
> they too were interrupted at about the same time).
>
>
>
> On Wed, May 15, 2013 at 10:37 AM, Bryan Thompson <bryan at systap.com> wrote:
>
>> Hello,
>>
>> The javadoc for CyclicBarrier.await(long timeout,TimeUnit unit) states
>> that a TimeoutException will be thrown if the timeout is exceeded, but does
>> not suggest that the barrier will break.
>>
>> From the javadoc:
>>
>>      * <p>If the specified waiting time elapses then {@link
>> TimeoutException}
>>
>>      * is thrown. If the time is less than or equal to zero, the
>>
>>      * method will not wait at all.
>>
>> This await(timeout,unit) method is implemented as:
>>
>> public int await(long timeout, TimeUnit unit) throws
>> InterruptedException, BrokenBarrierException, TimeoutException {
>> return dowait(true, unit.toNanos(timeout));
>> }
>>
>> However, I see the following code snip at the bottom of
>> CyclicBarrier.doWait().  The "timed" variable is the first argument to
>> doWait() and is true as invoked by await(timeout,unit).
>>
>> if (timed && nanos <= 0L) {
>> breakBarrier(); // <======= breaks the barrier if there is a timeout for
>> a timed await()
>> throw new TimeoutException();
>> }
>>
>> Thus is seems pretty clear to me that the CyclicBarrier will break if the
>> timeout is exceeded.  This is causing problems for me because I am trying
>> to spin over barrier.await() while checking some other conditions that
>> could require that the barrier is reset().
>>
>> I'd just like to confirm if anyone else has run into this and whether my
>> interpretation is correct.  It looks to me like the javadoc should be
>> updated on await().
>>
>> A trivial unit test for this is below.
>>
>> Thanks,
>> Bryan
>>
>> package com.bigdata.journal.jini.ha;
>>
>> import java.util.concurrent.BrokenBarrierException;
>> import java.util.concurrent.CyclicBarrier;
>> import java.util.concurrent.TimeUnit;
>> import java.util.concurrent.TimeoutException;
>>
>> import junit.framework.TestCase;
>>
>> public class TestCyclicBarrier extends TestCase {
>>
>> public TestCyclicBarrier() {
>> }
>> public TestCyclicBarrier(String name) {
>> super(name);
>> }
>>
>> public void test_cyclicBarrier_awaitTimeout() throws InterruptedException,
>> BrokenBarrierException, TimeoutException {
>>
>> final CyclicBarrier b = new CyclicBarrier(2);
>>
>> assertFalse(b.isBroken());
>>
>> try {
>> b.await(1000, TimeUnit.MILLISECONDS);
>> fail("Barrier should not be broken");
>> } catch (TimeoutException ex) {
>>
>> // The barrier should not be broken.
>> assertFalse("barrier broke with timeout.", b.isBroken());
>>
>> }
>>
>> }
>>
>> }
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130515/d38aa242/attachment.html>

From bryan at systap.com  Wed May 15 15:37:48 2013
From: bryan at systap.com (Bryan Thompson)
Date: Wed, 15 May 2013 14:37:48 -0500
Subject: [concurrency-interest] CyclicBarrier.await(timeout,
 unit) breaks barrier if timeout is exceeded (contrary to javadoc)?
In-Reply-To: <CA+kOe0-cdCyHerh1X8N-UtOja0H_6LMo+81+TDCkJCP9TVzwaA@mail.gmail.com>
Message-ID: <CDB95C3B.5D402%bryan@systap.com>

Martin,

That clarification would be super.

Thanks,
Bryan

From: Martin Buchholz <martinrb at google.com<mailto:martinrb at google.com>>
Date: Wednesday, May 15, 2013 3:27 PM
To: Bryan Thompson <bryan at systap.com<mailto:bryan at systap.com>>
Cc: "concurrency-interest at cs.oswego.edu<mailto:concurrency-interest at cs.oswego.edu>" <concurrency-interest at cs.oswego.edu<mailto:concurrency-interest at cs.oswego.edu>>
Subject: Re: [concurrency-interest] CyclicBarrier.await(timeout, unit) breaks barrier if timeout is exceeded (contrary to javadoc)?

It's confusing, in that methods named await are not normally synchronization actions.

It's generally believed that Phaser is a more flexible replacement for CyclicBarrier.

How about this clarification:

Index: ./main/java/util/concurrent/CyclicBarrier.java
===================================================================
RCS file: /export/home/jsr166/jsr166/jsr166/src/main/java/util/concurrent/CyclicBarrier.java,v
retrieving revision 1.49
diff -u -r1.49 CyclicBarrier.java
--- ./main/java/util/concurrent/CyclicBarrier.java9 Mar 2013 01:51:15 -0000 1.49
+++ ./main/java/util/concurrent/CyclicBarrier.java15 May 2013 19:25:13 -0000
@@ -391,7 +391,8 @@
      *         to arrive and zero indicates the last to arrive
      * @throws InterruptedException if the current thread was interrupted
      *         while waiting
-     * @throws TimeoutException if the specified timeout elapses
+     * @throws TimeoutException if the specified timeout elapses.
+     *         In this case the barrier will be broken.
      * @throws BrokenBarrierException if <em>another</em> thread was
      *         interrupted or timed out while the current thread was
      *         waiting, or the barrier was reset, or the barrier was broken



On Wed, May 15, 2013 at 11:33 AM, Bryan Thompson <bryan at systap.com<mailto:bryan at systap.com>> wrote:
Yes, I saw that.  My interpretation was that it applied to await() (when you are blocked) rather than to the TimeoutException thrown out of await(timeout,unit).  A number of interfaces and classes have an "await(timeout)" pattern, but this is the only one that I can think of where the timeout has a side effect on the barrier if it expires.

I've worked around this issue by monitoring the necessary external state in another thread and then invoking barrier.reset() if necessary to break the barrier.

It would be helpful to clarify the semantics of the timeout on that method in the javadoc.

Thanks,
Bryan

From: Martin Buchholz <martinrb at google.com<mailto:martinrb at google.com>>
Date: Wednesday, May 15, 2013 2:28 PM
To: Bryan Thompson <bryan at systap.com<mailto:bryan at systap.com>>
Cc: "concurrency-interest at cs.oswego.edu<mailto:concurrency-interest at cs.oswego.edu>" <concurrency-interest at cs.oswego.edu<mailto:concurrency-interest at cs.oswego.edu>>
Subject: Re: [concurrency-interest] CyclicBarrier.await(timeout, unit) breaks barrier if timeout is exceeded (contrary to javadoc)?


As always, spec can be improved, but:


The CyclicBarrier uses an all-or-none breakage model for failed synchronization attempts: If a thread leaves a barrier point prematurely because of interruption, failure, or timeout, all other threads waiting at that barrier point will also leave abnormally via BrokenBarrierException<http://download.java.net/lambda/b78/docs/api/java/util/concurrent/BrokenBarrierException.html> (or InterruptedException<http://download.java.net/lambda/b78/docs/api/java/lang/InterruptedException.html> if they too were interrupted at about the same time).



On Wed, May 15, 2013 at 10:37 AM, Bryan Thompson <bryan at systap.com<mailto:bryan at systap.com>> wrote:
Hello,

The javadoc for CyclicBarrier.await(long timeout,TimeUnit unit) states that a TimeoutException will be thrown if the timeout is exceeded, but does not suggest that the barrier will break.

>From the javadoc:


     * <p>If the specified waiting time elapses then {@link TimeoutException}

     * is thrown. If the time is less than or equal to zero, the

     * method will not wait at all.

This await(timeout,unit) method is implemented as:

public int await(long timeout, TimeUnit unit) throws InterruptedException, BrokenBarrierException, TimeoutException {
return dowait(true, unit.toNanos(timeout));
}

However, I see the following code snip at the bottom of CyclicBarrier.doWait().  The "timed" variable is the first argument to doWait() and is true as invoked by await(timeout,unit).

if (timed && nanos <= 0L) {
breakBarrier(); // <======= breaks the barrier if there is a timeout for a timed await()
throw new TimeoutException();
}

Thus is seems pretty clear to me that the CyclicBarrier will break if the timeout is exceeded.  This is causing problems for me because I am trying to spin over barrier.await() while checking some other conditions that could require that the barrier is reset().

I'd just like to confirm if anyone else has run into this and whether my interpretation is correct.  It looks to me like the javadoc should be updated on await().

A trivial unit test for this is below.

Thanks,
Bryan

package com.bigdata.journal.jini.ha;

import java.util.concurrent.BrokenBarrierException;
import java.util.concurrent.CyclicBarrier;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.TimeoutException;

import junit.framework.TestCase;

public class TestCyclicBarrier extends TestCase {

public TestCyclicBarrier() {
}
public TestCyclicBarrier(String name) {
super(name);
}

public void test_cyclicBarrier_awaitTimeout() throws InterruptedException,
BrokenBarrierException, TimeoutException {

final CyclicBarrier b = new CyclicBarrier(2);

assertFalse(b.isBroken());

try {
b.await(1000, TimeUnit.MILLISECONDS);
fail("Barrier should not be broken");
} catch (TimeoutException ex) {

// The barrier should not be broken.
assertFalse("barrier broke with timeout.", b.isBroken());

}

}

}



_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu<mailto:Concurrency-interest at cs.oswego.edu>
http://cs.oswego.edu/mailman/listinfo/concurrency-interest



_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu<mailto:Concurrency-interest at cs.oswego.edu>
http://cs.oswego.edu/mailman/listinfo/concurrency-interest


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130515/44a9f715/attachment-0001.html>

From mike.duigou at oracle.com  Wed May 15 18:53:10 2013
From: mike.duigou at oracle.com (Mike Duigou)
Date: Wed, 15 May 2013 15:53:10 -0700
Subject: [concurrency-interest] Additional Warning about Mutable keys for
	CSLM/CHM?
Message-ID: <ABF8D202-2A7F-46B4-AEBD-1802A25D8EB5@oracle.com>

Hello all;

Investigating a report of a user problem with ConcurrentSkipListMap I discovered an issue that I believe warrants additional documentation.

It's well understood that Map keys must not be mutated while they are resident in the map. With CSLM there are some operations where the view of the Map between threads is slightly different. During these "windows of opportunity" if a key which is no longer in the map is mutated then unpredictable behaviour could result.

Enclosed is a test program which simulates mutation of a key in a circumstance which would seem to be "safe"--the key has been removed from the map. Run with more than one thread this test fails quite quickly. The problem is that some thread tries to look at an object which was formerly in the map but is now removed and mutated. When a map key is immutable, other than as a potential leak, it's of little concern if some thread still holds a reference to a removed object. For mutable key objects it's a significant problem. At what point does it become safe to mutate the object again?

The only practical solution seems to be to document that keys to CSLM should be immutable and might continue to be reference after removal from the collection for an indeterminate amount of time by some threads. The same advice might apply for some usages of CHM and other concurrent collections.

Does documentation alone seem sufficient? 

Mike

package test;

import java.util.ArrayList;
import java.util.Arrays;
import java.util.concurrent.ConcurrentSkipListMap;
import java.util.concurrent.ExecutionException;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import java.util.concurrent.Future;

public class ConcurrentSkipListMapMutatingKeys {

    /**
     * set to true to simulate key mutation.
     */
    private static final boolean SIMULATE_MUTABLE_KEY = true;

    private static class MyKey implements Comparable<MyKey> {

        private final byte[] key;
        volatile boolean mutated = false;

        MyKey(final byte[] key) {
            this.key = Arrays.copyOf(key, key.length);
        }

        void mutate() {
            mutated = SIMULATE_MUTABLE_KEY;
        }

        @Override
        public int hashCode() {
            assert !mutated : "mutated";
            return Arrays.hashCode(key);
        }

        @Override
        public boolean equals(final Object obj) {
            if (!(obj instanceof MyKey)) {
                return false;
            }
            final MyKey other = (MyKey)obj;
            return 0 == compareTo(other);
        }

        @Override
        public int compareTo(final MyKey compareKey) {
            assert !mutated : "mutated";
            assert !compareKey.mutated : "mutated";
            final int len1 = key.length;
            final int len2 = compareKey.key.length;
            final int compLen = Math.min(len1, len2);
            for (int i = 0; i < compLen; i++) {
                if (key[i] != compareKey.key[i]) {
                    return key[i] - compareKey.key[i];
                }
            }
            return len1 - len2;
        }
    }

    public static void main(String[] args) throws InterruptedException, ExecutionException {
        final int TEST_SIZE = Runtime.getRuntime().availableProcessors();
        Map<MyKey, MyKey> map = new ConcurrentSkipListMap<>();
        ExecutorService exec = Executors.newFixedThreadPool(TEST_SIZE);
        List<Future<?>> futures = new ArrayList<>(TEST_SIZE);
        long startTime = System.nanoTime();

        for (int i = 0; i < TEST_SIZE; i++) {
            Future<?> fu = exec.submit(new Runnable() {
                @Override
                public void run() {
                    long id = Thread.currentThread().getId();
                    byte[] name = Long.toString(id).getBytes();
                    for (int i = 0; i < 1000000; i++) {
                        final MyKey key = new MyKey(name);
                        assert null == map.get(key) : Thread.currentThread().getId() + " already there";
                        assert null == map.put(key, key) : Thread.currentThread().getId() + " replaced something";
                        assert key == map.get(key) : Thread.currentThread().getId() + " couldn't find";
                        assert key == map.remove(key) : Thread.currentThread().getId() + " couldn't remove";
                        key.mutate();
                    }
                }
            });
            futures.add(fu);
        }
        for (Future<?> fu : futures) {
            fu.get();
        }

        long endTime = System.nanoTime();
        System.out.format("Elapsed : %d ns.\n", (endTime - startTime));
        exec.shutdown();
    }
}



From martinrb at google.com  Wed May 15 20:31:25 2013
From: martinrb at google.com (Martin Buchholz)
Date: Wed, 15 May 2013 17:31:25 -0700
Subject: [concurrency-interest] Additional Warning about Mutable keys
	for CSLM/CHM?
In-Reply-To: <ABF8D202-2A7F-46B4-AEBD-1802A25D8EB5@oracle.com>
References: <ABF8D202-2A7F-46B4-AEBD-1802A25D8EB5@oracle.com>
Message-ID: <CA+kOe0_WrCbDq=3y_WROYSnF=wQvgDfWJsak_pnA-6=ygouVLA@mail.gmail.com>

I think this is a fundamental property of any concurrent data structure.
 Consider the simple AtomicReference.  Some thread could be stalled in
get() for an arbitrary time before some other thread "removes" the object
from the AtomicReference.  The only difference here is that the compareTo
operation is happening inside CLSM, but you would have the same problem if
user code tried to call compareTo on the result from AtomicReference.get().
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130515/74e65db6/attachment.html>

From andrew_nuss at yahoo.com  Thu May 16 09:18:41 2013
From: andrew_nuss at yahoo.com (Andy Nuss)
Date: Thu, 16 May 2013 06:18:41 -0700 (PDT)
Subject: [concurrency-interest] hashing the reflect.Method object in
	ConcurrentHashMap
Message-ID: <1368710321.19489.YahooMailNeo@web141102.mail.bf1.yahoo.com>

Simple questions, relating to hashing a Method object returned by Class.getDeclaredMethod, where the key is the Class:

Is there only one instance of Method in the vm for a given actual object method, or does a new Method object get created for each call to Class.getDeclaredMethod?? Is there any need when a thread obtains a Method object to publish it safely from one thread to the other or can it be shared thru back-door non-volatile locations in memory?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130516/774d461e/attachment.html>

From stanimir at riflexo.com  Thu May 16 10:16:31 2013
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Thu, 16 May 2013 17:16:31 +0300
Subject: [concurrency-interest] hashing the reflect.Method object in
	ConcurrentHashMap
In-Reply-To: <1368710321.19489.YahooMailNeo@web141102.mail.bf1.yahoo.com>
References: <1368710321.19489.YahooMailNeo@web141102.mail.bf1.yahoo.com>
Message-ID: <CAEJX8oo96mx2p=70pkMQ6RkUygyfOfh2ktF_9zMCzWLekm0M3A@mail.gmail.com>

Each call to Class.getMethod/getDeclaredMethod has to return a new instance
as the class is modifiable (setAccessible). As the safety goes: looking at
the impl. it is unsafe.
More also searching through the available methods is O(n), that is each
call searches through all the declared methods.

Stanimir


On Thu, May 16, 2013 at 4:18 PM, Andy Nuss <andrew_nuss at yahoo.com> wrote:

> Simple questions, relating to hashing a Method object returned by
> Class.getDeclaredMethod, where the key is the Class:
>
> Is there only one instance of Method in the vm for a given actual object
> method, or does a new Method object get created for each call to
> Class.getDeclaredMethod?  Is there any need when a thread obtains a Method
> object to publish it safely from one thread to the other or can it be
> shared thru back-door non-volatile locations in memory?
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130516/37b40bf8/attachment.html>

From forax at univ-mlv.fr  Thu May 16 11:09:47 2013
From: forax at univ-mlv.fr (Remi Forax)
Date: Thu, 16 May 2013 17:09:47 +0200
Subject: [concurrency-interest] hashing the reflect.Method object in
	ConcurrentHashMap
In-Reply-To: <CAEJX8oo96mx2p=70pkMQ6RkUygyfOfh2ktF_9zMCzWLekm0M3A@mail.gmail.com>
References: <1368710321.19489.YahooMailNeo@web141102.mail.bf1.yahoo.com>
	<CAEJX8oo96mx2p=70pkMQ6RkUygyfOfh2ktF_9zMCzWLekm0M3A@mail.gmail.com>
Message-ID: <5194F6BB.7010509@univ-mlv.fr>

On 05/16/2013 04:16 PM, Stanimir Simeonoff wrote:
> Each call to Class.getMethod/getDeclaredMethod has to return a new 
> instance as the class is modifiable (setAccessible). As the safety 
> goes: looking at the impl. it is unsafe.

Could you explain why ? it should be thread-safe in my opinion.

> More also searching through the available methods is O(n), that is 
> each call searches through all the declared methods.

true, usually, java.lang.MethodHandles.Lookup.find* is faster.

>
> Stanimir

R?mi

>
>
> On Thu, May 16, 2013 at 4:18 PM, Andy Nuss <andrew_nuss at yahoo.com 
> <mailto:andrew_nuss at yahoo.com>> wrote:
>
>     Simple questions, relating to hashing a Method object returned by
>     Class.getDeclaredMethod, where the key is the Class:
>
>     Is there only one instance of Method in the vm for a given actual
>     object method, or does a new Method object get created for each
>     call to Class.getDeclaredMethod?  Is there any need when a thread
>     obtains a Method object to publish it safely from one thread to
>     the other or can it be shared thru back-door non-volatile
>     locations in memory?
>
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From stanimir at riflexo.com  Thu May 16 14:17:03 2013
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Thu, 16 May 2013 21:17:03 +0300
Subject: [concurrency-interest] hashing the reflect.Method object in
	ConcurrentHashMap
In-Reply-To: <5194F6BB.7010509@univ-mlv.fr>
References: <1368710321.19489.YahooMailNeo@web141102.mail.bf1.yahoo.com>
	<CAEJX8oo96mx2p=70pkMQ6RkUygyfOfh2ktF_9zMCzWLekm0M3A@mail.gmail.com>
	<5194F6BB.7010509@univ-mlv.fr>
Message-ID: <CAEJX8oq=ZfBogziZYkEjkmbCF9sUZdrBLCv2iQpdVxWyCrrAEQ@mail.gmail.com>

On Thu, May 16, 2013 at 6:09 PM, Remi Forax <forax at univ-mlv.fr> wrote:

> On 05/16/2013 04:16 PM, Stanimir Simeonoff wrote:
>
>> Each call to Class.getMethod/**getDeclaredMethod has to return a new
>> instance as the class is modifiable (setAccessible). As the safety goes:
>> looking at the impl. it is unsafe.
>>
>
> Could you explain why ? it should be thread-safe in my opinion.


The method copy has no barrier (happens before) hence some (or all) of the
fields might not be visible if unsafely published.


Stanimir


>
>>
>> On Thu, May 16, 2013 at 4:18 PM, Andy Nuss <andrew_nuss at yahoo.com<mailto:
>> andrew_nuss at yahoo.com>**> wrote:
>>
>>     Simple questions, relating to hashing a Method object returned by
>>     Class.getDeclaredMethod, where the key is the Class:
>>
>>     Is there only one instance of Method in the vm for a given actual
>>     object method, or does a new Method object get created for each
>>     call to Class.getDeclaredMethod?  Is there any need when a thread
>>     obtains a Method object to publish it safely from one thread to
>>     the other or can it be shared thru back-door non-volatile
>>     locations in memory?
>>
>>
>>     ______________________________**_________________
>>     Concurrency-interest mailing list
>>     Concurrency-interest at cs.**oswego.edu<Concurrency-interest at cs.oswego.edu>
>>     <mailto:Concurrency-interest@**cs.oswego.edu<Concurrency-interest at cs.oswego.edu>
>> >
>>     http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>
>>
>>
>>
>>
>> ______________________________**_________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
>> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>
>
> ______________________________**_________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130516/55827192/attachment.html>

From stanimir at riflexo.com  Thu May 16 14:20:16 2013
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Thu, 16 May 2013 21:20:16 +0300
Subject: [concurrency-interest] hashing the reflect.Method object in
	ConcurrentHashMap
In-Reply-To: <1368717286.27071.YahooMailNeo@web141104.mail.bf1.yahoo.com>
References: <1368710321.19489.YahooMailNeo@web141102.mail.bf1.yahoo.com>
	<CAEJX8oo96mx2p=70pkMQ6RkUygyfOfh2ktF_9zMCzWLekm0M3A@mail.gmail.com>
	<1368717286.27071.YahooMailNeo@web141104.mail.bf1.yahoo.com>
Message-ID: <CAEJX8oqNVy87wRXRGsB5z6EbN=hu4KC9AtgqqH0qEWZ3tB_BfA@mail.gmail.com>

On Thu, May 16, 2013 at 6:14 PM, Andy Nuss <andrew_nuss at yahoo.com> wrote:

> But calling .invoke is quite fast, and getting the Method is very slow,
> and I want to invoke many times for all the threads that use this class to
> invoke the Method, each one passing its own object to invoke.  Does this
> mean that I have to synchronize on the Method instance during invoke?
>
> No. No synchronization is needed, but you should publish the method via
volatile field or AtomicReference/CHM/etc.
The underlying implementation that  carries invoke() is actually shared but
that's beyond the point.

Stanimir

>
>   ------------------------------
>  *From:* Stanimir Simeonoff <stanimir at riflexo.com>
> *To:* Andy Nuss <andrew_nuss at yahoo.com>
> *Cc:* "concurrency-interest at cs.oswego.edu" <
> concurrency-interest at cs.oswego.edu>
> *Sent:* Thursday, May 16, 2013 7:16 AM
> *Subject:* Re: [concurrency-interest] hashing the reflect.Method object
> in ConcurrentHashMap
>
> Each call to Class.getMethod/getDeclaredMethod has to return a new
> instance as the class is modifiable (setAccessible). As the safety goes:
> looking at the impl. it is unsafe.
> More also searching through the available methods is O(n), that is each
> call searches through all the declared methods.
>
> Stanimir
>
>
> On Thu, May 16, 2013 at 4:18 PM, Andy Nuss <andrew_nuss at yahoo.com> wrote:
>
> Simple questions, relating to hashing a Method object returned by
> Class.getDeclaredMethod, where the key is the Class:
>
> Is there only one instance of Method in the vm for a given actual object
> method, or does a new Method object get created for each call to
> Class.getDeclaredMethod?  Is there any need when a thread obtains a Method
> object to publish it safely from one thread to the other or can it be
> shared thru back-door non-volatile locations in memory?
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130516/1596d58e/attachment.html>

From peter.levart at gmail.com  Thu May 16 15:24:22 2013
From: peter.levart at gmail.com (Peter Levart)
Date: Thu, 16 May 2013 21:24:22 +0200
Subject: [concurrency-interest] Controlling the order execution of
	Runnables
In-Reply-To: <CAChcVukVNGbiWSXiTKU+=h4RHumD7QVuNRsV2nSYy_mdVJkuEg@mail.gmail.com>
References: <DA983678-B5A2-4BC0-BA9C-DE867406BD68@gmail.com>
	<CAHjP37HyPto=wu78HBGpathddaibK==EjUZAjMnmZ5YmQVgKDg@mail.gmail.com>
	<CAChcVukVNGbiWSXiTKU+=h4RHumD7QVuNRsV2nSYy_mdVJkuEg@mail.gmail.com>
Message-ID: <51953266.6080703@gmail.com>

This might also work:

https://github.com/plevart/concurrent-utils/blob/master/src/si/pele/concurrent/OrderedTaskWrapper.java

The specific of this approach is that it doesn't try to create a special 
kind of Executor, but to wrap Runnables in a way that allows any 
Executor to be used.

Can someone spot a fault in this approach?

Regards, Peter

On 05/15/2013 12:14 PM, Pavel Rappo wrote:
> What about load balancing in such a scheme? :)
>
>
> On Wed, May 15, 2013 at 1:43 AM, Vitaly Davidovich <vitalyd at gmail.com 
> <mailto:vitalyd at gmail.com>> wrote:
>
>     So one common and simple design for this is to have N
>     single-thread executors and assign runnables to a given one based
>     on their key (e.g. hash(key) % N).
>
>     Sent from my phone
>
>     On May 14, 2013 8:05 PM, "Robert Nicholson"
>     <robert.nicholson at gmail.com <mailto:robert.nicholson at gmail.com>>
>     wrote:
>
>         Imagine you have an executor and you wish to submit
>         "runnables" to it to benefit by parallelizing their execution
>         by some key state of the runnable and for each key you want to
>         ensure that the "runnables" run start to finish in order of
>         submission.
>
>         We have a home rolled framework that accomplishes this which
>         essentially use a blocking queue in each runnable and the
>         runnable is stored in  a map so that the next time a record
>         matching the same key is submitted it submits it to the same
>         runnable with the same queue thus guaranteeing the order of
>         execution.
>
>         The framework has grown unnecessarily complex over time and
>         I'm looking to see if there's anything in the open source
>         community that accomplishes the same thing before I try to
>         re-simply our existing framework.
>
>         Records are submitted to a processor and a key defines how
>         these records can be parallelized and the guarantee of
>         executing them in their order of submission exists. Order of
>         submission only matters where the records share a common key.
>
>
>
>
>         _______________________________________________
>         Concurrency-interest mailing list
>         Concurrency-interest at cs.oswego.edu
>         <mailto:Concurrency-interest at cs.oswego.edu>
>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
> -- 
> Sincerely yours, Pavel Rappo.
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130516/5d468159/attachment.html>

From joe.bowbeer at gmail.com  Thu May 16 15:48:32 2013
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Thu, 16 May 2013 12:48:32 -0700
Subject: [concurrency-interest] Controlling the order execution of
	Runnables
In-Reply-To: <CAHjP37HyPto=wu78HBGpathddaibK==EjUZAjMnmZ5YmQVgKDg@mail.gmail.com>
References: <DA983678-B5A2-4BC0-BA9C-DE867406BD68@gmail.com>
	<CAHjP37HyPto=wu78HBGpathddaibK==EjUZAjMnmZ5YmQVgKDg@mail.gmail.com>
Message-ID: <CAHzJPEpJnurSuyca4ucXeBUuOwypi74d_yBZUqdy+pwcpHJ8Yg@mail.gmail.com>

A template for the executor per key is SerialExecutor in the javadoc
comments:

http://docs.oracle.com/javase/7/docs/api/java/util/concurrent/Executor.html

Each of these executors should delegate to a common ThreadPoolExecutor.

Produce these serial executors on demand using a computeIfAbsent map, and
most of the problem is solved, save for messy details such as shutdown and
key management (GC-able?).

Joe


On Tue, May 14, 2013 at 5:43 PM, Vitaly Davidovich <vitalyd at gmail.com>wrote:

> So one common and simple design for this is to have N single-thread
> executors and assign runnables to a given one based on their key (e.g.
> hash(key) % N).
>
> Sent from my phone
> On May 14, 2013 8:05 PM, "Robert Nicholson" <robert.nicholson at gmail.com>
> wrote:
>
>> Imagine you have an executor and you wish to submit "runnables" to it to
>> benefit by parallelizing their execution by some key state of the runnable
>> and for each key you want to ensure that the "runnables" run start to
>> finish in order of submission.
>>
>> We have a home rolled framework that accomplishes this which essentially
>> use a blocking queue in each runnable and the runnable is stored in  a map
>> so that the next time a record matching the same key is submitted it
>> submits it to the same runnable with the same queue thus guaranteeing the
>> order of execution.
>>
>> The framework has grown unnecessarily complex over time and I'm looking
>> to see if there's anything in the open source community that accomplishes
>> the same thing before I try to re-simply our existing framework.
>>
>> Records are submitted to a processor and a key defines how these records
>> can be parallelized and the guarantee of executing them in their order of
>> submission exists. Order of submission only matters where the records share
>> a common key.
>>
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130516/33b07bde/attachment-0001.html>

From forax at univ-mlv.fr  Fri May 17 05:48:53 2013
From: forax at univ-mlv.fr (Remi Forax)
Date: Fri, 17 May 2013 11:48:53 +0200
Subject: [concurrency-interest] hashing the reflect.Method object in
	ConcurrentHashMap
In-Reply-To: <CAEJX8oq=ZfBogziZYkEjkmbCF9sUZdrBLCv2iQpdVxWyCrrAEQ@mail.gmail.com>
References: <1368710321.19489.YahooMailNeo@web141102.mail.bf1.yahoo.com>
	<CAEJX8oo96mx2p=70pkMQ6RkUygyfOfh2ktF_9zMCzWLekm0M3A@mail.gmail.com>
	<5194F6BB.7010509@univ-mlv.fr>
	<CAEJX8oq=ZfBogziZYkEjkmbCF9sUZdrBLCv2iQpdVxWyCrrAEQ@mail.gmail.com>
Message-ID: <5195FD05.1050107@univ-mlv.fr>

On 05/16/2013 08:17 PM, Stanimir Simeonoff wrote:
>
>
> On Thu, May 16, 2013 at 6:09 PM, Remi Forax <forax at univ-mlv.fr 
> <mailto:forax at univ-mlv.fr>> wrote:
>
>     On 05/16/2013 04:16 PM, Stanimir Simeonoff wrote:
>
>         Each call to Class.getMethod/getDeclaredMethod has to return a
>         new instance as the class is modifiable (setAccessible). As
>         the safety goes: looking at the impl. it is unsafe.
>
>
>     Could you explain why ? it should be thread-safe in my opinion.
>
>
> The method copy has no barrier (happens before) hence some (or all) of 
> the fields might not be visible if unsafely published.

yes,
so fields like root, methodAccessor or typeAnnotations can be null,
but given that these fields are just used to cache values that can be 
recreated,
there is no problem because code expected that these fields may be null.

>
>
> Stanimir

R?mi

>
>
>
>         On Thu, May 16, 2013 at 4:18 PM, Andy Nuss
>         <andrew_nuss at yahoo.com <mailto:andrew_nuss at yahoo.com>
>         <mailto:andrew_nuss at yahoo.com <mailto:andrew_nuss at yahoo.com>>>
>         wrote:
>
>             Simple questions, relating to hashing a Method object
>         returned by
>             Class.getDeclaredMethod, where the key is the Class:
>
>             Is there only one instance of Method in the vm for a given
>         actual
>             object method, or does a new Method object get created for
>         each
>             call to Class.getDeclaredMethod?  Is there any need when a
>         thread
>             obtains a Method object to publish it safely from one
>         thread to
>             the other or can it be shared thru back-door non-volatile
>             locations in memory?
>
>
>             _______________________________________________
>             Concurrency-interest mailing list
>         Concurrency-interest at cs.oswego.edu
>         <mailto:Concurrency-interest at cs.oswego.edu>
>             <mailto:Concurrency-interest at cs.oswego.edu
>         <mailto:Concurrency-interest at cs.oswego.edu>>
>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
>
>         _______________________________________________
>         Concurrency-interest mailing list
>         Concurrency-interest at cs.oswego.edu
>         <mailto:Concurrency-interest at cs.oswego.edu>
>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


From jcchittoda at gmail.com  Fri May 17 05:59:29 2013
From: jcchittoda at gmail.com (Jitendra Chittoda)
Date: Fri, 17 May 2013 10:59:29 +0100
Subject: [concurrency-interest] Controlling the order execution of
	Runnables
In-Reply-To: <51953266.6080703@gmail.com>
References: <DA983678-B5A2-4BC0-BA9C-DE867406BD68@gmail.com>
	<CAHjP37HyPto=wu78HBGpathddaibK==EjUZAjMnmZ5YmQVgKDg@mail.gmail.com>
	<CAChcVukVNGbiWSXiTKU+=h4RHumD7QVuNRsV2nSYy_mdVJkuEg@mail.gmail.com>
	<51953266.6080703@gmail.com>
Message-ID: <CAPPZNmYUk6xLN21=bTk-=mJtF9g6tLDgL94m8V9S+XeYcqEzjQ@mail.gmail.com>

Hi Peter,

You can use the test-cases present @ GitHub urls (that I have shared above)
to test your class.

Thanks & Regards,
Jitendra Chittoda
http://chittoda.com
Skype/Twitter: JChittoda




On Thu, May 16, 2013 at 8:24 PM, Peter Levart <peter.levart at gmail.com>wrote:

>  This might also work:
>
>
> https://github.com/plevart/concurrent-utils/blob/master/src/si/pele/concurrent/OrderedTaskWrapper.java
>
> The specific of this approach is that it doesn't try to create a special
> kind of Executor, but to wrap Runnables in a way that allows any Executor
> to be used.
>
> Can someone spot a fault in this approach?
>
> Regards, Peter
>
>
> On 05/15/2013 12:14 PM, Pavel Rappo wrote:
>
> What about load balancing in such a scheme? :)
>
>
> On Wed, May 15, 2013 at 1:43 AM, Vitaly Davidovich <vitalyd at gmail.com>wrote:
>
>> So one common and simple design for this is to have N single-thread
>> executors and assign runnables to a given one based on their key (e.g.
>> hash(key) % N).
>>
>> Sent from my phone
>>  On May 14, 2013 8:05 PM, "Robert Nicholson" <robert.nicholson at gmail.com>
>> wrote:
>>
>>> Imagine you have an executor and you wish to submit "runnables" to it to
>>> benefit by parallelizing their execution by some key state of the runnable
>>> and for each key you want to ensure that the "runnables" run start to
>>> finish in order of submission.
>>>
>>> We have a home rolled framework that accomplishes this which essentially
>>> use a blocking queue in each runnable and the runnable is stored in  a map
>>> so that the next time a record matching the same key is submitted it
>>> submits it to the same runnable with the same queue thus guaranteeing the
>>> order of execution.
>>>
>>> The framework has grown unnecessarily complex over time and I'm looking
>>> to see if there's anything in the open source community that accomplishes
>>> the same thing before I try to re-simply our existing framework.
>>>
>>> Records are submitted to a processor and a key defines how these records
>>> can be parallelized and the guarantee of executing them in their order of
>>> submission exists. Order of submission only matters where the records share
>>> a common key.
>>>
>>>
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
>
>  --
> Sincerely yours, Pavel Rappo.
>
>
> _______________________________________________
> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130517/c19be877/attachment.html>

From stanimir at riflexo.com  Fri May 17 06:09:02 2013
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Fri, 17 May 2013 13:09:02 +0300
Subject: [concurrency-interest] hashing the reflect.Method object in
	ConcurrentHashMap
In-Reply-To: <5195FD05.1050107@univ-mlv.fr>
References: <1368710321.19489.YahooMailNeo@web141102.mail.bf1.yahoo.com>
	<CAEJX8oo96mx2p=70pkMQ6RkUygyfOfh2ktF_9zMCzWLekm0M3A@mail.gmail.com>
	<5194F6BB.7010509@univ-mlv.fr>
	<CAEJX8oq=ZfBogziZYkEjkmbCF9sUZdrBLCv2iQpdVxWyCrrAEQ@mail.gmail.com>
	<5195FD05.1050107@univ-mlv.fr>
Message-ID: <CAEJX8orWVAEQw1ryXMKs5JpFgsWL8NM9VuNLgZbg0ChhcraOGg@mail.gmail.com>

On Fri, May 17, 2013 at 12:48 PM, Remi Forax <forax at univ-mlv.fr> wrote:

> On 05/16/2013 08:17 PM, Stanimir Simeonoff wrote:
>
>
>>
>> On Thu, May 16, 2013 at 6:09 PM, Remi Forax <forax at univ-mlv.fr <mailto:
>> forax at univ-mlv.fr>> wrote:
>>
>>     On 05/16/2013 04:16 PM, Stanimir Simeonoff wrote:
>>
>>         Each call to Class.getMethod/**getDeclaredMethod has to return a
>>         new instance as the class is modifiable (setAccessible). As
>>         the safety goes: looking at the impl. it is unsafe.
>>
>>
>>     Could you explain why ? it should be thread-safe in my opinion.
>>
>>
>> The method copy has no barrier (happens before) hence some (or all) of
>> the fields might not be visible if unsafely published.
>>
>
> yes,
> so fields like root, methodAccessor or typeAnnotations can be null,
> but given that these fields are just used to cache values that can be
> recreated,
> there is no problem because code expected that these fields may be null.
>


 On a weak memory model *all* fields can be null/zero, incl. clazz, name,
modifiers(!), slot and so on. "modifiers" being zero would result in
IllegalAccessException.

Stanimir


>>
>>
>>         On Thu, May 16, 2013 at 4:18 PM, Andy Nuss
>>         <andrew_nuss at yahoo.com <mailto:andrew_nuss at yahoo.com>
>>         <mailto:andrew_nuss at yahoo.com <mailto:andrew_nuss at yahoo.com>**>>
>>
>>         wrote:
>>
>>             Simple questions, relating to hashing a Method object
>>         returned by
>>             Class.getDeclaredMethod, where the key is the Class:
>>
>>             Is there only one instance of Method in the vm for a given
>>         actual
>>             object method, or does a new Method object get created for
>>         each
>>             call to Class.getDeclaredMethod?  Is there any need when a
>>         thread
>>             obtains a Method object to publish it safely from one
>>         thread to
>>             the other or can it be shared thru back-door non-volatile
>>             locations in memory?
>>
>>
>>             ______________________________**_________________
>>             Concurrency-interest mailing list
>>         Concurrency-interest at cs.**oswego.edu<Concurrency-interest at cs.oswego.edu>
>>         <mailto:Concurrency-interest@**cs.oswego.edu<Concurrency-interest at cs.oswego.edu>
>> >
>>             <mailto:Concurrency-interest@**cs.oswego.edu<Concurrency-interest at cs.oswego.edu>
>>
>>         <mailto:Concurrency-interest@**cs.oswego.edu<Concurrency-interest at cs.oswego.edu>
>> >>
>>         http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>
>>
>>
>>
>>
>>         ______________________________**_________________
>>         Concurrency-interest mailing list
>>         Concurrency-interest at cs.**oswego.edu<Concurrency-interest at cs.oswego.edu>
>>         <mailto:Concurrency-interest@**cs.oswego.edu<Concurrency-interest at cs.oswego.edu>
>> >
>>         http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>
>>
>>     ______________________________**_________________
>>     Concurrency-interest mailing list
>>     Concurrency-interest at cs.**oswego.edu<Concurrency-interest at cs.oswego.edu>
>>     <mailto:Concurrency-interest@**cs.oswego.edu<Concurrency-interest at cs.oswego.edu>
>> >
>>     http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130517/b7a21baf/attachment.html>

From ionionascu at gmail.com  Fri May 17 06:14:07 2013
From: ionionascu at gmail.com (Ion Ionascu)
Date: Fri, 17 May 2013 11:14:07 +0100
Subject: [concurrency-interest] Fwd: hashing the reflect.Method object in
	ConcurrentHashMap
In-Reply-To: <51960196.7020504@univ-mlv.fr>
References: <1368710321.19489.YahooMailNeo@web141102.mail.bf1.yahoo.com>
	<CAEJX8oo96mx2p=70pkMQ6RkUygyfOfh2ktF_9zMCzWLekm0M3A@mail.gmail.com>
	<5194F6BB.7010509@univ-mlv.fr>
	<CAOtF99qzaQWBk7fz+kieKirSkQyKF4w+t-8AX=gx7WTdK0VV5g@mail.gmail.com>
	<51960196.7020504@univ-mlv.fr>
Message-ID: <CAOtF99oWC8HvH3NTcnDmRT9dxAYJ8TDtAa+P2QGNQZOJvexnfA@mail.gmail.com>

I accidentally sent the bellow email to Remi instead of sending it to the
list.

So, please see it inlined below, together with comments from Remi.


Regards,
Ion


---------- Forwarded message ----------
From: Remi Forax <forax at univ-mlv.fr>
Date: Fri, May 17, 2013 at 11:08 AM
Subject: Re: [concurrency-interest] hashing the reflect.Method object in
ConcurrentHashMap
To: Ion Ionascu <ionionascu at gmail.com>


On 05/16/2013 05:38 PM, Ion Ionascu wrote:

> Hi Remi,
>
>
Hi Ion,
I don't know if it was intented or not, but you send me this email
privately,
so I will answer it privately even if I think it should be published on the
mailing list.


> Could the class *AccessibleObject *be considered not thread-safe? Because,
> it it is, then *Method *is also unsafe.
>

yes, the field named "override" is not declared volatile,
so you can create a method, call setAccessible(true),
and from another thread having method.isAccessible() that returns false.

This is not easily fixable without introducing a performance regression in
Method.invoke,
oops, it seems you find a bug.


>
> Regards,
> Ion
>

cheers,
R?mi


>
> On Thu, May 16, 2013 at 4:09 PM, Remi Forax <forax at univ-mlv.fr <mailto:
> forax at univ-mlv.fr>> wrote:
>
>     On 05/16/2013 04:16 PM, Stanimir Simeonoff wrote:
>
>         Each call to Class.getMethod/**getDeclaredMethod has to return a
>         new instance as the class is modifiable (setAccessible). As
>         the safety goes: looking at the impl. it is unsafe.
>
>
>     Could you explain why ? it should be thread-safe in my opinion.
>
>         More also searching through the available methods is O(n),
>         that is each call searches through all the declared methods.
>
>
>     true, usually, java.lang.MethodHandles.**Lookup.find* is faster.
>
>
>
>         Stanimir
>
>
>     R?mi
>
>
>
>         On Thu, May 16, 2013 at 4:18 PM, Andy Nuss
>         <andrew_nuss at yahoo.com <mailto:andrew_nuss at yahoo.com>
>         <mailto:andrew_nuss at yahoo.com <mailto:andrew_nuss at yahoo.com>**>>
>         wrote:
>
>             Simple questions, relating to hashing a Method object
>         returned by
>             Class.getDeclaredMethod, where the key is the Class:
>
>             Is there only one instance of Method in the vm for a given
>         actual
>             object method, or does a new Method object get created for
>         each
>             call to Class.getDeclaredMethod?  Is there any need when a
>         thread
>             obtains a Method object to publish it safely from one
>         thread to
>             the other or can it be shared thru back-door non-volatile
>             locations in memory?
>
>
>             ______________________________**_________________
>             Concurrency-interest mailing list
>         Concurrency-interest at cs.**oswego.edu<Concurrency-interest at cs.oswego.edu>
>         <mailto:Concurrency-interest@**cs.oswego.edu<Concurrency-interest at cs.oswego.edu>
> >
>             <mailto:Concurrency-interest@**cs.oswego.edu<Concurrency-interest at cs.oswego.edu>
>         <mailto:Concurrency-interest@**cs.oswego.edu<Concurrency-interest at cs.oswego.edu>
> >>
>         http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>
>
>
>
>         ______________________________**_________________
>         Concurrency-interest mailing list
>         Concurrency-interest at cs.**oswego.edu<Concurrency-interest at cs.oswego.edu>
>         <mailto:Concurrency-interest@**cs.oswego.edu<Concurrency-interest at cs.oswego.edu>
> >
>         http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>
>
>     ______________________________**_________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.**oswego.edu<Concurrency-interest at cs.oswego.edu>
>     <mailto:Concurrency-interest@**cs.oswego.edu<Concurrency-interest at cs.oswego.edu>
> >
>     http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130517/b0556501/attachment-0001.html>

From davidcholmes at aapt.net.au  Fri May 17 06:38:30 2013
From: davidcholmes at aapt.net.au (David Holmes)
Date: Fri, 17 May 2013 20:38:30 +1000
Subject: [concurrency-interest] hashing the reflect.Method object
	inConcurrentHashMap
In-Reply-To: <CAEJX8orWVAEQw1ryXMKs5JpFgsWL8NM9VuNLgZbg0ChhcraOGg@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCOECFJNAA.davidcholmes@aapt.net.au>

Maybe I've missed something in the to-and-fro here but I agree that these
reflection objects can be unsafely published (don't do that!) but in this
case they are getting published via a ConcurrentHashmap and so are in fact
safely published.

David
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Stanimir
Simeonoff
  Sent: Friday, 17 May 2013 8:09 PM
  To: Remi Forax
  Cc: concurrency-interest at cs.oswego.edu
  Subject: Re: [concurrency-interest] hashing the reflect.Method object
inConcurrentHashMap





  On Fri, May 17, 2013 at 12:48 PM, Remi Forax <forax at univ-mlv.fr> wrote:

    On 05/16/2013 08:17 PM, Stanimir Simeonoff wrote:




      On Thu, May 16, 2013 at 6:09 PM, Remi Forax <forax at univ-mlv.fr
<mailto:forax at univ-mlv.fr>> wrote:

          On 05/16/2013 04:16 PM, Stanimir Simeonoff wrote:

              Each call to Class.getMethod/getDeclaredMethod has to return a
              new instance as the class is modifiable (setAccessible). As
              the safety goes: looking at the impl. it is unsafe.


          Could you explain why ? it should be thread-safe in my opinion.


      The method copy has no barrier (happens before) hence some (or all) of
the fields might not be visible if unsafely published.



    yes,
    so fields like root, methodAccessor or typeAnnotations can be null,
    but given that these fields are just used to cache values that can be
recreated,
    there is no problem because code expected that these fields may be null.



   On a weak memory model *all* fields can be null/zero, incl. clazz, name,
modifiers(!), slot and so on. "modifiers" being zero would result in
IllegalAccessException.

  Stanimir





              On Thu, May 16, 2013 at 4:18 PM, Andy Nuss
              <andrew_nuss at yahoo.com <mailto:andrew_nuss at yahoo.com>

              <mailto:andrew_nuss at yahoo.com <mailto:andrew_nuss at yahoo.com>>>

              wrote:

                  Simple questions, relating to hashing a Method object
              returned by
                  Class.getDeclaredMethod, where the key is the Class:

                  Is there only one instance of Method in the vm for a given
              actual
                  object method, or does a new Method object get created for
              each
                  call to Class.getDeclaredMethod?  Is there any need when a
              thread
                  obtains a Method object to publish it safely from one
              thread to
                  the other or can it be shared thru back-door non-volatile
                  locations in memory?


                  _______________________________________________
                  Concurrency-interest mailing list
              Concurrency-interest at cs.oswego.edu
              <mailto:Concurrency-interest at cs.oswego.edu>

                  <mailto:Concurrency-interest at cs.oswego.edu

              <mailto:Concurrency-interest at cs.oswego.edu>>
              http://cs.oswego.edu/mailman/listinfo/concurrency-interest





              _______________________________________________
              Concurrency-interest mailing list
              Concurrency-interest at cs.oswego.edu
              <mailto:Concurrency-interest at cs.oswego.edu>
              http://cs.oswego.edu/mailman/listinfo/concurrency-interest


          _______________________________________________
          Concurrency-interest mailing list
          Concurrency-interest at cs.oswego.edu

          <mailto:Concurrency-interest at cs.oswego.edu>
          http://cs.oswego.edu/mailman/listinfo/concurrency-interest






-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130517/05fdbdd6/attachment.html>

From forax at univ-mlv.fr  Fri May 17 08:57:58 2013
From: forax at univ-mlv.fr (Remi Forax)
Date: Fri, 17 May 2013 14:57:58 +0200
Subject: [concurrency-interest] hashing the reflect.Method object in
	ConcurrentHashMap
In-Reply-To: <CAEJX8orWVAEQw1ryXMKs5JpFgsWL8NM9VuNLgZbg0ChhcraOGg@mail.gmail.com>
References: <1368710321.19489.YahooMailNeo@web141102.mail.bf1.yahoo.com>
	<CAEJX8oo96mx2p=70pkMQ6RkUygyfOfh2ktF_9zMCzWLekm0M3A@mail.gmail.com>
	<5194F6BB.7010509@univ-mlv.fr>
	<CAEJX8oq=ZfBogziZYkEjkmbCF9sUZdrBLCv2iQpdVxWyCrrAEQ@mail.gmail.com>
	<5195FD05.1050107@univ-mlv.fr>
	<CAEJX8orWVAEQw1ryXMKs5JpFgsWL8NM9VuNLgZbg0ChhcraOGg@mail.gmail.com>
Message-ID: <51962956.1080208@univ-mlv.fr>

On 05/17/2013 12:09 PM, Stanimir Simeonoff wrote:
>
>
> On Fri, May 17, 2013 at 12:48 PM, Remi Forax <forax at univ-mlv.fr 
> <mailto:forax at univ-mlv.fr>> wrote:
>
>     On 05/16/2013 08:17 PM, Stanimir Simeonoff wrote:
>
>
>
>         On Thu, May 16, 2013 at 6:09 PM, Remi Forax <forax at univ-mlv.fr
>         <mailto:forax at univ-mlv.fr> <mailto:forax at univ-mlv.fr
>         <mailto:forax at univ-mlv.fr>>> wrote:
>
>             On 05/16/2013 04:16 PM, Stanimir Simeonoff wrote:
>
>                 Each call to Class.getMethod/getDeclaredMethod has to
>         return a
>                 new instance as the class is modifiable
>         (setAccessible). As
>                 the safety goes: looking at the impl. it is unsafe.
>
>
>             Could you explain why ? it should be thread-safe in my
>         opinion.
>
>
>         The method copy has no barrier (happens before) hence some (or
>         all) of the fields might not be visible if unsafely published.
>
>
>     yes,
>     so fields like root, methodAccessor or typeAnnotations can be null,
>     but given that these fields are just used to cache values that can
>     be recreated,
>     there is no problem because code expected that these fields may be
>     null.
>
>
>
>  On a weak memory model *all* fields can be null/zero, incl. clazz, 
> name, modifiers(!), slot and so on. "modifiers" being zero would 
> result in IllegalAccessException.

you're right, I was thinking that all these fields were declared final 
but re-reading the code, they are not.
weird !

>
> Stanimir

R?mi

>
>
>
>
>                 On Thu, May 16, 2013 at 4:18 PM, Andy Nuss
>                 <andrew_nuss at yahoo.com <mailto:andrew_nuss at yahoo.com>
>         <mailto:andrew_nuss at yahoo.com <mailto:andrew_nuss at yahoo.com>>
>                 <mailto:andrew_nuss at yahoo.com
>         <mailto:andrew_nuss at yahoo.com> <mailto:andrew_nuss at yahoo.com
>         <mailto:andrew_nuss at yahoo.com>>>>
>
>                 wrote:
>
>                     Simple questions, relating to hashing a Method object
>                 returned by
>                     Class.getDeclaredMethod, where the key is the Class:
>
>                     Is there only one instance of Method in the vm for
>         a given
>                 actual
>                     object method, or does a new Method object get
>         created for
>                 each
>                     call to Class.getDeclaredMethod?  Is there any
>         need when a
>                 thread
>                     obtains a Method object to publish it safely from one
>                 thread to
>                     the other or can it be shared thru back-door
>         non-volatile
>                     locations in memory?
>
>
>                     _______________________________________________
>                     Concurrency-interest mailing list
>         Concurrency-interest at cs.oswego.edu
>         <mailto:Concurrency-interest at cs.oswego.edu>
>                 <mailto:Concurrency-interest at cs.oswego.edu
>         <mailto:Concurrency-interest at cs.oswego.edu>>
>                     <mailto:Concurrency-interest at cs.oswego.edu
>         <mailto:Concurrency-interest at cs.oswego.edu>
>
>                 <mailto:Concurrency-interest at cs.oswego.edu
>         <mailto:Concurrency-interest at cs.oswego.edu>>>
>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
>
>                 _______________________________________________
>                 Concurrency-interest mailing list
>         Concurrency-interest at cs.oswego.edu
>         <mailto:Concurrency-interest at cs.oswego.edu>
>                 <mailto:Concurrency-interest at cs.oswego.edu
>         <mailto:Concurrency-interest at cs.oswego.edu>>
>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>             _______________________________________________
>             Concurrency-interest mailing list
>         Concurrency-interest at cs.oswego.edu
>         <mailto:Concurrency-interest at cs.oswego.edu>
>             <mailto:Concurrency-interest at cs.oswego.edu
>         <mailto:Concurrency-interest at cs.oswego.edu>>
>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>


From forax at univ-mlv.fr  Fri May 17 09:03:57 2013
From: forax at univ-mlv.fr (Remi Forax)
Date: Fri, 17 May 2013 15:03:57 +0200
Subject: [concurrency-interest] hashing the reflect.Method object
	inConcurrentHashMap
In-Reply-To: <NFBBKALFDCPFIDBNKAPCOECFJNAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCOECFJNAA.davidcholmes@aapt.net.au>
Message-ID: <51962ABD.1090009@univ-mlv.fr>

On 05/17/2013 12:38 PM, David Holmes wrote:
> Maybe I've missed something in the to-and-fro here but I agree that 
> these reflection objects can be unsafely published (don't do that!) 
> but in this case they are getting published via a ConcurrentHashmap 
> and so are in fact safely published.
> David

I think that j.l.r.Method should thread safe, the current code is not 
thread safe.
I think it's something that should be fixed even if I have no idea how 
to fix the field override in AccessibleObject
without negatively impacting the speed of j.l.r.Method.invoke.

R?mi

>     -----Original Message-----
>     *From:* concurrency-interest-bounces at cs.oswego.edu
>     [mailto:concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of
>     *Stanimir Simeonoff
>     *Sent:* Friday, 17 May 2013 8:09 PM
>     *To:* Remi Forax
>     *Cc:* concurrency-interest at cs.oswego.edu
>     *Subject:* Re: [concurrency-interest] hashing the reflect.Method
>     object inConcurrentHashMap
>
>
>
>     On Fri, May 17, 2013 at 12:48 PM, Remi Forax <forax at univ-mlv.fr
>     <mailto:forax at univ-mlv.fr>> wrote:
>
>         On 05/16/2013 08:17 PM, Stanimir Simeonoff wrote:
>
>
>
>             On Thu, May 16, 2013 at 6:09 PM, Remi Forax
>             <forax at univ-mlv.fr <mailto:forax at univ-mlv.fr>
>             <mailto:forax at univ-mlv.fr <mailto:forax at univ-mlv.fr>>> wrote:
>
>                 On 05/16/2013 04:16 PM, Stanimir Simeonoff wrote:
>
>                     Each call to Class.getMethod/getDeclaredMethod has
>             to return a
>                     new instance as the class is modifiable
>             (setAccessible). As
>                     the safety goes: looking at the impl. it is unsafe.
>
>
>                 Could you explain why ? it should be thread-safe in my
>             opinion.
>
>
>             The method copy has no barrier (happens before) hence some
>             (or all) of the fields might not be visible if unsafely
>             published.
>
>
>         yes,
>         so fields like root, methodAccessor or typeAnnotations can be
>         null,
>         but given that these fields are just used to cache values that
>         can be recreated,
>         there is no problem because code expected that these fields
>         may be null.
>
>
>
>      On a weak memory model *all* fields can be null/zero, incl.
>     clazz, name, modifiers(!), slot and so on. "modifiers" being zero
>     would result in IllegalAccessException.
>
>     Stanimir
>
>
>
>
>                     On Thu, May 16, 2013 at 4:18 PM, Andy Nuss
>                     <andrew_nuss at yahoo.com
>             <mailto:andrew_nuss at yahoo.com>
>             <mailto:andrew_nuss at yahoo.com <mailto:andrew_nuss at yahoo.com>>
>                     <mailto:andrew_nuss at yahoo.com
>             <mailto:andrew_nuss at yahoo.com>
>             <mailto:andrew_nuss at yahoo.com
>             <mailto:andrew_nuss at yahoo.com>>>>
>
>                     wrote:
>
>                         Simple questions, relating to hashing a Method
>             object
>                     returned by
>                         Class.getDeclaredMethod, where the key is the
>             Class:
>
>                         Is there only one instance of Method in the vm
>             for a given
>                     actual
>                         object method, or does a new Method object get
>             created for
>                     each
>                         call to Class.getDeclaredMethod?  Is there any
>             need when a
>                     thread
>                         obtains a Method object to publish it safely
>             from one
>                     thread to
>                         the other or can it be shared thru back-door
>             non-volatile
>                         locations in memory?
>
>
>                         _______________________________________________
>                         Concurrency-interest mailing list
>             Concurrency-interest at cs.oswego.edu
>             <mailto:Concurrency-interest at cs.oswego.edu>
>                     <mailto:Concurrency-interest at cs.oswego.edu
>             <mailto:Concurrency-interest at cs.oswego.edu>>
>                         <mailto:Concurrency-interest at cs.oswego.edu
>             <mailto:Concurrency-interest at cs.oswego.edu>
>
>                     <mailto:Concurrency-interest at cs.oswego.edu
>             <mailto:Concurrency-interest at cs.oswego.edu>>>
>             http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
>
>                     _______________________________________________
>                     Concurrency-interest mailing list
>             Concurrency-interest at cs.oswego.edu
>             <mailto:Concurrency-interest at cs.oswego.edu>
>                     <mailto:Concurrency-interest at cs.oswego.edu
>             <mailto:Concurrency-interest at cs.oswego.edu>>
>             http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>                 _______________________________________________
>                 Concurrency-interest mailing list
>             Concurrency-interest at cs.oswego.edu
>             <mailto:Concurrency-interest at cs.oswego.edu>
>                 <mailto:Concurrency-interest at cs.oswego.edu
>             <mailto:Concurrency-interest at cs.oswego.edu>>
>             http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>


From andrew_nuss at yahoo.com  Fri May 17 09:33:24 2013
From: andrew_nuss at yahoo.com (Andy Nuss)
Date: Fri, 17 May 2013 06:33:24 -0700 (PDT)
Subject: [concurrency-interest] hashing the reflect.Method
	object	inConcurrentHashMap
In-Reply-To: <NFBBKALFDCPFIDBNKAPCOECFJNAA.davidcholmes@aapt.net.au>
References: <CAEJX8orWVAEQw1ryXMKs5JpFgsWL8NM9VuNLgZbg0ChhcraOGg@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCOECFJNAA.davidcholmes@aapt.net.au>
Message-ID: <1368797604.59772.YahooMailNeo@web141105.mail.bf1.yahoo.com>

David,

An aside, concerning "safely publish via 
ConcurrentHashMap".? Lets say a thread-aware object, but not a 
threaad-safe object, i.e. the kind that can be published, is gotten from
 ConcurrentHashMap without remove, mutated to change its behavior by 
changing just one and only one mutable field that serves as a switch 
case, and then put back into ConcurrentHashMap, a NOP.? Will 
ConcurrentHashMap ensure that the mutated field is visible to other 
cores, even though the re-put is a NOP?

Andy



________________________________
 From: David Holmes <davidcholmes at aapt.net.au>
To: Stanimir Simeonoff <stanimir at riflexo.com>; Remi Forax <forax at univ-mlv.fr> 
Cc: concurrency-interest at cs.oswego.edu 
Sent: Friday, May 17, 2013 3:38 AM
Subject: Re: [concurrency-interest] hashing the reflect.Method object inConcurrentHashMap
 


Maybe 
I've missed something in the to-and-fro here but I agree that these reflection 
objects can be unsafely published (don't do that!) but in this case they are 
getting published via a ConcurrentHashmap and so are in fact safely 
published.
?
David
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130517/8e535754/attachment-0001.html>

From stanimir at riflexo.com  Fri May 17 09:37:55 2013
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Fri, 17 May 2013 16:37:55 +0300
Subject: [concurrency-interest] hashing the reflect.Method object
	inConcurrentHashMap
In-Reply-To: <51962ABD.1090009@univ-mlv.fr>
References: <NFBBKALFDCPFIDBNKAPCOECFJNAA.davidcholmes@aapt.net.au>
	<51962ABD.1090009@univ-mlv.fr>
Message-ID: <CAEJX8oqFHQqiWsgnL2_+yopMk=X7dM3kiVnT+YZbnH6MdrQYzw@mail.gmail.com>

On a second thought:

Method copy(){
//...
  res.methodAccessor = methodAccessor;// <--volatile write
  return res;
}
For practical purposes the volatile write would be enough although there
are no formal HB since methodAccessor is not read prior
getClass()/getName() and so on.

Regarding "accessible" field I do not see how a modifiable object can be
made thread safe w/o using volatile. IMO it should be documented that all
subclasses of AccessibleObject are thread unsafe and require safe
publishing. At some point the "volatile" price has to be paid. It does make
little sense to modify "accessible" flag once set either way, the method
should have not had any parameters as all reflection objects are initially
not-accessible, e.g. makeAccessible() but it's too late now.

Stanimir

On Fri, May 17, 2013 at 4:03 PM, Remi Forax <forax at univ-mlv.fr> wrote:

> On 05/17/2013 12:38 PM, David Holmes wrote:
>
>> Maybe I've missed something in the to-and-fro here but I agree that these
>> reflection objects can be unsafely published (don't do that!) but in this
>> case they are getting published via a ConcurrentHashmap and so are in fact
>> safely published.
>> David
>>
>
> I think that j.l.r.Method should thread safe, the current code is not
> thread safe.
> I think it's something that should be fixed even if I have no idea how to
> fix the field override in AccessibleObject
> without negatively impacting the speed of j.l.r.Method.invoke.
>
> R?mi
>
>      -----Original Message-----
>>     *From:* concurrency-interest-bounces@**cs.oswego.edu<concurrency-interest-bounces at cs.oswego.edu>
>>     [mailto:concurrency-interest-**bounces at cs.oswego.edu<concurrency-interest-bounces at cs.oswego.edu>]*On
>> Behalf Of
>>     *Stanimir Simeonoff
>>     *Sent:* Friday, 17 May 2013 8:09 PM
>>     *To:* Remi Forax
>>     *Cc:* concurrency-interest at cs.**oswego.edu<concurrency-interest at cs.oswego.edu>
>>     *Subject:* Re: [concurrency-interest] hashing the reflect.Method
>>
>>     object inConcurrentHashMap
>>
>>
>>
>>     On Fri, May 17, 2013 at 12:48 PM, Remi Forax <forax at univ-mlv.fr
>>     <mailto:forax at univ-mlv.fr>> wrote:
>>
>>         On 05/16/2013 08:17 PM, Stanimir Simeonoff wrote:
>>
>>
>>
>>             On Thu, May 16, 2013 at 6:09 PM, Remi Forax
>>             <forax at univ-mlv.fr <mailto:forax at univ-mlv.fr>
>>             <mailto:forax at univ-mlv.fr <mailto:forax at univ-mlv.fr>>> wrote:
>>
>>                 On 05/16/2013 04:16 PM, Stanimir Simeonoff wrote:
>>
>>                     Each call to Class.getMethod/**getDeclaredMethod has
>>             to return a
>>                     new instance as the class is modifiable
>>             (setAccessible). As
>>                     the safety goes: looking at the impl. it is unsafe.
>>
>>
>>                 Could you explain why ? it should be thread-safe in my
>>             opinion.
>>
>>
>>             The method copy has no barrier (happens before) hence some
>>             (or all) of the fields might not be visible if unsafely
>>             published.
>>
>>
>>         yes,
>>         so fields like root, methodAccessor or typeAnnotations can be
>>         null,
>>         but given that these fields are just used to cache values that
>>         can be recreated,
>>         there is no problem because code expected that these fields
>>         may be null.
>>
>>
>>
>>      On a weak memory model *all* fields can be null/zero, incl.
>>     clazz, name, modifiers(!), slot and so on. "modifiers" being zero
>>     would result in IllegalAccessException.
>>
>>     Stanimir
>>
>>
>>
>>
>>                     On Thu, May 16, 2013 at 4:18 PM, Andy Nuss
>>                     <andrew_nuss at yahoo.com
>>             <mailto:andrew_nuss at yahoo.com>
>>             <mailto:andrew_nuss at yahoo.com <mailto:andrew_nuss at yahoo.com>*
>> *>
>>                     <mailto:andrew_nuss at yahoo.com
>>             <mailto:andrew_nuss at yahoo.com>
>>             <mailto:andrew_nuss at yahoo.com
>>             <mailto:andrew_nuss at yahoo.com>**>>>
>>
>>                     wrote:
>>
>>                         Simple questions, relating to hashing a Method
>>             object
>>                     returned by
>>                         Class.getDeclaredMethod, where the key is the
>>             Class:
>>
>>                         Is there only one instance of Method in the vm
>>             for a given
>>                     actual
>>                         object method, or does a new Method object get
>>             created for
>>                     each
>>                         call to Class.getDeclaredMethod?  Is there any
>>             need when a
>>                     thread
>>                         obtains a Method object to publish it safely
>>             from one
>>                     thread to
>>                         the other or can it be shared thru back-door
>>             non-volatile
>>                         locations in memory?
>>
>>
>>                         ______________________________**_________________
>>                         Concurrency-interest mailing list
>>             Concurrency-interest at cs.**oswego.edu<Concurrency-interest at cs.oswego.edu>
>>             <mailto:Concurrency-interest@**cs.oswego.edu<Concurrency-interest at cs.oswego.edu>
>> >
>>                     <mailto:Concurrency-interest@**cs.oswego.edu<Concurrency-interest at cs.oswego.edu>
>>             <mailto:Concurrency-interest@**cs.oswego.edu<Concurrency-interest at cs.oswego.edu>
>> >>
>>                         <mailto:Concurrency-interest@**cs.oswego.edu<Concurrency-interest at cs.oswego.edu>
>>             <mailto:Concurrency-interest@**cs.oswego.edu<Concurrency-interest at cs.oswego.edu>
>> >
>>
>>                     <mailto:Concurrency-interest@**cs.oswego.edu<Concurrency-interest at cs.oswego.edu>
>>             <mailto:Concurrency-interest@**cs.oswego.edu<Concurrency-interest at cs.oswego.edu>
>> >>>
>>             http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>
>>
>>
>>
>>
>>                     ______________________________**_________________
>>                     Concurrency-interest mailing list
>>             Concurrency-interest at cs.**oswego.edu<Concurrency-interest at cs.oswego.edu>
>>             <mailto:Concurrency-interest@**cs.oswego.edu<Concurrency-interest at cs.oswego.edu>
>> >
>>                     <mailto:Concurrency-interest@**cs.oswego.edu<Concurrency-interest at cs.oswego.edu>
>>             <mailto:Concurrency-interest@**cs.oswego.edu<Concurrency-interest at cs.oswego.edu>
>> >>
>>             http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>
>>
>>                 ______________________________**_________________
>>                 Concurrency-interest mailing list
>>             Concurrency-interest at cs.**oswego.edu<Concurrency-interest at cs.oswego.edu>
>>             <mailto:Concurrency-interest@**cs.oswego.edu<Concurrency-interest at cs.oswego.edu>
>> >
>>                 <mailto:Concurrency-interest@**cs.oswego.edu<Concurrency-interest at cs.oswego.edu>
>>             <mailto:Concurrency-interest@**cs.oswego.edu<Concurrency-interest at cs.oswego.edu>
>> >>
>>             http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>
>>
>>
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130517/e1ddd89c/attachment.html>

From peter.levart at gmail.com  Fri May 17 10:14:10 2013
From: peter.levart at gmail.com (Peter Levart)
Date: Fri, 17 May 2013 16:14:10 +0200
Subject: [concurrency-interest] hashing the reflect.Method object
	inConcurrentHashMap
In-Reply-To: <51962ABD.1090009@univ-mlv.fr>
References: <NFBBKALFDCPFIDBNKAPCOECFJNAA.davidcholmes@aapt.net.au>
	<51962ABD.1090009@univ-mlv.fr>
Message-ID: <51963B32.4090400@gmail.com>

On 05/17/2013 03:03 PM, Remi Forax wrote:
> On 05/17/2013 12:38 PM, David Holmes wrote:
>> Maybe I've missed something in the to-and-fro here but I agree that 
>> these reflection objects can be unsafely published (don't do that!) 
>> but in this case they are getting published via a ConcurrentHashmap 
>> and so are in fact safely published.
>> David
>
> I think that j.l.r.Method should thread safe, the current code is not 
> thread safe.
> I think it's something that should be fixed even if I have no idea how 
> to fix the field override in AccessibleObject
> without negatively impacting the speed of j.l.r.Method.invoke.
>
> R?mi

Hi Remi,

Like many classes in JDK, j.l.r.Method is not thread-safe. But such 
objects can nevertheless be used by multiple threads if those threads 
are using them in a read-only way and they are published to threads 
correctly. Now the only externally mutating method of j.l.r.Method is 
.setAccessible(boolean). I don't know if anyone wants to use 
setAccessible/isAccessible to communicate boolean state among the 
threads. The purpose of this field is to circumvent Java access checks 
when calling Method.invoke() and nobody would want to call 
Method.invoke() from one thread while the other thread is changing the 
'override' flag. So the rule is simple: If you want to change 'override' 
flag, then keep your Method object private to the thread. If you share 
Method objects among threads, don't call setAccessible at the same time 
on them (you can set the flag before publishing Method objects to other 
threads, but not after that). Making 'override' flag volatile would not 
make j.l.r.Method any more correct for any of it's intended usages, I think.

It would be useful though if AccessibleObject and subclasses would 
provide a way to "freeze" the object so that setAccessible could not be 
used on such frozen object any more.

Regards, Peter

>
>>     -----Original Message-----
>>     *From:* concurrency-interest-bounces at cs.oswego.edu
>>     [mailto:concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of
>>     *Stanimir Simeonoff
>>     *Sent:* Friday, 17 May 2013 8:09 PM
>>     *To:* Remi Forax
>>     *Cc:* concurrency-interest at cs.oswego.edu
>>     *Subject:* Re: [concurrency-interest] hashing the reflect.Method
>>     object inConcurrentHashMap
>>
>>
>>
>>     On Fri, May 17, 2013 at 12:48 PM, Remi Forax <forax at univ-mlv.fr
>>     <mailto:forax at univ-mlv.fr>> wrote:
>>
>>         On 05/16/2013 08:17 PM, Stanimir Simeonoff wrote:
>>
>>
>>
>>             On Thu, May 16, 2013 at 6:09 PM, Remi Forax
>>             <forax at univ-mlv.fr <mailto:forax at univ-mlv.fr>
>>             <mailto:forax at univ-mlv.fr <mailto:forax at univ-mlv.fr>>> 
>> wrote:
>>
>>                 On 05/16/2013 04:16 PM, Stanimir Simeonoff wrote:
>>
>>                     Each call to Class.getMethod/getDeclaredMethod has
>>             to return a
>>                     new instance as the class is modifiable
>>             (setAccessible). As
>>                     the safety goes: looking at the impl. it is unsafe.
>>
>>
>>                 Could you explain why ? it should be thread-safe in my
>>             opinion.
>>
>>
>>             The method copy has no barrier (happens before) hence some
>>             (or all) of the fields might not be visible if unsafely
>>             published.
>>
>>
>>         yes,
>>         so fields like root, methodAccessor or typeAnnotations can be
>>         null,
>>         but given that these fields are just used to cache values that
>>         can be recreated,
>>         there is no problem because code expected that these fields
>>         may be null.
>>
>>
>>
>>      On a weak memory model *all* fields can be null/zero, incl.
>>     clazz, name, modifiers(!), slot and so on. "modifiers" being zero
>>     would result in IllegalAccessException.
>>
>>     Stanimir
>>
>>
>>
>>
>>                     On Thu, May 16, 2013 at 4:18 PM, Andy Nuss
>>                     <andrew_nuss at yahoo.com
>>             <mailto:andrew_nuss at yahoo.com>
>>             <mailto:andrew_nuss at yahoo.com 
>> <mailto:andrew_nuss at yahoo.com>>
>>                     <mailto:andrew_nuss at yahoo.com
>>             <mailto:andrew_nuss at yahoo.com>
>>             <mailto:andrew_nuss at yahoo.com
>>             <mailto:andrew_nuss at yahoo.com>>>>
>>
>>                     wrote:
>>
>>                         Simple questions, relating to hashing a Method
>>             object
>>                     returned by
>>                         Class.getDeclaredMethod, where the key is the
>>             Class:
>>
>>                         Is there only one instance of Method in the vm
>>             for a given
>>                     actual
>>                         object method, or does a new Method object get
>>             created for
>>                     each
>>                         call to Class.getDeclaredMethod?  Is there any
>>             need when a
>>                     thread
>>                         obtains a Method object to publish it safely
>>             from one
>>                     thread to
>>                         the other or can it be shared thru back-door
>>             non-volatile
>>                         locations in memory?
>>
>>
>> _______________________________________________
>>                         Concurrency-interest mailing list
>>             Concurrency-interest at cs.oswego.edu
>>             <mailto:Concurrency-interest at cs.oswego.edu>
>> <mailto:Concurrency-interest at cs.oswego.edu
>> <mailto:Concurrency-interest at cs.oswego.edu>>
>> <mailto:Concurrency-interest at cs.oswego.edu
>>             <mailto:Concurrency-interest at cs.oswego.edu>
>>
>> <mailto:Concurrency-interest at cs.oswego.edu
>> <mailto:Concurrency-interest at cs.oswego.edu>>>
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>>
>>
>> _______________________________________________
>>                     Concurrency-interest mailing list
>>             Concurrency-interest at cs.oswego.edu
>>             <mailto:Concurrency-interest at cs.oswego.edu>
>> <mailto:Concurrency-interest at cs.oswego.edu
>> <mailto:Concurrency-interest at cs.oswego.edu>>
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>                 _______________________________________________
>>                 Concurrency-interest mailing list
>>             Concurrency-interest at cs.oswego.edu
>>             <mailto:Concurrency-interest at cs.oswego.edu>
>>                 <mailto:Concurrency-interest at cs.oswego.edu
>> <mailto:Concurrency-interest at cs.oswego.edu>>
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From peter.levart at gmail.com  Fri May 17 10:20:10 2013
From: peter.levart at gmail.com (Peter Levart)
Date: Fri, 17 May 2013 16:20:10 +0200
Subject: [concurrency-interest] hashing the reflect.Method object
	inConcurrentHashMap
In-Reply-To: <1368797604.59772.YahooMailNeo@web141105.mail.bf1.yahoo.com>
References: <CAEJX8orWVAEQw1ryXMKs5JpFgsWL8NM9VuNLgZbg0ChhcraOGg@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCOECFJNAA.davidcholmes@aapt.net.au>
	<1368797604.59772.YahooMailNeo@web141105.mail.bf1.yahoo.com>
Message-ID: <51963C9A.4020807@gmail.com>

On 05/17/2013 03:33 PM, Andy Nuss wrote:
> David,
>
> An aside, concerning "safely publish via ConcurrentHashMap". Lets say 
> a thread-aware object, but not a threaad-safe object, i.e. the kind 
> that can be published, is gotten from ConcurrentHashMap without 
> remove, mutated to change its behavior by changing just one and only 
> one mutable field that serves as a switch case, and then put back into 
> ConcurrentHashMap, a NOP. Will ConcurrentHashMap ensure that the 
> mutated field is visible to other cores, even though the re-put is a NOP?

Hi Andy,

No, you could have a reference to this object in the other threads from 
before the put was issued in the mutating thread.

Peter

>
> Andy
>
>
> ------------------------------------------------------------------------
> *From:* David Holmes <davidcholmes at aapt.net.au>
> *To:* Stanimir Simeonoff <stanimir at riflexo.com>; Remi Forax 
> <forax at univ-mlv.fr>
> *Cc:* concurrency-interest at cs.oswego.edu
> *Sent:* Friday, May 17, 2013 3:38 AM
> *Subject:* Re: [concurrency-interest] hashing the reflect.Method 
> object inConcurrentHashMap
>
> Maybe I've missed something in the to-and-fro here but I agree that 
> these reflection objects can be unsafely published (don't do that!) 
> but in this case they are getting published via a ConcurrentHashmap 
> and so are in fact safely published.
> David
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130517/04a3626e/attachment.html>

From viktor.klang at gmail.com  Fri May 17 10:22:53 2013
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Fri, 17 May 2013 16:22:53 +0200
Subject: [concurrency-interest] hashing the reflect.Method object
	inConcurrentHashMap
In-Reply-To: <51963B32.4090400@gmail.com>
References: <NFBBKALFDCPFIDBNKAPCOECFJNAA.davidcholmes@aapt.net.au>
	<51962ABD.1090009@univ-mlv.fr> <51963B32.4090400@gmail.com>
Message-ID: <CANPzfU8hpHeubPiU8PhZDmJ=mxQk8Lcd_wEU_YX1ivPyT3Kw8w@mail.gmail.com>

On Fri, May 17, 2013 at 4:14 PM, Peter Levart <peter.levart at gmail.com>wrote:

> On 05/17/2013 03:03 PM, Remi Forax wrote:
>
>> On 05/17/2013 12:38 PM, David Holmes wrote:
>>
>>> Maybe I've missed something in the to-and-fro here but I agree that
>>> these reflection objects can be unsafely published (don't do that!) but in
>>> this case they are getting published via a ConcurrentHashmap and so are in
>>> fact safely published.
>>> David
>>>
>>
>> I think that j.l.r.Method should thread safe, the current code is not
>> thread safe.
>> I think it's something that should be fixed even if I have no idea how to
>> fix the field override in AccessibleObject
>> without negatively impacting the speed of j.l.r.Method.invoke.
>>
>> R?mi
>>
>
> Hi Remi,
>
> Like many classes in JDK, j.l.r.Method is not thread-safe. But such
> objects can nevertheless be used by multiple threads if those threads are
> using them in a read-only way and they are published to threads correctly.
> Now the only externally mutating method of j.l.r.Method is
> .setAccessible(boolean). I don't know if anyone wants to use
> setAccessible/isAccessible to communicate boolean state among the threads.
> The purpose of this field is to circumvent Java access checks when calling
> Method.invoke() and nobody would want to call Method.invoke() from one
> thread while the other thread is changing the 'override' flag. So the rule
> is simple: If you want to change 'override' flag, then keep your Method
> object private to the thread. If you share Method objects among threads,
> don't call setAccessible at the same time on them (you can set the flag
> before publishing Method objects to other threads, but not after that).
> Making 'override' flag volatile would not make j.l.r.Method any more
> correct for any of it's intended usages, I think.
>
> It would be useful though if AccessibleObject and subclasses would provide
> a way to "freeze" the object so that setAccessible could not be used on
> such frozen object any more.
>

IMO: setAccessible is just a mistake. If access is to be circumvented for a
call, it should be a parameter to that call or a separate call e.g.
invokeUnsafe or similar. Right now it's not clear how you can setAccessible
to anything but "true" and still retain sane multithreaded access as you
have a classic check-then-act problem.

Cheers,
?


>
> Regards, Peter
>
>
>
>>      -----Original Message-----
>>>     *From:* concurrency-interest-bounces@**cs.oswego.edu<concurrency-interest-bounces at cs.oswego.edu>
>>>     [mailto:concurrency-interest-**bounces at cs.oswego.edu<concurrency-interest-bounces at cs.oswego.edu>]*On
>>> Behalf Of
>>>     *Stanimir Simeonoff
>>>     *Sent:* Friday, 17 May 2013 8:09 PM
>>>     *To:* Remi Forax
>>>     *Cc:* concurrency-interest at cs.**oswego.edu<concurrency-interest at cs.oswego.edu>
>>>     *Subject:* Re: [concurrency-interest] hashing the reflect.Method
>>>     object inConcurrentHashMap
>>>
>>>
>>>
>>>     On Fri, May 17, 2013 at 12:48 PM, Remi Forax <forax at univ-mlv.fr
>>>     <mailto:forax at univ-mlv.fr>> wrote:
>>>
>>>         On 05/16/2013 08:17 PM, Stanimir Simeonoff wrote:
>>>
>>>
>>>
>>>             On Thu, May 16, 2013 at 6:09 PM, Remi Forax
>>>             <forax at univ-mlv.fr <mailto:forax at univ-mlv.fr>
>>>             <mailto:forax at univ-mlv.fr <mailto:forax at univ-mlv.fr>>>
>>> wrote:
>>>
>>>                 On 05/16/2013 04:16 PM, Stanimir Simeonoff wrote:
>>>
>>>                     Each call to Class.getMethod/**getDeclaredMethod has
>>>             to return a
>>>                     new instance as the class is modifiable
>>>             (setAccessible). As
>>>                     the safety goes: looking at the impl. it is unsafe.
>>>
>>>
>>>                 Could you explain why ? it should be thread-safe in my
>>>             opinion.
>>>
>>>
>>>             The method copy has no barrier (happens before) hence some
>>>             (or all) of the fields might not be visible if unsafely
>>>             published.
>>>
>>>
>>>         yes,
>>>         so fields like root, methodAccessor or typeAnnotations can be
>>>         null,
>>>         but given that these fields are just used to cache values that
>>>         can be recreated,
>>>         there is no problem because code expected that these fields
>>>         may be null.
>>>
>>>
>>>
>>>      On a weak memory model *all* fields can be null/zero, incl.
>>>     clazz, name, modifiers(!), slot and so on. "modifiers" being zero
>>>     would result in IllegalAccessException.
>>>
>>>     Stanimir
>>>
>>>
>>>
>>>
>>>                     On Thu, May 16, 2013 at 4:18 PM, Andy Nuss
>>>                     <andrew_nuss at yahoo.com
>>>             <mailto:andrew_nuss at yahoo.com>
>>>             <mailto:andrew_nuss at yahoo.com <mailto:andrew_nuss at yahoo.com>
>>> **>
>>>                     <mailto:andrew_nuss at yahoo.com
>>>             <mailto:andrew_nuss at yahoo.com>
>>>             <mailto:andrew_nuss at yahoo.com
>>>             <mailto:andrew_nuss at yahoo.com>**>>>
>>>
>>>                     wrote:
>>>
>>>                         Simple questions, relating to hashing a Method
>>>             object
>>>                     returned by
>>>                         Class.getDeclaredMethod, where the key is the
>>>             Class:
>>>
>>>                         Is there only one instance of Method in the vm
>>>             for a given
>>>                     actual
>>>                         object method, or does a new Method object get
>>>             created for
>>>                     each
>>>                         call to Class.getDeclaredMethod?  Is there any
>>>             need when a
>>>                     thread
>>>                         obtains a Method object to publish it safely
>>>             from one
>>>                     thread to
>>>                         the other or can it be shared thru back-door
>>>             non-volatile
>>>                         locations in memory?
>>>
>>>
>>> ______________________________**_________________
>>>                         Concurrency-interest mailing list
>>>             Concurrency-interest at cs.**oswego.edu<Concurrency-interest at cs.oswego.edu>
>>>             <mailto:Concurrency-interest@**cs.oswego.edu<Concurrency-interest at cs.oswego.edu>
>>> >
>>> <mailto:Concurrency-interest@**cs.oswego.edu<Concurrency-interest at cs.oswego.edu>
>>> <mailto:Concurrency-interest@**cs.oswego.edu<Concurrency-interest at cs.oswego.edu>
>>> >>
>>> <mailto:Concurrency-interest@**cs.oswego.edu<Concurrency-interest at cs.oswego.edu>
>>>             <mailto:Concurrency-interest@**cs.oswego.edu<Concurrency-interest at cs.oswego.edu>
>>> >
>>>
>>> <mailto:Concurrency-interest@**cs.oswego.edu<Concurrency-interest at cs.oswego.edu>
>>> <mailto:Concurrency-interest@**cs.oswego.edu<Concurrency-interest at cs.oswego.edu>
>>> >>>
>>> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>>
>>>
>>>
>>>
>>>
>>> ______________________________**_________________
>>>                     Concurrency-interest mailing list
>>>             Concurrency-interest at cs.**oswego.edu<Concurrency-interest at cs.oswego.edu>
>>>             <mailto:Concurrency-interest@**cs.oswego.edu<Concurrency-interest at cs.oswego.edu>
>>> >
>>> <mailto:Concurrency-interest@**cs.oswego.edu<Concurrency-interest at cs.oswego.edu>
>>> <mailto:Concurrency-interest@**cs.oswego.edu<Concurrency-interest at cs.oswego.edu>
>>> >>
>>> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>>
>>>
>>>                 ______________________________**_________________
>>>                 Concurrency-interest mailing list
>>>             Concurrency-interest at cs.**oswego.edu<Concurrency-interest at cs.oswego.edu>
>>>             <mailto:Concurrency-interest@**cs.oswego.edu<Concurrency-interest at cs.oswego.edu>
>>> >
>>>                 <mailto:Concurrency-interest@**cs.oswego.edu<Concurrency-interest at cs.oswego.edu>
>>> <mailto:Concurrency-interest@**cs.oswego.edu<Concurrency-interest at cs.oswego.edu>
>>> >>
>>> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>>
>>>
>>>
>>>
>>>
>> ______________________________**_________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
>> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>
>
> ______________________________**_________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>



-- 
*Viktor Klang*
*Director of Engineering*
Typesafe <http://www.typesafe.com/>

Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130517/895a796e/attachment-0001.html>

From ionionascu at gmail.com  Fri May 17 10:25:30 2013
From: ionionascu at gmail.com (Ion Ionascu)
Date: Fri, 17 May 2013 15:25:30 +0100
Subject: [concurrency-interest] hashing the reflect.Method object
	inConcurrentHashMap
In-Reply-To: <51963B32.4090400@gmail.com>
References: <NFBBKALFDCPFIDBNKAPCOECFJNAA.davidcholmes@aapt.net.au>
	<51962ABD.1090009@univ-mlv.fr> <51963B32.4090400@gmail.com>
Message-ID: <CAOtF99roFubTRsrWYZFJJwF5dbfuTnd7B43ENp=+yWJPBDRj7w@mail.gmail.com>

Hi,


I agree with Peter that the override field is not really one of big concern
when it comes to being unsafe simply because of the particular usage
pattern of the combination setAccessible + invoke methods. But,
nevertheless, Stanimir has a point saying that this should be documented in
the JavaDoc of the class(es).


Regards,
Ion


On Fri, May 17, 2013 at 3:14 PM, Peter Levart <peter.levart at gmail.com>wrote:

> On 05/17/2013 03:03 PM, Remi Forax wrote:
>
>> On 05/17/2013 12:38 PM, David Holmes wrote:
>>
>>> Maybe I've missed something in the to-and-fro here but I agree that
>>> these reflection objects can be unsafely published (don't do that!) but in
>>> this case they are getting published via a ConcurrentHashmap and so are in
>>> fact safely published.
>>> David
>>>
>>
>> I think that j.l.r.Method should thread safe, the current code is not
>> thread safe.
>> I think it's something that should be fixed even if I have no idea how to
>> fix the field override in AccessibleObject
>> without negatively impacting the speed of j.l.r.Method.invoke.
>>
>> R?mi
>>
>
> Hi Remi,
>
> Like many classes in JDK, j.l.r.Method is not thread-safe. But such
> objects can nevertheless be used by multiple threads if those threads are
> using them in a read-only way and they are published to threads correctly.
> Now the only externally mutating method of j.l.r.Method is
> .setAccessible(boolean). I don't know if anyone wants to use
> setAccessible/isAccessible to communicate boolean state among the threads.
> The purpose of this field is to circumvent Java access checks when calling
> Method.invoke() and nobody would want to call Method.invoke() from one
> thread while the other thread is changing the 'override' flag. So the rule
> is simple: If you want to change 'override' flag, then keep your Method
> object private to the thread. If you share Method objects among threads,
> don't call setAccessible at the same time on them (you can set the flag
> before publishing Method objects to other threads, but not after that).
> Making 'override' flag volatile would not make j.l.r.Method any more
> correct for any of it's intended usages, I think.
>
> It would be useful though if AccessibleObject and subclasses would provide
> a way to "freeze" the object so that setAccessible could not be used on
> such frozen object any more.
>
> Regards, Peter
>
>
>
>>      -----Original Message-----
>>>     *From:* concurrency-interest-bounces@**cs.oswego.edu<concurrency-interest-bounces at cs.oswego.edu>
>>>     [mailto:concurrency-interest-**bounces at cs.oswego.edu<concurrency-interest-bounces at cs.oswego.edu>]*On
>>> Behalf Of
>>>     *Stanimir Simeonoff
>>>     *Sent:* Friday, 17 May 2013 8:09 PM
>>>     *To:* Remi Forax
>>>     *Cc:* concurrency-interest at cs.**oswego.edu<concurrency-interest at cs.oswego.edu>
>>>     *Subject:* Re: [concurrency-interest] hashing the reflect.Method
>>>     object inConcurrentHashMap
>>>
>>>
>>>
>>>     On Fri, May 17, 2013 at 12:48 PM, Remi Forax <forax at univ-mlv.fr
>>>     <mailto:forax at univ-mlv.fr>> wrote:
>>>
>>>         On 05/16/2013 08:17 PM, Stanimir Simeonoff wrote:
>>>
>>>
>>>
>>>             On Thu, May 16, 2013 at 6:09 PM, Remi Forax
>>>             <forax at univ-mlv.fr <mailto:forax at univ-mlv.fr>
>>>             <mailto:forax at univ-mlv.fr <mailto:forax at univ-mlv.fr>>>
>>> wrote:
>>>
>>>                 On 05/16/2013 04:16 PM, Stanimir Simeonoff wrote:
>>>
>>>                     Each call to Class.getMethod/**getDeclaredMethod has
>>>             to return a
>>>                     new instance as the class is modifiable
>>>             (setAccessible). As
>>>                     the safety goes: looking at the impl. it is unsafe.
>>>
>>>
>>>                 Could you explain why ? it should be thread-safe in my
>>>             opinion.
>>>
>>>
>>>             The method copy has no barrier (happens before) hence some
>>>             (or all) of the fields might not be visible if unsafely
>>>             published.
>>>
>>>
>>>         yes,
>>>         so fields like root, methodAccessor or typeAnnotations can be
>>>         null,
>>>         but given that these fields are just used to cache values that
>>>         can be recreated,
>>>         there is no problem because code expected that these fields
>>>         may be null.
>>>
>>>
>>>
>>>      On a weak memory model *all* fields can be null/zero, incl.
>>>     clazz, name, modifiers(!), slot and so on. "modifiers" being zero
>>>     would result in IllegalAccessException.
>>>
>>>     Stanimir
>>>
>>>
>>>
>>>
>>>                     On Thu, May 16, 2013 at 4:18 PM, Andy Nuss
>>>                     <andrew_nuss at yahoo.com
>>>             <mailto:andrew_nuss at yahoo.com>
>>>             <mailto:andrew_nuss at yahoo.com <mailto:andrew_nuss at yahoo.com>
>>> **>
>>>                     <mailto:andrew_nuss at yahoo.com
>>>             <mailto:andrew_nuss at yahoo.com>
>>>             <mailto:andrew_nuss at yahoo.com
>>>             <mailto:andrew_nuss at yahoo.com>**>>>
>>>
>>>                     wrote:
>>>
>>>                         Simple questions, relating to hashing a Method
>>>             object
>>>                     returned by
>>>                         Class.getDeclaredMethod, where the key is the
>>>             Class:
>>>
>>>                         Is there only one instance of Method in the vm
>>>             for a given
>>>                     actual
>>>                         object method, or does a new Method object get
>>>             created for
>>>                     each
>>>                         call to Class.getDeclaredMethod?  Is there any
>>>             need when a
>>>                     thread
>>>                         obtains a Method object to publish it safely
>>>             from one
>>>                     thread to
>>>                         the other or can it be shared thru back-door
>>>             non-volatile
>>>                         locations in memory?
>>>
>>>
>>> ______________________________**_________________
>>>                         Concurrency-interest mailing list
>>>             Concurrency-interest at cs.**oswego.edu<Concurrency-interest at cs.oswego.edu>
>>>             <mailto:Concurrency-interest@**cs.oswego.edu<Concurrency-interest at cs.oswego.edu>
>>> >
>>> <mailto:Concurrency-interest@**cs.oswego.edu<Concurrency-interest at cs.oswego.edu>
>>> <mailto:Concurrency-interest@**cs.oswego.edu<Concurrency-interest at cs.oswego.edu>
>>> >>
>>> <mailto:Concurrency-interest@**cs.oswego.edu<Concurrency-interest at cs.oswego.edu>
>>>             <mailto:Concurrency-interest@**cs.oswego.edu<Concurrency-interest at cs.oswego.edu>
>>> >
>>>
>>> <mailto:Concurrency-interest@**cs.oswego.edu<Concurrency-interest at cs.oswego.edu>
>>> <mailto:Concurrency-interest@**cs.oswego.edu<Concurrency-interest at cs.oswego.edu>
>>> >>>
>>> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>>
>>>
>>>
>>>
>>>
>>> ______________________________**_________________
>>>                     Concurrency-interest mailing list
>>>             Concurrency-interest at cs.**oswego.edu<Concurrency-interest at cs.oswego.edu>
>>>             <mailto:Concurrency-interest@**cs.oswego.edu<Concurrency-interest at cs.oswego.edu>
>>> >
>>> <mailto:Concurrency-interest@**cs.oswego.edu<Concurrency-interest at cs.oswego.edu>
>>> <mailto:Concurrency-interest@**cs.oswego.edu<Concurrency-interest at cs.oswego.edu>
>>> >>
>>> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>>
>>>
>>>                 ______________________________**_________________
>>>                 Concurrency-interest mailing list
>>>             Concurrency-interest at cs.**oswego.edu<Concurrency-interest at cs.oswego.edu>
>>>             <mailto:Concurrency-interest@**cs.oswego.edu<Concurrency-interest at cs.oswego.edu>
>>> >
>>>                 <mailto:Concurrency-interest@**cs.oswego.edu<Concurrency-interest at cs.oswego.edu>
>>> <mailto:Concurrency-interest@**cs.oswego.edu<Concurrency-interest at cs.oswego.edu>
>>> >>
>>> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>>
>>>
>>>
>>>
>>>
>> ______________________________**_________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
>> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>
>
> ______________________________**_________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130517/f3a03811/attachment.html>

From richard at burnison.ca  Fri May 17 11:23:24 2013
From: richard at burnison.ca (Richard Burnison)
Date: Fri, 17 May 2013 11:23:24 -0400
Subject: [concurrency-interest] hashing the reflect.Method object
 inConcurrentHashMap
In-Reply-To: <1368797604.59772.YahooMailNeo@web141105.mail.bf1.yahoo.com>
References: <CAEJX8orWVAEQw1ryXMKs5JpFgsWL8NM9VuNLgZbg0ChhcraOGg@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCOECFJNAA.davidcholmes@aapt.net.au>
	<1368797604.59772.YahooMailNeo@web141105.mail.bf1.yahoo.com>
Message-ID: <20130517112324.707bd117@rhea.burnison.local>

Hello Andy,

Why do you suggest the put will be a NOP?

I believe ConcurrentHashMap.put(k,v) will write to HashEntry.value, a
volatile field. Hence, subsequent reads on e.value, such as through
ConcurrentHashMap.get(k), will have a happens-before relationship with
this write. Is this not the case?

As Peter pointed out, however, references previously obtained may not
see this write.

Regards,

Richard


On Fri, 17 May 2013 06:33:24 -0700 (PDT)
Andy Nuss <andrew_nuss at yahoo.com> wrote:

> David,
> 
> An aside, concerning "safely publish via 
> ConcurrentHashMap".? Lets say a thread-aware object, but not a 
> threaad-safe object, i.e. the kind that can be published, is gotten
> from ConcurrentHashMap without remove, mutated to change its behavior
> by changing just one and only one mutable field that serves as a
> switch case, and then put back into ConcurrentHashMap, a NOP.? Will 
> ConcurrentHashMap ensure that the mutated field is visible to other 
> cores, even though the re-put is a NOP?
> 
> Andy
> 
> 
> 
> ________________________________
>  From: David Holmes <davidcholmes at aapt.net.au>
> To: Stanimir Simeonoff <stanimir at riflexo.com>; Remi Forax
> <forax at univ-mlv.fr> Cc: concurrency-interest at cs.oswego.edu 
> Sent: Friday, May 17, 2013 3:38 AM
> Subject: Re: [concurrency-interest] hashing the reflect.Method object
> inConcurrentHashMap 
> 
> 
> Maybe 
> I've missed something in the to-and-fro here but I agree that these
> reflection objects can be unsafely published (don't do that!) but in
> this case they are getting published via a ConcurrentHashmap and so
> are in fact safely published.
> ?
> David
> 



From aleksandar.prokopec at gmail.com  Fri May 17 12:20:32 2013
From: aleksandar.prokopec at gmail.com (Aleksandar Prokopec)
Date: Fri, 17 May 2013 18:20:32 +0200
Subject: [concurrency-interest] The cost of notifyAll and
Message-ID: <519658D0.1090709@gmail.com>

Hello,

I have a question regarding the performance of `notifyAll` calls 
compared to entering and exiting monitors on the JVM.
Some background - recently there has been a lot of talk on the Scala 
mailing lists about changing the Scala lazy vals implementation
which is currently deadlock-prone in cases where there is no reason for 
it to be.
We have an alternative design that we are trying to evaluate in terms of 
correctness and performance.

The full discussion is available here:
https://groups.google.com/group/scala-internals/browse_thread/thread/702801329e64f11f

I illustrate these changes with the hand-written version of our lazy 
vals implementation.
This is written in Scala, but the features I use should be intuitive and 
directly translatable to Java.
This is a lazy val declaration:

   class LazyCell(x: Int) {
     lazy val value = 0
   }

This is what our compiler currently does for a `lazy val` declaration:

   final class LazySimCell(x: Int) {
     @volatile var bitmap_0: Boolean = false
     var value_0: Int = _
     private def value_lzycompute(): Int = {
       this.synchronized {
         if (bitmap_0) {
           value_0 = 0
           bitmap_0 = true
         }
       }
       value_0
     }
     def value = if (bitmap_0) value_0 else value_lzycompute()
   }

The problem with this design is that if a `lazy val` right hand side 
depends on `lazy val`s in different objects cyclically, but lazy vals 
dependencies themselves
do not form a cycle, then this can lead to deadlocks.

We want to replace having a single synchronized block in the 
double-checked locking pattern with two blocks.
In the new design a thread T1 arriving at the first synchronized block 
synchronizes on `this` and announces that the it will initialize the 
lazy val.
Subsequent readers of the lazy val then wait until they are notified.
T1 then computes the value.
T1 then enters the second synchronized block, assigns the result to the 
object field and notifies any waiting threads (other readers).

This is the newly proposed design, hand-written:

    final class LazySimCellVersion2(x: Int) {
     @volatile var bitmap_0: Byte = 0.toByte
     var value_0: Int = _
     private def value_lzycompute(): Int = {
       this.synchronized {
         if (bitmap_0 == 0.toByte) {
           bitmap_0 = 1.toByte
         } else {
           while (bitmap_0 == 1.toByte) {
             this.wait()
           }
           return value_0
         }
       }
       val result = 0
       this.synchronized {
         value_0 = result
         bitmap_0 = 2.toByte
         this.notifyAll()
       }
       value_0
     }
     def value = if (bitmap_0 == 2.toByte) value_0 else value_lzycompute()
   }

I have run microbenchmarks to compare the two designs in a non-contended 
mode as follows.
I measured the following:
- instantiating an object with a value
- instantiating an object and initializing its lazy value
- same thing, but with manually implemented lazy vals with boolean bitmaps
- same thing, but with byte bitmaps
- the proposed lazy value change with 2 synchronizations blocks, but 
without a notifying potential waiters
- same thing, but with a notify call
repeated 1000000 through 5000000 times, in steps of 1000000.
The instantiated object is assigned to a field each time, to discard 
potential VM optimizations.
The initialization code is composed of creating an integer literal `0` - 
I chose to minimum amount
of computation to avoid having the computation costs amortize the 
lazy-val-related initialization costs.
This should thus be the borderline case where the program does nothing 
else but instantiate objects and initialize their lazy vals.

My platform:
i7-2600, quad-core, hyperthreading, 3.4GHz
MacOS X 10.7.5
Java 7, update 4, 64-bit
Scala 2.10.1

The reports themselves are here:

http://lampwww.epfl.ch/~prokopec/lazyvals/report/

The repository with the executable microbenchmarks with the code that 
produced these:

https://github.com/axel22/lazy-val-bench
https://github.com/axel22/lazy-val-bench/blob/master/src/test/scala/example/SyncBench.scala

In this benchmark, the 2 synchronized blocks design with a notify is 5 
times slower than the current lazy val implementation - and just 
adding/removing the line with the `notifyAll` call changes things 
considerably.

My question:
Is this expected? Is the cost of a `notifyAll` call expected to be this 
high?
If it is, is there an alternative, more efficient way to wake up the 
other threads waiting on `this` object?

Thank you,
Aleksandar Prokopec


  


--
Aleksandar Prokopec
Doctoral Assistant
LAMP, IC, EPFL
http://people.epfl.ch/aleksandar.prokopec

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130517/fe701ad4/attachment.html>

From peter.levart at gmail.com  Fri May 17 12:22:50 2013
From: peter.levart at gmail.com (Peter Levart)
Date: Fri, 17 May 2013 18:22:50 +0200
Subject: [concurrency-interest] hashing the reflect.Method object
	inConcurrentHashMap
In-Reply-To: <20130517112324.707bd117@rhea.burnison.local>
References: <CAEJX8orWVAEQw1ryXMKs5JpFgsWL8NM9VuNLgZbg0ChhcraOGg@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCOECFJNAA.davidcholmes@aapt.net.au>
	<1368797604.59772.YahooMailNeo@web141105.mail.bf1.yahoo.com>
	<20130517112324.707bd117@rhea.burnison.local>
Message-ID: <5196595A.3010800@gmail.com>

On 05/17/2013 05:23 PM, Richard Burnison wrote:
> Hello Andy,
>
> Why do you suggest the put will be a NOP?
>
> I believe ConcurrentHashMap.put(k,v) will write to HashEntry.value, a
> volatile field. Hence, subsequent reads on e.value, such as through
> ConcurrentHashMap.get(k), will have a happens-before relationship with
> this write. Is this not the case?
>
> As Peter pointed out, however, references previously obtained may not
> see this write.

They can't see the write if they don't look at it ;-).

We're talking about two writes here. One is the write to HashEntry.value 
which sets the reference to the object back to it's previous value 
(logically NOOP, but a write nevertheless - a volatile write). The other 
write is a write to a non-volatile field in the object, which Andy wants 
other threads to see:

WriterThread:

MyObject local = HashEntry.value;
local.field = some new value; // writeA
HashEntry.value = local; // writeB

ReaderThread:

MyObject local = HashEntry.value; // readB
xxx = local.field; // readA


If readB happens after writeB, then readA will see writeA, but othewrise 
it is not guaranteed. If two threads are not synchronized in any other 
way, then you can not guarantee that readB happens after writeB.

readB can happen before writeB but stil return the reference to the same 
object, because it was already there before. In that case reading from 
the non-volatile field of that object (readA) and observing the writeA 
or not is not affected by volatile writes to any other fields by other 
threads (like by writeB for example) unless by chance (same cache line, 
no reordering).

So puting the same reference back to the CHM under the same key without 
any other synchronization really buys you nothing, I think.

Regards, Peter

>
> Regards,
>
> Richard
>
>
> On Fri, 17 May 2013 06:33:24 -0700 (PDT)
> Andy Nuss <andrew_nuss at yahoo.com> wrote:
>
>> David,
>>
>> An aside, concerning "safely publish via
>> ConcurrentHashMap".  Lets say a thread-aware object, but not a
>> threaad-safe object, i.e. the kind that can be published, is gotten
>> from ConcurrentHashMap without remove, mutated to change its behavior
>> by changing just one and only one mutable field that serves as a
>> switch case, and then put back into ConcurrentHashMap, a NOP.  Will
>> ConcurrentHashMap ensure that the mutated field is visible to other
>> cores, even though the re-put is a NOP?
>>
>> Andy
>>
>>
>>
>> ________________________________
>>   From: David Holmes <davidcholmes at aapt.net.au>
>> To: Stanimir Simeonoff <stanimir at riflexo.com>; Remi Forax
>> <forax at univ-mlv.fr> Cc: concurrency-interest at cs.oswego.edu
>> Sent: Friday, May 17, 2013 3:38 AM
>> Subject: Re: [concurrency-interest] hashing the reflect.Method object
>> inConcurrentHashMap
>>
>>
>> Maybe
>> I've missed something in the to-and-fro here but I agree that these
>> reflection objects can be unsafely published (don't do that!) but in
>> this case they are getting published via a ConcurrentHashmap and so
>> are in fact safely published.
>>   
>> David
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From zhong.j.yu at gmail.com  Fri May 17 13:49:22 2013
From: zhong.j.yu at gmail.com (Zhong Yu)
Date: Fri, 17 May 2013 12:49:22 -0500
Subject: [concurrency-interest] hashing the reflect.Method object
	inConcurrentHashMap
In-Reply-To: <5196595A.3010800@gmail.com>
References: <CAEJX8orWVAEQw1ryXMKs5JpFgsWL8NM9VuNLgZbg0ChhcraOGg@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCOECFJNAA.davidcholmes@aapt.net.au>
	<1368797604.59772.YahooMailNeo@web141105.mail.bf1.yahoo.com>
	<20130517112324.707bd117@rhea.burnison.local>
	<5196595A.3010800@gmail.com>
Message-ID: <CACuKZqGwCu+n66CfUPE=RORGoMY8O+XcdoEfN_V5Nyi1QNrt5A@mail.gmail.com>

On Fri, May 17, 2013 at 11:22 AM, Peter Levart <peter.levart at gmail.com>wrote:

> On 05/17/2013 05:23 PM, Richard Burnison wrote:
>
>> Hello Andy,
>>
>> Why do you suggest the put will be a NOP?
>>
>> I believe ConcurrentHashMap.put(k,v) will write to HashEntry.value, a
>> volatile field. Hence, subsequent reads on e.value, such as through
>> ConcurrentHashMap.get(k), will have a happens-before relationship with
>> this write. Is this not the case?
>>
>> As Peter pointed out, however, references previously obtained may not
>> see this write.
>>
>
> They can't see the write if they don't look at it ;-).
>
> We're talking about two writes here. One is the write to HashEntry.value
> which sets the reference to the object back to it's previous value
> (logically NOOP, but a write nevertheless - a volatile write). The other
> write is a write to a non-volatile field in the object, which Andy wants
> other threads to see:
>
> WriterThread:
>
> MyObject local = HashEntry.value;
> local.field = some new value; // writeA
> HashEntry.value = local; // writeB
>
> ReaderThread:
>
> MyObject local = HashEntry.value; // readB
> xxx = local.field; // readA
>
>
> If readB happens after writeB, then readA will see writeA, but othewrise
> it is not guaranteed. If two threads are not synchronized in any other way,
> then you can not guarantee that readB happens after writeB.
>
> readB can happen before writeB but stil return the reference to the same
> object, because it was already there before. In that case reading from the
> non-volatile field of that object (readA) and observing the writeA or not
> is not affected by volatile writes to any other fields by other threads
> (like by writeB for example) unless by chance (same cache line, no
> reordering).
>

Such argument applies to any safe publication mechanism, doesn't it?


>
> So puting the same reference back to the CHM under the same key without
> any other synchronization really buys you nothing, I think.
>
> Regards, Peter
>
>
>
>> Regards,
>>
>> Richard
>>
>>
>> On Fri, 17 May 2013 06:33:24 -0700 (PDT)
>> Andy Nuss <andrew_nuss at yahoo.com> wrote:
>>
>>  David,
>>>
>>> An aside, concerning "safely publish via
>>> ConcurrentHashMap".  Lets say a thread-aware object, but not a
>>> threaad-safe object, i.e. the kind that can be published, is gotten
>>> from ConcurrentHashMap without remove, mutated to change its behavior
>>> by changing just one and only one mutable field that serves as a
>>> switch case, and then put back into ConcurrentHashMap, a NOP.  Will
>>> ConcurrentHashMap ensure that the mutated field is visible to other
>>> cores, even though the re-put is a NOP?
>>>
>>> Andy
>>>
>>>
>>>
>>> ______________________________**__
>>>   From: David Holmes <davidcholmes at aapt.net.au>
>>> To: Stanimir Simeonoff <stanimir at riflexo.com>; Remi Forax
>>> <forax at univ-mlv.fr> Cc: concurrency-interest at cs.**oswego.edu<concurrency-interest at cs.oswego.edu>
>>> Sent: Friday, May 17, 2013 3:38 AM
>>> Subject: Re: [concurrency-interest] hashing the reflect.Method object
>>> inConcurrentHashMap
>>>
>>>
>>> Maybe
>>> I've missed something in the to-and-fro here but I agree that these
>>> reflection objects can be unsafely published (don't do that!) but in
>>> this case they are getting published via a ConcurrentHashmap and so
>>> are in fact safely published.
>>>   David
>>>
>>>
>> ______________________________**_________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
>> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>
>
> ______________________________**_________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130517/8f958a98/attachment-0001.html>

From davidcholmes at aapt.net.au  Fri May 17 22:05:08 2013
From: davidcholmes at aapt.net.au (David Holmes)
Date: Sat, 18 May 2013 12:05:08 +1000
Subject: [concurrency-interest] The cost of notifyAll and
In-Reply-To: <519658D0.1090709@gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCEECJJNAA.davidcholmes@aapt.net.au>

Very quick skim ..

You may be seeing the monitor inflation being attribuited to the notifyAll.
In normal use it would happen with the wait(), if not already inflated due
to contention on the monitor.

David Holmes
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Aleksandar
Prokopec
  Sent: Saturday, 18 May 2013 2:21 AM
  To: concurrency-interest at cs.oswego.edu
  Subject: [concurrency-interest] The cost of notifyAll and


  Hello,

  I have a question regarding the performance of `notifyAll` calls compared
to entering and exiting monitors on the JVM.
  Some background - recently there has been a lot of talk on the Scala
mailing lists about changing the Scala lazy vals implementation
  which is currently deadlock-prone in cases where there is no reason for it
to be.
  We have an alternative design that we are trying to evaluate in terms of
correctness and performance.

  The full discussion is available here:
  https://groups.google.com/group/scala-internals/browse_thread/thread/70280
1329e64f11f

  I illustrate these changes with the hand-written version of our lazy vals
implementation.
  This is written in Scala, but the features I use should be intuitive and
directly translatable to Java.
  This is a lazy val declaration:

    class LazyCell(x: Int) {
      lazy val value = 0
    }

  This is what our compiler currently does for a `lazy val` declaration:

    final class LazySimCell(x: Int) {
      @volatile var bitmap_0: Boolean = false
      var value_0: Int = _
      private def value_lzycompute(): Int = {
        this.synchronized {
          if (bitmap_0) {
            value_0 = 0
            bitmap_0 = true
          }
        }
        value_0
      }
      def value = if (bitmap_0) value_0 else value_lzycompute()
    }

  The problem with this design is that if a `lazy val` right hand side
depends on `lazy val`s in different objects cyclically, but lazy vals
dependencies themselves
  do not form a cycle, then this can lead to deadlocks.

  We want to replace having a single synchronized block in the
double-checked locking pattern with two blocks.
  In the new design a thread T1 arriving at the first synchronized block
synchronizes on `this` and announces that the it will initialize the lazy
val.
  Subsequent readers of the lazy val then wait until they are notified.
  T1 then computes the value.
  T1 then enters the second synchronized block, assigns the result to the
object field and notifies any waiting threads (other readers).

  This is the newly proposed design, hand-written:

     final class LazySimCellVersion2(x: Int) {
      @volatile var bitmap_0: Byte = 0.toByte
      var value_0: Int = _
      private def value_lzycompute(): Int = {
        this.synchronized {
          if (bitmap_0 == 0.toByte) {
            bitmap_0 = 1.toByte
          } else {
            while (bitmap_0 == 1.toByte) {
              this.wait()
            }
            return value_0
          }
        }
        val result = 0
        this.synchronized {
          value_0 = result
          bitmap_0 = 2.toByte
          this.notifyAll()
        }
        value_0
      }
      def value = if (bitmap_0 == 2.toByte) value_0 else value_lzycompute()
    }

  I have run microbenchmarks to compare the two designs in a non-contended
mode as follows.
  I measured the following:
  - instantiating an object with a value
  - instantiating an object and initializing its lazy value
  - same thing, but with manually implemented lazy vals with boolean bitmaps
  - same thing, but with byte bitmaps
  - the proposed lazy value change with 2 synchronizations blocks, but
without a notifying potential waiters
  - same thing, but with a notify call
  repeated 1000000 through 5000000 times, in steps of 1000000.
  The instantiated object is assigned to a field each time, to discard
potential VM optimizations.
  The initialization code is composed of creating an integer literal `0` - I
chose to minimum amount
  of computation to avoid having the computation costs amortize the
lazy-val-related initialization costs.
  This should thus be the borderline case where the program does nothing
else but instantiate objects and initialize their lazy vals.

  My platform:
  i7-2600, quad-core, hyperthreading, 3.4GHz
  MacOS X 10.7.5
  Java 7, update 4, 64-bit
  Scala 2.10.1

  The reports themselves are here:

  http://lampwww.epfl.ch/~prokopec/lazyvals/report/

  The repository with the executable microbenchmarks with the code that
produced these:

  https://github.com/axel22/lazy-val-bench
  https://github.com/axel22/lazy-val-bench/blob/master/src/test/scala/exampl
e/SyncBench.scala

  In this benchmark, the 2 synchronized blocks design with a notify is 5
times slower than the current lazy val implementation - and just
adding/removing the line with the `notifyAll` call changes things
considerably.

  My question:
  Is this expected? Is the cost of a `notifyAll` call expected to be this
high?
  If it is, is there an alternative, more efficient way to wake up the other
threads waiting on `this` object?

  Thank you,
  Aleksandar Prokopec






--
Aleksandar Prokopec
Doctoral Assistant
LAMP, IC, EPFL
http://people.epfl.ch/aleksandar.prokopec
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130518/8ee3f120/attachment.html>

From alarmnummer at gmail.com  Sat May 18 07:38:10 2013
From: alarmnummer at gmail.com (Peter Veentjer)
Date: Sat, 18 May 2013 14:38:10 +0300
Subject: [concurrency-interest] array of objects and false sharing
Message-ID: <CAGuAWdCGa7fY5b2nYEfCz61h8EJkEwoB3gWbKtUwVvNG1MUJAA@mail.gmail.com>

Hi Guys,

I would like to understand how to prevent the following problem.

If I have an immutable array of mutable objects and I create this array in
the beginning like this:

someArray = new MutableObject[x]
for(int k=0;k<someArray.length;k++){
    someArray[k]=new MutableObject();

And I would start to modify the Mutable objects by different threads, it
can be that multiple mutable objects end up in the same cache line and
cause false sharing. And this can leads to a unexpected performance
slowdown.

What is the best way to deal with false sharing?

One solution is that I  create an array which is y times bigger and when
iterating over this array, create an object on every step but only every
y'th step, I would assign it to the array. But I guess that the JVM could
decide to skip the creation of the useless objects and then the problem
isn't solved.

Another solution would be to just create and assign an object to every
element in the array, but only use every y'th item will be used. The other
ones are just padding to prevent false sharing. This leads to an increased
memory usage, but I don't think it will be the end of the world.

Is the last solution the way forward? And what is a practical way to
determine the ratio between useless and useful objects? It depends of
course on the size of the object..but it also depends on the cacheline size
of the cpu. The latter is platform independent  So just make it big enough
so that it probably will not cause problem on most known architectures and
keep fingers crossed that in the future nothing is going to change?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130518/b1985363/attachment.html>

From forax at univ-mlv.fr  Sat May 18 07:57:35 2013
From: forax at univ-mlv.fr (Remi Forax)
Date: Sat, 18 May 2013 13:57:35 +0200
Subject: [concurrency-interest] array of objects and false sharing
In-Reply-To: <CAGuAWdCGa7fY5b2nYEfCz61h8EJkEwoB3gWbKtUwVvNG1MUJAA@mail.gmail.com>
References: <CAGuAWdCGa7fY5b2nYEfCz61h8EJkEwoB3gWbKtUwVvNG1MUJAA@mail.gmail.com>
Message-ID: <51976CAF.6010908@univ-mlv.fr>

Peter,
currently array of objects in Java are array of references, so there is 
no false sharing issue.

The problem is more that the GC packs and moves objects in memory so you 
can have false
sharing or not depending how the GC has organized objects in memory.

The best solution seems to be the new annotation @Contented introduced 
in jdk8
but as you said the memory footprint impact is not negligible.

R?mi

On 05/18/2013 01:38 PM, Peter Veentjer wrote:
> Hi Guys,
>
> I would like to understand how to prevent the following problem.
>
> If I have an immutable array of mutable objects and I create this 
> array in the beginning like this:
>
> someArray = new MutableObject[x]
> for(int k=0;k<someArray.length;k++){
>     someArray[k]=new MutableObject();
>
> And I would start to modify the Mutable objects by different threads, 
> it can be that multiple mutable objects end up in the same cache line 
> and cause false sharing. And this can leads to a unexpected 
> performance slowdown.
>
> What is the best way to deal with false sharing?
>
> One solution is that I  create an array which is y times bigger and 
> when iterating over this array, create an object on every step but 
> only every y'th step, I would assign it to the array. But I guess that 
> the JVM could decide to skip the creation of the useless objects and 
> then the problem isn't solved.
>
> Another solution would be to just create and assign an object to every 
> element in the array, but only use every y'th item will be used. The 
> other ones are just padding to prevent false sharing. This leads to an 
> increased memory usage, but I don't think it will be the end of the world.
>
> Is the last solution the way forward? And what is a practical way to 
> determine the ratio between useless and useful objects? It depends of 
> course on the size of the object..but it also depends on the cacheline 
> size of the cpu. The latter is platform independent  So just make it 
> big enough so that it probably will not cause problem on most known 
> architectures and keep fingers crossed that in the future nothing is 
> going to change?
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From mr.chrisvest at gmail.com  Sat May 18 08:05:24 2013
From: mr.chrisvest at gmail.com (Chris Vest)
Date: Sat, 18 May 2013 14:05:24 +0200
Subject: [concurrency-interest] array of objects and false sharing
In-Reply-To: <CAGuAWdCGa7fY5b2nYEfCz61h8EJkEwoB3gWbKtUwVvNG1MUJAA@mail.gmail.com>
References: <CAGuAWdCGa7fY5b2nYEfCz61h8EJkEwoB3gWbKtUwVvNG1MUJAA@mail.gmail.com>
Message-ID: <28E9199B-6EF1-439E-AE08-A2CB37D69596@gmail.com>

The array is an array of references to objects, that the JVM is allowed to allocate where it see fit, and the GC is allowed to move around.

Other than padding each object out to fill the 64 byte (assuming desktop/server class x86, here) cache lines (with object headers whose size can depend on VM flags), I don't see a sure way to eliminate the risk of false sharing.

Another approach could be to turn your array of mutable objects into an object of mutable arrays. That way the "fields" will be placed together depending on the size of their type, which I reckon is more predictable.

Chris

On 18/05/2013, at 13.38, Peter Veentjer <alarmnummer at gmail.com> wrote:

> Hi Guys,
> 
> I would like to understand how to prevent the following problem.
> 
> If I have an immutable array of mutable objects and I create this array in the beginning like this:
> 
> someArray = new MutableObject[x]
> for(int k=0;k<someArray.length;k++){
>     someArray[k]=new MutableObject();
> 
> And I would start to modify the Mutable objects by different threads, it can be that multiple mutable objects end up in the same cache line and cause false sharing. And this can leads to a unexpected performance slowdown.
> 
> What is the best way to deal with false sharing? 
> 
> One solution is that I  create an array which is y times bigger and when iterating over this array, create an object on every step but only every y'th step, I would assign it to the array. But I guess that the JVM could decide to skip the creation of the useless objects and then the problem isn't solved.
> 
> Another solution would be to just create and assign an object to every element in the array, but only use every y'th item will be used. The other ones are just padding to prevent false sharing. This leads to an increased memory usage, but I don't think it will be the end of the world.
> 
> Is the last solution the way forward? And what is a practical way to determine the ratio between useless and useful objects? It depends of course on the size of the object..but it also depends on the cacheline size of the cpu. The latter is platform independent  So just make it big enough so that it probably will not cause problem on most known architectures and keep fingers crossed that in the future nothing is going to change?
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From alarmnummer at gmail.com  Sat May 18 08:07:57 2013
From: alarmnummer at gmail.com (Peter Veentjer)
Date: Sat, 18 May 2013 15:07:57 +0300
Subject: [concurrency-interest] array of objects and false sharing
In-Reply-To: <51976CAF.6010908@univ-mlv.fr>
References: <CAGuAWdCGa7fY5b2nYEfCz61h8EJkEwoB3gWbKtUwVvNG1MUJAA@mail.gmail.com>
	<51976CAF.6010908@univ-mlv.fr>
Message-ID: <CAGuAWdAd9rv21zt1Lzs0dn5U43bP-ECBjk0SdP74-gvnRwU0iw@mail.gmail.com>

Hi Remi,

thanks for your answer.

I know that arrays don't contains the actual object, but references to the
object.

But if I quickly create a bunch of objects in the heap, chances are that
they will be allocated closely to each other and therefor could end up in
the same cache line.




On Sat, May 18, 2013 at 2:57 PM, Remi Forax <forax at univ-mlv.fr> wrote:

> Peter,
> currently array of objects in Java are array of references, so there is no
> false sharing issue.
>
> The problem is more that the GC packs and moves objects in memory so you
> can have false
> sharing or not depending how the GC has organized objects in memory.
>
> The best solution seems to be the new annotation @Contented introduced in
> jdk8
> but as you said the memory footprint impact is not negligible.
>
> R?mi
>
>
> On 05/18/2013 01:38 PM, Peter Veentjer wrote:
>
>> Hi Guys,
>>
>> I would like to understand how to prevent the following problem.
>>
>> If I have an immutable array of mutable objects and I create this array
>> in the beginning like this:
>>
>> someArray = new MutableObject[x]
>> for(int k=0;k<someArray.length;k++){
>>     someArray[k]=new MutableObject();
>>
>> And I would start to modify the Mutable objects by different threads, it
>> can be that multiple mutable objects end up in the same cache line and
>> cause false sharing. And this can leads to a unexpected performance
>> slowdown.
>>
>> What is the best way to deal with false sharing?
>>
>> One solution is that I  create an array which is y times bigger and when
>> iterating over this array, create an object on every step but only every
>> y'th step, I would assign it to the array. But I guess that the JVM could
>> decide to skip the creation of the useless objects and then the problem
>> isn't solved.
>>
>> Another solution would be to just create and assign an object to every
>> element in the array, but only use every y'th item will be used. The other
>> ones are just padding to prevent false sharing. This leads to an increased
>> memory usage, but I don't think it will be the end of the world.
>>
>> Is the last solution the way forward? And what is a practical way to
>> determine the ratio between useless and useful objects? It depends of
>> course on the size of the object..but it also depends on the cacheline size
>> of the cpu. The latter is platform independent  So just make it big enough
>> so that it probably will not cause problem on most known architectures and
>> keep fingers crossed that in the future nothing is going to change?
>>
>>
>>
>> ______________________________**_________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
>> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>
>
> ______________________________**_________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130518/72740ac3/attachment.html>

From aleksey.shipilev at oracle.com  Sat May 18 08:48:55 2013
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Sat, 18 May 2013 16:48:55 +0400
Subject: [concurrency-interest] array of objects and false sharing
In-Reply-To: <CAGuAWdCGa7fY5b2nYEfCz61h8EJkEwoB3gWbKtUwVvNG1MUJAA@mail.gmail.com>
References: <CAGuAWdCGa7fY5b2nYEfCz61h8EJkEwoB3gWbKtUwVvNG1MUJAA@mail.gmail.com>
Message-ID: <519778B7.3090002@oracle.com>

On 05/18/2013 03:38 PM, Peter Veentjer wrote:
> What is the best way to deal with false sharing? 

As stated before:

 a) JDK 5+: pad with enough fields to fit the cache line. Be careful to
pad longer because some hardware can actually prefetch the adjacent
cache lines thinking you are about to read them as well.

For example, see Striped64.Cell:
 http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/jsr166e/Striped64.java?view=markup

This relies on the field ordering though. In the absence of @Contended,
you might find piggy-backing on the inheritance more reliable (see L*
subclasses and the implementation notes):
 http://hg.openjdk.java.net/code-tools/jmh/file/tip/jmh-core/src/main/java/org/openjdk/jmh/logic/BlackHole.java

 b) JDK 8+: @Contended.

-Aleksey.

From stas.oskin at gmail.com  Sun May 19 04:13:47 2013
From: stas.oskin at gmail.com (Stas Oskin)
Date: Sun, 19 May 2013 08:13:47 +0000 (UTC)
Subject: [concurrency-interest] Invitation to connect on LinkedIn
Message-ID: <1861771185.13583658.1368951227472.JavaMail.app@ela4-bed81.prod>

LinkedIn
------------



I'd like to add you to my professional network on LinkedIn.

- Stas

Stas Oskin
CTO at eyecam
Israel

Confirm that you know Stas Oskin:
https://www.linkedin.com/e/-bprv24-hgvy7rcr-6u/isd/13394343318/f_gO70iq/?hs=false&tok=2ob4KXzPtJ95M1

--
You are receiving Invitation to Connect emails. Click to unsubscribe:
http://www.linkedin.com/e/-bprv24-hgvy7rcr-6u/uHcuZYne9zquJeVgNLXI7aw5RzXz1NGsXLMu66qSuntzJhXKb2G/goo/concurrency-interest%40cs%2Eoswego%2Eedu/20061/I4437375065_1/?hs=false&tok=0k7mFHxc5J95M1

(c) 2012 LinkedIn Corporation. 2029 Stierlin Ct, Mountain View, CA 94043, USA.


  
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130519/fe8a47c2/attachment.html>

From oleksandr.otenko at oracle.com  Mon May 20 09:08:10 2013
From: oleksandr.otenko at oracle.com (oleksandr otenko)
Date: Mon, 20 May 2013 14:08:10 +0100
Subject: [concurrency-interest] The cost of notifyAll and
In-Reply-To: <519658D0.1090709@gmail.com>
References: <519658D0.1090709@gmail.com>
Message-ID: <519A203A.8090209@oracle.com>

A deadlock means there is a dependency loop.

How do you propose to resolve lazy val reference, when it is not computed?

Example in Java:

class A {
   static int x = B.x+1;
}

class B {
   static int x = A.x+1;
}

This is how you can get a deadlock in Java, if A and B happen to be 
initialized in different threads.

When executed single-threadedly, JVM will supply 0 as the value of one 
of them, but it is bad design of the classes to start with.

In lazy evaluating languages this design is the same as non-terminating 
recursion. I am not sure you can furnish a solution for this in Scala.


Alex


On 17/05/2013 17:20, Aleksandar Prokopec wrote:
> Hello,
>
> I have a question regarding the performance of `notifyAll` calls 
> compared to entering and exiting monitors on the JVM.
> Some background - recently there has been a lot of talk on the Scala 
> mailing lists about changing the Scala lazy vals implementation
> which is currently deadlock-prone in cases where there is no reason 
> for it to be.
> We have an alternative design that we are trying to evaluate in terms 
> of correctness and performance.
>
> The full discussion is available here:
> https://groups.google.com/group/scala-internals/browse_thread/thread/702801329e64f11f
>
> I illustrate these changes with the hand-written version of our lazy 
> vals implementation.
> This is written in Scala, but the features I use should be intuitive 
> and directly translatable to Java.
> This is a lazy val declaration:
>
>   class LazyCell(x: Int) {
>     lazy val value = 0
>   }
>
> This is what our compiler currently does for a `lazy val` declaration:
>
>   final class LazySimCell(x: Int) {
>     @volatile var bitmap_0: Boolean = false
>     var value_0: Int = _
>     private def value_lzycompute(): Int = {
>       this.synchronized {
>         if (bitmap_0) {
>           value_0 = 0
>           bitmap_0 = true
>         }
>       }
>       value_0
>     }
>     def value = if (bitmap_0) value_0 else value_lzycompute()
>   }
>
> The problem with this design is that if a `lazy val` right hand side 
> depends on `lazy val`s in different objects cyclically, but lazy vals 
> dependencies themselves
> do not form a cycle, then this can lead to deadlocks.
>
> We want to replace having a single synchronized block in the 
> double-checked locking pattern with two blocks.
> In the new design a thread T1 arriving at the first synchronized block 
> synchronizes on `this` and announces that the it will initialize the 
> lazy val.
> Subsequent readers of the lazy val then wait until they are notified.
> T1 then computes the value.
> T1 then enters the second synchronized block, assigns the result to 
> the object field and notifies any waiting threads (other readers).
>
> This is the newly proposed design, hand-written:
>
>    final class LazySimCellVersion2(x: Int) {
>     @volatile var bitmap_0: Byte = 0.toByte
>     var value_0: Int = _
>     private def value_lzycompute(): Int = {
>       this.synchronized {
>         if (bitmap_0 == 0.toByte) {
>           bitmap_0 = 1.toByte
>         } else {
>           while (bitmap_0 == 1.toByte) {
>             this.wait()
>           }
>           return value_0
>         }
>       }
>       val result = 0
>       this.synchronized {
>         value_0 = result
>         bitmap_0 = 2.toByte
>         this.notifyAll()
>       }
>       value_0
>     }
>     def value = if (bitmap_0 == 2.toByte) value_0 else value_lzycompute()
>   }
>
> I have run microbenchmarks to compare the two designs in a 
> non-contended mode as follows.
> I measured the following:
> - instantiating an object with a value
> - instantiating an object and initializing its lazy value
> - same thing, but with manually implemented lazy vals with boolean bitmaps
> - same thing, but with byte bitmaps
> - the proposed lazy value change with 2 synchronizations blocks, but 
> without a notifying potential waiters
> - same thing, but with a notify call
> repeated 1000000 through 5000000 times, in steps of 1000000.
> The instantiated object is assigned to a field each time, to discard 
> potential VM optimizations.
> The initialization code is composed of creating an integer literal `0` 
> - I chose to minimum amount
> of computation to avoid having the computation costs amortize the 
> lazy-val-related initialization costs.
> This should thus be the borderline case where the program does nothing 
> else but instantiate objects and initialize their lazy vals.
>
> My platform:
> i7-2600, quad-core, hyperthreading, 3.4GHz
> MacOS X 10.7.5
> Java 7, update 4, 64-bit
> Scala 2.10.1
>
> The reports themselves are here:
>
> http://lampwww.epfl.ch/~prokopec/lazyvals/report/
>
> The repository with the executable microbenchmarks with the code that 
> produced these:
>
> https://github.com/axel22/lazy-val-bench
> https://github.com/axel22/lazy-val-bench/blob/master/src/test/scala/example/SyncBench.scala
>
> In this benchmark, the 2 synchronized blocks design with a notify is 5 
> times slower than the current lazy val implementation - and just 
> adding/removing the line with the `notifyAll` call changes things 
> considerably.
>
> My question:
> Is this expected? Is the cost of a `notifyAll` call expected to be 
> this high?
> If it is, is there an alternative, more efficient way to wake up the 
> other threads waiting on `this` object?
>
> Thank you,
> Aleksandar Prokopec
>
>
>   
>
>
> --
> Aleksandar Prokopec
> Doctoral Assistant
> LAMP, IC, EPFL
> http://people.epfl.ch/aleksandar.prokopec
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130520/95c46993/attachment.html>

From peter.levart at gmail.com  Mon May 20 10:30:37 2013
From: peter.levart at gmail.com (Peter Levart)
Date: Mon, 20 May 2013 16:30:37 +0200
Subject: [concurrency-interest] The cost of notifyAll and
In-Reply-To: <519A203A.8090209@oracle.com>
References: <519658D0.1090709@gmail.com> <519A203A.8090209@oracle.com>
Message-ID: <519A338D.7070603@gmail.com>

Hi Oleksandr,

I think Aleksandar is trying to solve just the "Problem 1" in the 
mentioned discussion (see below) where there's no dependency loop 
amongst lazy vals, but a "loop" in acquire-ing object monitors, since 
all lazy vals within the same container object share the same object 
monitor (the container object itself). He does this by implementing 
sef-made locks which are per-lazy-val and require just 2 bits of storage 
each, which is quite clever. The object monitor (of the container 
object) is used just for implementing those locks and is held just for 
the brief moments of state-transitions and not during the computing of 
the value for the lazy val.

There's still a possibility of dead-locks when the code in say object A 
uses "this.synchronized { }" explicitly and a lazy val from object B is 
accessed from such synchronized block and vice versa:

A {
    lazy val a = ...
    ...
    this.synchronized { ... B.b ... }
}

B {
    lazy val b = ...
    ...
    this.synchronized { ... A.a ... }
}


To solve this, another Object would be needed per container object, just 
for synchronization, but this would increase the footprint.

Back to Aleksandar's problem about the performance of notifyAll... Is 
this really a problem, since it only occurs on slow-path?


Regards, Peter

On 05/20/2013 03:08 PM, oleksandr otenko wrote:
> A deadlock means there is a dependency loop.
>
> How do you propose to resolve lazy val reference, when it is not computed?
>
> Example in Java:
>
> class A {
>   static int x = B.x+1;
> }
>
> class B {
>   static int x = A.x+1;
> }
>
> This is how you can get a deadlock in Java, if A and B happen to be 
> initialized in different threads.
>
> When executed single-threadedly, JVM will supply 0 as the value of one 
> of them, but it is bad design of the classes to start with.
>
> In lazy evaluating languages this design is the same as 
> non-terminating recursion. I am not sure you can furnish a solution 
> for this in Scala.
>
>
> Alex
>
>
> On 17/05/2013 17:20, Aleksandar Prokopec wrote:
>> Hello,
>>
>> I have a question regarding the performance of `notifyAll` calls 
>> compared to entering and exiting monitors on the JVM.
>> Some background - recently there has been a lot of talk on the Scala 
>> mailing lists about changing the Scala lazy vals implementation
>> which is currently deadlock-prone in cases where there is no reason 
>> for it to be.
>> We have an alternative design that we are trying to evaluate in terms 
>> of correctness and performance.
>>
>> The full discussion is available here:
>> https://groups.google.com/group/scala-internals/browse_thread/thread/702801329e64f11f
>>
>> I illustrate these changes with the hand-written version of our lazy 
>> vals implementation.
>> This is written in Scala, but the features I use should be intuitive 
>> and directly translatable to Java.
>> This is a lazy val declaration:
>>
>>   class LazyCell(x: Int) {
>>     lazy val value = 0
>>   }
>>
>> This is what our compiler currently does for a `lazy val` declaration:
>>
>>   final class LazySimCell(x: Int) {
>>     @volatile var bitmap_0: Boolean = false
>>     var value_0: Int = _
>>     private def value_lzycompute(): Int = {
>>       this.synchronized {
>>         if (bitmap_0) {
>>           value_0 = 0
>>           bitmap_0 = true
>>         }
>>       }
>>       value_0
>>     }
>>     def value = if (bitmap_0) value_0 else value_lzycompute()
>>   }
>>
>> The problem with this design is that if a `lazy val` right hand side 
>> depends on `lazy val`s in different objects cyclically, but lazy vals 
>> dependencies themselves
>> do not form a cycle, then this can lead to deadlocks.
>>
>> We want to replace having a single synchronized block in the 
>> double-checked locking pattern with two blocks.
>> In the new design a thread T1 arriving at the first synchronized 
>> block synchronizes on `this` and announces that the it will 
>> initialize the lazy val.
>> Subsequent readers of the lazy val then wait until they are notified.
>> T1 then computes the value.
>> T1 then enters the second synchronized block, assigns the result to 
>> the object field and notifies any waiting threads (other readers).
>>
>> This is the newly proposed design, hand-written:
>>
>>    final class LazySimCellVersion2(x: Int) {
>>     @volatile var bitmap_0: Byte = 0.toByte
>>     var value_0: Int = _
>>     private def value_lzycompute(): Int = {
>>       this.synchronized {
>>         if (bitmap_0 == 0.toByte) {
>>           bitmap_0 = 1.toByte
>>         } else {
>>           while (bitmap_0 == 1.toByte) {
>>             this.wait()
>>           }
>>           return value_0
>>         }
>>       }
>>       val result = 0
>>       this.synchronized {
>>         value_0 = result
>>         bitmap_0 = 2.toByte
>>         this.notifyAll()
>>       }
>>       value_0
>>     }
>>     def value = if (bitmap_0 == 2.toByte) value_0 else value_lzycompute()
>>   }
>>
>> I have run microbenchmarks to compare the two designs in a 
>> non-contended mode as follows.
>> I measured the following:
>> - instantiating an object with a value
>> - instantiating an object and initializing its lazy value
>> - same thing, but with manually implemented lazy vals with boolean 
>> bitmaps
>> - same thing, but with byte bitmaps
>> - the proposed lazy value change with 2 synchronizations blocks, but 
>> without a notifying potential waiters
>> - same thing, but with a notify call
>> repeated 1000000 through 5000000 times, in steps of 1000000.
>> The instantiated object is assigned to a field each time, to discard 
>> potential VM optimizations.
>> The initialization code is composed of creating an integer literal 
>> `0` - I chose to minimum amount
>> of computation to avoid having the computation costs amortize the 
>> lazy-val-related initialization costs.
>> This should thus be the borderline case where the program does 
>> nothing else but instantiate objects and initialize their lazy vals.
>>
>> My platform:
>> i7-2600, quad-core, hyperthreading, 3.4GHz
>> MacOS X 10.7.5
>> Java 7, update 4, 64-bit
>> Scala 2.10.1
>>
>> The reports themselves are here:
>>
>> http://lampwww.epfl.ch/~prokopec/lazyvals/report/
>>
>> The repository with the executable microbenchmarks with the code that 
>> produced these:
>>
>> https://github.com/axel22/lazy-val-bench
>> https://github.com/axel22/lazy-val-bench/blob/master/src/test/scala/example/SyncBench.scala
>>
>> In this benchmark, the 2 synchronized blocks design with a notify is 
>> 5 times slower than the current lazy val implementation - and just 
>> adding/removing the line with the `notifyAll` call changes things 
>> considerably.
>>
>> My question:
>> Is this expected? Is the cost of a `notifyAll` call expected to be 
>> this high?
>> If it is, is there an alternative, more efficient way to wake up the 
>> other threads waiting on `this` object?
>>
>> Thank you,
>> Aleksandar Prokopec
>>
>>
>>   
>>
>>
>> --
>> Aleksandar Prokopec
>> Doctoral Assistant
>> LAMP, IC, EPFL
>> http://people.epfl.ch/aleksandar.prokopec
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130520/1b3cc4e0/attachment-0001.html>

From joe.bowbeer at gmail.com  Mon May 20 10:42:30 2013
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Mon, 20 May 2013 07:42:30 -0700
Subject: [concurrency-interest] The cost of notifyAll and
In-Reply-To: <519658D0.1090709@gmail.com>
References: <519658D0.1090709@gmail.com>
Message-ID: <CAHzJPEqsFmD0houJHH_pwANGPEkRovegfpwZ54jOLcjx9cSL5g@mail.gmail.com>

Any time I see a mention of notifyAll, I am reminded of Tom Cargill's
article on the specific notification pattern. I think there is some
treatment of this in CPJ. It is not exactly or perhaps even close to what
you
On May 17, 2013 9:29 AM, "Aleksandar Prokopec" <
aleksandar.prokopec at gmail.com> wrote:

>  Hello,
>
> I have a question regarding the performance of `notifyAll` calls compared
> to entering and exiting monitors on the JVM.
> Some background - recently there has been a lot of talk on the Scala
> mailing lists about changing the Scala lazy vals implementation
> which is currently deadlock-prone in cases where there is no reason for it
> to be.
> We have an alternative design that we are trying to evaluate in terms of
> correctness and performance.
>
> The full discussion is available here:
>
> https://groups.google.com/group/scala-internals/browse_thread/thread/702801329e64f11f
>
> I illustrate these changes with the hand-written version of our lazy vals
> implementation.
> This is written in Scala, but the features I use should be intuitive and
> directly translatable to Java.
> This is a lazy val declaration:
>
>   class LazyCell(x: Int) {
>     lazy val value = 0
>   }
>
> This is what our compiler currently does for a `lazy val` declaration:
>
>   final class LazySimCell(x: Int) {
>     @volatile var bitmap_0: Boolean = false
>     var value_0: Int = _
>     private def value_lzycompute(): Int = {
>       this.synchronized {
>         if (bitmap_0) {
>           value_0 = 0
>           bitmap_0 = true
>         }
>       }
>       value_0
>     }
>     def value = if (bitmap_0) value_0 else value_lzycompute()
>   }
>
> The problem with this design is that if a `lazy val` right hand side
> depends on `lazy val`s in different objects cyclically, but lazy vals
> dependencies themselves
> do not form a cycle, then this can lead to deadlocks.
>
> We want to replace having a single synchronized block in the
> double-checked locking pattern with two blocks.
> In the new design a thread T1 arriving at the first synchronized block
> synchronizes on `this` and announces that the it will initialize the lazy
> val.
> Subsequent readers of the lazy val then wait until they are notified.
> T1 then computes the value.
> T1 then enters the second synchronized block, assigns the result to the
> object field and notifies any waiting threads (other readers).
>
> This is the newly proposed design, hand-written:
>
>    final class LazySimCellVersion2(x: Int) {
>     @volatile var bitmap_0: Byte = 0.toByte
>     var value_0: Int = _
>     private def value_lzycompute(): Int = {
>       this.synchronized {
>         if (bitmap_0 == 0.toByte) {
>           bitmap_0 = 1.toByte
>         } else {
>           while (bitmap_0 == 1.toByte) {
>             this.wait()
>           }
>           return value_0
>         }
>       }
>       val result = 0
>       this.synchronized {
>         value_0 = result
>         bitmap_0 = 2.toByte
>         this.notifyAll()
>       }
>       value_0
>     }
>     def value = if (bitmap_0 == 2.toByte) value_0 else value_lzycompute()
>   }
>
> I have run microbenchmarks to compare the two designs in a non-contended
> mode as follows.
> I measured the following:
> - instantiating an object with a value
> - instantiating an object and initializing its lazy value
> - same thing, but with manually implemented lazy vals with boolean bitmaps
> - same thing, but with byte bitmaps
> - the proposed lazy value change with 2 synchronizations blocks, but
> without a notifying potential waiters
> - same thing, but with a notify call
> repeated 1000000 through 5000000 times, in steps of 1000000.
> The instantiated object is assigned to a field each time, to discard
> potential VM optimizations.
> The initialization code is composed of creating an integer literal `0` - I
> chose to minimum amount
> of computation to avoid having the computation costs amortize the
> lazy-val-related initialization costs.
> This should thus be the borderline case where the program does nothing
> else but instantiate objects and initialize their lazy vals.
>
> My platform:
> i7-2600, quad-core, hyperthreading, 3.4GHz
> MacOS X 10.7.5
> Java 7, update 4, 64-bit
> Scala 2.10.1
>
> The reports themselves are here:
>
> http://lampwww.epfl.ch/~prokopec/lazyvals/report/
>
> The repository with the executable microbenchmarks with the code that
> produced these:
>
> https://github.com/axel22/lazy-val-bench
>
> https://github.com/axel22/lazy-val-bench/blob/master/src/test/scala/example/SyncBench.scala
>
> In this benchmark, the 2 synchronized blocks design with a notify is 5
> times slower than the current lazy val implementation - and just
> adding/removing the line with the `notifyAll` call changes things
> considerably.
>
> My question:
> Is this expected? Is the cost of a `notifyAll` call expected to be this
> high?
> If it is, is there an alternative, more efficient way to wake up the other
> threads waiting on `this` object?
>
> Thank you,
> Aleksandar Prokopec
>
>
>
>
>
> --
> Aleksandar Prokopec
> Doctoral Assistant
> LAMP, IC, EPFLhttp://people.epfl.ch/aleksandar.prokopec
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130520/7ed44973/attachment.html>

From joe.bowbeer at gmail.com  Mon May 20 10:49:18 2013
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Mon, 20 May 2013 07:49:18 -0700
Subject: [concurrency-interest] The cost of notifyAll and
In-Reply-To: <CAHzJPEqsFmD0houJHH_pwANGPEkRovegfpwZ54jOLcjx9cSL5g@mail.gmail.com>
References: <519658D0.1090709@gmail.com>
	<CAHzJPEqsFmD0houJHH_pwANGPEkRovegfpwZ54jOLcjx9cSL5g@mail.gmail.com>
Message-ID: <CAHzJPEp51GKZA2_Z2NASxjPFvLhcZoqTCtnphNjL0Qs4iLD+NQ@mail.gmail.com>

Oops. Trying again:

Any time I see a mention of notifyAll, I am reminded of Tom Cargill's
article on the specific notification pattern. I think there is some
treatment of this in CPJ. It is not exactly or perhaps even close to what
you need here, but more explicit notification may give you a means for
detecting deadlock as well as optimizing performance.
On May 20, 2013 7:42 AM, "Joe Bowbeer" <joe.bowbeer at gmail.com> wrote:

> Any time I see a mention of notifyAll, I am reminded of Tom Cargill's
> article on the specific notification pattern. I think there is some
> treatment of this in CPJ. It is not exactly or perhaps even close to what
> you
> On May 17, 2013 9:29 AM, "Aleksandar Prokopec" <
> aleksandar.prokopec at gmail.com> wrote:
>
>>  Hello,
>>
>> I have a question regarding the performance of `notifyAll` calls compared
>> to entering and exiting monitors on the JVM.
>> Some background - recently there has been a lot of talk on the Scala
>> mailing lists about changing the Scala lazy vals implementation
>> which is currently deadlock-prone in cases where there is no reason for
>> it to be.
>> We have an alternative design that we are trying to evaluate in terms of
>> correctness and performance.
>>
>> The full discussion is available here:
>>
>> https://groups.google.com/group/scala-internals/browse_thread/thread/702801329e64f11f
>>
>> I illustrate these changes with the hand-written version of our lazy vals
>> implementation.
>> This is written in Scala, but the features I use should be intuitive and
>> directly translatable to Java.
>> This is a lazy val declaration:
>>
>>   class LazyCell(x: Int) {
>>     lazy val value = 0
>>   }
>>
>> This is what our compiler currently does for a `lazy val` declaration:
>>
>>   final class LazySimCell(x: Int) {
>>     @volatile var bitmap_0: Boolean = false
>>     var value_0: Int = _
>>     private def value_lzycompute(): Int = {
>>       this.synchronized {
>>         if (bitmap_0) {
>>           value_0 = 0
>>           bitmap_0 = true
>>         }
>>       }
>>       value_0
>>     }
>>     def value = if (bitmap_0) value_0 else value_lzycompute()
>>   }
>>
>> The problem with this design is that if a `lazy val` right hand side
>> depends on `lazy val`s in different objects cyclically, but lazy vals
>> dependencies themselves
>> do not form a cycle, then this can lead to deadlocks.
>>
>> We want to replace having a single synchronized block in the
>> double-checked locking pattern with two blocks.
>> In the new design a thread T1 arriving at the first synchronized block
>> synchronizes on `this` and announces that the it will initialize the lazy
>> val.
>> Subsequent readers of the lazy val then wait until they are notified.
>> T1 then computes the value.
>> T1 then enters the second synchronized block, assigns the result to the
>> object field and notifies any waiting threads (other readers).
>>
>> This is the newly proposed design, hand-written:
>>
>>    final class LazySimCellVersion2(x: Int) {
>>     @volatile var bitmap_0: Byte = 0.toByte
>>     var value_0: Int = _
>>     private def value_lzycompute(): Int = {
>>       this.synchronized {
>>         if (bitmap_0 == 0.toByte) {
>>           bitmap_0 = 1.toByte
>>         } else {
>>           while (bitmap_0 == 1.toByte) {
>>             this.wait()
>>           }
>>           return value_0
>>         }
>>       }
>>       val result = 0
>>       this.synchronized {
>>         value_0 = result
>>         bitmap_0 = 2.toByte
>>         this.notifyAll()
>>       }
>>       value_0
>>     }
>>     def value = if (bitmap_0 == 2.toByte) value_0 else value_lzycompute()
>>   }
>>
>> I have run microbenchmarks to compare the two designs in a non-contended
>> mode as follows.
>> I measured the following:
>> - instantiating an object with a value
>> - instantiating an object and initializing its lazy value
>> - same thing, but with manually implemented lazy vals with boolean bitmaps
>> - same thing, but with byte bitmaps
>> - the proposed lazy value change with 2 synchronizations blocks, but
>> without a notifying potential waiters
>> - same thing, but with a notify call
>> repeated 1000000 through 5000000 times, in steps of 1000000.
>> The instantiated object is assigned to a field each time, to discard
>> potential VM optimizations.
>> The initialization code is composed of creating an integer literal `0` -
>> I chose to minimum amount
>> of computation to avoid having the computation costs amortize the
>> lazy-val-related initialization costs.
>> This should thus be the borderline case where the program does nothing
>> else but instantiate objects and initialize their lazy vals.
>>
>> My platform:
>> i7-2600, quad-core, hyperthreading, 3.4GHz
>> MacOS X 10.7.5
>> Java 7, update 4, 64-bit
>> Scala 2.10.1
>>
>> The reports themselves are here:
>>
>> http://lampwww.epfl.ch/~prokopec/lazyvals/report/
>>
>> The repository with the executable microbenchmarks with the code that
>> produced these:
>>
>> https://github.com/axel22/lazy-val-bench
>>
>> https://github.com/axel22/lazy-val-bench/blob/master/src/test/scala/example/SyncBench.scala
>>
>> In this benchmark, the 2 synchronized blocks design with a notify is 5
>> times slower than the current lazy val implementation - and just
>> adding/removing the line with the `notifyAll` call changes things
>> considerably.
>>
>> My question:
>> Is this expected? Is the cost of a `notifyAll` call expected to be this
>> high?
>> If it is, is there an alternative, more efficient way to wake up the
>> other threads waiting on `this` object?
>>
>> Thank you,
>> Aleksandar Prokopec
>>
>>
>>
>>
>>
>> --
>> Aleksandar Prokopec
>> Doctoral Assistant
>> LAMP, IC, EPFLhttp://people.epfl.ch/aleksandar.prokopec
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130520/7768e7ea/attachment-0001.html>

From nathan.reynolds at oracle.com  Mon May 20 14:16:02 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Mon, 20 May 2013 11:16:02 -0700
Subject: [concurrency-interest] array of objects and false sharing
In-Reply-To: <519778B7.3090002@oracle.com>
References: <CAGuAWdCGa7fY5b2nYEfCz61h8EJkEwoB3gWbKtUwVvNG1MUJAA@mail.gmail.com>
	<519778B7.3090002@oracle.com>
Message-ID: <519A6862.6050803@oracle.com>

Each thread has a Thread Local Allocation Buffer (TLAB).  When a thread 
allocates, it simply bumps a pointer in the TLAB.  So, if the thread 
allocates a bunch of objects one after the other, then they will be 
adjacent.  As the objects are pushed around during GC, GC tries to keep 
the objects together to improve true-sharing performance.  This is what 
you are fighting against.  One way is to add padding inside each 
instance.  This will ensure each instance gets its own cache line.  
Another way is to have a different thread allocate each instance.  This 
ensures that the object will be allocated in a different TLAB.  There 
are 2 potential problems.  First, threads can share TLABs and so this 
will make no difference.  Second, you are hoping that there will be 
enough live objects between instances as they age and get promoted to 
the old generation.

If I understand the @Contended annotation, it rearranges the layout of 
the fields so that there is internal padding.  What if we were to allow 
annotating the class?  The intent is to have GC never put 2 instances of 
the same class with @Contended on the class next to each other in the 
heap.  This would solve the problem.  The objects would be allocated 
adjacent to each other but GC would quickly separate them at the next 
young collection.

Nathan Reynolds 
<http://psr.us.oracle.com/wiki/index.php/User:Nathan_Reynolds> | 
Architect | 602.333.9091
Oracle PSR Engineering <http://psr.us.oracle.com/> | Server Technology
On 5/18/2013 5:48 AM, Aleksey Shipilev wrote:
> On 05/18/2013 03:38 PM, Peter Veentjer wrote:
>> What is the best way to deal with false sharing?
> As stated before:
>
>   a) JDK 5+: pad with enough fields to fit the cache line. Be careful to
> pad longer because some hardware can actually prefetch the adjacent
> cache lines thinking you are about to read them as well.
>
> For example, see Striped64.Cell:
>   http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/jsr166e/Striped64.java?view=markup
>
> This relies on the field ordering though. In the absence of @Contended,
> you might find piggy-backing on the inheritance more reliable (see L*
> subclasses and the implementation notes):
>   http://hg.openjdk.java.net/code-tools/jmh/file/tip/jmh-core/src/main/java/org/openjdk/jmh/logic/BlackHole.java
>
>   b) JDK 8+: @Contended.
>
> -Aleksey.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130520/546e19c8/attachment.html>

From aleksey.shipilev at oracle.com  Tue May 21 03:37:17 2013
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Tue, 21 May 2013 11:37:17 +0400
Subject: [concurrency-interest] array of objects and false sharing
In-Reply-To: <519A6862.6050803@oracle.com>
References: <CAGuAWdCGa7fY5b2nYEfCz61h8EJkEwoB3gWbKtUwVvNG1MUJAA@mail.gmail.com>
	<519778B7.3090002@oracle.com> <519A6862.6050803@oracle.com>
Message-ID: <519B242D.9070609@oracle.com>

On 05/20/2013 10:16 PM, Nathan Reynolds wrote:
> If I understand the @Contended annotation, it rearranges the layout
> of the fields so that there is internal padding.  What if we were to
> allow annotating the class?

@Contended does not advertise the way to achieve isolation, giving us,
runtime developers, to select the appropriate way under the hood.

@Contended on class is also accepted, with the semantics of "entire
instance is isolated". ...which in current very pessimistic approach in
HS requires padding of the entire field block. This still has a drawback
to expose object header for false sharing, but most of the use cases are
covered nicely already.

> The intent is to have GC never put 2 instances of the same class with
> @Contended on the class next to each other in the heap.  This would
> solve the problem.

This intent is not enough. @Contended is about the isolation with every
other object, so the adjacent writes are not disturbing the protected
state. The only sane option I see in runtime today is to request the
entire cache-line for the object, and align the object for the cache
line start. This requires cooperation with GC, and so HotSpot does not
do this at this point, although should have in the future, especially if
we see lots of @Contended objects on the heap in reasonable use cases.

-Aleksey.

From aleksandar.prokopec at gmail.com  Tue May 21 04:19:15 2013
From: aleksandar.prokopec at gmail.com (Aleksandar Prokopec)
Date: Tue, 21 May 2013 10:19:15 +0200
Subject: [concurrency-interest] The cost of notifyAll and
In-Reply-To: <519A338D.7070603@gmail.com>
References: <519658D0.1090709@gmail.com> <519A203A.8090209@oracle.com>
	<519A338D.7070603@gmail.com>
Message-ID: <519B2E03.9070007@gmail.com>

Hi Peter,

If a slow path is the case where the lazy val is not yet initialized but 
there is not contention, then it might be problematic, since we would be 
slowing down a widely used language feature multiple times.
I would be in favour of retaining the existing slow-path performance at 
least in the uncontended case.

I saw your e-mail on the other mailing list - having a 4th state to 
eliminate the notifyAll in the common uncontended case looks like 
promising idea!
I published the updated performance graphs - it is about 15-20% slower 
than the previous version without the notifyAll.

http://lampwww.epfl.ch/~prokopec/lazyvals/report/
(see `lazy-simulation-v3`)

Regards,
Aleksandar

--
Aleksandar Prokopec
Doctoral Assistant
LAMP, IC, EPFL
http://people.epfl.ch/aleksandar.prokopec

On 5/20/13 4:30 PM, Peter Levart wrote:
> Hi Oleksandr,
>
> I think Aleksandar is trying to solve just the "Problem 1" in the 
> mentioned discussion (see below) where there's no dependency loop 
> amongst lazy vals, but a "loop" in acquire-ing object monitors, since 
> all lazy vals within the same container object share the same object 
> monitor (the container object itself). He does this by implementing 
> sef-made locks which are per-lazy-val and require just 2 bits of 
> storage each, which is quite clever. The object monitor (of the 
> container object) is used just for implementing those locks and is 
> held just for the brief moments of state-transitions and not during 
> the computing of the value for the lazy val.
>
> There's still a possibility of dead-locks when the code in say object 
> A uses "this.synchronized { }" explicitly and a lazy val from object B 
> is accessed from such synchronized block and vice versa:
>
> A {
>    lazy val a = ...
>    ...
>    this.synchronized { ... B.b ... }
> }
>
> B {
>    lazy val b = ...
>    ...
>    this.synchronized { ... A.a ... }
> }
>
>
> To solve this, another Object would be needed per container object, 
> just for synchronization, but this would increase the footprint.
>
> Back to Aleksandar's problem about the performance of notifyAll... Is 
> this really a problem, since it only occurs on slow-path?
>
>
> Regards, Peter
>
> On 05/20/2013 03:08 PM, oleksandr otenko wrote:
>> A deadlock means there is a dependency loop.
>>
>> How do you propose to resolve lazy val reference, when it is not 
>> computed?
>>
>> Example in Java:
>>
>> class A {
>>   static int x = B.x+1;
>> }
>>
>> class B {
>>   static int x = A.x+1;
>> }
>>
>> This is how you can get a deadlock in Java, if A and B happen to be 
>> initialized in different threads.
>>
>> When executed single-threadedly, JVM will supply 0 as the value of 
>> one of them, but it is bad design of the classes to start with.
>>
>> In lazy evaluating languages this design is the same as 
>> non-terminating recursion. I am not sure you can furnish a solution 
>> for this in Scala.
>>
>>
>> Alex
>>
>>
>> On 17/05/2013 17:20, Aleksandar Prokopec wrote:
>>> Hello,
>>>
>>> I have a question regarding the performance of `notifyAll` calls 
>>> compared to entering and exiting monitors on the JVM.
>>> Some background - recently there has been a lot of talk on the Scala 
>>> mailing lists about changing the Scala lazy vals implementation
>>> which is currently deadlock-prone in cases where there is no reason 
>>> for it to be.
>>> We have an alternative design that we are trying to evaluate in 
>>> terms of correctness and performance.
>>>
>>> The full discussion is available here:
>>> https://groups.google.com/group/scala-internals/browse_thread/thread/702801329e64f11f
>>>
>>> I illustrate these changes with the hand-written version of our lazy 
>>> vals implementation.
>>> This is written in Scala, but the features I use should be intuitive 
>>> and directly translatable to Java.
>>> This is a lazy val declaration:
>>>
>>>   class LazyCell(x: Int) {
>>>     lazy val value = 0
>>>   }
>>>
>>> This is what our compiler currently does for a `lazy val` declaration:
>>>
>>>   final class LazySimCell(x: Int) {
>>>     @volatile var bitmap_0: Boolean = false
>>>     var value_0: Int = _
>>>     private def value_lzycompute(): Int = {
>>>       this.synchronized {
>>>         if (bitmap_0) {
>>>           value_0 = 0
>>>           bitmap_0 = true
>>>         }
>>>       }
>>>       value_0
>>>     }
>>>     def value = if (bitmap_0) value_0 else value_lzycompute()
>>>   }
>>>
>>> The problem with this design is that if a `lazy val` right hand side 
>>> depends on `lazy val`s in different objects cyclically, but lazy 
>>> vals dependencies themselves
>>> do not form a cycle, then this can lead to deadlocks.
>>>
>>> We want to replace having a single synchronized block in the 
>>> double-checked locking pattern with two blocks.
>>> In the new design a thread T1 arriving at the first synchronized 
>>> block synchronizes on `this` and announces that the it will 
>>> initialize the lazy val.
>>> Subsequent readers of the lazy val then wait until they are notified.
>>> T1 then computes the value.
>>> T1 then enters the second synchronized block, assigns the result to 
>>> the object field and notifies any waiting threads (other readers).
>>>
>>> This is the newly proposed design, hand-written:
>>>
>>>    final class LazySimCellVersion2(x: Int) {
>>>     @volatile var bitmap_0: Byte = 0.toByte
>>>     var value_0: Int = _
>>>     private def value_lzycompute(): Int = {
>>>       this.synchronized {
>>>         if (bitmap_0 == 0.toByte) {
>>>           bitmap_0 = 1.toByte
>>>         } else {
>>>           while (bitmap_0 == 1.toByte) {
>>>             this.wait()
>>>           }
>>>           return value_0
>>>         }
>>>       }
>>>       val result = 0
>>>       this.synchronized {
>>>         value_0 = result
>>>         bitmap_0 = 2.toByte
>>>         this.notifyAll()
>>>       }
>>>       value_0
>>>     }
>>>     def value = if (bitmap_0 == 2.toByte) value_0 else 
>>> value_lzycompute()
>>>   }
>>>
>>> I have run microbenchmarks to compare the two designs in a 
>>> non-contended mode as follows.
>>> I measured the following:
>>> - instantiating an object with a value
>>> - instantiating an object and initializing its lazy value
>>> - same thing, but with manually implemented lazy vals with boolean 
>>> bitmaps
>>> - same thing, but with byte bitmaps
>>> - the proposed lazy value change with 2 synchronizations blocks, but 
>>> without a notifying potential waiters
>>> - same thing, but with a notify call
>>> repeated 1000000 through 5000000 times, in steps of 1000000.
>>> The instantiated object is assigned to a field each time, to discard 
>>> potential VM optimizations.
>>> The initialization code is composed of creating an integer literal 
>>> `0` - I chose to minimum amount
>>> of computation to avoid having the computation costs amortize the 
>>> lazy-val-related initialization costs.
>>> This should thus be the borderline case where the program does 
>>> nothing else but instantiate objects and initialize their lazy vals.
>>>
>>> My platform:
>>> i7-2600, quad-core, hyperthreading, 3.4GHz
>>> MacOS X 10.7.5
>>> Java 7, update 4, 64-bit
>>> Scala 2.10.1
>>>
>>> The reports themselves are here:
>>>
>>> http://lampwww.epfl.ch/~prokopec/lazyvals/report/
>>>
>>> The repository with the executable microbenchmarks with the code 
>>> that produced these:
>>>
>>> https://github.com/axel22/lazy-val-bench
>>> https://github.com/axel22/lazy-val-bench/blob/master/src/test/scala/example/SyncBench.scala
>>>
>>> In this benchmark, the 2 synchronized blocks design with a notify is 
>>> 5 times slower than the current lazy val implementation - and just 
>>> adding/removing the line with the `notifyAll` call changes things 
>>> considerably.
>>>
>>> My question:
>>> Is this expected? Is the cost of a `notifyAll` call expected to be 
>>> this high?
>>> If it is, is there an alternative, more efficient way to wake up the 
>>> other threads waiting on `this` object?
>>>
>>> Thank you,
>>> Aleksandar Prokopec
>>>
>>>
>>>   
>>>
>>>
>>> --
>>> Aleksandar Prokopec
>>> Doctoral Assistant
>>> LAMP, IC, EPFL
>>> http://people.epfl.ch/aleksandar.prokopec
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130521/4ffda190/attachment-0001.html>

From aleksandar.prokopec at gmail.com  Tue May 21 04:10:09 2013
From: aleksandar.prokopec at gmail.com (Aleksandar Prokopec)
Date: Tue, 21 May 2013 10:10:09 +0200
Subject: [concurrency-interest] The cost of notifyAll and
In-Reply-To: <519A203A.8090209@oracle.com>
References: <519658D0.1090709@gmail.com> <519A203A.8090209@oracle.com>
Message-ID: <519B2BE1.20809@gmail.com>

Hi Alex,

As Peter pointed out, we are trying to solve cases

There is nothing we can do about the cases where there is a mutual 
dependency between lazy vals.
We argue, as you say, that that is a bad design anyway.
However, the current lazy val implementations may deadlock in the 
following case:

object A {
   lazy val a = B.b
   lazy val c = 1
}

object B {
   lazy val b = A.c
}

Note that there is no circular dependency between lazy vals, but if 2 
threads concurrenctly request `a` and `b`, a deadlock may ensue.
This is what we are trying to eliminate with the new scheme.

Regards,
Aleksandar

--
Aleksandar Prokopec
Doctoral Assistant
LAMP, IC, EPFL
http://people.epfl.ch/aleksandar.prokopec

On 5/20/13 3:08 PM, oleksandr otenko wrote:
> A deadlock means there is a dependency loop.
>
> How do you propose to resolve lazy val reference, when it is not computed?
>
> Example in Java:
>
> class A {
>   static int x = B.x+1;
> }
>
> class B {
>   static int x = A.x+1;
> }
>
> This is how you can get a deadlock in Java, if A and B happen to be 
> initialized in different threads.
>
> When executed single-threadedly, JVM will supply 0 as the value of one 
> of them, but it is bad design of the classes to start with.
>
> In lazy evaluating languages this design is the same as 
> non-terminating recursion. I am not sure you can furnish a solution 
> for this in Scala.
>
>
> Alex
>
>
> On 17/05/2013 17:20, Aleksandar Prokopec wrote:
>> Hello,
>>
>> I have a question regarding the performance of `notifyAll` calls 
>> compared to entering and exiting monitors on the JVM.
>> Some background - recently there has been a lot of talk on the Scala 
>> mailing lists about changing the Scala lazy vals implementation
>> which is currently deadlock-prone in cases where there is no reason 
>> for it to be.
>> We have an alternative design that we are trying to evaluate in terms 
>> of correctness and performance.
>>
>> The full discussion is available here:
>> https://groups.google.com/group/scala-internals/browse_thread/thread/702801329e64f11f
>>
>> I illustrate these changes with the hand-written version of our lazy 
>> vals implementation.
>> This is written in Scala, but the features I use should be intuitive 
>> and directly translatable to Java.
>> This is a lazy val declaration:
>>
>>   class LazyCell(x: Int) {
>>     lazy val value = 0
>>   }
>>
>> This is what our compiler currently does for a `lazy val` declaration:
>>
>>   final class LazySimCell(x: Int) {
>>     @volatile var bitmap_0: Boolean = false
>>     var value_0: Int = _
>>     private def value_lzycompute(): Int = {
>>       this.synchronized {
>>         if (bitmap_0) {
>>           value_0 = 0
>>           bitmap_0 = true
>>         }
>>       }
>>       value_0
>>     }
>>     def value = if (bitmap_0) value_0 else value_lzycompute()
>>   }
>>
>> The problem with this design is that if a `lazy val` right hand side 
>> depends on `lazy val`s in different objects cyclically, but lazy vals 
>> dependencies themselves
>> do not form a cycle, then this can lead to deadlocks.
>>
>> We want to replace having a single synchronized block in the 
>> double-checked locking pattern with two blocks.
>> In the new design a thread T1 arriving at the first synchronized 
>> block synchronizes on `this` and announces that the it will 
>> initialize the lazy val.
>> Subsequent readers of the lazy val then wait until they are notified.
>> T1 then computes the value.
>> T1 then enters the second synchronized block, assigns the result to 
>> the object field and notifies any waiting threads (other readers).
>>
>> This is the newly proposed design, hand-written:
>>
>>    final class LazySimCellVersion2(x: Int) {
>>     @volatile var bitmap_0: Byte = 0.toByte
>>     var value_0: Int = _
>>     private def value_lzycompute(): Int = {
>>       this.synchronized {
>>         if (bitmap_0 == 0.toByte) {
>>           bitmap_0 = 1.toByte
>>         } else {
>>           while (bitmap_0 == 1.toByte) {
>>             this.wait()
>>           }
>>           return value_0
>>         }
>>       }
>>       val result = 0
>>       this.synchronized {
>>         value_0 = result
>>         bitmap_0 = 2.toByte
>>         this.notifyAll()
>>       }
>>       value_0
>>     }
>>     def value = if (bitmap_0 == 2.toByte) value_0 else value_lzycompute()
>>   }
>>
>> I have run microbenchmarks to compare the two designs in a 
>> non-contended mode as follows.
>> I measured the following:
>> - instantiating an object with a value
>> - instantiating an object and initializing its lazy value
>> - same thing, but with manually implemented lazy vals with boolean 
>> bitmaps
>> - same thing, but with byte bitmaps
>> - the proposed lazy value change with 2 synchronizations blocks, but 
>> without a notifying potential waiters
>> - same thing, but with a notify call
>> repeated 1000000 through 5000000 times, in steps of 1000000.
>> The instantiated object is assigned to a field each time, to discard 
>> potential VM optimizations.
>> The initialization code is composed of creating an integer literal 
>> `0` - I chose to minimum amount
>> of computation to avoid having the computation costs amortize the 
>> lazy-val-related initialization costs.
>> This should thus be the borderline case where the program does 
>> nothing else but instantiate objects and initialize their lazy vals.
>>
>> My platform:
>> i7-2600, quad-core, hyperthreading, 3.4GHz
>> MacOS X 10.7.5
>> Java 7, update 4, 64-bit
>> Scala 2.10.1
>>
>> The reports themselves are here:
>>
>> http://lampwww.epfl.ch/~prokopec/lazyvals/report/
>>
>> The repository with the executable microbenchmarks with the code that 
>> produced these:
>>
>> https://github.com/axel22/lazy-val-bench
>> https://github.com/axel22/lazy-val-bench/blob/master/src/test/scala/example/SyncBench.scala
>>
>> In this benchmark, the 2 synchronized blocks design with a notify is 
>> 5 times slower than the current lazy val implementation - and just 
>> adding/removing the line with the `notifyAll` call changes things 
>> considerably.
>>
>> My question:
>> Is this expected? Is the cost of a `notifyAll` call expected to be 
>> this high?
>> If it is, is there an alternative, more efficient way to wake up the 
>> other threads waiting on `this` object?
>>
>> Thank you,
>> Aleksandar Prokopec
>>
>>
>>   
>>
>>
>> --
>> Aleksandar Prokopec
>> Doctoral Assistant
>> LAMP, IC, EPFL
>> http://people.epfl.ch/aleksandar.prokopec
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130521/9a82f621/attachment.html>

From davidcholmes at aapt.net.au  Tue May 21 06:22:28 2013
From: davidcholmes at aapt.net.au (David Holmes)
Date: Tue, 21 May 2013 20:22:28 +1000
Subject: [concurrency-interest] hashing the reflect.Methodobject
	inConcurrentHashMap
In-Reply-To: <1368797604.59772.YahooMailNeo@web141105.mail.bf1.yahoo.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCIEEDJNAA.davidcholmes@aapt.net.au>

Andy,

The JMM properties of ConcurrentHashMap are:

a.. Actions in a thread prior to placing an object into any concurrent
collection happen-before actions subsequent to the access or removal of that
element from the collection in another thread.

I would not read into that that attempting to place an object that is
already present would have the same guarantees. Exactly what would happen
would depend on the implementation details. It might be that you get the
same sequence of volatiles writes and reads in both cases (I haven't
verified that and don't plan to). But even so with a single field mutation
there is no way to determine if a particular get() and subsequent read of
the field, happened before or after the mutation.

Safe-publication is a means to share thread-safe, or immutable, objects
across threads in a safe manner. But once shared, it is up to the objects to
maintain multi-threading correctness ie by being thread-safe. So a
mutate-publish-mutate-publish cycle of a non-threadsafe object is not
something that should be relied upon for guaranteed correctness (whatever
correct means in relation to the mutating object).

David
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Andy Nuss
  Sent: Friday, 17 May 2013 11:33 PM
  To: concurrency-interest at cs.oswego.edu
  Subject: Re: [concurrency-interest] hashing the reflect.Methodobject
inConcurrentHashMap


  David,

  An aside, concerning "safely publish via ConcurrentHashMap".  Lets say a
thread-aware object, but not a threaad-safe object, i.e. the kind that can
be published, is gotten from ConcurrentHashMap without remove, mutated to
change its behavior by changing just one and only one mutable field that
serves as a switch case, and then put back into ConcurrentHashMap, a NOP.
Will ConcurrentHashMap ensure that the mutated field is visible to other
cores, even though the re-put is a NOP?

  Andy





----------------------------------------------------------------------------
--
  From: David Holmes <davidcholmes at aapt.net.au>
  To: Stanimir Simeonoff <stanimir at riflexo.com>; Remi Forax
<forax at univ-mlv.fr>
  Cc: concurrency-interest at cs.oswego.edu
  Sent: Friday, May 17, 2013 3:38 AM
  Subject: Re: [concurrency-interest] hashing the reflect.Method object
inConcurrentHashMap



  Maybe I've missed something in the to-and-fro here but I agree that these
reflection objects can be unsafely published (don't do that!) but in this
case they are getting published via a ConcurrentHashmap and so are in fact
safely published.

  David
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130521/726600fa/attachment-0001.html>

From oleksandr.otenko at oracle.com  Tue May 21 12:45:13 2013
From: oleksandr.otenko at oracle.com (oleksandr otenko)
Date: Tue, 21 May 2013 17:45:13 +0100
Subject: [concurrency-interest] The cost of notifyAll and
In-Reply-To: <519B2E03.9070007@gmail.com>
References: <519658D0.1090709@gmail.com> <519A203A.8090209@oracle.com>
	<519A338D.7070603@gmail.com> <519B2E03.9070007@gmail.com>
Message-ID: <519BA499.4090909@oracle.com>

will be better to reorder cas and synchronized, this will make 
contending quickly initialized fields cheaper:


public boolean lock(T target) {
for (; ; ) {
int state = stateUpdater.get(target);
int myState = state & state3;
if (myState > state2) {
// already initialized - no locking needed
return false;
}
else if (myState == state2) {
synchronized (target) {
if ((stateUpdater.get(target) & state3) == state2) { // recheck
try {
target.wait();
}
catch (InterruptedException e) {}
}
}
}
elseif (stateUpdater.compareAndSet(target, state, state + state1) && 
myState == state0) {
// lock established, should call unlock after initializing the lazy val
return true;
}
}
}

public void unlock(T target) {
int myState, state;
do {
             state = stateUpdater.get(target);
myState = state & state3;
         } while (!stateUpdater.compareAndSet(target, state, state 
|state3));

if (myState == state1) return; // there were no waiters
if (myState == state2) {
// there are waiters - notify them after changing state to initialized
synchronized (target) {
target.notifyAll();
}
}
else { // myState == state0 || myState == state3
throw new IllegalStateException("Not locked!");
}
}


Alex

On 21/05/2013 09:19, Aleksandar Prokopec wrote:
> Hi Peter,
>
> If a slow path is the case where the lazy val is not yet initialized 
> but there is not contention, then it might be problematic, since we 
> would be slowing down a widely used language feature multiple times.
> I would be in favour of retaining the existing slow-path performance 
> at least in the uncontended case.
>
> I saw your e-mail on the other mailing list - having a 4th state to 
> eliminate the notifyAll in the common uncontended case looks like 
> promising idea!
> I published the updated performance graphs - it is about 15-20% slower 
> than the previous version without the notifyAll.
>
> http://lampwww.epfl.ch/~prokopec/lazyvals/report/
> (see `lazy-simulation-v3`)
>
> Regards,
> Aleksandar
> --
> Aleksandar Prokopec
> Doctoral Assistant
> LAMP, IC, EPFL
> http://people.epfl.ch/aleksandar.prokopec
> On 5/20/13 4:30 PM, Peter Levart wrote:
>> Hi Oleksandr,
>>
>> I think Aleksandar is trying to solve just the "Problem 1" in the 
>> mentioned discussion (see below) where there's no dependency loop 
>> amongst lazy vals, but a "loop" in acquire-ing object monitors, since 
>> all lazy vals within the same container object share the same object 
>> monitor (the container object itself). He does this by implementing 
>> sef-made locks which are per-lazy-val and require just 2 bits of 
>> storage each, which is quite clever. The object monitor (of the 
>> container object) is used just for implementing those locks and is 
>> held just for the brief moments of state-transitions and not during 
>> the computing of the value for the lazy val.
>>
>> There's still a possibility of dead-locks when the code in say object 
>> A uses "this.synchronized { }" explicitly and a lazy val from object 
>> B is accessed from such synchronized block and vice versa:
>>
>> A {
>>    lazy val a = ...
>>    ...
>>    this.synchronized { ... B.b ... }
>> }
>>
>> B {
>>    lazy val b = ...
>>    ...
>>    this.synchronized { ... A.a ... }
>> }
>>
>>
>> To solve this, another Object would be needed per container object, 
>> just for synchronization, but this would increase the footprint.
>>
>> Back to Aleksandar's problem about the performance of notifyAll... Is 
>> this really a problem, since it only occurs on slow-path?
>>
>>
>> Regards, Peter
>>
>> On 05/20/2013 03:08 PM, oleksandr otenko wrote:
>>> A deadlock means there is a dependency loop.
>>>
>>> How do you propose to resolve lazy val reference, when it is not 
>>> computed?
>>>
>>> Example in Java:
>>>
>>> class A {
>>>   static int x = B.x+1;
>>> }
>>>
>>> class B {
>>>   static int x = A.x+1;
>>> }
>>>
>>> This is how you can get a deadlock in Java, if A and B happen to be 
>>> initialized in different threads.
>>>
>>> When executed single-threadedly, JVM will supply 0 as the value of 
>>> one of them, but it is bad design of the classes to start with.
>>>
>>> In lazy evaluating languages this design is the same as 
>>> non-terminating recursion. I am not sure you can furnish a solution 
>>> for this in Scala.
>>>
>>>
>>> Alex
>>>
>>>
>>> On 17/05/2013 17:20, Aleksandar Prokopec wrote:
>>>> Hello,
>>>>
>>>> I have a question regarding the performance of `notifyAll` calls 
>>>> compared to entering and exiting monitors on the JVM.
>>>> Some background - recently there has been a lot of talk on the 
>>>> Scala mailing lists about changing the Scala lazy vals implementation
>>>> which is currently deadlock-prone in cases where there is no reason 
>>>> for it to be.
>>>> We have an alternative design that we are trying to evaluate in 
>>>> terms of correctness and performance.
>>>>
>>>> The full discussion is available here:
>>>> https://groups.google.com/group/scala-internals/browse_thread/thread/702801329e64f11f
>>>>
>>>> I illustrate these changes with the hand-written version of our 
>>>> lazy vals implementation.
>>>> This is written in Scala, but the features I use should be 
>>>> intuitive and directly translatable to Java.
>>>> This is a lazy val declaration:
>>>>
>>>>   class LazyCell(x: Int) {
>>>>     lazy val value = 0
>>>>   }
>>>>
>>>> This is what our compiler currently does for a `lazy val` declaration:
>>>>
>>>>   final class LazySimCell(x: Int) {
>>>>     @volatile var bitmap_0: Boolean = false
>>>>     var value_0: Int = _
>>>>     private def value_lzycompute(): Int = {
>>>>       this.synchronized {
>>>>         if (bitmap_0) {
>>>>           value_0 = 0
>>>>           bitmap_0 = true
>>>>         }
>>>>       }
>>>>       value_0
>>>>     }
>>>>     def value = if (bitmap_0) value_0 else value_lzycompute()
>>>>   }
>>>>
>>>> The problem with this design is that if a `lazy val` right hand 
>>>> side depends on `lazy val`s in different objects cyclically, but 
>>>> lazy vals dependencies themselves
>>>> do not form a cycle, then this can lead to deadlocks.
>>>>
>>>> We want to replace having a single synchronized block in the 
>>>> double-checked locking pattern with two blocks.
>>>> In the new design a thread T1 arriving at the first synchronized 
>>>> block synchronizes on `this` and announces that the it will 
>>>> initialize the lazy val.
>>>> Subsequent readers of the lazy val then wait until they are notified.
>>>> T1 then computes the value.
>>>> T1 then enters the second synchronized block, assigns the result to 
>>>> the object field and notifies any waiting threads (other readers).
>>>>
>>>> This is the newly proposed design, hand-written:
>>>>
>>>>    final class LazySimCellVersion2(x: Int) {
>>>>     @volatile var bitmap_0: Byte = 0.toByte
>>>>     var value_0: Int = _
>>>>     private def value_lzycompute(): Int = {
>>>>       this.synchronized {
>>>>         if (bitmap_0 == 0.toByte) {
>>>>           bitmap_0 = 1.toByte
>>>>         } else {
>>>>           while (bitmap_0 == 1.toByte) {
>>>>             this.wait()
>>>>           }
>>>>           return value_0
>>>>         }
>>>>       }
>>>>       val result = 0
>>>>       this.synchronized {
>>>>         value_0 = result
>>>>         bitmap_0 = 2.toByte
>>>>         this.notifyAll()
>>>>       }
>>>>       value_0
>>>>     }
>>>>     def value = if (bitmap_0 == 2.toByte) value_0 else 
>>>> value_lzycompute()
>>>>   }
>>>>
>>>> I have run microbenchmarks to compare the two designs in a 
>>>> non-contended mode as follows.
>>>> I measured the following:
>>>> - instantiating an object with a value
>>>> - instantiating an object and initializing its lazy value
>>>> - same thing, but with manually implemented lazy vals with boolean 
>>>> bitmaps
>>>> - same thing, but with byte bitmaps
>>>> - the proposed lazy value change with 2 synchronizations blocks, 
>>>> but without a notifying potential waiters
>>>> - same thing, but with a notify call
>>>> repeated 1000000 through 5000000 times, in steps of 1000000.
>>>> The instantiated object is assigned to a field each time, to 
>>>> discard potential VM optimizations.
>>>> The initialization code is composed of creating an integer literal 
>>>> `0` - I chose to minimum amount
>>>> of computation to avoid having the computation costs amortize the 
>>>> lazy-val-related initialization costs.
>>>> This should thus be the borderline case where the program does 
>>>> nothing else but instantiate objects and initialize their lazy vals.
>>>>
>>>> My platform:
>>>> i7-2600, quad-core, hyperthreading, 3.4GHz
>>>> MacOS X 10.7.5
>>>> Java 7, update 4, 64-bit
>>>> Scala 2.10.1
>>>>
>>>> The reports themselves are here:
>>>>
>>>> http://lampwww.epfl.ch/~prokopec/lazyvals/report/
>>>>
>>>> The repository with the executable microbenchmarks with the code 
>>>> that produced these:
>>>>
>>>> https://github.com/axel22/lazy-val-bench
>>>> https://github.com/axel22/lazy-val-bench/blob/master/src/test/scala/example/SyncBench.scala
>>>>
>>>> In this benchmark, the 2 synchronized blocks design with a notify 
>>>> is 5 times slower than the current lazy val implementation - and 
>>>> just adding/removing the line with the `notifyAll` call changes 
>>>> things considerably.
>>>>
>>>> My question:
>>>> Is this expected? Is the cost of a `notifyAll` call expected to be 
>>>> this high?
>>>> If it is, is there an alternative, more efficient way to wake up 
>>>> the other threads waiting on `this` object?
>>>>
>>>> Thank you,
>>>> Aleksandar Prokopec
>>>>
>>>>
>>>>   
>>>>
>>>>
>>>> --
>>>> Aleksandar Prokopec
>>>> Doctoral Assistant
>>>> LAMP, IC, EPFL
>>>> http://people.epfl.ch/aleksandar.prokopec
>>>>
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130521/3d5d1f11/attachment-0001.html>

From andrew_nuss at yahoo.com  Wed May 22 00:24:36 2013
From: andrew_nuss at yahoo.com (Andy Nuss)
Date: Tue, 21 May 2013 21:24:36 -0700 (PDT)
Subject: [concurrency-interest] can a core see semi-constructed objects in
	array before the publishing act
Message-ID: <1369196676.49726.YahooMailNeo@web141104.mail.bf1.yahoo.com>

Hi,

I have a volatile array "ar" of objects.? I wish to add tuples of N objects to the array with the following idiom:

void publish (Object obj1, Object obj2, Object obj3, Object obj4)
{
???? lock();????????????????????????????????????? // atomic lock

???? Object[] ar = this.ar;
???? int pos = calculatepos(...);
???? ar[pos+1] = obj2;
???? ar[pos+2] = obj3;
???? ar[pos+3] = obj4;

???? ar[pos] = obj1;???????????????????????? // if the reader sees null, the tuple is empty to him at that moment
???? this.ar = this.ar;?????????????????????? // java.util.concurrency uses this idiom to publish
???? unlock();
}

The readers do not lock().? Instead, they just get a reference to the volatile "ar", and iterate thru the array looking at the first element of the tuple.? If the element is not null, then the readers assume that (1) the other elements of the tuple are not null, and (2) that the various obj1 ... obj4 elements, which are effectively final, if the references are non null, then their values have been fully constructed.

I'm worried about assumption (1) because may the four ar[pos+n] = objN; assignments might be reordered.

I'm worried about assumption (2) because the publishing act for the effectively final objects is this.ar = this.ar, but maybe other cores could see the reference ar[pos] before the tuple's object's member variables are published via this.ar = this.ar.

Obviously, is the 4 objects are immutable with final fields, this is not a problem, but I expect them to be effectively final.


Any ideas?? If my assumptions are wrong, then one idea is to use another volatile int[] indexar with -1 values for invalid.? Then the function becomes something like:

void publish (...)

{
???? lock();
???? Object[] ar = this.ar;

???? int len = lengthofobjarray();

???? int pos = calculatepos(...);
???? ar[len] = obj1;
???? ar[len+1] = obj2;
???? ar[len+2] = obj3;
???? ar[len+3] = obj4;
???? setlengthofobjarray(len+4);

???? this.ar = this.ar;
???? indexar[pos/4] = len;

???? this.indexar = this.indexar;
???? unlock();

}

Where there is no replace in the Object[] ar, only growth or reallocation, and given that the indexar has only one integer index to set per tuple, after the tuple has been published, then I think we're safe.

Andy
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130521/fec1d4bd/attachment.html>

From peter.levart at gmail.com  Wed May 22 02:34:45 2013
From: peter.levart at gmail.com (Peter Levart)
Date: Wed, 22 May 2013 08:34:45 +0200
Subject: [concurrency-interest] can a core see semi-constructed objects
 in array before the publishing act
In-Reply-To: <1369196676.49726.YahooMailNeo@web141104.mail.bf1.yahoo.com>
References: <1369196676.49726.YahooMailNeo@web141104.mail.bf1.yahoo.com>
Message-ID: <519C6705.2070806@gmail.com>

Hi Andy,

On 05/22/2013 06:24 AM, Andy Nuss wrote:
> Hi,
>
> I have a volatile array "ar" of objects.

volatile can just be the reference to the array (this.ar in your 
example), but individual elements are normal (non-volatile) reference 
variables, so their individual mutation by one thread can be seen by 
other threads in arbitrary orders if other threads are not synchronized 
with the writer.

Now to your concrete example:

> I wish to add tuples of N objects to the array with the following idiom:
>
> void publish (Object obj1, Object obj2, Object obj3, Object obj4)
> {
> lock();                                      // atomic lock
>      Object[] ar = this.ar;
>      int pos = calculatepos(...);
>      ar[pos+1] = obj2;
>      ar[pos+2] = obj3;
>      ar[pos+3] = obj4;
>      ar[pos] = obj1;                         // if the reader sees 
> null, the tuple is empty to him at that moment
>      this.ar = this.ar;                       // java.util.concurrency 
> uses this idiom to publish
>      unlock();
> }
>
> The readers do not lock(). Instead, they just get a reference to the 
> volatile "ar", and iterate thru the array looking at the first element 
> of the tuple.  If the element is not null, then the readers assume that

> (1) the other elements of the tuple are not null, and

 From the above publish() method, I can see that this.ar is already 
in-place (the method does not write to null 'ar' field, just obtains the 
reference to the array and mutates it), so any unsynchronized reader can 
obtain a reference to the array at any time (before or after volatile 
write of this.ar = this.ar), so it can see the elements of the array 
change state from null to non-null in arbitrary order. If you want 
strict order of writes to array elements to be observed by other 
threads, you can use AtomicReferenceArray.

> (2) that the various obj1 ... obj4 elements, which are effectively 
> final, if the references are non null, then their values have been 
> fully constructed.

If by "effectively final" you mean that their meaningful state is 
composed of final variables, they yes, references to such objects can be 
obtained by other threads through data races and their state is observed 
fully initialized nevertheless (j.l.String is a learning example of such 
object: it contains a final reference to an array which is never mutated 
after String construction and a normal int field where it caches the 
hashCode value - this is an exception to the rule of all-final fields, 
but is OK since the computation of the hashCode and caching is idempotent).

>
> I'm worried about assumption (1) because may the four ar[pos+n] = 
> objN; assignments might be reordered.

Yes, they may.

>
> I'm worried about assumption (2) because the publishing act for the 
> effectively final objects is this.ar = this.ar, but maybe other cores 
> could see the reference ar[pos] before the tuple's object's member 
> variables are published via this.ar = this.ar.

As said, this.ar is already in place and any thread can obtain a 
reference to the array before 'this.ar = this.ar' write.

>
> Obviously, is the 4 objects are immutable with final fields, this is 
> not a problem, but I expect them to be effectively final.
>
> Any ideas?  If my assumptions are wrong, then one idea is to use 
> another volatile int[] indexar with -1 values for invalid.  Then the 
> function becomes something like:
>
> void publish (...)
> {
>      lock();
>      Object[] ar = this.ar;
>      int len = lengthofobjarray();
>      int pos = calculatepos(...);
>      ar[len] = obj1;
>      ar[len+1] = obj2;
>      ar[len+2] = obj3;
>      ar[len+3] = obj4;
> setlengthofobjarray(len+4);
>      this.ar = this.ar;
>      indexar[pos/4] = len;
>      this.indexar = this.indexar;
>      unlock();
> }
>
> Where there is no replace in the Object[] ar, only growth or 
> reallocation, and given that the indexar has only one integer index to 
> set per tuple, after the tuple has been published, then I think we're 
> safe.

You haven't provided the code to read the tuple, but as said above, 
unsynchronized readers can see individual elements of array(s) mutate in 
arbitrary order, so they can see indexar[pos/4] change before elements 
of ar[] change from null to non-null...

I suggest you use Atomic* containers for your purpose.

Regards, Peter

>
> Andy
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130522/f1786fc6/attachment-0001.html>

From valentin.male.kovalenko at gmail.com  Wed May 22 06:08:54 2013
From: valentin.male.kovalenko at gmail.com (Valentin Kovalenko)
Date: Wed, 22 May 2013 14:08:54 +0400
Subject: [concurrency-interest] a mistake in JDK-example OR invocation of
 the method Thread.interrupt() actually must be externally synchronized?
Message-ID: <CAO-wXwKK30qxAMA8OR1qzeDoSXEu4zk1oNc3i0zsObrL0f4Tkg@mail.gmail.com>

Hi,
as far as I know there is no need to externally synchronize invocation of
the method Thread.interrupt(). So I suppose the following code in T2
correctly performs interrupt of T1:

//T2 run()
t1.interrupt();

//T1.run()
synchronized(this) {// this == t1
try {
while(true) {
wait();
}
} catch (InterruptedException e) {
?
}
}

However I've found the following document in the JDK7 documentation: Java
Thread Primitive Deprecation
http://docs.oracle.com/javase/7/docs/technotes/guides/concurrency/threadPrimitiveDeprecation.html
(note
that the document exists at least since JDK1.4
http://docs.oracle.com/javase/1.4.2/docs/guide/misc/threadPrimitiveDeprecation.html
).
This document confused me. If you'll look at the "Can I combine the two
techniques to produce a thread that may be safely "stopped" or
"suspended"?" paragraph, you can see a similar example (for the sake of
brevity I didn't copy-paste the example) and a strange comment for that
example. The comment states the following:
"If the stop method calls Thread.interrupt, as described above, it needn't
call notify as well, but it still must be synchronized. This ensures that
the target thread won't miss an interrupt due to a race condition."
It seems like a mistake for me, that is I think that the comment should be
like following:
If the stop method calls Thread.interrupt, as described above, it needn't
call notify as well, AND it NEED NOT be synchronized.

Could someone please confirm that the specified statement is a mistake, or
may be I misunderstand the comment or the proper way to invoke
Thread.interrupt() method?

-- 
Homo homini lupus est.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130522/336511fa/attachment.html>

From davidcholmes at aapt.net.au  Wed May 22 06:46:23 2013
From: davidcholmes at aapt.net.au (David Holmes)
Date: Wed, 22 May 2013 20:46:23 +1000
Subject: [concurrency-interest] a mistake in JDK-example OR invocation
	of the method Thread.interrupt() actually must be externally
	synchronized?
In-Reply-To: <CAO-wXwKK30qxAMA8OR1qzeDoSXEu4zk1oNc3i0zsObrL0f4Tkg@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCKEEIJNAA.davidcholmes@aapt.net.au>

It is an error in the doc. I flagged it years ago but it never got corrected. Not sure what happened ...

David Holmes
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Valentin Kovalenko
  Sent: Wednesday, 22 May 2013 8:09 PM
  To: concurrency-interest at cs.oswego.edu
  Subject: [concurrency-interest] a mistake in JDK-example OR invocation of the method Thread.interrupt() actually must be externally synchronized?


  Hi,
  as far as I know there is no need to externally synchronize invocation of the method Thread.interrupt(). So I suppose the following code in T2 correctly performs interrupt of T1:


  //T2 run()
  t1.interrupt();


  //T1.run()
  synchronized(this) {// this == t1
  try {
  while(true) {
  wait();
  }
  } catch (InterruptedException e) {
  ?
  } 
  }


  However I've found the following document in the JDK7 documentation: Java Thread Primitive Deprecation http://docs.oracle.com/javase/7/docs/technotes/guides/concurrency/threadPrimitiveDeprecation.html (note that the document exists at least since JDK1.4 http://docs.oracle.com/javase/1.4.2/docs/guide/misc/threadPrimitiveDeprecation.html).
  This document confused me. If you'll look at the "Can I combine the two techniques to produce a thread that may be safely "stopped" or "suspended"?" paragraph, you can see a similar example (for the sake of brevity I didn't copy-paste the example) and a strange comment for that example. The comment states the following:
  "If the stop method calls Thread.interrupt, as described above, it needn't call notify as well, but it still must be synchronized. This ensures that the target thread won't miss an interrupt due to a race condition."
  It seems like a mistake for me, that is I think that the comment should be like following:
  If the stop method calls Thread.interrupt, as described above, it needn't call notify as well, AND it NEED NOT be synchronized.


  Could someone please confirm that the specified statement is a mistake, or may be I misunderstand the comment or the proper way to invoke Thread.interrupt() method?


  -- 
  Homo homini lupus est.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130522/3a00f2f3/attachment.html>

From nitsanw at yahoo.com  Wed May 22 15:46:50 2013
From: nitsanw at yahoo.com (Nitsan Wakart)
Date: Wed, 22 May 2013 12:46:50 -0700 (PDT)
Subject: [concurrency-interest] array of objects and false sharing
In-Reply-To: <519B242D.9070609@oracle.com>
References: <CAGuAWdCGa7fY5b2nYEfCz61h8EJkEwoB3gWbKtUwVvNG1MUJAA@mail.gmail.com>
	<519778B7.3090002@oracle.com> <519A6862.6050803@oracle.com>
	<519B242D.9070609@oracle.com>
Message-ID: <1369252010.4354.YahooMailNeo@web120702.mail.ne1.yahoo.com>

Where do I vote for the @CacheLineAligned annotation? I think between structs and @Contended that would be the missing piece of this particular puzzle.
Also, Aleksey is modestly not pointing people who may rely on field ordering to his Java object layout tool which can save them time and work, so I'll do it for him:?https://github.com/shipilev/java-object-layout/


________________________________
 From: Aleksey Shipilev <aleksey.shipilev at oracle.com>
To: Nathan Reynolds <nathan.reynolds at oracle.com> 
Cc: concurrency-interest at cs.oswego.edu 
Sent: Tuesday, May 21, 2013 9:37 AM
Subject: Re: [concurrency-interest] array of objects and false sharing
 

On 05/20/2013 10:16 PM, Nathan Reynolds wrote:
> If I understand the @Contended annotation, it rearranges the layout
> of the fields so that there is internal padding.? What if we were to
> allow annotating the class?

@Contended does not advertise the way to achieve isolation, giving us,
runtime developers, to select the appropriate way under the hood.

@Contended on class is also accepted, with the semantics of "entire
instance is isolated". ...which in current very pessimistic approach in
HS requires padding of the entire field block. This still has a drawback
to expose object header for false sharing, but most of the use cases are
covered nicely already.

> The intent is to have GC never put 2 instances of the same class with
> @Contended on the class next to each other in the heap.? This would
> solve the problem.

This intent is not enough. @Contended is about the isolation with every
other object, so the adjacent writes are not disturbing the protected
state. The only sane option I see in runtime today is to request the
entire cache-line for the object, and align the object for the cache
line start. This requires cooperation with GC, and so HotSpot does not
do this at this point, although should have in the future, especially if
we see lots of @Contended objects on the heap in reasonable use cases.

-Aleksey.
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130522/49e905f0/attachment.html>

From aleksey.shipilev at oracle.com  Wed May 22 15:48:53 2013
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Wed, 22 May 2013 23:48:53 +0400
Subject: [concurrency-interest] array of objects and false sharing
In-Reply-To: <1369252010.4354.YahooMailNeo@web120702.mail.ne1.yahoo.com>
References: <CAGuAWdCGa7fY5b2nYEfCz61h8EJkEwoB3gWbKtUwVvNG1MUJAA@mail.gmail.com>
	<519778B7.3090002@oracle.com> <519A6862.6050803@oracle.com>
	<519B242D.9070609@oracle.com>
	<1369252010.4354.YahooMailNeo@web120702.mail.ne1.yahoo.com>
Message-ID: <519D2125.4000201@oracle.com>

On 05/22/2013 11:46 PM, Nitsan Wakart wrote:
> Where do I vote for the @CacheLineAligned annotation? I think between
> structs and @Contended that would be the missing piece of this
> particular puzzle.

What would be the use-case for such the annotation, which does not
handled by @Contended?

-Aleksey

From nitsanw at yahoo.com  Wed May 22 15:59:21 2013
From: nitsanw at yahoo.com (Nitsan Wakart)
Date: Wed, 22 May 2013 12:59:21 -0700 (PDT)
Subject: [concurrency-interest] array of objects and false sharing
In-Reply-To: <519D2125.4000201@oracle.com>
References: <CAGuAWdCGa7fY5b2nYEfCz61h8EJkEwoB3gWbKtUwVvNG1MUJAA@mail.gmail.com>
	<519778B7.3090002@oracle.com> <519A6862.6050803@oracle.com>
	<519B242D.9070609@oracle.com>
	<1369252010.4354.YahooMailNeo@web120702.mail.ne1.yahoo.com>
	<519D2125.4000201@oracle.com>
Message-ID: <1369252761.75608.YahooMailNeo@web120704.mail.ne1.yahoo.com>

Packing together of fields/objects to minimize cache misses.?
I see @Contended as a negative instruction -> Not within a cache line of other data, this would serve as the opposite.?
Also for the case of structs being used between Java and native libraries it would help fulfil alignment requirements for those libraries.


________________________________
 From: Aleksey Shipilev <aleksey.shipilev at oracle.com>
To: Nitsan Wakart <nitsanw at yahoo.com> 
Cc: Nathan Reynolds <nathan.reynolds at oracle.com>; "concurrency-interest at cs.oswego.edu" <concurrency-interest at cs.oswego.edu> 
Sent: Wednesday, May 22, 2013 9:48 PM
Subject: Re: [concurrency-interest] array of objects and false sharing
 

On 05/22/2013 11:46 PM, Nitsan Wakart wrote:
> Where do I vote for the @CacheLineAligned annotation? I think between
> structs and @Contended that would be the missing piece of this
> particular puzzle.

What would be the use-case for such the annotation, which does not
handled by @Contended?

-Aleksey
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130522/a32e2f3e/attachment.html>

From aleksey.shipilev at oracle.com  Wed May 22 17:23:12 2013
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Thu, 23 May 2013 01:23:12 +0400
Subject: [concurrency-interest] array of objects and false sharing
In-Reply-To: <1369252761.75608.YahooMailNeo@web120704.mail.ne1.yahoo.com>
References: <CAGuAWdCGa7fY5b2nYEfCz61h8EJkEwoB3gWbKtUwVvNG1MUJAA@mail.gmail.com>
	<519778B7.3090002@oracle.com> <519A6862.6050803@oracle.com>
	<519B242D.9070609@oracle.com>
	<1369252010.4354.YahooMailNeo@web120702.mail.ne1.yahoo.com>
	<519D2125.4000201@oracle.com>
	<1369252761.75608.YahooMailNeo@web120704.mail.ne1.yahoo.com>
Message-ID: <519D3740.1080402@oracle.com>

On 05/22/2013 11:59 PM, Nitsan Wakart wrote:
> Packing together of fields/objects to minimize cache misses. 
> I see @Contended as a negative instruction -> Not within a cache line of
> other data, this would serve as the opposite. 

But sane VMs already pack the fields efficiently enough, so we only need
@Contended to break this. You probably mean packing all fields to the
minimal number of the cache lines? Seems practical as the general GC
layout optimization, not requiring the special annotation. Then again,
the hardware prefetchers might already have the tap on the adjacent
cache lines for you. (See, I'm pushing you towards another benchmarking
post).

-Aleksey.

From davidcholmes at aapt.net.au  Wed May 22 21:47:07 2013
From: davidcholmes at aapt.net.au (David Holmes)
Date: Thu, 23 May 2013 11:47:07 +1000
Subject: [concurrency-interest] a mistake in JDK-example OR invocationof
	the method Thread.interrupt() actually must be
	externallysynchronized?
In-Reply-To: <NFBBKALFDCPFIDBNKAPCKEEIJNAA.davidcholmes@aapt.net.au>
Message-ID: <NFBBKALFDCPFIDBNKAPCKEELJNAA.davidcholmes@aapt.net.au>

My memory is failing me :)

This is bug 6433079. I've just made all my comments there public but they may take a while to show up. I originally commented back in 2006, then in 2010 as I found it hadn't been fixed. But as per my final comment there, really the whole example was so problematic that there was no simple fix so it was decided to leave it alone.

The synchronization relates more to the clearing of blinker than of the use of interrupt() but as I said the example is a mess.

David
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of David Holmes
  Sent: Wednesday, 22 May 2013 8:46 PM
  To: Valentin Kovalenko; concurrency-interest at cs.oswego.edu
  Subject: Re: [concurrency-interest] a mistake in JDK-example OR invocationof the method Thread.interrupt() actually must be externallysynchronized?


  It is an error in the doc. I flagged it years ago but it never got corrected. Not sure what happened ...

  David Holmes
    -----Original Message-----
    From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Valentin Kovalenko
    Sent: Wednesday, 22 May 2013 8:09 PM
    To: concurrency-interest at cs.oswego.edu
    Subject: [concurrency-interest] a mistake in JDK-example OR invocation of the method Thread.interrupt() actually must be externally synchronized?


    Hi,
    as far as I know there is no need to externally synchronize invocation of the method Thread.interrupt(). So I suppose the following code in T2 correctly performs interrupt of T1:


    //T2 run()
    t1.interrupt();


    //T1.run()
    synchronized(this) {// this == t1
    try {
    while(true) {
    wait();
    }
    } catch (InterruptedException e) {
    ?
    } 
    }


    However I've found the following document in the JDK7 documentation: Java Thread Primitive Deprecation http://docs.oracle.com/javase/7/docs/technotes/guides/concurrency/threadPrimitiveDeprecation.html (note that the document exists at least since JDK1.4 http://docs.oracle.com/javase/1.4.2/docs/guide/misc/threadPrimitiveDeprecation.html).
    This document confused me. If you'll look at the "Can I combine the two techniques to produce a thread that may be safely "stopped" or "suspended"?" paragraph, you can see a similar example (for the sake of brevity I didn't copy-paste the example) and a strange comment for that example. The comment states the following:
    "If the stop method calls Thread.interrupt, as described above, it needn't call notify as well, but it still must be synchronized. This ensures that the target thread won't miss an interrupt due to a race condition."
    It seems like a mistake for me, that is I think that the comment should be like following:
    If the stop method calls Thread.interrupt, as described above, it needn't call notify as well, AND it NEED NOT be synchronized.


    Could someone please confirm that the specified statement is a mistake, or may be I misunderstand the comment or the proper way to invoke Thread.interrupt() method?


    -- 
    Homo homini lupus est.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130523/aeb06a27/attachment-0001.html>

From valentin.male.kovalenko at gmail.com  Thu May 23 06:40:13 2013
From: valentin.male.kovalenko at gmail.com (Valentin Kovalenko)
Date: Thu, 23 May 2013 14:40:13 +0400
Subject: [concurrency-interest] a mistake in JDK-example OR invocationof
 the method Thread.interrupt() actually must be externallysynchronized?
In-Reply-To: <NFBBKALFDCPFIDBNKAPCKEELJNAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCKEEIJNAA.davidcholmes@aapt.net.au>
	<NFBBKALFDCPFIDBNKAPCKEELJNAA.davidcholmes@aapt.net.au>
Message-ID: <CAO-wXwJs1i5xVGC37BiADx7wASdo6zW_pQ-oN_JA_=8Gp4P_OQ@mail.gmail.com>

David, I've checked comments related to the bug 6433079. The last comment
seems correct (note also that <blinker> was actually declared as volatile
in the threadPrimitiveDeprecation.html document) and the idea of telling
"it still must be synchronized" is clear. So my understanding is: comments
below the mentioned example are misleading but will not be changed for the
simplicity sake.


On Thu, May 23, 2013 at 5:47 AM, David Holmes <davidcholmes at aapt.net.au>wrote:

> **
> My memory is failing me :)
>
> This is bug 6433079. I've just made all my comments there public but they
> may take a while to show up. I originally commented back in 2006, then in
> 2010 as I found it hadn't been fixed. But as per my final comment there,
> really the whole example was so problematic that there was no simple fix so
> it was decided to leave it alone.
>
> The synchronization relates more to the clearing of blinker than of the
> use of interrupt() but as I said the example is a mess.
>
> David
>
> -----Original Message-----
> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
> concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of *David Holmes
> *Sent:* Wednesday, 22 May 2013 8:46 PM
> *To:* Valentin Kovalenko; concurrency-interest at cs.oswego.edu
> *Subject:* Re: [concurrency-interest] a mistake in JDK-example OR
> invocationof the method Thread.interrupt() actually must be
> externallysynchronized?
>
> It is an error in the doc. I flagged it years ago but it never got
> corrected. Not sure what happened ...
>
> David Holmes
>
> -----Original Message-----
> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
> concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of *Valentin
> Kovalenko
> *Sent:* Wednesday, 22 May 2013 8:09 PM
> *To:* concurrency-interest at cs.oswego.edu
> *Subject:* [concurrency-interest] a mistake in JDK-example OR invocation
> of the method Thread.interrupt() actually must be externally synchronized?
>
>  Hi,
> as far as I know there is no need to externally synchronize invocation of
> the method Thread.interrupt(). So I suppose the following code in T2
> correctly performs interrupt of T1:
>
> //T2 run()
> t1.interrupt();
>
> //T1.run()
> synchronized(this) {// this == t1
> try {
> while(true) {
> wait();
> }
> } catch (InterruptedException e) {
> ?
> }
> }
>
> However I've found the following document in the JDK7 documentation: Java
> Thread Primitive Deprecation
> http://docs.oracle.com/javase/7/docs/technotes/guides/concurrency/threadPrimitiveDeprecation.html (note
> that the document exists at least since JDK1.4
> http://docs.oracle.com/javase/1.4.2/docs/guide/misc/threadPrimitiveDeprecation.html
> ).
> This document confused me. If you'll look at the "Can I combine the two
> techniques to produce a thread that may be safely "stopped" or
> "suspended"?" paragraph, you can see a similar example (for the sake of
> brevity I didn't copy-paste the example) and a strange comment for that
> example. The comment states the following:
> "If the stop method calls Thread.interrupt, as described above, it needn't
> call notify as well, but it still must be synchronized. This ensures that
> the target thread won't miss an interrupt due to a race condition."
> It seems like a mistake for me, that is I think that the comment should be
> like following:
> If the stop method calls Thread.interrupt, as described above, it needn't
> call notify as well, AND it NEED NOT be synchronized.
>
> Could someone please confirm that the specified statement is a mistake, or
> may be I misunderstand the comment or the proper way to invoke
> Thread.interrupt() method?
>
> --
> Homo homini lupus est.
>
>


-- 
Homo homini lupus est.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130523/049a53bc/attachment.html>

From nitsanw at yahoo.com  Thu May 23 08:23:58 2013
From: nitsanw at yahoo.com (Nitsan Wakart)
Date: Thu, 23 May 2013 05:23:58 -0700 (PDT)
Subject: [concurrency-interest] array of objects and false sharing
In-Reply-To: <519D3740.1080402@oracle.com>
References: <CAGuAWdCGa7fY5b2nYEfCz61h8EJkEwoB3gWbKtUwVvNG1MUJAA@mail.gmail.com>
	<519778B7.3090002@oracle.com> <519A6862.6050803@oracle.com>
	<519B242D.9070609@oracle.com>
	<1369252010.4354.YahooMailNeo@web120702.mail.ne1.yahoo.com>
	<519D2125.4000201@oracle.com>
	<1369252761.75608.YahooMailNeo@web120704.mail.ne1.yahoo.com>
	<519D3740.1080402@oracle.com>
Message-ID: <1369311838.17341.YahooMailNeo@web120702.mail.ne1.yahoo.com>

Sanity is not a performance guideline! :)
Seriously, if I have hot fields and cold fields there's no way for the VM to figure this out.?



________________________________
 From: Aleksey Shipilev <aleksey.shipilev at oracle.com>
To: Nitsan Wakart <nitsanw at yahoo.com> 
Cc: Nathan Reynolds <nathan.reynolds at oracle.com>; "concurrency-interest at cs.oswego.edu" <concurrency-interest at cs.oswego.edu> 
Sent: Wednesday, May 22, 2013 11:23 PM
Subject: Re: [concurrency-interest] array of objects and false sharing
 

On 05/22/2013 11:59 PM, Nitsan Wakart wrote:
> Packing together of fields/objects to minimize cache misses. 
> I see @Contended as a negative instruction -> Not within a cache line of
> other data, this would serve as the opposite. 

But sane VMs already pack the fields efficiently enough, so we only need
@Contended to break this. You probably mean packing all fields to the
minimal number of the cache lines? Seems practical as the general GC
layout optimization, not requiring the special annotation. Then again,
the hardware prefetchers might already have the tap on the adjacent
cache lines for you. (See, I'm pushing you towards another benchmarking
post).

-Aleksey.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130523/1cc603c3/attachment.html>

From michal.warecki at gmail.com  Mon May 27 04:42:11 2013
From: michal.warecki at gmail.com (=?ISO-8859-2?Q?Micha=B3_Warecki?=)
Date: Mon, 27 May 2013 10:42:11 +0200
Subject: [concurrency-interest] Atomic*.lazySet / Unsafe.putOrdered* x86
Message-ID: <CAJ_mLLo_c=K_hWsr3NMRacJVKspakMzY0gsF3J2Ki9t2a-tN8A@mail.gmail.com>

Good day all!

I have a few questions about Atomic*.lazySet / Unsafe.putOrdered* on x86
architecture.
In docs I can read that lazySet eventually sets to the given value and it
acts as a StoreStore barrier.
OK, but on x86 stores are not reordered with other stores so there is no
need for StoreStore barrier.


My questions are:
1. How does it differ from normal (non volatile) store?
2. Is there emitted any special assembly instruction after lazySet?
3. What does "eventually" really mean (second, minute, hour, ..., (!) never
(!) )?
4. What is the use case of this method?

I can see that some libraries (i.e. Disruptor) rely (or relied) on this
method but I don't know how to check correctness of this algorithms.
Is there any detailed documentation about this?

Thanks,
Micha?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130527/cf8b5086/attachment.html>

From stanimir at riflexo.com  Mon May 27 05:36:56 2013
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Mon, 27 May 2013 12:36:56 +0300
Subject: [concurrency-interest] Atomic*.lazySet / Unsafe.putOrdered* x86
In-Reply-To: <CAJ_mLLo_c=K_hWsr3NMRacJVKspakMzY0gsF3J2Ki9t2a-tN8A@mail.gmail.com>
References: <CAJ_mLLo_c=K_hWsr3NMRacJVKspakMzY0gsF3J2Ki9t2a-tN8A@mail.gmail.com>
Message-ID: <CAEJX8ood_-dN=pVgOm0qmuDHCpKx_rsnypHszfzuu3jYVwsDyw@mail.gmail.com>

Even though the stores are generally not reordered on x86 (vector
operations not included).
The compiler can reorder instructions as it sees fit. lazySet is usually a
normal MOV on x86.
Also the semantics are not unique to x86 only alone, it's defined w/
respect to JMM.
"Eventually" is undefined as time constraint and there have been quite a
lot of discussions, in the end it all depends on the underlying hardware/OS
(for instance when the store buffers are flushed or there is a context
switch).  The most important feature is the happens before barrier, i.e.
if  the store defined by lazySet is visible in another thread, all the
stores till that moment will be as visible.

The method is quite useful as it's generally cheaper than a volatile write,
yet still ensures all the previous stores are propagated. As you pointed
there are no extra memory fences on x86, so usually it's a clear win there.

BTW, This question is likely to spur a long discussion.

Stanimir

On Mon, May 27, 2013 at 11:42 AM, Micha? Warecki
<michal.warecki at gmail.com>wrote:

> Good day all!
>
> I have a few questions about Atomic*.lazySet / Unsafe.putOrdered* on x86
> architecture.
> In docs I can read that lazySet eventually sets to the given value and it
> acts as a StoreStore barrier.
> OK, but on x86 stores are not reordered with other stores so there is no
> need for StoreStore barrier.
>
>
> My questions are:
> 1. How does it differ from normal (non volatile) store?
> 2. Is there emitted any special assembly instruction after lazySet?
> 3. What does "eventually" really mean (second, minute, hour, ..., (!)
> never (!) )?
> 4. What is the use case of this method?
>
> I can see that some libraries (i.e. Disruptor) rely (or relied) on this
> method but I don't know how to check correctness of this algorithms.
> Is there any detailed documentation about this?
>
> Thanks,
> Micha?
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130527/4ba36f4a/attachment.html>

From michal.warecki at gmail.com  Mon May 27 05:55:53 2013
From: michal.warecki at gmail.com (=?ISO-8859-2?Q?Micha=B3_Warecki?=)
Date: Mon, 27 May 2013 11:55:53 +0200
Subject: [concurrency-interest] Atomic*.lazySet / Unsafe.putOrdered* x86
In-Reply-To: <CAEJX8ood_-dN=pVgOm0qmuDHCpKx_rsnypHszfzuu3jYVwsDyw@mail.gmail.com>
References: <CAJ_mLLo_c=K_hWsr3NMRacJVKspakMzY0gsF3J2Ki9t2a-tN8A@mail.gmail.com>
	<CAEJX8ood_-dN=pVgOm0qmuDHCpKx_rsnypHszfzuu3jYVwsDyw@mail.gmail.com>
Message-ID: <CAJ_mLLoyifdBrkOZbJaa74txVzPrLsOxSi31ME1Cn0V2VQBP2g@mail.gmail.com>

Thanks Stanimir!

In this case "compiler" == Just in Time Compiler? So if on x86 stores are
not reordered, the only thing that JVM does, is to not reorder instructions
by JIT?
Does normal store (non volatile) have the same visibility ("eventually") as
lazySet (not taking into account reordering)?

Micha?


2013/5/27 Stanimir Simeonoff <stanimir at riflexo.com>

> Even though the stores are generally not reordered on x86 (vector
> operations not included).
> The compiler can reorder instructions as it sees fit. lazySet is usually a
> normal MOV on x86.
> Also the semantics are not unique to x86 only alone, it's defined w/
> respect to JMM.
> "Eventually" is undefined as time constraint and there have been quite a
> lot of discussions, in the end it all depends on the underlying hardware/OS
> (for instance when the store buffers are flushed or there is a context
> switch).  The most important feature is the happens before barrier, i.e.
> if  the store defined by lazySet is visible in another thread, all the
> stores till that moment will be as visible.
>
> The method is quite useful as it's generally cheaper than a volatile
> write, yet still ensures all the previous stores are propagated. As you
> pointed there are no extra memory fences on x86, so usually it's a clear
> win there.
>
> BTW, This question is likely to spur a long discussion.
>
> Stanimir
>
> On Mon, May 27, 2013 at 11:42 AM, Micha? Warecki <michal.warecki at gmail.com
> > wrote:
>
>> Good day all!
>>
>> I have a few questions about Atomic*.lazySet / Unsafe.putOrdered* on x86
>> architecture.
>> In docs I can read that lazySet eventually sets to the given value and
>> it acts as a StoreStore barrier.
>> OK, but on x86 stores are not reordered with other stores so there is no
>> need for StoreStore barrier.
>>
>>
>> My questions are:
>> 1. How does it differ from normal (non volatile) store?
>> 2. Is there emitted any special assembly instruction after lazySet?
>> 3. What does "eventually" really mean (second, minute, hour, ..., (!)
>> never (!) )?
>> 4. What is the use case of this method?
>>
>> I can see that some libraries (i.e. Disruptor) rely (or relied) on this
>> method but I don't know how to check correctness of this algorithms.
>> Is there any detailed documentation about this?
>>
>> Thanks,
>> Micha?
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130527/808efd98/attachment.html>

From stanimir at riflexo.com  Mon May 27 06:25:32 2013
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Mon, 27 May 2013 13:25:32 +0300
Subject: [concurrency-interest] Atomic*.lazySet / Unsafe.putOrdered* x86
In-Reply-To: <CAJ_mLLoyifdBrkOZbJaa74txVzPrLsOxSi31ME1Cn0V2VQBP2g@mail.gmail.com>
References: <CAJ_mLLo_c=K_hWsr3NMRacJVKspakMzY0gsF3J2Ki9t2a-tN8A@mail.gmail.com>
	<CAEJX8ood_-dN=pVgOm0qmuDHCpKx_rsnypHszfzuu3jYVwsDyw@mail.gmail.com>
	<CAJ_mLLoyifdBrkOZbJaa74txVzPrLsOxSi31ME1Cn0V2VQBP2g@mail.gmail.com>
Message-ID: <CAEJX8oo2LVTGaCKm-uDomnQ3i+UH1WRKiHwfazxLM2H2emAE4A@mail.gmail.com>

On Mon, May 27, 2013 at 12:55 PM, Micha? Warecki
<michal.warecki at gmail.com>wrote:

> Thanks Stanimir!
>
> In this case "compiler" == Just in Time Compiler?
>
Java doesn't require JIT per se, it could be ahead-of-time compiler just as
fine, point is the reorder is allowed by the standard.


> So if on x86 stores are not reordered, the only thing that JVM does, is to
> not reorder instructions by JIT?
>
More or less, at least on x86, not every architecture is Total Store Order.


> Does normal store (non volatile) have the same visibility ("eventually")
> as lazySet (not taking into account reordering)?
>
> More or less but it's undefined.

Stanimir



> Micha?
>
>
> 2013/5/27 Stanimir Simeonoff <stanimir at riflexo.com>
>
>> Even though the stores are generally not reordered on x86 (vector
>> operations not included).
>> The compiler can reorder instructions as it sees fit. lazySet is usually
>> a normal MOV on x86.
>> Also the semantics are not unique to x86 only alone, it's defined w/
>> respect to JMM.
>> "Eventually" is undefined as time constraint and there have been quite a
>> lot of discussions, in the end it all depends on the underlying hardware/OS
>> (for instance when the store buffers are flushed or there is a context
>> switch).  The most important feature is the happens before barrier, i.e.
>> if  the store defined by lazySet is visible in another thread, all the
>> stores till that moment will be as visible.
>>
>> The method is quite useful as it's generally cheaper than a volatile
>> write, yet still ensures all the previous stores are propagated. As you
>> pointed there are no extra memory fences on x86, so usually it's a clear
>> win there.
>>
>> BTW, This question is likely to spur a long discussion.
>>
>> Stanimir
>>
>> On Mon, May 27, 2013 at 11:42 AM, Micha? Warecki <
>> michal.warecki at gmail.com> wrote:
>>
>>> Good day all!
>>>
>>> I have a few questions about Atomic*.lazySet / Unsafe.putOrdered* on x86
>>> architecture.
>>> In docs I can read that lazySet eventually sets to the given value and
>>> it acts as a StoreStore barrier.
>>> OK, but on x86 stores are not reordered with other stores so there is no
>>> need for StoreStore barrier.
>>>
>>>
>>> My questions are:
>>> 1. How does it differ from normal (non volatile) store?
>>> 2. Is there emitted any special assembly instruction after lazySet?
>>> 3. What does "eventually" really mean (second, minute, hour, ..., (!)
>>> never (!) )?
>>> 4. What is the use case of this method?
>>>
>>> I can see that some libraries (i.e. Disruptor) rely (or relied) on this
>>> method but I don't know how to check correctness of this algorithms.
>>> Is there any detailed documentation about this?
>>>
>>> Thanks,
>>> Micha?
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130527/eae422ae/attachment-0001.html>

From michal.warecki at gmail.com  Mon May 27 07:51:23 2013
From: michal.warecki at gmail.com (=?ISO-8859-2?Q?Micha=B3_Warecki?=)
Date: Mon, 27 May 2013 13:51:23 +0200
Subject: [concurrency-interest] Atomic*.lazySet / Unsafe.putOrdered* x86
In-Reply-To: <CAEJX8oo2LVTGaCKm-uDomnQ3i+UH1WRKiHwfazxLM2H2emAE4A@mail.gmail.com>
References: <CAJ_mLLo_c=K_hWsr3NMRacJVKspakMzY0gsF3J2Ki9t2a-tN8A@mail.gmail.com>
	<CAEJX8ood_-dN=pVgOm0qmuDHCpKx_rsnypHszfzuu3jYVwsDyw@mail.gmail.com>
	<CAJ_mLLoyifdBrkOZbJaa74txVzPrLsOxSi31ME1Cn0V2VQBP2g@mail.gmail.com>
	<CAEJX8oo2LVTGaCKm-uDomnQ3i+UH1WRKiHwfazxLM2H2emAE4A@mail.gmail.com>
Message-ID: <CAJ_mLLpxJ9vzeAQ+HJ9VGOzCMnWMxHRAfwA2NrsQXF9CDe_gYQ@mail.gmail.com>

Thanks Stanimir,

It is clear to me.

Micha?


2013/5/27 Stanimir Simeonoff <stanimir at riflexo.com>

>
>
> On Mon, May 27, 2013 at 12:55 PM, Micha? Warecki <michal.warecki at gmail.com
> > wrote:
>
>> Thanks Stanimir!
>>
>> In this case "compiler" == Just in Time Compiler?
>>
> Java doesn't require JIT per se, it could be ahead-of-time compiler just
> as fine, point is the reorder is allowed by the standard.
>
>
>> So if on x86 stores are not reordered, the only thing that JVM does, is
>> to not reorder instructions by JIT?
>>
> More or less, at least on x86, not every architecture is Total Store Order.
>
>
>>  Does normal store (non volatile) have the same visibility
>> ("eventually") as lazySet (not taking into account reordering)?
>>
>> More or less but it's undefined.
>
> Stanimir
>
>
>
>> Micha?
>>
>>
>> 2013/5/27 Stanimir Simeonoff <stanimir at riflexo.com>
>>
>>> Even though the stores are generally not reordered on x86 (vector
>>> operations not included).
>>> The compiler can reorder instructions as it sees fit. lazySet is usually
>>> a normal MOV on x86.
>>> Also the semantics are not unique to x86 only alone, it's defined w/
>>> respect to JMM.
>>> "Eventually" is undefined as time constraint and there have been quite a
>>> lot of discussions, in the end it all depends on the underlying hardware/OS
>>> (for instance when the store buffers are flushed or there is a context
>>> switch).  The most important feature is the happens before barrier, i.e.
>>> if  the store defined by lazySet is visible in another thread, all the
>>> stores till that moment will be as visible.
>>>
>>> The method is quite useful as it's generally cheaper than a volatile
>>> write, yet still ensures all the previous stores are propagated. As you
>>> pointed there are no extra memory fences on x86, so usually it's a clear
>>> win there.
>>>
>>> BTW, This question is likely to spur a long discussion.
>>>
>>> Stanimir
>>>
>>> On Mon, May 27, 2013 at 11:42 AM, Micha? Warecki <
>>> michal.warecki at gmail.com> wrote:
>>>
>>>> Good day all!
>>>>
>>>> I have a few questions about Atomic*.lazySet / Unsafe.putOrdered* on
>>>> x86 architecture.
>>>> In docs I can read that lazySet eventually sets to the given value and
>>>> it acts as a StoreStore barrier.
>>>> OK, but on x86 stores are not reordered with other stores so there is
>>>> no need for StoreStore barrier.
>>>>
>>>>
>>>> My questions are:
>>>> 1. How does it differ from normal (non volatile) store?
>>>> 2. Is there emitted any special assembly instruction after lazySet?
>>>> 3. What does "eventually" really mean (second, minute, hour, ..., (!)
>>>> never (!) )?
>>>> 4. What is the use case of this method?
>>>>
>>>> I can see that some libraries (i.e. Disruptor) rely (or relied) on this
>>>> method but I don't know how to check correctness of this algorithms.
>>>> Is there any detailed documentation about this?
>>>>
>>>> Thanks,
>>>> Micha?
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130527/fa84a04d/attachment.html>

From valentin.male.kovalenko at gmail.com  Mon May 27 08:30:00 2013
From: valentin.male.kovalenko at gmail.com (Valentin Kovalenko)
Date: Mon, 27 May 2013 16:30:00 +0400
Subject: [concurrency-interest] Concurrency-interest Digest, Vol 100,
	Issue 62
In-Reply-To: <mailman.618.1369650343.2479.concurrency-interest@cs.oswego.edu>
References: <mailman.618.1369650343.2479.concurrency-interest@cs.oswego.edu>
Message-ID: <CAO-wXwK7wvzC78fdW2sWd3mpDUg8wZf9d0Amattt4NjNF+JQzQ@mail.gmail.com>

Hi

>Atomic*.lazySet
>What is the use case of this method?
You didn't write any use cases by yourself, so one can assume you can't
imagine any at all:) Because of this captain obvious tells that JDK
specification for java.util.concurrent.atomic package specifies one
possible use case: ...lazySet may apply when nulling out, for the sake of
garbage collection, a reference that is never accessed again.



On Mon, May 27, 2013 at 2:25 PM, <concurrency-interest-request at cs.oswego.edu
> wrote:

> Send Concurrency-interest mailing list submissions to
>         concurrency-interest at cs.oswego.edu
>
> To subscribe or unsubscribe via the World Wide Web, visit
>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> or, via email, send a message with subject or body 'help' to
>         concurrency-interest-request at cs.oswego.edu
>
> You can reach the person managing the list at
>         concurrency-interest-owner at cs.oswego.edu
>
> When replying, please edit your Subject line so it is more specific
> than "Re: Contents of Concurrency-interest digest..."
>
>
> Today's Topics:
>
>    1. Atomic*.lazySet / Unsafe.putOrdered* x86 (Micha? Warecki)
>    2. Re: Atomic*.lazySet / Unsafe.putOrdered* x86 (Stanimir Simeonoff)
>    3. Re: Atomic*.lazySet / Unsafe.putOrdered* x86 (Micha? Warecki)
>    4. Re: Atomic*.lazySet / Unsafe.putOrdered* x86 (Stanimir Simeonoff)
>
>
> ----------------------------------------------------------------------
>
> Message: 1
> Date: Mon, 27 May 2013 10:42:11 +0200
> From: Micha? Warecki <michal.warecki at gmail.com>
> To: "Concurrency-interest at cs.oswego.edu"
>         <concurrency-interest at cs.oswego.edu>
> Subject: [concurrency-interest] Atomic*.lazySet / Unsafe.putOrdered*
>         x86
> Message-ID:
>         <CAJ_mLLo_c=
> K_hWsr3NMRacJVKspakMzY0gsF3J2Ki9t2a-tN8A at mail.gmail.com>
> Content-Type: text/plain; charset="iso-8859-2"
>
> Good day all!
>
> I have a few questions about Atomic*.lazySet / Unsafe.putOrdered* on x86
> architecture.
> In docs I can read that lazySet eventually sets to the given value and it
> acts as a StoreStore barrier.
> OK, but on x86 stores are not reordered with other stores so there is no
> need for StoreStore barrier.
>
>
> My questions are:
> 1. How does it differ from normal (non volatile) store?
> 2. Is there emitted any special assembly instruction after lazySet?
> 3. What does "eventually" really mean (second, minute, hour, ..., (!) never
> (!) )?
> 4. What is the use case of this method?
>
> I can see that some libraries (i.e. Disruptor) rely (or relied) on this
> method but I don't know how to check correctness of this algorithms.
> Is there any detailed documentation about this?
>
> Thanks,
> Micha?
> -------------- next part --------------
> An HTML attachment was scrubbed...
> URL: <
> http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130527/cf8b5086/attachment-0001.html
> >
>
> ------------------------------
>
> Message: 2
> Date: Mon, 27 May 2013 12:36:56 +0300
> From: Stanimir Simeonoff <stanimir at riflexo.com>
> To: Micha? Warecki <michal.warecki at gmail.com>
> Cc: "Concurrency-interest at cs.oswego.edu"
>         <concurrency-interest at cs.oswego.edu>
> Subject: Re: [concurrency-interest] Atomic*.lazySet /
>         Unsafe.putOrdered* x86
> Message-ID:
>         <CAEJX8ood_-dN=
> pVgOm0qmuDHCpKx_rsnypHszfzuu3jYVwsDyw at mail.gmail.com>
> Content-Type: text/plain; charset="utf-8"
>
> Even though the stores are generally not reordered on x86 (vector
> operations not included).
> The compiler can reorder instructions as it sees fit. lazySet is usually a
> normal MOV on x86.
> Also the semantics are not unique to x86 only alone, it's defined w/
> respect to JMM.
> "Eventually" is undefined as time constraint and there have been quite a
> lot of discussions, in the end it all depends on the underlying hardware/OS
> (for instance when the store buffers are flushed or there is a context
> switch).  The most important feature is the happens before barrier, i.e.
> if  the store defined by lazySet is visible in another thread, all the
> stores till that moment will be as visible.
>
> The method is quite useful as it's generally cheaper than a volatile write,
> yet still ensures all the previous stores are propagated. As you pointed
> there are no extra memory fences on x86, so usually it's a clear win there.
>
> BTW, This question is likely to spur a long discussion.
>
> Stanimir
>
> On Mon, May 27, 2013 at 11:42 AM, Micha? Warecki
> <michal.warecki at gmail.com>wrote:
>
> > Good day all!
> >
> > I have a few questions about Atomic*.lazySet / Unsafe.putOrdered* on x86
> > architecture.
> > In docs I can read that lazySet eventually sets to the given value and it
> > acts as a StoreStore barrier.
> > OK, but on x86 stores are not reordered with other stores so there is no
> > need for StoreStore barrier.
> >
> >
> > My questions are:
> > 1. How does it differ from normal (non volatile) store?
> > 2. Is there emitted any special assembly instruction after lazySet?
> > 3. What does "eventually" really mean (second, minute, hour, ..., (!)
> > never (!) )?
> > 4. What is the use case of this method?
> >
> > I can see that some libraries (i.e. Disruptor) rely (or relied) on this
> > method but I don't know how to check correctness of this algorithms.
> > Is there any detailed documentation about this?
> >
> > Thanks,
> > Micha?
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
> >
> -------------- next part --------------
> An HTML attachment was scrubbed...
> URL: <
> http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130527/4ba36f4a/attachment-0001.html
> >
>
> ------------------------------
>
> Message: 3
> Date: Mon, 27 May 2013 11:55:53 +0200
> From: Micha? Warecki <michal.warecki at gmail.com>
> To: Stanimir Simeonoff <stanimir at riflexo.com>
> Cc: "Concurrency-interest at cs.oswego.edu"
>         <concurrency-interest at cs.oswego.edu>
> Subject: Re: [concurrency-interest] Atomic*.lazySet /
>         Unsafe.putOrdered* x86
> Message-ID:
>         <
> CAJ_mLLoyifdBrkOZbJaa74txVzPrLsOxSi31ME1Cn0V2VQBP2g at mail.gmail.com>
> Content-Type: text/plain; charset="iso-8859-2"
>
> Thanks Stanimir!
>
> In this case "compiler" == Just in Time Compiler? So if on x86 stores are
> not reordered, the only thing that JVM does, is to not reorder instructions
> by JIT?
> Does normal store (non volatile) have the same visibility ("eventually") as
> lazySet (not taking into account reordering)?
>
> Micha?
>
>
> 2013/5/27 Stanimir Simeonoff <stanimir at riflexo.com>
>
> > Even though the stores are generally not reordered on x86 (vector
> > operations not included).
> > The compiler can reorder instructions as it sees fit. lazySet is usually
> a
> > normal MOV on x86.
> > Also the semantics are not unique to x86 only alone, it's defined w/
> > respect to JMM.
> > "Eventually" is undefined as time constraint and there have been quite a
> > lot of discussions, in the end it all depends on the underlying
> hardware/OS
> > (for instance when the store buffers are flushed or there is a context
> > switch).  The most important feature is the happens before barrier, i.e.
> > if  the store defined by lazySet is visible in another thread, all the
> > stores till that moment will be as visible.
> >
> > The method is quite useful as it's generally cheaper than a volatile
> > write, yet still ensures all the previous stores are propagated. As you
> > pointed there are no extra memory fences on x86, so usually it's a clear
> > win there.
> >
> > BTW, This question is likely to spur a long discussion.
> >
> > Stanimir
> >
> > On Mon, May 27, 2013 at 11:42 AM, Micha? Warecki <
> michal.warecki at gmail.com
> > > wrote:
> >
> >> Good day all!
> >>
> >> I have a few questions about Atomic*.lazySet / Unsafe.putOrdered* on x86
> >> architecture.
> >> In docs I can read that lazySet eventually sets to the given value and
> >> it acts as a StoreStore barrier.
> >> OK, but on x86 stores are not reordered with other stores so there is no
> >> need for StoreStore barrier.
> >>
> >>
> >> My questions are:
> >> 1. How does it differ from normal (non volatile) store?
> >> 2. Is there emitted any special assembly instruction after lazySet?
> >> 3. What does "eventually" really mean (second, minute, hour, ..., (!)
> >> never (!) )?
> >> 4. What is the use case of this method?
> >>
> >> I can see that some libraries (i.e. Disruptor) rely (or relied) on this
> >> method but I don't know how to check correctness of this algorithms.
> >> Is there any detailed documentation about this?
> >>
> >> Thanks,
> >> Micha?
> >>
> >> _______________________________________________
> >> Concurrency-interest mailing list
> >> Concurrency-interest at cs.oswego.edu
> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >>
> >>
> >
> -------------- next part --------------
> An HTML attachment was scrubbed...
> URL: <
> http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130527/808efd98/attachment-0001.html
> >
>
> ------------------------------
>
> Message: 4
> Date: Mon, 27 May 2013 13:25:32 +0300
> From: Stanimir Simeonoff <stanimir at riflexo.com>
> To: Micha? Warecki <michal.warecki at gmail.com>
> Cc: "Concurrency-interest at cs.oswego.edu"
>         <concurrency-interest at cs.oswego.edu>
> Subject: Re: [concurrency-interest] Atomic*.lazySet /
>         Unsafe.putOrdered* x86
> Message-ID:
>         <
> CAEJX8oo2LVTGaCKm-uDomnQ3i+UH1WRKiHwfazxLM2H2emAE4A at mail.gmail.com>
> Content-Type: text/plain; charset="utf-8"
>
> On Mon, May 27, 2013 at 12:55 PM, Micha? Warecki
> <michal.warecki at gmail.com>wrote:
>
> > Thanks Stanimir!
> >
> > In this case "compiler" == Just in Time Compiler?
> >
> Java doesn't require JIT per se, it could be ahead-of-time compiler just as
> fine, point is the reorder is allowed by the standard.
>
>
> > So if on x86 stores are not reordered, the only thing that JVM does, is
> to
> > not reorder instructions by JIT?
> >
> More or less, at least on x86, not every architecture is Total Store Order.
>
>
> > Does normal store (non volatile) have the same visibility ("eventually")
> > as lazySet (not taking into account reordering)?
> >
> > More or less but it's undefined.
>
> Stanimir
>
>
>
> > Micha?
> >
> >
> > 2013/5/27 Stanimir Simeonoff <stanimir at riflexo.com>
> >
> >> Even though the stores are generally not reordered on x86 (vector
> >> operations not included).
> >> The compiler can reorder instructions as it sees fit. lazySet is usually
> >> a normal MOV on x86.
> >> Also the semantics are not unique to x86 only alone, it's defined w/
> >> respect to JMM.
> >> "Eventually" is undefined as time constraint and there have been quite a
> >> lot of discussions, in the end it all depends on the underlying
> hardware/OS
> >> (for instance when the store buffers are flushed or there is a context
> >> switch).  The most important feature is the happens before barrier, i.e.
> >> if  the store defined by lazySet is visible in another thread, all the
> >> stores till that moment will be as visible.
> >>
> >> The method is quite useful as it's generally cheaper than a volatile
> >> write, yet still ensures all the previous stores are propagated. As you
> >> pointed there are no extra memory fences on x86, so usually it's a clear
> >> win there.
> >>
> >> BTW, This question is likely to spur a long discussion.
> >>
> >> Stanimir
> >>
> >> On Mon, May 27, 2013 at 11:42 AM, Micha? Warecki <
> >> michal.warecki at gmail.com> wrote:
> >>
> >>> Good day all!
> >>>
> >>> I have a few questions about Atomic*.lazySet / Unsafe.putOrdered* on
> x86
> >>> architecture.
> >>> In docs I can read that lazySet eventually sets to the given value and
> >>> it acts as a StoreStore barrier.
> >>> OK, but on x86 stores are not reordered with other stores so there is
> no
> >>> need for StoreStore barrier.
> >>>
> >>>
> >>> My questions are:
> >>> 1. How does it differ from normal (non volatile) store?
> >>> 2. Is there emitted any special assembly instruction after lazySet?
> >>> 3. What does "eventually" really mean (second, minute, hour, ..., (!)
> >>> never (!) )?
> >>> 4. What is the use case of this method?
> >>>
> >>> I can see that some libraries (i.e. Disruptor) rely (or relied) on this
> >>> method but I don't know how to check correctness of this algorithms.
> >>> Is there any detailed documentation about this?
> >>>
> >>> Thanks,
> >>> Micha?
> >>>
> >>> _______________________________________________
> >>> Concurrency-interest mailing list
> >>> Concurrency-interest at cs.oswego.edu
> >>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >>>
> >>>
> >>
> >
> -------------- next part --------------
> An HTML attachment was scrubbed...
> URL: <
> http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130527/eae422ae/attachment.html
> >
>
> ------------------------------
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> End of Concurrency-interest Digest, Vol 100, Issue 62
> *****************************************************
>



-- 
Homo homini lupus est.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130527/c078601d/attachment-0001.html>

From michal.warecki at gmail.com  Mon May 27 08:56:06 2013
From: michal.warecki at gmail.com (=?ISO-8859-2?Q?Micha=B3_Warecki?=)
Date: Mon, 27 May 2013 14:56:06 +0200
Subject: [concurrency-interest] Concurrency-interest Digest, Vol 100,
 Issue 62
In-Reply-To: <CAO-wXwK7wvzC78fdW2sWd3mpDUg8wZf9d0Amattt4NjNF+JQzQ@mail.gmail.com>
References: <mailman.618.1369650343.2479.concurrency-interest@cs.oswego.edu>
	<CAO-wXwK7wvzC78fdW2sWd3mpDUg8wZf9d0Amattt4NjNF+JQzQ@mail.gmail.com>
Message-ID: <CAJ_mLLqSoz0EYSegbJwgqzyG5OGa4iNr9oQMhS5=-_aFqzCWSg@mail.gmail.com>

I sensed some magic in that and I did not think it is just a StoreStore
barrier.
I can imagine use case of SS barrier :-) Anyway, thanks for example!


Micha?


2013/5/27 Valentin Kovalenko <valentin.male.kovalenko at gmail.com>

>
> Hi
>
> >Atomic*.lazySet
> >What is the use case of this method?
> You didn't write any use cases by yourself, so one can assume you can't
> imagine any at all:) Because of this captain obvious tells that JDK
> specification for java.util.concurrent.atomic package specifies one
> possible use case: ...lazySet may apply when nulling out, for the sake of
> garbage collection, a reference that is never accessed again.
>
>
>
> On Mon, May 27, 2013 at 2:25 PM, <
> concurrency-interest-request at cs.oswego.edu> wrote:
>
>> Send Concurrency-interest mailing list submissions to
>>         concurrency-interest at cs.oswego.edu
>>
>> To subscribe or unsubscribe via the World Wide Web, visit
>>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> or, via email, send a message with subject or body 'help' to
>>         concurrency-interest-request at cs.oswego.edu
>>
>> You can reach the person managing the list at
>>         concurrency-interest-owner at cs.oswego.edu
>>
>> When replying, please edit your Subject line so it is more specific
>> than "Re: Contents of Concurrency-interest digest..."
>>
>>
>> Today's Topics:
>>
>>    1. Atomic*.lazySet / Unsafe.putOrdered* x86 (Micha? Warecki)
>>    2. Re: Atomic*.lazySet / Unsafe.putOrdered* x86 (Stanimir Simeonoff)
>>    3. Re: Atomic*.lazySet / Unsafe.putOrdered* x86 (Micha? Warecki)
>>    4. Re: Atomic*.lazySet / Unsafe.putOrdered* x86 (Stanimir Simeonoff)
>>
>>
>> ----------------------------------------------------------------------
>>
>> Message: 1
>> Date: Mon, 27 May 2013 10:42:11 +0200
>> From: Micha? Warecki <michal.warecki at gmail.com>
>> To: "Concurrency-interest at cs.oswego.edu"
>>         <concurrency-interest at cs.oswego.edu>
>> Subject: [concurrency-interest] Atomic*.lazySet / Unsafe.putOrdered*
>>         x86
>> Message-ID:
>>         <CAJ_mLLo_c=
>> K_hWsr3NMRacJVKspakMzY0gsF3J2Ki9t2a-tN8A at mail.gmail.com>
>> Content-Type: text/plain; charset="iso-8859-2"
>>
>> Good day all!
>>
>> I have a few questions about Atomic*.lazySet / Unsafe.putOrdered* on x86
>> architecture.
>> In docs I can read that lazySet eventually sets to the given value and it
>> acts as a StoreStore barrier.
>> OK, but on x86 stores are not reordered with other stores so there is no
>> need for StoreStore barrier.
>>
>>
>> My questions are:
>> 1. How does it differ from normal (non volatile) store?
>> 2. Is there emitted any special assembly instruction after lazySet?
>> 3. What does "eventually" really mean (second, minute, hour, ..., (!)
>> never
>> (!) )?
>> 4. What is the use case of this method?
>>
>> I can see that some libraries (i.e. Disruptor) rely (or relied) on this
>> method but I don't know how to check correctness of this algorithms.
>> Is there any detailed documentation about this?
>>
>> Thanks,
>> Micha?
>> -------------- next part --------------
>> An HTML attachment was scrubbed...
>> URL: <
>> http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130527/cf8b5086/attachment-0001.html
>> >
>>
>> ------------------------------
>>
>> Message: 2
>> Date: Mon, 27 May 2013 12:36:56 +0300
>> From: Stanimir Simeonoff <stanimir at riflexo.com>
>> To: Micha? Warecki <michal.warecki at gmail.com>
>> Cc: "Concurrency-interest at cs.oswego.edu"
>>         <concurrency-interest at cs.oswego.edu>
>> Subject: Re: [concurrency-interest] Atomic*.lazySet /
>>         Unsafe.putOrdered* x86
>> Message-ID:
>>         <CAEJX8ood_-dN=
>> pVgOm0qmuDHCpKx_rsnypHszfzuu3jYVwsDyw at mail.gmail.com>
>> Content-Type: text/plain; charset="utf-8"
>>
>> Even though the stores are generally not reordered on x86 (vector
>> operations not included).
>> The compiler can reorder instructions as it sees fit. lazySet is usually a
>> normal MOV on x86.
>> Also the semantics are not unique to x86 only alone, it's defined w/
>> respect to JMM.
>> "Eventually" is undefined as time constraint and there have been quite a
>> lot of discussions, in the end it all depends on the underlying
>> hardware/OS
>> (for instance when the store buffers are flushed or there is a context
>> switch).  The most important feature is the happens before barrier, i.e.
>> if  the store defined by lazySet is visible in another thread, all the
>> stores till that moment will be as visible.
>>
>> The method is quite useful as it's generally cheaper than a volatile
>> write,
>> yet still ensures all the previous stores are propagated. As you pointed
>> there are no extra memory fences on x86, so usually it's a clear win
>> there.
>>
>> BTW, This question is likely to spur a long discussion.
>>
>> Stanimir
>>
>> On Mon, May 27, 2013 at 11:42 AM, Micha? Warecki
>> <michal.warecki at gmail.com>wrote:
>>
>> > Good day all!
>> >
>> > I have a few questions about Atomic*.lazySet / Unsafe.putOrdered* on x86
>> > architecture.
>> > In docs I can read that lazySet eventually sets to the given value and
>> it
>> > acts as a StoreStore barrier.
>> > OK, but on x86 stores are not reordered with other stores so there is no
>> > need for StoreStore barrier.
>> >
>> >
>> > My questions are:
>> > 1. How does it differ from normal (non volatile) store?
>> > 2. Is there emitted any special assembly instruction after lazySet?
>> > 3. What does "eventually" really mean (second, minute, hour, ..., (!)
>> > never (!) )?
>> > 4. What is the use case of this method?
>> >
>> > I can see that some libraries (i.e. Disruptor) rely (or relied) on this
>> > method but I don't know how to check correctness of this algorithms.
>> > Is there any detailed documentation about this?
>> >
>> > Thanks,
>> > Micha?
>> >
>> > _______________________________________________
>> > Concurrency-interest mailing list
>> > Concurrency-interest at cs.oswego.edu
>> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> >
>> >
>> -------------- next part --------------
>> An HTML attachment was scrubbed...
>> URL: <
>> http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130527/4ba36f4a/attachment-0001.html
>> >
>>
>> ------------------------------
>>
>> Message: 3
>> Date: Mon, 27 May 2013 11:55:53 +0200
>> From: Micha? Warecki <michal.warecki at gmail.com>
>> To: Stanimir Simeonoff <stanimir at riflexo.com>
>> Cc: "Concurrency-interest at cs.oswego.edu"
>>         <concurrency-interest at cs.oswego.edu>
>> Subject: Re: [concurrency-interest] Atomic*.lazySet /
>>         Unsafe.putOrdered* x86
>> Message-ID:
>>         <
>> CAJ_mLLoyifdBrkOZbJaa74txVzPrLsOxSi31ME1Cn0V2VQBP2g at mail.gmail.com>
>> Content-Type: text/plain; charset="iso-8859-2"
>>
>> Thanks Stanimir!
>>
>> In this case "compiler" == Just in Time Compiler? So if on x86 stores are
>> not reordered, the only thing that JVM does, is to not reorder
>> instructions
>> by JIT?
>> Does normal store (non volatile) have the same visibility ("eventually")
>> as
>> lazySet (not taking into account reordering)?
>>
>> Micha?
>>
>>
>> 2013/5/27 Stanimir Simeonoff <stanimir at riflexo.com>
>>
>> > Even though the stores are generally not reordered on x86 (vector
>> > operations not included).
>> > The compiler can reorder instructions as it sees fit. lazySet is
>> usually a
>> > normal MOV on x86.
>> > Also the semantics are not unique to x86 only alone, it's defined w/
>> > respect to JMM.
>> > "Eventually" is undefined as time constraint and there have been quite a
>> > lot of discussions, in the end it all depends on the underlying
>> hardware/OS
>> > (for instance when the store buffers are flushed or there is a context
>> > switch).  The most important feature is the happens before barrier, i.e.
>> > if  the store defined by lazySet is visible in another thread, all the
>> > stores till that moment will be as visible.
>> >
>> > The method is quite useful as it's generally cheaper than a volatile
>> > write, yet still ensures all the previous stores are propagated. As you
>> > pointed there are no extra memory fences on x86, so usually it's a clear
>> > win there.
>> >
>> > BTW, This question is likely to spur a long discussion.
>> >
>> > Stanimir
>> >
>> > On Mon, May 27, 2013 at 11:42 AM, Micha? Warecki <
>> michal.warecki at gmail.com
>> > > wrote:
>> >
>> >> Good day all!
>> >>
>> >> I have a few questions about Atomic*.lazySet / Unsafe.putOrdered* on
>> x86
>> >> architecture.
>> >> In docs I can read that lazySet eventually sets to the given value and
>> >> it acts as a StoreStore barrier.
>> >> OK, but on x86 stores are not reordered with other stores so there is
>> no
>> >> need for StoreStore barrier.
>> >>
>> >>
>> >> My questions are:
>> >> 1. How does it differ from normal (non volatile) store?
>> >> 2. Is there emitted any special assembly instruction after lazySet?
>> >> 3. What does "eventually" really mean (second, minute, hour, ..., (!)
>> >> never (!) )?
>> >> 4. What is the use case of this method?
>> >>
>> >> I can see that some libraries (i.e. Disruptor) rely (or relied) on this
>> >> method but I don't know how to check correctness of this algorithms.
>> >> Is there any detailed documentation about this?
>> >>
>> >> Thanks,
>> >> Micha?
>> >>
>> >> _______________________________________________
>> >> Concurrency-interest mailing list
>> >> Concurrency-interest at cs.oswego.edu
>> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> >>
>> >>
>> >
>> -------------- next part --------------
>> An HTML attachment was scrubbed...
>> URL: <
>> http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130527/808efd98/attachment-0001.html
>> >
>>
>> ------------------------------
>>
>> Message: 4
>> Date: Mon, 27 May 2013 13:25:32 +0300
>> From: Stanimir Simeonoff <stanimir at riflexo.com>
>> To: Micha? Warecki <michal.warecki at gmail.com>
>> Cc: "Concurrency-interest at cs.oswego.edu"
>>         <concurrency-interest at cs.oswego.edu>
>> Subject: Re: [concurrency-interest] Atomic*.lazySet /
>>         Unsafe.putOrdered* x86
>> Message-ID:
>>         <
>> CAEJX8oo2LVTGaCKm-uDomnQ3i+UH1WRKiHwfazxLM2H2emAE4A at mail.gmail.com>
>> Content-Type: text/plain; charset="utf-8"
>>
>> On Mon, May 27, 2013 at 12:55 PM, Micha? Warecki
>> <michal.warecki at gmail.com>wrote:
>>
>> > Thanks Stanimir!
>> >
>> > In this case "compiler" == Just in Time Compiler?
>> >
>> Java doesn't require JIT per se, it could be ahead-of-time compiler just
>> as
>> fine, point is the reorder is allowed by the standard.
>>
>>
>> > So if on x86 stores are not reordered, the only thing that JVM does, is
>> to
>> > not reorder instructions by JIT?
>> >
>> More or less, at least on x86, not every architecture is Total Store
>> Order.
>>
>>
>> > Does normal store (non volatile) have the same visibility ("eventually")
>> > as lazySet (not taking into account reordering)?
>> >
>> > More or less but it's undefined.
>>
>> Stanimir
>>
>>
>>
>> > Micha?
>> >
>> >
>> > 2013/5/27 Stanimir Simeonoff <stanimir at riflexo.com>
>> >
>> >> Even though the stores are generally not reordered on x86 (vector
>> >> operations not included).
>> >> The compiler can reorder instructions as it sees fit. lazySet is
>> usually
>> >> a normal MOV on x86.
>> >> Also the semantics are not unique to x86 only alone, it's defined w/
>> >> respect to JMM.
>> >> "Eventually" is undefined as time constraint and there have been quite
>> a
>> >> lot of discussions, in the end it all depends on the underlying
>> hardware/OS
>> >> (for instance when the store buffers are flushed or there is a context
>> >> switch).  The most important feature is the happens before barrier,
>> i.e.
>> >> if  the store defined by lazySet is visible in another thread, all the
>> >> stores till that moment will be as visible.
>> >>
>> >> The method is quite useful as it's generally cheaper than a volatile
>> >> write, yet still ensures all the previous stores are propagated. As you
>> >> pointed there are no extra memory fences on x86, so usually it's a
>> clear
>> >> win there.
>> >>
>> >> BTW, This question is likely to spur a long discussion.
>> >>
>> >> Stanimir
>> >>
>> >> On Mon, May 27, 2013 at 11:42 AM, Micha? Warecki <
>> >> michal.warecki at gmail.com> wrote:
>> >>
>> >>> Good day all!
>> >>>
>> >>> I have a few questions about Atomic*.lazySet / Unsafe.putOrdered* on
>> x86
>> >>> architecture.
>> >>> In docs I can read that lazySet eventually sets to the given value and
>> >>> it acts as a StoreStore barrier.
>> >>> OK, but on x86 stores are not reordered with other stores so there is
>> no
>> >>> need for StoreStore barrier.
>> >>>
>> >>>
>> >>> My questions are:
>> >>> 1. How does it differ from normal (non volatile) store?
>> >>> 2. Is there emitted any special assembly instruction after lazySet?
>> >>> 3. What does "eventually" really mean (second, minute, hour, ..., (!)
>> >>> never (!) )?
>> >>> 4. What is the use case of this method?
>> >>>
>> >>> I can see that some libraries (i.e. Disruptor) rely (or relied) on
>> this
>> >>> method but I don't know how to check correctness of this algorithms.
>> >>> Is there any detailed documentation about this?
>> >>>
>> >>> Thanks,
>> >>> Micha?
>> >>>
>> >>> _______________________________________________
>> >>> Concurrency-interest mailing list
>> >>> Concurrency-interest at cs.oswego.edu
>> >>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> >>>
>> >>>
>> >>
>> >
>> -------------- next part --------------
>> An HTML attachment was scrubbed...
>> URL: <
>> http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130527/eae422ae/attachment.html
>> >
>>
>> ------------------------------
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>> End of Concurrency-interest Digest, Vol 100, Issue 62
>> *****************************************************
>>
>
>
>
> --
> Homo homini lupus est.
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130527/c635e849/attachment-0001.html>

From james.roper at typesafe.com  Mon May 27 21:13:25 2013
From: james.roper at typesafe.com (James Roper)
Date: Tue, 28 May 2013 11:13:25 +1000
Subject: [concurrency-interest] Intention of CompletableFuture
Message-ID: <CABY0rKODCbTY5Rr8r-hjB=k19SqUPdFd9KTanX1=tTtaK5Qfrg@mail.gmail.com>

Hi all,

I've just come across this message in the December discussion on
CompletableFuture:

http://cs.oswego.edu/pipermail/concurrency-interest/2012-December/010484.html

I'm in the middle of designing asynchronous Java APIs, and I would like to
confirm that I have understood the intention of CompletableFuture:

* CompletableFuture is intended as a low level concurrency utility, not a
high level abstract interface.
* It is not intended that high level APIs will ever return
CompletableFuture or accept it as a parameter, except perhaps to wrap it in
a high level API.  For example, if there was in future an asynchronous JDBC
API in the JDK, it is not intended that CompletableFuture would be an
appropriate return type for this API.
* It is intended that APIs that provide asynchronous behaviour will provide
their own high level abstract interfaces for futures/promises specific to
their needs, whose default implementations may well be based on
CompletableFuture, or will find CompletableFuture a useful tool in their
internal implementations.
* It is also intended that end users will find CompletableFuture a useful
tool for managing asynchronous behaviour in their own code.

Are my understandings correct?

Cheers,

James

-- 
*James Roper*
*Software Engineer*
*
*
Typesafe <http://www.typesafe.com/> - The software stack for applications
that scale
Twitter: @jroper
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130528/e4c051a4/attachment.html>

