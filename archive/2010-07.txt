From bryan at systap.com  Fri Jul  2 08:14:21 2010
From: bryan at systap.com (Bryan Thompson)
Date: Fri, 2 Jul 2010 07:14:21 -0500
Subject: [concurrency-interest] Upcoming FJ Simplifications
In-Reply-To: <4C2A0ADD.60002@cs.oswego.edu>
References: <4C2A0ADD.60002@cs.oswego.edu>
Message-ID: <DE10B00CCE0DC54883734F3060AC9ED44DCA037673@AUSP01VMBX06.collaborationhost.net>

Doug,

My question would be whether removing hints to govern the thread pool size will make it more difficult for applications which want to control the balance of effort directed at different families of tasks.  For example, tasks oriented at local computations vs async network I/O vs async file I/O (w/ Java 7).  

The application which motivates this question is a distributed database with integrated reasoning, network I/O for internode communication, and we will begin using async file I/O later this year for query execution.

Thanks,
Bryan

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu 
> [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf 
> Of Doug Lea
> Sent: Tuesday, June 29, 2010 11:02 AM
> To: Concurrency-interest at cs.oswego.edu
> Subject: [concurrency-interest] Upcoming FJ Simplifications
> 
> 
> Over the past month I've talked (mainly at various 
> conferences) to a bunch of people (including some of you on 
> this list) who are currently using or layering other 
> frameworks on top of ForkJoin.
> One of my conclusions is that I should simplify some of the 
> APIs, mainly by removing some methods; at the same time 
> reworking some of the internals to provide more uniformly 
> good performance without the need to experiment with various 
> tunings and special case methods. By removing some of the 
> visible controls, we can continue to improve base algorithms 
> without being constrained in ways that may become obsolete.
> 
> Unless I hear some good arguments otherwise, I plan the 
> following changes, that should be ready to try out within a 
> week or so. A mock up of revised APIs is at 
> http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166ydocs/
> 
> 1. Disable dynamic reconfiguration of ForkJoinPool and 
> provide fewer configuration parameters. The main rationale is 
> that we know of no compelling use cases where these are 
> helpful and several where they are harmful.
> This entails removing methods setParallelism, 
> setMaintainsParallelism, setMaximumPoolSize, etc.
> The only on-construction parameters left are parallelism, 
> factory, uncaughtExceptionHandler, and async mode. This also 
> entails removing "maintainParallelism" arguments from a few 
> other methods.
> 
> 2. Recast ForkJoinPool.invoke to be equivalent to 
> ForkJoinTask.invoke when issued by a task in the current 
> pool. While these methods must be distinct (because the 
> former can invoke a task in a different pool), the semantic 
> differences are otherwise subtle and error-prone. Also, 
> improve documentation about the various execute vs submit vs 
> invoke vs fork methods.
> 
> 4. Remove ForkJoinTask.helpJoin() and variants. Instead, 
> plain join() conservatively, safely mixes helping- vs spare- 
> based joins internally.
> 
> Any further suggestions for ways to remove, combine, or 
> simplify functionality would be welcome.
> 
> -Doug
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 

From dl at cs.oswego.edu  Fri Jul  2 10:54:03 2010
From: dl at cs.oswego.edu (Doug Lea)
Date: Fri, 02 Jul 2010 10:54:03 -0400
Subject: [concurrency-interest] Upcoming FJ Simplifications
In-Reply-To: <DE10B00CCE0DC54883734F3060AC9ED44DCA037673@AUSP01VMBX06.collaborationhost.net>
References: <4C2A0ADD.60002@cs.oswego.edu>
	<DE10B00CCE0DC54883734F3060AC9ED44DCA037673@AUSP01VMBX06.collaborationhost.net>
Message-ID: <4C2DFD8B.5070905@cs.oswego.edu>

On 07/02/10 08:14, Bryan Thompson wrote:

> My question would be whether removing hints to govern the thread pool size
> will make it more difficult for applications which want to control the
> balance of effort directed at different families of tasks.  For example,
> tasks oriented at local computations vs async network I/O vs async file I/O
> (w/ Java 7).

This general category of question arises a lot lately,
so here's a shot at a more general description of the
the issues here. I'll follow with a separate answer to your
actual question :-)

Originally, Executors were primarily designed for
async usages -- arrange for execution of some work,
without knowing or caring about how, when, or even
whether it completes.

In contrast, ForkJoin was designed for mostly-synchronous
parallel applications (including "deterministic parallel" --
see http://dpj.cs.uiuc.edu/DPJ/Home.html)  -- parallel
divide-and-conquer, apply-to-all operations, and so on,
that carry some usage restrictions mainly amounting to
nicely supporting very simple forms of sync (mainly just join())
and not at all supporting some forms of arbitrary blocking and IO.
But because FJ can be a lot faster and more scalable
than other j.u.c Executors, people are naturally tempted
to use it for less-structured async usages as well. We
even support this style to some extent via "async mode"
which provides task processing in the mostly-FIFO ordering
that people expect in event-style and actor-like frameworks.
The support for this mode is OK but not great: For example,
since event-like asyncs do not typically burst into a
highly parallel set of subtasks, the internal efforts to
quickly set all threads in motion is often wasted. Also,
people using asyncs are likely to ignore the usage
guidelines about blocking/IO and so risk thread starvation,
which is not at all a nice problem for them to experience.

So in the spirit of further simplifying FJ, I'm contemplating
splitting out support for this async/event/actor-ish style of
usage into a couple of classes (perhaps called EventTask
and EventExecutor?) that preserve most of the lower overhead
and better throughput and scalability advantages of FJ
but without all the scheduling and mechanics mismatches.
All thoughts about this would be welcome.

Until then, if you ignore the overhead and scalability issues,
typically the most appropriate vehicle for such usages
is a Executors.newCachedThreadPool.

-Doug


From dl at cs.oswego.edu  Fri Jul  2 11:30:44 2010
From: dl at cs.oswego.edu (Doug Lea)
Date: Fri, 02 Jul 2010 11:30:44 -0400
Subject: [concurrency-interest] Upcoming FJ Simplifications
In-Reply-To: <DE10B00CCE0DC54883734F3060AC9ED44DCA037673@AUSP01VMBX06.collaborationhost.net>
References: <4C2A0ADD.60002@cs.oswego.edu>
	<DE10B00CCE0DC54883734F3060AC9ED44DCA037673@AUSP01VMBX06.collaborationhost.net>
Message-ID: <4C2E0624.4080606@cs.oswego.edu>

On 07/02/10 08:14, Bryan Thompson wrote:
> My question would be whether removing hints to govern the thread pool size
> will make it more difficult for applications which want to control the
> balance of effort directed at different families of tasks.

Modulo the issues in my last post ...

FJ does not give you all that much control to begin with
since it accepts only target parallelism level, not
numbers of threads. Internally, to ensure liveness/progress,
we must sometimes transiently increase the numbers of workers.
It was a bad idea to allow users to bound these values. Not only
can it lead to stalled computations in the worst case, it can also
lead to poorer internal adaptation in common cases. So they are
going away.

A second consequence of internal adaptation is that under
some patterns of join dependencies, you may see transient peaks
in which, almost regardless of the nominal parallelism level,
FJ has momentarily taken over all your CPUs. This is a feature,
not a bug! -- you would rather have us do this than indefinitely
postpone completion. If this is a concern, I suppose you could
heuristically reduce impact by subclassing ForkJoinWorkerThread
to adjust Thread priorities, but I don't particularly recommend
it because priorities have so little impact on OS scheduling on
most multiprocessors.

-Doug

From dl at cs.oswego.edu  Fri Jul  2 13:48:58 2010
From: dl at cs.oswego.edu (Doug Lea)
Date: Fri, 02 Jul 2010 13:48:58 -0400
Subject: [concurrency-interest] CFP: CAP10 -- Splash Workshop on Concurrency
 for the Application Programmer
Message-ID: <4C2E268A.4070407@cs.oswego.edu>

Please consider submitting to and attending the following workshop
we are holding at SPLASH (aka OOPSLA).




                         Call For Participation
                        SPLASH 2010 Workshop on
               Concurrency for the Application Programmer
                        Monday, October 18, 2010
                          Splash, Reno Nevada

For more information and discussion about this workshop,
see the CFP at http://gee.cs.oswego.edu/dl/html/CAP2010.html
and join the CAP Facebook group at
http://www.facebook.com/group.php?gid=129568167064179

Forced by architectural and commercial considerations, programmers now
have to confront multi-core systems, heterogeneity, clusters, clouds.

What does this revolution mean for the application programmer,
typically removed from the hardware through many layers of middle-ware
(often on top of managed run-time environments)? How should the
capabilities of heterogeneous processors (including GPUs, FPGAs,
streaming processors) and heterogeneous memory (including non-coherent
memory) be made available to the application programmer? Should
abstractions for the application programmer focus primarily on
application-level concurrency rather than implementation-level
concurrency? Should application-level concurrency abstractions be
fundamentally determinate? Fundamentally declarative? Resilient in the
face of node- and network- failure? How can high-performance
concurrent programs be written in garbage-collected languages? How can
they not be written in garbage-collected languages?

This workshop aims to bring together practitioners and thinkers to
address all topics around concurrency for the application
programmer. We intend to try to keep the workshop small (ideally,
between 20 - 40 participants). Participants are expected to have
significant experience developing either applications, or concurrent
application frameworks (e.g. Hadoop, data-streaming languages) or
domain-specific languages, or tooling for application programmers.

Potential participants are requested to submit either 10-page
technical papers or 3-page position papers, using 10-point ACM SIGPLAN
templates to http://www.easychair.org/conferences/?conf=cap100. The
Program Committee will choose from the selected
submissions. Participation in the workshop will be through invitations
and on the basis of submissions.

Accepted papers will be posted in advance of the meeting, thereby
offering a chance for participants to read each others' papers and be
prepared for a more meaningful discussion.

Program Committee
     * Bob Blainey (IBM)
     * Joshua Bloch (Google)
     * Ras Bodik (UC Berkeley/Par Lab)
     * Amol Ghoting (IBM)
     * Kevlin Henney (Curbrain Ltd)
     * David Holmes (Oracle)
     * Jim Larus (Microsoft)
     * Doug Lea (SUNY Oswego) -- Co-chair
     * Martin Odersky (EPFL)
     * Bill Pugh (U Maryland)
     * Vijay Saraswat (IBM) -- Co-chair
     * Adam Welc (Intel)

Dates
     * Submission deadline: September 13, 2010
     * Acceptance notification: September 20, 2010
     * Final copy deadline: October 4, 2010
     * Workshop: October 18, 2010

From eshioji at gmail.com  Fri Jul  2 16:17:17 2010
From: eshioji at gmail.com (Enno Shioji)
Date: Sat, 3 Jul 2010 01:47:17 +0530
Subject: [concurrency-interest] How to schedule important periodic task.
Message-ID: <AANLkTikACnbsgy8XWj46-npzf-0DMP3talO-tvRa_eTr@mail.gmail.com>

I used to believe that the following idiom gives protection against
thread death. Namely, that a new  thread is instantiated and that the
task  continues to be executed in case of RuntimeException etc.


        ScheduledExecutorService ses =
Executors.newSingleThreadScheduledExecutor();

        ses.scheduleAtFixedRate(new Runnable() {
            public void run() {

System.out.println("RuntimeException:"+Thread.currentThread().getName());
                throw new RuntimeException();
            }
        }, 1, 1, TimeUnit.MILLISECONDS);


However, as it says in the javadoc of ScheduledExecutorService, tasks
are not executed anymore once a RuntimeException is encountered.


The following idiom works, but it kinda looks hacky, so I'm wondering
why the authors of ScheduledExecutorService chose the above behavior.

        final ScheduledExecutorService exec =
Executors.newSingleThreadScheduledExecutor();
        Runnable dieHard = new Runnable(){
           public void run() {
                try{

System.out.println("RuntimeException:"+Thread.currentThread().getName());
                    throw new RuntimeException();
                }finally{
                    exec.schedule(this,1,TimeUnit.MILLISECONDS);
                }
          }
        };
        exec.schedule(dieHard,1,TimeUnit.MILLISECONDS);




Regards,
Enno

From joe.bowbeer at gmail.com  Fri Jul  2 16:55:34 2010
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Fri, 2 Jul 2010 13:55:34 -0700
Subject: [concurrency-interest] How to schedule important periodic task.
In-Reply-To: <AANLkTikACnbsgy8XWj46-npzf-0DMP3talO-tvRa_eTr@mail.gmail.com>
References: <AANLkTikACnbsgy8XWj46-npzf-0DMP3talO-tvRa_eTr@mail.gmail.com>
Message-ID: <AANLkTim1ZsySmc8BJYT84mUlMqQTIB6H-bAFAZCqZqVW@mail.gmail.com>

The javadoc says:

"If any execution of the task encounters an exception, subsequent executions
are suppressed."

This is to protect everyone from aberrant tasks repeatedly running amok.

If you want your task to be rescheduled, catch and handle all exceptions.
 (Don't re-throw.)

Joe

On Fri, Jul 2, 2010 at 1:17 PM, Enno Shioji wrote:

> I used to believe that the following idiom gives protection against
> thread death. Namely, that a new  thread is instantiated and that the
> task  continues to be executed in case of RuntimeException etc.
>
>
>        ScheduledExecutorService ses =
> Executors.newSingleThreadScheduledExecutor();
>
>        ses.scheduleAtFixedRate(new Runnable() {
>            public void run() {
>
> System.out.println("RuntimeException:"+Thread.currentThread().getName());
>                throw new RuntimeException();
>            }
>        }, 1, 1, TimeUnit.MILLISECONDS);
>
>
> However, as it says in the javadoc of ScheduledExecutorService, tasks
> are not executed anymore once a RuntimeException is encountered.
>
>
> The following idiom works, but it kinda looks hacky, so I'm wondering
> why the authors of ScheduledExecutorService chose the above behavior.
>
>        final ScheduledExecutorService exec =
> Executors.newSingleThreadScheduledExecutor();
>        Runnable dieHard = new Runnable(){
>           public void run() {
>                try{
>
> System.out.println("RuntimeException:"+Thread.currentThread().getName());
>                    throw new RuntimeException();
>                }finally{
>                    exec.schedule(this,1,TimeUnit.MILLISECONDS);
>                }
>          }
>        };
>        exec.schedule(dieHard,1,TimeUnit.MILLISECONDS);
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100702/8586d574/attachment.html>

From ariel at weisberg.ws  Fri Jul  2 16:56:17 2010
From: ariel at weisberg.ws (Ariel Weisberg)
Date: Fri, 02 Jul 2010 16:56:17 -0400
Subject: [concurrency-interest] How to schedule important periodic task.
In-Reply-To: <AANLkTikACnbsgy8XWj46-npzf-0DMP3talO-tvRa_eTr@mail.gmail.com>
References: <AANLkTikACnbsgy8XWj46-npzf-0DMP3talO-tvRa_eTr@mail.gmail.com>
Message-ID: <1278104177.15778.1383053447@webmail.messagingengine.com>

Hi Enno,

I'll go out on a limb and say that it is because a task that throws a
RuntimeException is indicating that it is broken. If the task can fail
in a way that can be handled then it should throw a checked Exception
which can be caught and logged by top level task code. I believe that
other thread pool executors (created via java.util.concurrent.Executors)
will allow the thread running a task to terminate from a
RuntimeException, but they will spawn replacement threads. The failed
task will not be retried in that case either which seems consistent.

Regards,
Ariel Weisberg

On Sat, 03 Jul 2010 01:47 +0530, "Enno Shioji" <eshioji at gmail.com>
wrote:
> I used to believe that the following idiom gives protection against
> thread death. Namely, that a new  thread is instantiated and that the
> task  continues to be executed in case of RuntimeException etc.
> 
> 
>         ScheduledExecutorService ses =
> Executors.newSingleThreadScheduledExecutor();
> 
>         ses.scheduleAtFixedRate(new Runnable() {
>             public void run() {
> 
> System.out.println("RuntimeException:"+Thread.currentThread().getName());
>                 throw new RuntimeException();
>             }
>         }, 1, 1, TimeUnit.MILLISECONDS);
> 
> 
> However, as it says in the javadoc of ScheduledExecutorService, tasks
> are not executed anymore once a RuntimeException is encountered.
> 
> 
> The following idiom works, but it kinda looks hacky, so I'm wondering
> why the authors of ScheduledExecutorService chose the above behavior.
> 
>         final ScheduledExecutorService exec =
> Executors.newSingleThreadScheduledExecutor();
>         Runnable dieHard = new Runnable(){
>            public void run() {
>                 try{
> 
> System.out.println("RuntimeException:"+Thread.currentThread().getName());
>                     throw new RuntimeException();
>                 }finally{
>                     exec.schedule(this,1,TimeUnit.MILLISECONDS);
>                 }
>           }
>         };
>         exec.schedule(dieHard,1,TimeUnit.MILLISECONDS);
> 
> 
> 
> 
> Regards,
> Enno
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 

From eshioji at gmail.com  Fri Jul  2 17:19:38 2010
From: eshioji at gmail.com (Enno Shioji)
Date: Sat, 3 Jul 2010 02:49:38 +0530
Subject: [concurrency-interest] How to schedule important periodic task.
In-Reply-To: <1278104177.15778.1383053447@webmail.messagingengine.com>
References: <AANLkTikACnbsgy8XWj46-npzf-0DMP3talO-tvRa_eTr@mail.gmail.com>
	<1278104177.15778.1383053447@webmail.messagingengine.com>
Message-ID: <AANLkTilYeYcCjQnb33rPush0BtHCYVHJuOEo30tmNta5@mail.gmail.com>

Hi Ariel,

I see the logic, but consider the following situation (which happened
to me a few days ago..)

I had a scheduledExecutor that was de-queueing from a message queue.
The client posted mistakenly a "toxic message" that caused a
RuntimeException, and the de-queueing stopped (and it was a big
problem). From the app. perspective, it had been much better if that
was simply logged and the app. continued to process healthy messages.
(Maybe I should have checked the message for this, but it's hard to
foresee all possible illegal states)

I see that RuntimeExceptions are not supposed to be thrown, but in
production, it's often better to log and keep going, right?

From ariel at weisberg.ws  Sat Jul  3 16:55:28 2010
From: ariel at weisberg.ws (Ariel Weisberg)
Date: Sat, 03 Jul 2010 16:55:28 -0400
Subject: [concurrency-interest] How to schedule important periodic task.
In-Reply-To: <AANLkTilYeYcCjQnb33rPush0BtHCYVHJuOEo30tmNta5@mail.gmail.com>
References: <AANLkTikACnbsgy8XWj46-npzf-0DMP3talO-tvRa_eTr@mail.gmail.com><1278104177.15778.1383053447@webmail.messagingengine.com>
	<AANLkTilYeYcCjQnb33rPush0BtHCYVHJuOEo30tmNta5@mail.gmail.com>
Message-ID: <1278190528.4281.1383158413@webmail.messagingengine.com>

Hi Enno,

The way to achieve that is to supply your own ThreadFactory which
creates threads that catch and log any exceptions thrown by
task/Executor code. I am guessing that you still need to terminate the
thread when you catch an exception because the Executor has already seen
the thread leave the run method and is probably expecting to replace it.
I have not checked the code but I assume there is a try finally in
there. If you want to prevent the task from not being scheduled you
probably need to place the try catch in each task because allowing the
task to throw a RuntimeException is what causes it to not be scheduled.

Regards,
Ariel Weisberg

On Sat, 03 Jul 2010 02:49 +0530, "Enno Shioji" <eshioji at gmail.com>
wrote:
> Hi Ariel,
> 
> I see the logic, but consider the following situation (which happened
> to me a few days ago..)
> 
> I had a scheduledExecutor that was de-queueing from a message queue.
> The client posted mistakenly a "toxic message" that caused a
> RuntimeException, and the de-queueing stopped (and it was a big
> problem). From the app. perspective, it had been much better if that
> was simply logged and the app. continued to process healthy messages.
> (Maybe I should have checked the message for this, but it's hard to
> foresee all possible illegal states)
> 
> I see that RuntimeExceptions are not supposed to be thrown, but in
> production, it's often better to log and keep going, right?
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 

From eshioji at gmail.com  Sun Jul  4 06:24:19 2010
From: eshioji at gmail.com (Enno Shioji)
Date: Sun, 4 Jul 2010 19:24:19 +0900
Subject: [concurrency-interest] How to schedule important periodic task.
In-Reply-To: <1278190528.4281.1383158413@webmail.messagingengine.com>
References: <AANLkTikACnbsgy8XWj46-npzf-0DMP3talO-tvRa_eTr@mail.gmail.com>
	<1278104177.15778.1383053447@webmail.messagingengine.com>
	<AANLkTilYeYcCjQnb33rPush0BtHCYVHJuOEo30tmNta5@mail.gmail.com>
	<1278190528.4281.1383158413@webmail.messagingengine.com>
Message-ID: <AANLkTilkRY57_IKIhyrupjrhkeE92l6EyozAUzljOBSh@mail.gmail.com>

Hi Ariel, Joe,


Thanks for your answers. Duh, I could have caught everything..

It's just that I thought I read somewhere that
ScheduledExecutorService is an improvement over TimerTask, in that it
can reschedule in case of thread death, which was not consistent with
the behavior.

I guess I misread something..


Regards,
Enno








On Sun, Jul 4, 2010 at 5:55 AM, Ariel Weisberg <ariel at weisberg.ws> wrote:
> Hi Enno,
>
> The way to achieve that is to supply your own ThreadFactory which
> creates threads that catch and log any exceptions thrown by
> task/Executor code. I am guessing that you still need to terminate the
> thread when you catch an exception because the Executor has already seen
> the thread leave the run method and is probably expecting to replace it.
> I have not checked the code but I assume there is a try finally in
> there. If you want to prevent the task from not being scheduled you
> probably need to place the try catch in each task because allowing the
> task to throw a RuntimeException is what causes it to not be scheduled.
>
> Regards,
> Ariel Weisberg
>
> On Sat, 03 Jul 2010 02:49 +0530, "Enno Shioji" <eshioji at gmail.com>
> wrote:
>> Hi Ariel,
>>
>> I see the logic, but consider the following situation (which happened
>> to me a few days ago..)
>>
>> I had a scheduledExecutor that was de-queueing from a message queue.
>> The client posted mistakenly a "toxic message" that caused a
>> RuntimeException, and the de-queueing stopped (and it was a big
>> problem). From the app. perspective, it had been much better if that
>> was simply logged and the app. continued to process healthy messages.
>> (Maybe I should have checked the message for this, but it's hard to
>> foresee all possible illegal states)
>>
>> I see that RuntimeExceptions are not supposed to be thrown, but in
>> production, it's often better to log and keep going, right?
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>

From adrian.tarau at gmail.com  Sun Jul  4 09:40:40 2010
From: adrian.tarau at gmail.com (Adrian Tarau)
Date: Sun, 04 Jul 2010 09:40:40 -0400
Subject: [concurrency-interest] How to schedule important periodic task.
In-Reply-To: <AANLkTilkRY57_IKIhyrupjrhkeE92l6EyozAUzljOBSh@mail.gmail.com>
References: <AANLkTikACnbsgy8XWj46-npzf-0DMP3talO-tvRa_eTr@mail.gmail.com>	<1278104177.15778.1383053447@webmail.messagingengine.com>	<AANLkTilYeYcCjQnb33rPush0BtHCYVHJuOEo30tmNta5@mail.gmail.com>	<1278190528.4281.1383158413@webmail.messagingengine.com>
	<AANLkTilkRY57_IKIhyrupjrhkeE92l6EyozAUzljOBSh@mail.gmail.com>
Message-ID: <4C308F58.80302@gmail.com>

I had this problem myself and this is how I "solve" it - actually I 
wanted to have also a way to skip executing the job is one with the same 
class is already in execution(not sure if is possible with 
ScheduledExecutorService but when I looked back then I couldn't find a 
way to do it).

So I married a regular Timer with an Executor and the Worker (similar 
with Callable) is wrapped into a TimerTask. At schedule time I check if 
is not already running or allows concurrent executions and if condition 
holds I push it to an Executor. Even if the Executor decides to kill the 
thread, the scheduled job keeps running(will be rescheduled) until you 
cancel it.

It works well for me, you can find the source code here : 
http://svn.adrian.tarau.org/svn/intellicommons/trunk/worker/src/main/java/com/intellisoftsystems/commons/worker.

Best regards,
Adrian Tarau

On 07/04/2010 06:24 AM, Enno Shioji wrote:
> Hi Ariel, Joe,
>
>
> Thanks for your answers. Duh, I could have caught everything..
>
> It's just that I thought I read somewhere that
> ScheduledExecutorService is an improvement over TimerTask, in that it
> can reschedule in case of thread death, which was not consistent with
> the behavior.
>
> I guess I misread something..
>
>
> Regards,
> Enno
>
>
>
>
>
>
>
>
> On Sun, Jul 4, 2010 at 5:55 AM, Ariel Weisberg<ariel at weisberg.ws>  wrote:
>    
>> Hi Enno,
>>
>> The way to achieve that is to supply your own ThreadFactory which
>> creates threads that catch and log any exceptions thrown by
>> task/Executor code. I am guessing that you still need to terminate the
>> thread when you catch an exception because the Executor has already seen
>> the thread leave the run method and is probably expecting to replace it.
>> I have not checked the code but I assume there is a try finally in
>> there. If you want to prevent the task from not being scheduled you
>> probably need to place the try catch in each task because allowing the
>> task to throw a RuntimeException is what causes it to not be scheduled.
>>
>> Regards,
>> Ariel Weisberg
>>
>> On Sat, 03 Jul 2010 02:49 +0530, "Enno Shioji"<eshioji at gmail.com>
>> wrote:
>>      
>>> Hi Ariel,
>>>
>>> I see the logic, but consider the following situation (which happened
>>> to me a few days ago..)
>>>
>>> I had a scheduledExecutor that was de-queueing from a message queue.
>>> The client posted mistakenly a "toxic message" that caused a
>>> RuntimeException, and the de-queueing stopped (and it was a big
>>> problem). From the app. perspective, it had been much better if that
>>> was simply logged and the app. continued to process healthy messages.
>>> (Maybe I should have checked the message for this, but it's hard to
>>> foresee all possible illegal states)
>>>
>>> I see that RuntimeExceptions are not supposed to be thrown, but in
>>> production, it's often better to log and keep going, right?
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>        
>>      
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>    


From ariel at weisberg.ws  Sun Jul  4 11:15:58 2010
From: ariel at weisberg.ws (Ariel Weisberg)
Date: Sun, 04 Jul 2010 11:15:58 -0400
Subject: [concurrency-interest] How to schedule important periodic task.
In-Reply-To: <AANLkTilkRY57_IKIhyrupjrhkeE92l6EyozAUzljOBSh@mail.gmail.com>
References: <AANLkTikACnbsgy8XWj46-npzf-0DMP3talO-tvRa_eTr@mail.gmail.com><1278104177.15778.1383053447@webmail.messagingengine.com><AANLkTilYeYcCjQnb33rPush0BtHCYVHJuOEo30tmNta5@mail.gmail.com><1278190528.4281.1383158413@webmail.messagingengine.com>
	<AANLkTilkRY57_IKIhyrupjrhkeE92l6EyozAUzljOBSh@mail.gmail.com>
Message-ID: <1278256558.19680.1383227135@webmail.messagingengine.com>

Hi Enno,

It is an improvement in the sense that it will spawn a new thread (or
catch the Exception) and not die like a Timer so your other scheduled
tasks remain unaffected. What to do with a task after it throws an
unchecked Exception is matter of policy and Executors are more about
providing a mechanism with which you can impose your own policy. If a
task throws it might be leaking file descriptors, pooled database
connections or ByteBuffers etc. so the default behavior of not
rescheduling seems reasonable to me. If the Executor runs the task again
and it throws a 2nd time should it continue to schedule it?

Apparently you can implement
http://java.sun.com/javase/6/docs/api/java/util/concurrent/ThreadPoolExecutor.html#afterExecute(java.lang.Runnable,%20java.lang.Throwable)
in order to customize error handling behavior. It catches Throwable
which includes Error so you probably want to crash if you can an actual
Error.

Regards,
Ariel Weisberg

On Sun, 04 Jul 2010 19:24 +0900, "Enno Shioji" <eshioji at gmail.com>
wrote:
> Hi Ariel, Joe,
> 
> 
> Thanks for your answers. Duh, I could have caught everything..
> 
> It's just that I thought I read somewhere that
> ScheduledExecutorService is an improvement over TimerTask, in that it
> can reschedule in case of thread death, which was not consistent with
> the behavior.
> 
> I guess I misread something..
> 
> 
> Regards,
> Enno
> 
> 
> 
> 
> 
> 
> 
> 
> On Sun, Jul 4, 2010 at 5:55 AM, Ariel Weisberg <ariel at weisberg.ws> wrote:
> > Hi Enno,
> >
> > The way to achieve that is to supply your own ThreadFactory which
> > creates threads that catch and log any exceptions thrown by
> > task/Executor code. I am guessing that you still need to terminate the
> > thread when you catch an exception because the Executor has already seen
> > the thread leave the run method and is probably expecting to replace it.
> > I have not checked the code but I assume there is a try finally in
> > there. If you want to prevent the task from not being scheduled you
> > probably need to place the try catch in each task because allowing the
> > task to throw a RuntimeException is what causes it to not be scheduled.
> >
> > Regards,
> > Ariel Weisberg
> >
> > On Sat, 03 Jul 2010 02:49 +0530, "Enno Shioji" <eshioji at gmail.com>
> > wrote:
> >> Hi Ariel,
> >>
> >> I see the logic, but consider the following situation (which happened
> >> to me a few days ago..)
> >>
> >> I had a scheduledExecutor that was de-queueing from a message queue.
> >> The client posted mistakenly a "toxic message" that caused a
> >> RuntimeException, and the de-queueing stopped (and it was a big
> >> problem). From the app. perspective, it had been much better if that
> >> was simply logged and the app. continued to process healthy messages.
> >> (Maybe I should have checked the message for this, but it's hard to
> >> foresee all possible illegal states)
> >>
> >> I see that RuntimeExceptions are not supposed to be thrown, but in
> >> production, it's often better to log and keep going, right?
> >> _______________________________________________
> >> Concurrency-interest mailing list
> >> Concurrency-interest at cs.oswego.edu
> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >>
> >
> 

From gregg at cytetech.com  Tue Jul  6 15:57:02 2010
From: gregg at cytetech.com (Gregg Wonderly)
Date: Tue, 06 Jul 2010 14:57:02 -0500
Subject: [concurrency-interest] How to schedule important periodic task.
In-Reply-To: <1278104177.15778.1383053447@webmail.messagingengine.com>
References: <AANLkTikACnbsgy8XWj46-npzf-0DMP3talO-tvRa_eTr@mail.gmail.com>
	<1278104177.15778.1383053447@webmail.messagingengine.com>
Message-ID: <4C338A8E.9050903@cytetech.com>

Ariel Weisberg wrote:
> I'll go out on a limb and say that it is because a task that throws a
> RuntimeException is indicating that it is broken. If the task can fail
> in a way that can be handled then it should throw a checked Exception
> which can be caught and logged by top level task code. I believe that
> other thread pool executors (created via java.util.concurrent.Executors)
> will allow the thread running a task to terminate from a
> RuntimeException, but they will spawn replacement threads. The failed
> task will not be retried in that case either which seems consistent.

One of the primary things about creating fault tolerant software, is to tolerate 
faults.  The most common RuntimeException instances thrown, are perhaps 
NullPointerException and ArrayIndexOutOfBoundsException based on my experiences.

One of the predominate things that I learned while working at AT&T Bell Labs on 
the 5ESS Switch Software development, is that software will always have bugs. 
It is impossible to make bug free software, especially in environments where 
inputs are not in your control, and where the environment can change dynamically 
(cards failing, unplugged, plugged in etc).

Any software library that calls out to software which it is not in charge of, 
needs to protect itself from faults, recurring and otherwise.  It needs to 
continue to provide the service that it is designed to provide, so that it's 
behavior doesn't control or redirect the behavior of the faulting software. 
This makes diagnoses so much easier, because you don't wonder if the repeating 
algorithm is somehow broken and this is why something is not occurring. 
Instead, you will see the "fault" reoccurring as that software is executing.

There may be some situations where you'd like to create different behavior over 
time, such as retrying something at a slower pace.  But, I really thing that 
software like STPE should persist with the schedule requested and not try and 
"save the system", which is not something in its domain of expertise.

Also, when you call out to "listeners", you need to use try{}catch{} to make 
sure that any one listener fault doesn't keep you from notifying all listeners. 
  It doesn't make sense for the behavior of software to be controlled by faults 
in external code when the two "modules" are unrelated.  Utility software should 
just provide the function needed and let the user shape their system around 
fault handling and control themselves.

It is always way more beneficial to the user when an exception is logged then 
when a thread ceases execution, silently.

Gregg Wonderly

From gregg at cytetech.com  Tue Jul  6 16:21:13 2010
From: gregg at cytetech.com (Gregg Wonderly)
Date: Tue, 06 Jul 2010 15:21:13 -0500
Subject: [concurrency-interest] How to schedule important periodic task.
In-Reply-To: <1278256558.19680.1383227135@webmail.messagingengine.com>
References: <AANLkTikACnbsgy8XWj46-npzf-0DMP3talO-tvRa_eTr@mail.gmail.com>
	<1278104177.15778.1383053447@webmail.messagingengine.com>
	<AANLkTilYeYcCjQnb33rPush0BtHCYVHJuOEo30tmNta5@mail.gmail.com>
	<1278190528.4281.1383158413@webmail.messagingengine.com>
	<AANLkTilkRY57_IKIhyrupjrhkeE92l6EyozAUzljOBSh@mail.gmail.com>
	<1278256558.19680.1383227135@webmail.messagingengine.com>
Message-ID: <4C339039.5030204@cytetech.com>

Ariel Weisberg wrote:
> Hi Enno,
> 
> It is an improvement in the sense that it will spawn a new thread (or
> catch the Exception) and not die like a Timer so your other scheduled
> tasks remain unaffected. What to do with a task after it throws an
> unchecked Exception is matter of policy and Executors are more about
> providing a mechanism with which you can impose your own policy. If a
> task throws it might be leaking file descriptors, pooled database
> connections or ByteBuffers etc. so the default behavior of not
> rescheduling seems reasonable to me. If the Executor runs the task again
> and it throws a 2nd time should it continue to schedule it?

Yes, you should continue to schedule it.  STPE has no domain over fault 
protection, resolution or handling.  Additionally, as I already said in my 
earlier post, anything that calls into an object that is foreign to it's domain 
should always have

try {
     object.doSomething(...);
} catch( RuntimeException ex ) {
     log.log( Level.SEVERE, ex.toString(), ex );
}

as a general mechanism in that call.  It's just the most useful structure of 
control to use because it is completely informative about where the problem was 
encountered, and it protects the containing code from having its behavior 
altered by the presence of bugs in other code.

Yes, we can all stand around and point fingers and say "not my problem", but we 
are talking about core library behaviors that should be as beneficial to all as 
possible.

If you stop providing the features of your API because of a bug in another piece 
of software, it will cause a quadratic expansion in the number of places and 
combinations of problems that a user has to "diagnose" before they can discover 
and fix the true bug in their software.

Gregg Wonderly

From ariel at weisberg.ws  Tue Jul  6 17:35:32 2010
From: ariel at weisberg.ws (Ariel Weisberg)
Date: Tue, 06 Jul 2010 17:35:32 -0400
Subject: [concurrency-interest] How to schedule important periodic task.
In-Reply-To: <4C339039.5030204@cytetech.com>
References: <AANLkTikACnbsgy8XWj46-npzf-0DMP3talO-tvRa_eTr@mail.gmail.com>
	<1278104177.15778.1383053447@webmail.messagingengine.com>
	<AANLkTilYeYcCjQnb33rPush0BtHCYVHJuOEo30tmNta5@mail.gmail.com>
	<1278190528.4281.1383158413@webmail.messagingengine.com>
	<AANLkTilkRY57_IKIhyrupjrhkeE92l6EyozAUzljOBSh@mail.gmail.com>
	<1278256558.19680.1383227135@webmail.messagingengine.com>
	<4C339039.5030204@cytetech.com>
Message-ID: <1278452132.9776.1383596933@webmail.messagingengine.com>

Hi Gregg,

I think we are for the most part agreeing, especially WRT to catching
and logging exceptions rather then leaking them. A better and more
explicit (more than afterExecute) method of handling uncaught exceptions
in STPE would be nice. Something like Thread.UncaughtExceptionHandler
that returns a boolean indicating whether the task should be rescheduled
(save some boilerplate caching scheduling info) and that can be
associated with an individual task or be a default for all tasks. Why
the default uncaught exception policy is not equivalent to Thread's
default uncaught exception handler is an interesting question. The
verbosity might encourage developers to provide their own.

Saying that STPE has no domain over fault protection, resolution, or
handling doesn't make a whole lot of sense considering that it exists to
schedule and execute tasks while protecting them from each other (unlike
Timer). Like it or not it has to implement a policy of rescheduling (or
not) and there are people who will argue for either approach. I think
what one prefers has more to do with the type of software you are
accustomed to developing (fail hard and fast vs. pacemakers).
Rescheduling a silently failed task can turn a show stopper that shows
up in development into a transient nigh impossible to debug daily,
monthly, or yearly bug.

If I had my way you wouldn't be able to instantiate an TPE/STPE without
providing an uncaught exception handler. -Werror, -Wall, -Wextra, belt,
and suspenders.

Regards,
Ariel Weisberg

On Tue, 06 Jul 2010 15:21 -0500, "Gregg Wonderly" <gregg at cytetech.com>
wrote:
> Ariel Weisberg wrote:
> > Hi Enno,
> > 
> > It is an improvement in the sense that it will spawn a new thread (or
> > catch the Exception) and not die like a Timer so your other scheduled
> > tasks remain unaffected. What to do with a task after it throws an
> > unchecked Exception is matter of policy and Executors are more about
> > providing a mechanism with which you can impose your own policy. If a
> > task throws it might be leaking file descriptors, pooled database
> > connections or ByteBuffers etc. so the default behavior of not
> > rescheduling seems reasonable to me. If the Executor runs the task again
> > and it throws a 2nd time should it continue to schedule it?
> 
> Yes, you should continue to schedule it.  STPE has no domain over fault 
> protection, resolution or handling.  Additionally, as I already said in
> my 
> earlier post, anything that calls into an object that is foreign to it's
> domain 
> should always have
> 
> try {
>      object.doSomething(...);
> } catch( RuntimeException ex ) {
>      log.log( Level.SEVERE, ex.toString(), ex );
> }
> 
> as a general mechanism in that call.  It's just the most useful structure
> of 
> control to use because it is completely informative about where the
> problem was 
> encountered, and it protects the containing code from having its behavior 
> altered by the presence of bugs in other code.
> 
> Yes, we can all stand around and point fingers and say "not my problem",
> but we 
> are talking about core library behaviors that should be as beneficial to
> all as 
> possible.
> 
> If you stop providing the features of your API because of a bug in
> another piece 
> of software, it will cause a quadratic expansion in the number of places
> and 
> combinations of problems that a user has to "diagnose" before they can
> discover 
> and fix the true bug in their software.
> 
> Gregg Wonderly
> 

From gregg at cytetech.com  Tue Jul  6 17:48:10 2010
From: gregg at cytetech.com (Gregg Wonderly)
Date: Tue, 06 Jul 2010 16:48:10 -0500
Subject: [concurrency-interest] How to schedule important periodic task.
In-Reply-To: <1278452132.9776.1383596933@webmail.messagingengine.com>
References: <AANLkTikACnbsgy8XWj46-npzf-0DMP3talO-tvRa_eTr@mail.gmail.com>
	<1278104177.15778.1383053447@webmail.messagingengine.com>
	<AANLkTilYeYcCjQnb33rPush0BtHCYVHJuOEo30tmNta5@mail.gmail.com>
	<1278190528.4281.1383158413@webmail.messagingengine.com>
	<AANLkTilkRY57_IKIhyrupjrhkeE92l6EyozAUzljOBSh@mail.gmail.com>
	<1278256558.19680.1383227135@webmail.messagingengine.com>
	<4C339039.5030204@cytetech.com>
	<1278452132.9776.1383596933@webmail.messagingengine.com>
Message-ID: <4C33A49A.7060206@cytetech.com>

RuntimeException makes runtime errors seem like non-recoverable bugs in many 
peoples books.  I consider them transient error conditions, which through 
outside stimulations can be remedied just like outside stimulations can cause 
them to appear.  When you stop a thread in running software, without controls to 
restart it, things go down hill, often, just as fast as you might imagine they 
would in the worst situations if execution continued.

Good or bad, right or wrong, stopping something that doesn't have a bug (the 
repeated execution of a block of code on a scheduled time interval) is not any 
better of a solution.  It makes a visible bug "disappear" which can make other 
bugs (lack of expected activity on a schedule) "appear".  This is highly 
problematic as a general behavior of a focused task.

Much higher level software should be in charge of system sanity.  If you really 
care about stability, you will have an auditing subsystem that can detect 
unexpected states and manage those states to recover the system.  Alarming 
modules can also signal, externally, that something unexpected is happening.

In the end, STPE works the way it works, so once you discover this "feature", 
you can then plan how you will recover from such problems.

It seems that STPE authors considered RuntimeException to be a fatal flaw. I 
think that silently failing is a real problem.  Given the halt in system 
behavior that this causes, they should of just called System.exit() so that it 
was clear that things would no longer function it seems to me.

Gregg Wonderly

Ariel Weisberg wrote:
> Hi Gregg,
> 
> I think we are for the most part agreeing, especially WRT to catching
> and logging exceptions rather then leaking them. A better and more
> explicit (more than afterExecute) method of handling uncaught exceptions
> in STPE would be nice. Something like Thread.UncaughtExceptionHandler
> that returns a boolean indicating whether the task should be rescheduled
> (save some boilerplate caching scheduling info) and that can be
> associated with an individual task or be a default for all tasks. Why
> the default uncaught exception policy is not equivalent to Thread's
> default uncaught exception handler is an interesting question. The
> verbosity might encourage developers to provide their own.
> 
> Saying that STPE has no domain over fault protection, resolution, or
> handling doesn't make a whole lot of sense considering that it exists to
> schedule and execute tasks while protecting them from each other (unlike
> Timer). Like it or not it has to implement a policy of rescheduling (or
> not) and there are people who will argue for either approach. I think
> what one prefers has more to do with the type of software you are
> accustomed to developing (fail hard and fast vs. pacemakers).
> Rescheduling a silently failed task can turn a show stopper that shows
> up in development into a transient nigh impossible to debug daily,
> monthly, or yearly bug.
> 
> If I had my way you wouldn't be able to instantiate an TPE/STPE without
> providing an uncaught exception handler. -Werror, -Wall, -Wextra, belt,
> and suspenders.
> 
> Regards,
> Ariel Weisberg
> 
> On Tue, 06 Jul 2010 15:21 -0500, "Gregg Wonderly" <gregg at cytetech.com>
> wrote:
>> Ariel Weisberg wrote:
>>> Hi Enno,
>>>
>>> It is an improvement in the sense that it will spawn a new thread (or
>>> catch the Exception) and not die like a Timer so your other scheduled
>>> tasks remain unaffected. What to do with a task after it throws an
>>> unchecked Exception is matter of policy and Executors are more about
>>> providing a mechanism with which you can impose your own policy. If a
>>> task throws it might be leaking file descriptors, pooled database
>>> connections or ByteBuffers etc. so the default behavior of not
>>> rescheduling seems reasonable to me. If the Executor runs the task again
>>> and it throws a 2nd time should it continue to schedule it?
>> Yes, you should continue to schedule it.  STPE has no domain over fault 
>> protection, resolution or handling.  Additionally, as I already said in
>> my 
>> earlier post, anything that calls into an object that is foreign to it's
>> domain 
>> should always have
>>
>> try {
>>      object.doSomething(...);
>> } catch( RuntimeException ex ) {
>>      log.log( Level.SEVERE, ex.toString(), ex );
>> }
>>
>> as a general mechanism in that call.  It's just the most useful structure
>> of 
>> control to use because it is completely informative about where the
>> problem was 
>> encountered, and it protects the containing code from having its behavior 
>> altered by the presence of bugs in other code.
>>
>> Yes, we can all stand around and point fingers and say "not my problem",
>> but we 
>> are talking about core library behaviors that should be as beneficial to
>> all as 
>> possible.
>>
>> If you stop providing the features of your API because of a bug in
>> another piece 
>> of software, it will cause a quadratic expansion in the number of places
>> and 
>> combinations of problems that a user has to "diagnose" before they can
>> discover 
>> and fix the true bug in their software.
>>
>> Gregg Wonderly
>>
> 
> 


From vgrazi at gmail.com  Tue Jul  6 20:03:19 2010
From: vgrazi at gmail.com (Victor Grazi)
Date: Tue, 6 Jul 2010 20:03:19 -0400
Subject: [concurrency-interest] Inconsistent behavior when running a
	ReentrantReadWriteLock between Java 5 and Java 6.
Message-ID: <AANLkTilaPpHfX4nXeifqXWcs31uOPLJUFI-ykwCYb72z@mail.gmail.com>

It seems that there is inconsistent behavior when running a
ReentrantReadWriteLock between Java 5 and Java 6.
The case occurs when a reader has the lock, a writer is waiting, and a new
reader tries to grab the lock.

In Java 5, the new reader can get in.
In Java 6, the new reader is blocked.

I noticed this while reviewing the Java Concurrent Animated (
https://sourceforge.net/projects/javaconcurrenta/) animation for
ReadWriteLock with Dr. Heinz Kabutz.

Heinz predicted that in that scenario the new reader would get in. I
predicted it would block. Turns out we were both right, depending on the
JDK.

Is this a feature or a bug? In any case it is a major difference between the
2 JDK?s, and can cause unpredicted results when migrating existing programs.

Regards, Victor Grazi

Here are the screen shots? (blue arrows are readers, red arrows are writers,
newer threads appear beneath older threads.)

Java 5

[image: cid:image005.jpg at 01CB1C7D.6460B420]



                                                Java 6

[image: cid:image006.jpg at 01CB1C7D.6460B420]
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100706/4c4818bf/attachment-0001.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: image/jpeg
Size: 5484 bytes
Desc: not available
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100706/4c4818bf/attachment-0002.jpe>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: image/jpeg
Size: 5214 bytes
Desc: not available
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100706/4c4818bf/attachment-0003.jpe>

From davidcholmes at aapt.net.au  Tue Jul  6 20:19:44 2010
From: davidcholmes at aapt.net.au (David Holmes)
Date: Wed, 7 Jul 2010 10:19:44 +1000
Subject: [concurrency-interest] Inconsistent behavior when running
	aReentrantReadWriteLock between Java 5 and Java 6.
In-Reply-To: <AANLkTilaPpHfX4nXeifqXWcs31uOPLJUFI-ykwCYb72z@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCAECCIHAA.davidcholmes@aapt.net.au>

There were problems with the spec and implementation for RRWL in Java 5 that
were fixed in Java 6. In Java 6 the implementation should be consistent with
the expected behaviour defined by the spec.

In general you want the reader to block to prevent indefinite postponement
of writers. This matches the expected use-case of many readers but few
writers.

There's a lot of discussion in various bug reports and also in the archives
of this list, but I don't have any direct pointers handy.

David Holmes
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Victor Grazi
  Sent: Wednesday, 7 July 2010 10:03 AM
  To: concurrency-interest at cs.oswego.edu
  Subject: [concurrency-interest] Inconsistent behavior when running
aReentrantReadWriteLock between Java 5 and Java 6.



  It seems that there is inconsistent behavior when running a
ReentrantReadWriteLock between Java 5 and Java 6.
  The case occurs when a reader has the lock, a writer is waiting, and a new
reader tries to grab the lock.

  In Java 5, the new reader can get in.
  In Java 6, the new reader is blocked.

  I noticed this while reviewing the Java Concurrent Animated
(https://sourceforge.net/projects/javaconcurrenta/) animation for
ReadWriteLock with Dr. Heinz Kabutz.

  Heinz predicted that in that scenario the new reader would get in. I
predicted it would block. Turns out we were both right, depending on the
JDK.

  Is this a feature or a bug? In any case it is a major difference between
the 2 JDK?s, and can cause unpredicted results when migrating existing
programs.

  Regards, Victor Grazi

  Here are the screen shots? (blue arrows are readers, red arrows are
writers, newer threads appear beneath older threads.)

  Java 5







                                                  Java 6






-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100707/862fd78f/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: image/jpeg
Size: 5214 bytes
Desc: not available
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100707/862fd78f/attachment.jpe>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: image/jpeg
Size: 5484 bytes
Desc: not available
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100707/862fd78f/attachment-0001.jpe>

From joe.bowbeer at gmail.com  Tue Jul  6 20:47:11 2010
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Tue, 6 Jul 2010 17:47:11 -0700
Subject: [concurrency-interest] Inconsistent behavior when running
	aReentrantReadWriteLock between Java 5 and Java 6.
In-Reply-To: <NFBBKALFDCPFIDBNKAPCAECCIHAA.davidcholmes@aapt.net.au>
References: <AANLkTilaPpHfX4nXeifqXWcs31uOPLJUFI-ykwCYb72z@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCAECCIHAA.davidcholmes@aapt.net.au>
Message-ID: <AANLkTimUkiadO_Nav4uL5KbCUM8dGMUKONc2VOvza5Wq@mail.gmail.com>

These differences are observed in fair mode, right?

The fair mode description changed a lot between 5 and 6, including the
sentence:

"A thread that tries to acquire a fair read lock (non-reentrantly) will
block if either the write lock is held, or there is a waiting writer
thread."

Joe

On Tue, Jul 6, 2010 at 5:19 PM, David Holmes wrote:

>  There were problems with the spec and implementation for RRWL in Java 5
> that were fixed in Java 6. In Java 6 the implementation should be consistent
> with the expected behaviour defined by the spec.
>
> In general you want the reader to block to prevent indefinite postponement
> of writers. This matches the expected use-case of many readers but few
> writers.
>
> There's a lot of discussion in various bug reports and also in the archives
> of this list, but I don't have any direct pointers handy.
>
> David Holmes
>
> -----Original Message-----
> *From:* <concurrency-interest-bounces at cs.oswego.edu>Victor Grazi
> *Sent:* Wednesday, 7 July 2010 10:03 AM
> *To:* concurrency-interest at cs.oswego.edu
> *Subject:* [concurrency-interest] Inconsistent behavior when running
> aReentrantReadWriteLock between Java 5 and Java 6.
>
>   It seems that there is inconsistent behavior when running a
> ReentrantReadWriteLock between Java 5 and Java 6.
> The case occurs when a reader has the lock, a writer is waiting, and a new
> reader tries to grab the lock.
>
> In Java 5, the new reader can get in.
> In Java 6, the new reader is blocked.
>
> I noticed this while reviewing the Java Concurrent Animated (
> https://sourceforge.net/projects/javaconcurrenta/) animation for
> ReadWriteLock with Dr. Heinz Kabutz.
>
> Heinz predicted that in that scenario the new reader would get in. I
> predicted it would block. Turns out we were both right, depending on the
> JDK.
>
> Is this a feature or a bug? In any case it is a major difference between
> the 2 JDK?s, and can cause unpredicted results when migrating existing
> programs.
>
> Regards, Victor Grazi
>
> Here are the screen shots? (blue arrows are readers, red arrows are
> writers, newer threads appear beneath older threads.)
>
> Java 5
>
> [image: cid:image005.jpg at 01CB1C7D.6460B420]
>
>
>
>                                                 Java 6
>
> [image: cid:image006.jpg at 01CB1C7D.6460B420]
>
>
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100706/bc5f2907/attachment-0001.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: image/jpeg
Size: 5484 bytes
Desc: not available
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100706/bc5f2907/attachment-0002.jpe>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: image/jpeg
Size: 5214 bytes
Desc: not available
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100706/bc5f2907/attachment-0003.jpe>

From vgrazi at gmail.com  Tue Jul  6 20:54:00 2010
From: vgrazi at gmail.com (Victor Grazi)
Date: Tue, 6 Jul 2010 20:54:00 -0400
Subject: [concurrency-interest] Inconsistent behavior when running
	aReentrantReadWriteLock between Java 5 and Java 6.
In-Reply-To: <NFBBKALFDCPFIDBNKAPCAECCIHAA.davidcholmes@aapt.net.au>
References: <AANLkTilaPpHfX4nXeifqXWcs31uOPLJUFI-ykwCYb72z@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCAECCIHAA.davidcholmes@aapt.net.au>
Message-ID: <AANLkTilhrq7_uW1L50_JGS7uKIeHB3SdyDLK0_T0HP6I@mail.gmail.com>

Interesting. That explains it and it makes good sense.

Thanks Victor

On Jul 6, 2010 8:19 PM, "David Holmes" <davidcholmes at aapt.net.au> wrote:

 There were problems with the spec and implementation for RRWL in Java 5
that were fixed in Java 6. In Java 6 the implementation should be consistent
with the expected behaviour defined by the spec.

In general you want the reader to block to prevent indefinite postponement
of writers. This matches the expected use-case of many readers but few
writers.

There's a lot of discussion in various bug reports and also in the archives
of this list, but I don't have any direct pointers handy.

David Holmes


>
> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurren...
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100706/34024d9d/attachment.html>

From joe.bowbeer at gmail.com  Tue Jul  6 20:57:26 2010
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Tue, 6 Jul 2010 17:57:26 -0700
Subject: [concurrency-interest] Inconsistent behavior when running
	aReentrantReadWriteLock between Java 5 and Java 6.
In-Reply-To: <AANLkTimUkiadO_Nav4uL5KbCUM8dGMUKONc2VOvza5Wq@mail.gmail.com>
References: <AANLkTilaPpHfX4nXeifqXWcs31uOPLJUFI-ykwCYb72z@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCAECCIHAA.davidcholmes@aapt.net.au>
	<AANLkTimUkiadO_Nav4uL5KbCUM8dGMUKONc2VOvza5Wq@mail.gmail.com>
Message-ID: <AANLkTikvMgAXVnQ_sQsrtEbDn3-uI2iH4o0RyueqvqUa@mail.gmail.com>

On second thought, that sentence applies to RWL in general.

Anyway, I think it would be useful to know if you're referring to fair or
non-fair mode.

On Tue, Jul 6, 2010 at 5:47 PM, Joe Bowbeer wrote:

> These differences are observed in fair mode, right?
>
> The fair mode description changed a lot between 5 and 6, including the
> sentence:
>
> "A thread that tries to acquire a fair read lock (non-reentrantly) will
> block if either the write lock is held, or there is a waiting writer
> thread."
>
> Joe
>
> On Tue, Jul 6, 2010 at 5:19 PM, David Holmes wrote:
>
>>  There were problems with the spec and implementation for RRWL in Java 5
>> that were fixed in Java 6. In Java 6 the implementation should be consistent
>> with the expected behaviour defined by the spec.
>>
>> In general you want the reader to block to prevent indefinite postponement
>> of writers. This matches the expected use-case of many readers but few
>> writers.
>>
>> There's a lot of discussion in various bug reports and also in the
>> archives of this list, but I don't have any direct pointers handy.
>>
>> David Holmes
>>
>> -----Original Message-----
>> *From:* <concurrency-interest-bounces at cs.oswego.edu>Victor Grazi
>> *Sent:* Wednesday, 7 July 2010 10:03 AM
>> *To:* concurrency-interest at cs.oswego.edu
>> *Subject:* [concurrency-interest] Inconsistent behavior when running
>> aReentrantReadWriteLock between Java 5 and Java 6.
>>
>>   It seems that there is inconsistent behavior when running a
>> ReentrantReadWriteLock between Java 5 and Java 6.
>> The case occurs when a reader has the lock, a writer is waiting, and a new
>> reader tries to grab the lock.
>>
>> In Java 5, the new reader can get in.
>> In Java 6, the new reader is blocked.
>>
>> I noticed this while reviewing the Java Concurrent Animated (
>> https://sourceforge.net/projects/javaconcurrenta/) animation for
>> ReadWriteLock with Dr. Heinz Kabutz.
>>
>> Heinz predicted that in that scenario the new reader would get in. I
>> predicted it would block. Turns out we were both right, depending on the
>> JDK.
>>
>> Is this a feature or a bug? In any case it is a major difference between
>> the 2 JDK?s, and can cause unpredicted results when migrating existing
>> programs.
>>
>> Regards, Victor Grazi
>>
>> Here are the screen shots? (blue arrows are readers, red arrows are
>> writers, newer threads appear beneath older threads.)
>>
>> Java 5
>>
>> [image: cid:image005.jpg at 01CB1C7D.6460B420]
>>
>>
>>
>>                                                 Java 6
>>
>> [image: cid:image006.jpg at 01CB1C7D.6460B420]
>>
>>
>>
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100706/8047ae0b/attachment-0001.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: image/jpeg
Size: 5484 bytes
Desc: not available
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100706/8047ae0b/attachment-0002.jpe>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: not available
Type: image/jpeg
Size: 5214 bytes
Desc: not available
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100706/8047ae0b/attachment-0003.jpe>

From davidcholmes at aapt.net.au  Wed Jul  7 03:49:52 2010
From: davidcholmes at aapt.net.au (David Holmes)
Date: Wed, 7 Jul 2010 17:49:52 +1000
Subject: [concurrency-interest] How to schedule important periodic task.
In-Reply-To: <4C33A49A.7060206@cytetech.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCEECGIHAA.davidcholmes@aapt.net.au>

I have no problem with STPE refusing to reschedule a task that throws an
exception. If the exception is truly a transient and recoverable failure
then the task is responsible for dealing with that (how could the STPE ever
know?). So if the task handles the recoverable failures by not re-throwing
the exception then the STPE will continue to schedule it. If the task throws
then STPE considers it a bad task and discards it.

I do agree however that silent failure as happens here is very bad. The
problem arises from the internal decorating using a ScheduledFutureTask.
This is a form of FutureTask and so it never allows exceptions to propogate,
instead catching them and setting the exception status. But in the case of a
periodicly scheduled task there is no synchronous interaction at which the
execution exception could be rethrown (nor the exception state explicitly
queried) so the exception remains undetectable and the failure is silent.

I don't recall how we arrived at the current situation but in the current
context it certainly seems less than desirable.

David Holmes
------------

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Gregg
> Wonderly
> Sent: Wednesday, 7 July 2010 7:48 AM
> To: Ariel Weisberg
> Cc: Concurrency-interest at cs.oswego.edu; gregg.wonderly at pobox.com
> Subject: Re: [concurrency-interest] How to schedule important periodic
> task.
>
>
> RuntimeException makes runtime errors seem like non-recoverable
> bugs in many
> peoples books.  I consider them transient error conditions, which through
> outside stimulations can be remedied just like outside
> stimulations can cause
> them to appear.  When you stop a thread in running software,
> without controls to
> restart it, things go down hill, often, just as fast as you might
> imagine they
> would in the worst situations if execution continued.
>
> Good or bad, right or wrong, stopping something that doesn't have
> a bug (the
> repeated execution of a block of code on a scheduled time
> interval) is not any
> better of a solution.  It makes a visible bug "disappear" which
> can make other
> bugs (lack of expected activity on a schedule) "appear".  This is highly
> problematic as a general behavior of a focused task.
>
> Much higher level software should be in charge of system sanity.
> If you really
> care about stability, you will have an auditing subsystem that can detect
> unexpected states and manage those states to recover the system.
> Alarming
> modules can also signal, externally, that something unexpected is
> happening.
>
> In the end, STPE works the way it works, so once you discover
> this "feature",
> you can then plan how you will recover from such problems.
>
> It seems that STPE authors considered RuntimeException to be a
> fatal flaw. I
> think that silently failing is a real problem.  Given the halt in system
> behavior that this causes, they should of just called
> System.exit() so that it
> was clear that things would no longer function it seems to me.
>
> Gregg Wonderly
>
> Ariel Weisberg wrote:
> > Hi Gregg,
> >
> > I think we are for the most part agreeing, especially WRT to catching
> > and logging exceptions rather then leaking them. A better and more
> > explicit (more than afterExecute) method of handling uncaught exceptions
> > in STPE would be nice. Something like Thread.UncaughtExceptionHandler
> > that returns a boolean indicating whether the task should be rescheduled
> > (save some boilerplate caching scheduling info) and that can be
> > associated with an individual task or be a default for all tasks. Why
> > the default uncaught exception policy is not equivalent to Thread's
> > default uncaught exception handler is an interesting question. The
> > verbosity might encourage developers to provide their own.
> >
> > Saying that STPE has no domain over fault protection, resolution, or
> > handling doesn't make a whole lot of sense considering that it exists to
> > schedule and execute tasks while protecting them from each other (unlike
> > Timer). Like it or not it has to implement a policy of rescheduling (or
> > not) and there are people who will argue for either approach. I think
> > what one prefers has more to do with the type of software you are
> > accustomed to developing (fail hard and fast vs. pacemakers).
> > Rescheduling a silently failed task can turn a show stopper that shows
> > up in development into a transient nigh impossible to debug daily,
> > monthly, or yearly bug.
> >
> > If I had my way you wouldn't be able to instantiate an TPE/STPE without
> > providing an uncaught exception handler. -Werror, -Wall, -Wextra, belt,
> > and suspenders.
> >
> > Regards,
> > Ariel Weisberg
> >
> > On Tue, 06 Jul 2010 15:21 -0500, "Gregg Wonderly" <gregg at cytetech.com>
> > wrote:
> >> Ariel Weisberg wrote:
> >>> Hi Enno,
> >>>
> >>> It is an improvement in the sense that it will spawn a new thread (or
> >>> catch the Exception) and not die like a Timer so your other scheduled
> >>> tasks remain unaffected. What to do with a task after it throws an
> >>> unchecked Exception is matter of policy and Executors are more about
> >>> providing a mechanism with which you can impose your own policy. If a
> >>> task throws it might be leaking file descriptors, pooled database
> >>> connections or ByteBuffers etc. so the default behavior of not
> >>> rescheduling seems reasonable to me. If the Executor runs the
> task again
> >>> and it throws a 2nd time should it continue to schedule it?
> >> Yes, you should continue to schedule it.  STPE has no domain
> over fault
> >> protection, resolution or handling.  Additionally, as I already said in
> >> my
> >> earlier post, anything that calls into an object that is
> foreign to it's
> >> domain
> >> should always have
> >>
> >> try {
> >>      object.doSomething(...);
> >> } catch( RuntimeException ex ) {
> >>      log.log( Level.SEVERE, ex.toString(), ex );
> >> }
> >>
> >> as a general mechanism in that call.  It's just the most
> useful structure
> >> of
> >> control to use because it is completely informative about where the
> >> problem was
> >> encountered, and it protects the containing code from having
> its behavior
> >> altered by the presence of bugs in other code.
> >>
> >> Yes, we can all stand around and point fingers and say "not my
> problem",
> >> but we
> >> are talking about core library behaviors that should be as
> beneficial to
> >> all as
> >> possible.
> >>
> >> If you stop providing the features of your API because of a bug in
> >> another piece
> >> of software, it will cause a quadratic expansion in the number
> of places
> >> and
> >> combinations of problems that a user has to "diagnose" before they can
> >> discover
> >> and fix the true bug in their software.
> >>
> >> Gregg Wonderly
> >>
> >
> >
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From gdenys at yahoo.com  Wed Jul  7 12:02:34 2010
From: gdenys at yahoo.com (Denys Geert)
Date: Wed, 7 Jul 2010 09:02:34 -0700 (PDT)
Subject: [concurrency-interest] Upcoming FJ Simplifications
In-Reply-To: <4C2A0ADD.60002@cs.oswego.edu>
References: <4C2A0ADD.60002@cs.oswego.edu>
Message-ID: <733988.83620.qm@web51404.mail.re2.yahoo.com>

Doug wrote:

> 4. Remove ForkJoinTask.helpJoin() and variants. Instead, plain join()
> conservatively, safely mixes helping- vs spare- based joins internally.

We have felt the need to use helpJoin in combination with a limited thread count to ensure all threads keep running. When I switch to a regular join (using the FJ version of 5/27) on the tasks, most of the threads start blocking after a while and only one or two threads are still doing anything useful. 

With the proposed FJ changes (specifically the removal of helpJoin), will the join be adapted in such a way that it behaves similar to the current helpJoin? What can we expect?

Thanks,
Geert


      

From tim at peierls.net  Wed Jul  7 12:23:05 2010
From: tim at peierls.net (Tim Peierls)
Date: Wed, 7 Jul 2010 12:23:05 -0400
Subject: [concurrency-interest] How to schedule important periodic task.
In-Reply-To: <NFBBKALFDCPFIDBNKAPCEECGIHAA.davidcholmes@aapt.net.au>
References: <4C33A49A.7060206@cytetech.com>
	<NFBBKALFDCPFIDBNKAPCEECGIHAA.davidcholmes@aapt.net.au>
Message-ID: <AANLkTikudFu4-jsuf5p-A5eS6y0XrSsHWJMNXRT0MdRu@mail.gmail.com>

I don't remember the details, either, but no one came up with a compelling
alternative that didn't impose extra API and overhead on everyone (including
those who could tolerate silent failure).

But the SES (STPE?) javadocs would benefit from the addition of examples of
dealing with exceptions thrown in periodic tasks.

--tim

On Wed, Jul 7, 2010 at 3:49 AM, David Holmes <davidcholmes at aapt.net.au>wrote:

> I have no problem with STPE refusing to reschedule a task that throws an
> exception. If the exception is truly a transient and recoverable failure
> then the task is responsible for dealing with that (how could the STPE ever
> know?). So if the task handles the recoverable failures by not re-throwing
> the exception then the STPE will continue to schedule it. If the task
> throws
> then STPE considers it a bad task and discards it.
>
> I do agree however that silent failure as happens here is very bad. The
> problem arises from the internal decorating using a ScheduledFutureTask.
> This is a form of FutureTask and so it never allows exceptions to
> propogate,
> instead catching them and setting the exception status. But in the case of
> a
> periodicly scheduled task there is no synchronous interaction at which the
> execution exception could be rethrown (nor the exception state explicitly
> queried) so the exception remains undetectable and the failure is silent.
>
> I don't recall how we arrived at the current situation but in the current
> context it certainly seems less than desirable.
>
> David Holmes
> ------------
>
> > -----Original Message-----
> > From: concurrency-interest-bounces at cs.oswego.edu
> > [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Gregg
> > Wonderly
> > Sent: Wednesday, 7 July 2010 7:48 AM
> > To: Ariel Weisberg
> > Cc: Concurrency-interest at cs.oswego.edu; gregg.wonderly at pobox.com
> > Subject: Re: [concurrency-interest] How to schedule important periodic
> > task.
> >
> >
> > RuntimeException makes runtime errors seem like non-recoverable
> > bugs in many
> > peoples books.  I consider them transient error conditions, which through
> > outside stimulations can be remedied just like outside
> > stimulations can cause
> > them to appear.  When you stop a thread in running software,
> > without controls to
> > restart it, things go down hill, often, just as fast as you might
> > imagine they
> > would in the worst situations if execution continued.
> >
> > Good or bad, right or wrong, stopping something that doesn't have
> > a bug (the
> > repeated execution of a block of code on a scheduled time
> > interval) is not any
> > better of a solution.  It makes a visible bug "disappear" which
> > can make other
> > bugs (lack of expected activity on a schedule) "appear".  This is highly
> > problematic as a general behavior of a focused task.
> >
> > Much higher level software should be in charge of system sanity.
> > If you really
> > care about stability, you will have an auditing subsystem that can detect
> > unexpected states and manage those states to recover the system.
> > Alarming
> > modules can also signal, externally, that something unexpected is
> > happening.
> >
> > In the end, STPE works the way it works, so once you discover
> > this "feature",
> > you can then plan how you will recover from such problems.
> >
> > It seems that STPE authors considered RuntimeException to be a
> > fatal flaw. I
> > think that silently failing is a real problem.  Given the halt in system
> > behavior that this causes, they should of just called
> > System.exit() so that it
> > was clear that things would no longer function it seems to me.
> >
> > Gregg Wonderly
> >
> > Ariel Weisberg wrote:
> > > Hi Gregg,
> > >
> > > I think we are for the most part agreeing, especially WRT to catching
> > > and logging exceptions rather then leaking them. A better and more
> > > explicit (more than afterExecute) method of handling uncaught
> exceptions
> > > in STPE would be nice. Something like Thread.UncaughtExceptionHandler
> > > that returns a boolean indicating whether the task should be
> rescheduled
> > > (save some boilerplate caching scheduling info) and that can be
> > > associated with an individual task or be a default for all tasks. Why
> > > the default uncaught exception policy is not equivalent to Thread's
> > > default uncaught exception handler is an interesting question. The
> > > verbosity might encourage developers to provide their own.
> > >
> > > Saying that STPE has no domain over fault protection, resolution, or
> > > handling doesn't make a whole lot of sense considering that it exists
> to
> > > schedule and execute tasks while protecting them from each other
> (unlike
> > > Timer). Like it or not it has to implement a policy of rescheduling (or
> > > not) and there are people who will argue for either approach. I think
> > > what one prefers has more to do with the type of software you are
> > > accustomed to developing (fail hard and fast vs. pacemakers).
> > > Rescheduling a silently failed task can turn a show stopper that shows
> > > up in development into a transient nigh impossible to debug daily,
> > > monthly, or yearly bug.
> > >
> > > If I had my way you wouldn't be able to instantiate an TPE/STPE without
> > > providing an uncaught exception handler. -Werror, -Wall, -Wextra, belt,
> > > and suspenders.
> > >
> > > Regards,
> > > Ariel Weisberg
> > >
> > > On Tue, 06 Jul 2010 15:21 -0500, "Gregg Wonderly" <gregg at cytetech.com>
> > > wrote:
> > >> Ariel Weisberg wrote:
> > >>> Hi Enno,
> > >>>
> > >>> It is an improvement in the sense that it will spawn a new thread (or
> > >>> catch the Exception) and not die like a Timer so your other scheduled
> > >>> tasks remain unaffected. What to do with a task after it throws an
> > >>> unchecked Exception is matter of policy and Executors are more about
> > >>> providing a mechanism with which you can impose your own policy. If a
> > >>> task throws it might be leaking file descriptors, pooled database
> > >>> connections or ByteBuffers etc. so the default behavior of not
> > >>> rescheduling seems reasonable to me. If the Executor runs the
> > task again
> > >>> and it throws a 2nd time should it continue to schedule it?
> > >> Yes, you should continue to schedule it.  STPE has no domain
> > over fault
> > >> protection, resolution or handling.  Additionally, as I already said
> in
> > >> my
> > >> earlier post, anything that calls into an object that is
> > foreign to it's
> > >> domain
> > >> should always have
> > >>
> > >> try {
> > >>      object.doSomething(...);
> > >> } catch( RuntimeException ex ) {
> > >>      log.log( Level.SEVERE, ex.toString(), ex );
> > >> }
> > >>
> > >> as a general mechanism in that call.  It's just the most
> > useful structure
> > >> of
> > >> control to use because it is completely informative about where the
> > >> problem was
> > >> encountered, and it protects the containing code from having
> > its behavior
> > >> altered by the presence of bugs in other code.
> > >>
> > >> Yes, we can all stand around and point fingers and say "not my
> > problem",
> > >> but we
> > >> are talking about core library behaviors that should be as
> > beneficial to
> > >> all as
> > >> possible.
> > >>
> > >> If you stop providing the features of your API because of a bug in
> > >> another piece
> > >> of software, it will cause a quadratic expansion in the number
> > of places
> > >> and
> > >> combinations of problems that a user has to "diagnose" before they can
> > >> discover
> > >> and fix the true bug in their software.
> > >>
> > >> Gregg Wonderly
> > >>
> > >
> > >
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100707/0ddc38a1/attachment.html>

From dl at cs.oswego.edu  Wed Jul  7 16:04:54 2010
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed, 07 Jul 2010 16:04:54 -0400
Subject: [concurrency-interest] Upcoming FJ Simplifications
In-Reply-To: <733988.83620.qm@web51404.mail.re2.yahoo.com>
References: <4C2A0ADD.60002@cs.oswego.edu>
	<733988.83620.qm@web51404.mail.re2.yahoo.com>
Message-ID: <4C34DDE6.80802@cs.oswego.edu>

On 07/07/10 12:02, Denys Geert wrote:
>
> With the proposed FJ changes (specifically the removal of helpJoin), will the
> join be adapted in such a way that it behaves similar to the current
> helpJoin? What can we expect?
>

Please try the versions just checked in that include the
changes I posted  about last week.

You should see performance at least as good using plain
join() as previously under manual combinations of join+helpJoin.
If not, please send me (offlist) some code that doesn't.

Get updates at the usual places (only jsr166y versions,
not the j.u.c versions):
# API specs:  http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166ydocs/
# jar file: http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166y.jar (compiled using 
Java6 javac).
# Browsable CVS sources: 
http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/jsr166y/

These don't include the other possible method naming changes
people suggested last week. Or some other internal changes I'm still
considering.

-Doug






From gregg at cytetech.com  Wed Jul  7 16:08:35 2010
From: gregg at cytetech.com (Gregg Wonderly)
Date: Wed, 07 Jul 2010 15:08:35 -0500
Subject: [concurrency-interest] How to schedule important periodic task.
In-Reply-To: <NFBBKALFDCPFIDBNKAPCEECGIHAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCEECGIHAA.davidcholmes@aapt.net.au>
Message-ID: <4C34DEC3.6080303@cytetech.com>

David Holmes wrote:
> I have no problem with STPE refusing to reschedule a task that throws an
> exception. If the exception is truly a transient and recoverable failure
> then the task is responsible for dealing with that (how could the STPE ever
> know?). So if the task handles the recoverable failures by not re-throwing
> the exception then the STPE will continue to schedule it. If the task throws
> then STPE considers it a bad task and discards it.

While STPE may have been designed with this idea, my argument is about how much 
this behavior doesn't actually help anyone.  STPE is not, in my opinion, tasked 
with Java VM stability and application "friendliness".  It seems to me, based on 
it's name, to schedule things to run for the user, and run them.  Stopping a 
task requested by the user of the API, without the user requesting it, just 
doesn't make much sense.

Programmers, in general, do not seem to do a great job of estimating and 
accounting for every single possible problem, or else we could have bug free 
software, and we wouldn't need to waste time with test suite development.

Adding to a developers problems by creating a new bug (yes, I realize that I am 
calling this a bug, and some would consider it "as designed") in the system, 
purposefully, seems counterproductive.

> I do agree however that silent failure as happens here is very bad. The
> problem arises from the internal decorating using a ScheduledFutureTask.
> This is a form of FutureTask and so it never allows exceptions to propogate,
> instead catching them and setting the exception status. But in the case of a
> periodicly scheduled task there is no synchronous interaction at which the
> execution exception could be rethrown (nor the exception state explicitly
> queried) so the exception remains undetectable and the failure is silent.
> 
> I don't recall how we arrived at the current situation but in the current
> context it certainly seems less than desirable.

I believe that it is possible to add

try {
	...
} catch( RuntimeException ex ) {
	log.log( Level.SEVERE, ex.toString(), ex );
}

into the existing code quite easily, is it not?  Is there some hesitation 
because the overhead of the try{}catch block is considered a performance 
problem?  If everyone else needs to add this to keep the scheduling running, it 
would seem that STPE could provide this as a service to the API user without 
there being a lot of hesitation or push back.

Gregg Wonderly

From martinrb at google.com  Wed Jul  7 16:28:24 2010
From: martinrb at google.com (Martin Buchholz)
Date: Wed, 7 Jul 2010 13:28:24 -0700
Subject: [concurrency-interest] How to schedule important periodic task.
In-Reply-To: <4C34DEC3.6080303@cytetech.com>
References: <NFBBKALFDCPFIDBNKAPCEECGIHAA.davidcholmes@aapt.net.au>
	<4C34DEC3.6080303@cytetech.com>
Message-ID: <AANLkTilAu0uecd15FsHuT4qDgujtcugF8HRr41MA3hwZ@mail.gmail.com>

On Wed, Jul 7, 2010 at 13:08, Gregg Wonderly <gregg at cytetech.com> wrote:

> I believe that it is possible to add
>
> try {
> ? ? ? ?...
> } catch( RuntimeException ex ) {
> ? ? ? ?log.log( Level.SEVERE, ex.toString(), ex );
> }
>
> into the existing code quite easily, is it not?

j.u.c. (and in general, jdk core libraries) don't currently do any logging.

 ?Is there some hesitation
> because the overhead of the try{}catch block is considered a performance
> problem? ?If everyone else needs to add this to keep the scheduling running,
> it would seem that STPE could provide this as a service to the API user
> without there being a lot of hesitation or push back.


From sberlin at gmail.com  Wed Jul  7 16:41:58 2010
From: sberlin at gmail.com (Sam Berlin)
Date: Wed, 7 Jul 2010 16:41:58 -0400
Subject: [concurrency-interest] How to schedule important periodic task.
In-Reply-To: <4C34DEC3.6080303@cytetech.com>
References: <NFBBKALFDCPFIDBNKAPCEECGIHAA.davidcholmes@aapt.net.au>
	<4C34DEC3.6080303@cytetech.com>
Message-ID: <AANLkTilSbI5Vdc6qelGlNehR2-k961o8c4LPYAmA3bZ3@mail.gmail.com>

Rather than going to log, I would toss it right to the
Thread.getUncaughtExceptionHandler() (or default uncaught exception handler,
etc..).  This is what I've done previously, where application bootstrapping
code setup uncaught exception handlers.

sam

On Wed, Jul 7, 2010 at 4:08 PM, Gregg Wonderly <gregg at cytetech.com> wrote:

> David Holmes wrote:
>
>> I have no problem with STPE refusing to reschedule a task that throws an
>> exception. If the exception is truly a transient and recoverable failure
>> then the task is responsible for dealing with that (how could the STPE
>> ever
>> know?). So if the task handles the recoverable failures by not re-throwing
>> the exception then the STPE will continue to schedule it. If the task
>> throws
>> then STPE considers it a bad task and discards it.
>>
>
> While STPE may have been designed with this idea, my argument is about how
> much this behavior doesn't actually help anyone.  STPE is not, in my
> opinion, tasked with Java VM stability and application "friendliness".  It
> seems to me, based on it's name, to schedule things to run for the user, and
> run them.  Stopping a task requested by the user of the API, without the
> user requesting it, just doesn't make much sense.
>
> Programmers, in general, do not seem to do a great job of estimating and
> accounting for every single possible problem, or else we could have bug free
> software, and we wouldn't need to waste time with test suite development.
>
> Adding to a developers problems by creating a new bug (yes, I realize that
> I am calling this a bug, and some would consider it "as designed") in the
> system, purposefully, seems counterproductive.
>
>
>  I do agree however that silent failure as happens here is very bad. The
>> problem arises from the internal decorating using a ScheduledFutureTask.
>> This is a form of FutureTask and so it never allows exceptions to
>> propogate,
>> instead catching them and setting the exception status. But in the case of
>> a
>> periodicly scheduled task there is no synchronous interaction at which the
>> execution exception could be rethrown (nor the exception state explicitly
>> queried) so the exception remains undetectable and the failure is silent.
>>
>> I don't recall how we arrived at the current situation but in the current
>> context it certainly seems less than desirable.
>>
>
> I believe that it is possible to add
>
> try {
>        ...
>
> } catch( RuntimeException ex ) {
>        log.log( Level.SEVERE, ex.toString(), ex );
> }
>
> into the existing code quite easily, is it not?  Is there some hesitation
> because the overhead of the try{}catch block is considered a performance
> problem?  If everyone else needs to add this to keep the scheduling running,
> it would seem that STPE could provide this as a service to the API user
> without there being a lot of hesitation or push back.
>
>
> Gregg Wonderly
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100707/c95d04be/attachment.html>

From gregg at cytetech.com  Wed Jul  7 17:17:59 2010
From: gregg at cytetech.com (Gregg Wonderly)
Date: Wed, 07 Jul 2010 16:17:59 -0500
Subject: [concurrency-interest] How to schedule important periodic task.
In-Reply-To: <AANLkTilAu0uecd15FsHuT4qDgujtcugF8HRr41MA3hwZ@mail.gmail.com>
References: <NFBBKALFDCPFIDBNKAPCEECGIHAA.davidcholmes@aapt.net.au>
	<4C34DEC3.6080303@cytetech.com>
	<AANLkTilAu0uecd15FsHuT4qDgujtcugF8HRr41MA3hwZ@mail.gmail.com>
Message-ID: <4C34EF07.8050508@cytetech.com>

Martin Buchholz wrote:
> On Wed, Jul 7, 2010 at 13:08, Gregg Wonderly <gregg at cytetech.com> wrote:
> 
>> I believe that it is possible to add
>>
>> try {
>>        ...
>> } catch( RuntimeException ex ) {
>>        log.log( Level.SEVERE, ex.toString(), ex );
>> }
>>
>> into the existing code quite easily, is it not?
> 
> j.u.c. (and in general, jdk core libraries) don't currently do any logging.

Static logger creation for use in logging exceptional cases wouldn't seem to add 
any real overhead.  It would help tremendously for developers to see logging 
being used in productive ways, instead of redemonstrating what many developers 
already do,

	try {
		...
	} catch( Exception ex ) {}

so that there software will keep running because they much prefer that to seeing 
it stop with those silly exception messages.  I'd really like to be able to 
count how many mailing lists I've seen postings with blocks of code like above, 
and then a question of "what is going wrong" because they don't understand how 
exception processing should really be done.

People read the JVM java src.zip for examples and for education.  Having all 
aspects of good software design visible is a great thing in my opinion.

There could be a whole set of debates on the default Formatter provided in Java 
itself, but at least something would come out somewhere that would tell the 
story of the problem.  I know that I always use loggers everywhere because I
really value knowing what is going on and being able to turn logging on to see 
what paths are causing the behavior I am seeing.  When a class has complex 
decision logic inside of it, logging is often the easiest way to see flow 
through the code.

Gregg Wonderly

>   Is there some hesitation
>> because the overhead of the try{}catch block is considered a performance
>> problem?  If everyone else needs to add this to keep the scheduling running,
>> it would seem that STPE could provide this as a service to the API user
>> without there being a lot of hesitation or push back.

From ariel at weisberg.ws  Wed Jul  7 17:32:21 2010
From: ariel at weisberg.ws (Ariel Weisberg)
Date: Wed, 07 Jul 2010 17:32:21 -0400
Subject: [concurrency-interest] How to schedule important periodic task.
In-Reply-To: <AANLkTilSbI5Vdc6qelGlNehR2-k961o8c4LPYAmA3bZ3@mail.gmail.com>
References: <NFBBKALFDCPFIDBNKAPCEECGIHAA.davidcholmes@aapt.net.au><4C34DEC3.6080303@cytetech.com><AANLkTilSbI5Vdc6qelGlNehR2-k961o8c4LPYAmA3bZ3@mail.gmail.com>
Message-ID: <1278538341.8733.1383784381@webmail.messagingengine.com>

Hi all,

The code (1.6.0_20) in ScheduledThreadPoolExecutor wraps the
provided task
in ScheduledFutureTask which extends FutureTask. Future task
does:
boolean innerRunAndReset() {
    if (!compareAndSetState(READY, RUNNING))
        return false;
    try {
        runner = Thread.currentThread();
        if (getState() == RUNNING)
            callable.call(); // don't set result
        runner = null;
        return compareAndSetState(RUNNING, READY);
    } catch (Throwable ex) {
        setException(ex);
        return false;
    }
}
It caches the exception so that it can be retrieved from the
Future later on. ScheduledFutureTask overrides FutureTask.run()
to:
        /**
         * Overrides FutureTask version so as to reset/requeue if
periodic.
         */
        public void run() {
            boolean periodic = isPeriodic();
            if (!canRunInCurrentRunState(periodic))
                cancel(false);
            else if (!periodic)
                ScheduledFutureTask.super.run();
            else if (ScheduledFutureTask.super.runAndReset()) {
                setNextRunTime();
                reExecutePeriodic(outerTask);
            }
        }
runAndReset returns false if the task throws so the task is not
rescheduled. The overhead of a try catch is already there.
There is only one instance where a task that throws an exception
will cause output by default and that is a task submitted via
execute to a ThreadPoolExecutor (not a
ScheduledThreadPoolExecutor) coupled with the default thread
uncaught exception handler. Everything else is wrapped in a
FutureTask (or subclass of) which never throws (including execute
invoked against a ScheduledThreadPoolExecutor).
Worker threads run:
            while (task != null || (task = getTask()) != null) {
                w.lock();
                clearInterruptsForTaskRun();
                try {
                    beforeExecute(w.thread, task);
                    Throwable thrown = null;
                    try {
                        task.run();
                    } catch (RuntimeException x) {
                        thrown = x; throw x;
                    } catch (Error x) {
                        thrown = x; throw x;
                    } catch (Throwable x) {
                        thrown = x; throw new Error(x);
                    } finally {
                        afterExecute(task, thrown);
                    }
                } finally {
                    task = null;
                    w.completedTasks++;
                    w.unlock();
                }
            }
So they only pass on exceptions from tasks that are not wrapped
by FutureTask. The attached app shows how you can get
notification of such exceptions. It is possible for afterExecute
to rethrow an exception as a RuntimeException. This would get you
to the default UncaughtExceptionHandler or whatever your
ThreadFactory used.

It is a little strange that some tasks go to the thread's
UncaughtExceptionHandler and others don't.

Ariel
On Wed, 07 Jul 2010 16:41 -0400, "Sam Berlin" <sberlin at gmail.com>
wrote:

  Rather than going to log, I would toss it right to the
  Thread.getUncaughtExceptionHandler() (or default uncaught
  exception handler, etc..).  This is what I've done previously,
  where application bootstrapping code setup uncaught exception
  handlers.



sam
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100707/75bd8738/attachment-0001.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: STPETest.java
Type: text/x-java
Size: 1758 bytes
Desc: not available
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100707/75bd8738/attachment-0001.bin>

From joe.bowbeer at gmail.com  Wed Jul  7 18:18:20 2010
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Wed, 7 Jul 2010 15:18:20 -0700
Subject: [concurrency-interest] How to schedule important periodic task.
In-Reply-To: <4C34EF07.8050508@cytetech.com>
References: <NFBBKALFDCPFIDBNKAPCEECGIHAA.davidcholmes@aapt.net.au>
	<4C34DEC3.6080303@cytetech.com>
	<AANLkTilAu0uecd15FsHuT4qDgujtcugF8HRr41MA3hwZ@mail.gmail.com>
	<4C34EF07.8050508@cytetech.com>
Message-ID: <AANLkTim8ZPDyHE0wcD4uMxhaXavizPaoL3_scymJstv7@mail.gmail.com>

Too bad about the logging, but this code is used in a wide variety of places
(from handsets to servers) and until some logging strategy is found that
works in all those deployments, it's best to leave the logging in the hands
of the clients, and to be consistent throughout the core java.* classes.  No
logging, that is.

I think it's worth mentioning that STPE was written as a thread pool
replacement for Timer.  In Timer, an errant task will kill the timer thread
and render the timer inoperative (as if the timer were canceled).  But the
uncaughtExceptionHandler will be invoked...

On Wed, Jul 7, 2010 at 2:17 PM, Gregg Wonderly wrote:

> Martin Buchholz wrote:
>
>> On Wed, Jul 7, 2010 at 13:08, Gregg Wonderly wrote:
>>
>>  I believe that it is possible to add
>>>
>>> try {
>>>       ...
>>> } catch( RuntimeException ex ) {
>>>       log.log( Level.SEVERE, ex.toString(), ex );
>>> }
>>>
>>> into the existing code quite easily, is it not?
>>>
>>
>> j.u.c. (and in general, jdk core libraries) don't currently do any
>> logging.
>>
>
> Static logger creation for use in logging exceptional cases wouldn't seem
> to add any real overhead.  It would help tremendously for developers to see
> logging being used in productive ways, instead of redemonstrating what many
> developers already do,
>
>        try {
>                ...
>        } catch( Exception ex ) {}
>
> so that there software will keep running because they much prefer that to
> seeing it stop with those silly exception messages.  I'd really like to be
> able to count how many mailing lists I've seen postings with blocks of code
> like above, and then a question of "what is going wrong" because they don't
> understand how exception processing should really be done.
>
> People read the JVM java src.zip for examples and for education.  Having
> all aspects of good software design visible is a great thing in my opinion.
>
> There could be a whole set of debates on the default Formatter provided in
> Java itself, but at least something would come out somewhere that would tell
> the story of the problem.  I know that I always use loggers everywhere
> because I
> really value knowing what is going on and being able to turn logging on to
> see what paths are causing the behavior I am seeing.  When a class has
> complex decision logic inside of it, logging is often the easiest way to see
> flow through the code.
>
> Gregg Wonderly
>
>
>   Is there some hesitation
>>
>>> because the overhead of the try{}catch block is considered a performance
>>> problem?  If everyone else needs to add this to keep the scheduling
>>> running,
>>> it would seem that STPE could provide this as a service to the API user
>>> without there being a lot of hesitation or push back.
>>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100707/fa0a176a/attachment.html>

From vgrazi at gmail.com  Wed Jul  7 23:42:24 2010
From: vgrazi at gmail.com (Victor Grazi)
Date: Wed, 7 Jul 2010 23:42:24 -0400
Subject: [concurrency-interest] Inconsistent behavior when running a
	ReentrantReadWriteLock between Java 5 and Java 6.
Message-ID: <AANLkTilTFTRv9p1E7s2sJ0bf3PVvMIRignmnqX5Qfe05@mail.gmail.com>

I was talking about un-fair locks. Fair locks have not changed, and the fair
RW locks in Java 5 and 6 both function as I had described the Java 6 un-fair
locks - i.e. a waiting writer prevents waiting readers from getting the
lock.

I guess that is consistent with the label "fair".

But we can't write our code in a vacuum - I would say that the Java 5 model
at least gave us the flexibility that if we know we have frequent writers
and infrequent readers, the Java 5 non-fair version would have served us
well.

And if we were worried about writers being starved out by readers, so in
that case you could have chosen a fair lock.

That was in Java 5. In Java 6 we seem to have lost that flexibility.

Just a thought
Thanks, Victor

> ----------------------------------------------------------------------
>
> Message: 1
> Date: Tue, 6 Jul 2010 20:54:00 -0400
> From: Victor Grazi <vgrazi at gmail.com>
> Subject: Re: [concurrency-interest] Inconsistent behavior when running
>        aReentrantReadWriteLock between Java 5 and Java 6.
> To: dholmes at ieee.org
> Cc: concurrency-interest at cs.oswego.edu
> Message-ID:
>        <AANLkTilhrq7_uW1L50_JGS7uKIeHB3SdyDLK0_T0HP6I at mail.gmail.com>
> Content-Type: text/plain; charset="iso-8859-1"
>
> Interesting. That explains it and it makes good sense.
>
> Thanks Victor
>
> On Jul 6, 2010 8:19 PM, "David Holmes" <davidcholmes at aapt.net.au> wrote:
>
>  There were problems with the spec and implementation for RRWL in Java 5
> that were fixed in Java 6. In Java 6 the implementation should be
> consistent
> with the expected behaviour defined by the spec.
>
> In general you want the reader to block to prevent indefinite postponement
> of writers. This matches the expected use-case of many readers but few
> writers.
>
> There's a lot of discussion in various bug reports and also in the archives
> of this list, but I don't have any direct pointers handy.
>
> David Holmes
>
>
> >
> > -----Original Message-----
> > From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurren...
> -------------- next part --------------
> An HTML attachment was scrubbed...
> URL: <
> http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100706/34024d9d/attachment-0001.html
> >
>
> ------------------------------
>
> Message: 2
> Date: Tue, 6 Jul 2010 17:57:26 -0700
> From: Joe Bowbeer <joe.bowbeer at gmail.com>
> Subject: Re: [concurrency-interest] Inconsistent behavior when running
>        aReentrantReadWriteLock between Java 5 and Java 6.
> To: concurrency-interest <concurrency-interest at cs.oswego.edu>
> Message-ID:
>        <AANLkTikvMgAXVnQ_sQsrtEbDn3-uI2iH4o0RyueqvqUa at mail.gmail.com>
> Content-Type: text/plain; charset="windows-1252"
>
> On second thought, that sentence applies to RWL in general.
>
> Anyway, I think it would be useful to know if you're referring to fair or
> non-fair mode.
>
> On Tue, Jul 6, 2010 at 5:47 PM, Joe Bowbeer wrote:
>
> > These differences are observed in fair mode, right?
> >
> > The fair mode description changed a lot between 5 and 6, including the
> > sentence:
> >
> > "A thread that tries to acquire a fair read lock (non-reentrantly) will
> > block if either the write lock is held, or there is a waiting writer
> > thread."
> >
> > Joe
> >
> > On Tue, Jul 6, 2010 at 5:19 PM, David Holmes wrote:
> >
> >>  There were problems with the spec and implementation for RRWL in Java 5
> >> that were fixed in Java 6. In Java 6 the implementation should be
> consistent
> >> with the expected behaviour defined by the spec.
> >>
> >> In general you want the reader to block to prevent indefinite
> postponement
> >> of writers. This matches the expected use-case of many readers but few
> >> writers.
> >>
> >> There's a lot of discussion in various bug reports and also in the
> >> archives of this list, but I don't have any direct pointers handy.
> >>
> >> David Holmes
> >>
> >> -----Original Message-----
> >> *From:* <concurrency-interest-bounces at cs.oswego.edu>Victor Grazi
> >> *Sent:* Wednesday, 7 July 2010 10:03 AM
> >> *To:* concurrency-interest at cs.oswego.edu
> >> *Subject:* [concurrency-interest] Inconsistent behavior when running
> >> aReentrantReadWriteLock between Java 5 and Java 6.
> >>
> >>   It seems that there is inconsistent behavior when running a
> >> ReentrantReadWriteLock between Java 5 and Java 6.
> >> The case occurs when a reader has the lock, a writer is waiting, and a
> new
> >> reader tries to grab the lock.
> >>
> >> In Java 5, the new reader can get in.
> >> In Java 6, the new reader is blocked.
> >>
> >> I noticed this while reviewing the Java Concurrent Animated (
> >> https://sourceforge.net/projects/javaconcurrenta/) animation for
> >> ReadWriteLock with Dr. Heinz Kabutz.
> >>
> >> Heinz predicted that in that scenario the new reader would get in. I
> >> predicted it would block. Turns out we were both right, depending on the
> >> JDK.
> >>
> >> Is this a feature or a bug? In any case it is a major difference between
> >> the 2 JDK?s, and can cause unpredicted results when migrating existing
> >> programs.
> >>
> >> Regards, Victor Grazi
> >>
> >> Here are the screen shots? (blue arrows are readers, red arrows are
> >> writers, newer threads appear beneath older threads.)
> >>
> >> Java 5
> >>
> >> [image: cid:image005.jpg at 01CB1C7D.6460B420]
> >>
> >>
> >>
> >>                                                 Java 6
> >>
> >> [image: cid:image006.jpg at 01CB1C7D.6460B420]
> >>
> >>
> >>
> >>
> >>
> >
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100707/8fb10a8d/attachment.html>

From davidcholmes at aapt.net.au  Wed Jul  7 23:52:14 2010
From: davidcholmes at aapt.net.au (David Holmes)
Date: Thu, 8 Jul 2010 13:52:14 +1000
Subject: [concurrency-interest] Inconsistent behavior when running
	aReentrantReadWriteLock between Java 5 and Java 6.
In-Reply-To: <AANLkTilTFTRv9p1E7s2sJ0bf3PVvMIRignmnqX5Qfe05@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCOEDDIHAA.davidcholmes@aapt.net.au>

If you have frequent writers and infrequent readers there is little point in
using a ReadWriteLock. The overhead of the RWL has to be amortized across
the additional concurrency it provides for readers. If concurrent readers
are not common then you get more overhead than benefit at which point you
are better off using a regular ReentrantLock.

Of course you need to do the math for a given situation to see where the
switch over point is.

David
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Victor Grazi
  Sent: Thursday, 8 July 2010 1:42 PM
  To: concurrency-interest at cs.oswego.edu
  Subject: Re: [concurrency-interest] Inconsistent behavior when running
aReentrantReadWriteLock between Java 5 and Java 6.


  I was talking about un-fair locks. Fair locks have not changed, and the
fair RW locks in Java 5 and 6 both function as I had described the Java 6
un-fair locks - i.e. a waiting writer prevents waiting readers from getting
the lock.


  I guess that is consistent with the label "fair".


  But we can't write our code in a vacuum - I would say that the Java 5
model at least gave us the flexibility that if we know we have frequent
writers and infrequent readers, the Java 5 non-fair version would have
served us well.


  And if we were worried about writers being starved out by readers, so in
that case you could have chosen a fair lock.


  That was in Java 5. In Java 6 we seem to have lost that flexibility.


  Just a thought
  Thanks, Victor
    ----------------------------------------------------------------------

    Message: 1
    Date: Tue, 6 Jul 2010 20:54:00 -0400
    From: Victor Grazi <vgrazi at gmail.com>
    Subject: Re: [concurrency-interest] Inconsistent behavior when running
           aReentrantReadWriteLock between Java 5 and Java 6.
    To: dholmes at ieee.org
    Cc: concurrency-interest at cs.oswego.edu
    Message-ID:
           <AANLkTilhrq7_uW1L50_JGS7uKIeHB3SdyDLK0_T0HP6I at mail.gmail.com>
    Content-Type: text/plain; charset="iso-8859-1"

    Interesting. That explains it and it makes good sense.

    Thanks Victor

    On Jul 6, 2010 8:19 PM, "David Holmes" <davidcholmes at aapt.net.au> wrote:

     There were problems with the spec and implementation for RRWL in Java 5
    that were fixed in Java 6. In Java 6 the implementation should be
consistent
    with the expected behaviour defined by the spec.

    In general you want the reader to block to prevent indefinite
postponement
    of writers. This matches the expected use-case of many readers but few
    writers.

    There's a lot of discussion in various bug reports and also in the
archives
    of this list, but I don't have any direct pointers handy.

    David Holmes


    >
    > -----Original Message-----
    > From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurren...
    -------------- next part --------------
    An HTML attachment was scrubbed...
    URL:
<http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100706/34
024d9d/attachment-0001.html>

    ------------------------------

    Message: 2
    Date: Tue, 6 Jul 2010 17:57:26 -0700
    From: Joe Bowbeer <joe.bowbeer at gmail.com>
    Subject: Re: [concurrency-interest] Inconsistent behavior when running
           aReentrantReadWriteLock between Java 5 and Java 6.
    To: concurrency-interest <concurrency-interest at cs.oswego.edu>
    Message-ID:
           <AANLkTikvMgAXVnQ_sQsrtEbDn3-uI2iH4o0RyueqvqUa at mail.gmail.com>
    Content-Type: text/plain; charset="windows-1252"

    On second thought, that sentence applies to RWL in general.

    Anyway, I think it would be useful to know if you're referring to fair
or
    non-fair mode.

    On Tue, Jul 6, 2010 at 5:47 PM, Joe Bowbeer wrote:

    > These differences are observed in fair mode, right?
    >
    > The fair mode description changed a lot between 5 and 6, including the
    > sentence:
    >
    > "A thread that tries to acquire a fair read lock (non-reentrantly)
will
    > block if either the write lock is held, or there is a waiting writer
    > thread."
    >
    > Joe
    >
    > On Tue, Jul 6, 2010 at 5:19 PM, David Holmes wrote:
    >
    >>  There were problems with the spec and implementation for RRWL in
Java 5
    >> that were fixed in Java 6. In Java 6 the implementation should be
consistent
    >> with the expected behaviour defined by the spec.
    >>
    >> In general you want the reader to block to prevent indefinite
postponement
    >> of writers. This matches the expected use-case of many readers but
few
    >> writers.
    >>
    >> There's a lot of discussion in various bug reports and also in the
    >> archives of this list, but I don't have any direct pointers handy.
    >>
    >> David Holmes
    >>
    >> -----Original Message-----
    >> *From:* <concurrency-interest-bounces at cs.oswego.edu>Victor Grazi
    >> *Sent:* Wednesday, 7 July 2010 10:03 AM
    >> *To:* concurrency-interest at cs.oswego.edu
    >> *Subject:* [concurrency-interest] Inconsistent behavior when running
    >> aReentrantReadWriteLock between Java 5 and Java 6.
    >>
    >>   It seems that there is inconsistent behavior when running a
    >> ReentrantReadWriteLock between Java 5 and Java 6.
    >> The case occurs when a reader has the lock, a writer is waiting, and
a new
    >> reader tries to grab the lock.
    >>
    >> In Java 5, the new reader can get in.
    >> In Java 6, the new reader is blocked.
    >>
    >> I noticed this while reviewing the Java Concurrent Animated (
    >> https://sourceforge.net/projects/javaconcurrenta/) animation for
    >> ReadWriteLock with Dr. Heinz Kabutz.
    >>
    >> Heinz predicted that in that scenario the new reader would get in. I
    >> predicted it would block. Turns out we were both right, depending on
the
    >> JDK.
    >>
    >> Is this a feature or a bug? In any case it is a major difference
between
    >> the 2 JDK?s, and can cause unpredicted results when migrating
existing
    >> programs.
    >>
    >> Regards, Victor Grazi
    >>
    >> Here are the screen shots? (blue arrows are readers, red arrows are
    >> writers, newer threads appear beneath older threads.)
    >>
    >> Java 5
    >>
    >> [image: cid:image005.jpg at 01CB1C7D.6460B420]
    >>
    >>
    >>
    >>                                                 Java 6
    >>
    >> [image: cid:image006.jpg at 01CB1C7D.6460B420]
    >>
    >>
    >>
    >>
    >>
    >
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100708/b64541ee/attachment-0001.html>

From holger.hoffstaette at googlemail.com  Thu Jul  8 04:10:41 2010
From: holger.hoffstaette at googlemail.com (=?ISO-8859-1?Q?Holger_Hoffst=E4tte?=)
Date: Thu, 08 Jul 2010 10:10:41 +0200
Subject: [concurrency-interest] How to schedule important periodic task.
In-Reply-To: <AANLkTilAu0uecd15FsHuT4qDgujtcugF8HRr41MA3hwZ@mail.gmail.com>
References: <NFBBKALFDCPFIDBNKAPCEECGIHAA.davidcholmes@aapt.net.au>	<4C34DEC3.6080303@cytetech.com>
	<AANLkTilAu0uecd15FsHuT4qDgujtcugF8HRr41MA3hwZ@mail.gmail.com>
Message-ID: <4C358801.1020402@googlemail.com>

Martin Buchholz wrote:
> On Wed, Jul 7, 2010 at 13:08, Gregg Wonderly <gregg at cytetech.com> wrote:
> 
>> I believe that it is possible to add
>>
>> try {
>>        ...
>> } catch( RuntimeException ex ) {
>>        log.log( Level.SEVERE, ex.toString(), ex );
>> }
>>
>> into the existing code quite easily, is it not?
> 
> j.u.c. (and in general, jdk core libraries) don't currently do any logging.

And that's a good thing, because logging is completely, utterly useless.
One of the best ways to make software *more* reliable is to *remove* what
most Java developers call "logging".

-h

From davidcholmes at aapt.net.au  Thu Jul  8 06:34:30 2010
From: davidcholmes at aapt.net.au (David Holmes)
Date: Thu, 8 Jul 2010 20:34:30 +1000
Subject: [concurrency-interest] Lock hiding.
In-Reply-To: <AANLkTikBiTWZPHEXXYP6aM-sR5qh0POaZPjqYCdJMKE_@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCEEDGIHAA.davidcholmes@aapt.net.au>

Paulo,

Sorry for the late reply here ...

There is a cost associated with acquiring an already held lock. What that
cost is will depend on a number of factors - including whether or not your
JVM is using biased-locking.

You'd need to benchmark this to quantify the costs in your situation.

David Holmes
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Paulo Levi
  Sent: Saturday, 26 June 2010 3:56 AM
  To: concurrency-interest at cs.oswego.edu
  Subject: [concurrency-interest] Lock hiding.


  Hi. I have a performance question about some locks. In this case they are
glazed list library locks, but i am interested in the answer for java locks
if you can't comment on those.

  I have a simple concurrent data structure that hides its lock internally.
However i wanted to allow a user to compose the public operations
exclusively for those cases.

  I cooked up this function:
  public void withLock(Runable computation), that just acquires the lock,
runs the computation and releases the lock.

  My question is: if the lock is already acquired, do the hidden lock
operations inside the the other functions of the public interface delay the
computation as if the outer lock wasn't acquired?

  Or put another way : Is the delay short-circuited in some way or do the
inner lock operations delay the computation even with the lack of concurrent
access?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100708/67249ac4/attachment.html>

From vgrazi at gmail.com  Thu Jul  8 07:21:32 2010
From: vgrazi at gmail.com (Victor Grazi)
Date: Thu, 8 Jul 2010 07:21:32 -0400
Subject: [concurrency-interest] Inconsistent behavior when running
	aReentrantReadWriteLock between Java 5 and Java 6.
In-Reply-To: <NFBBKALFDCPFIDBNKAPCOEDDIHAA.davidcholmes@aapt.net.au>
References: <AANLkTilTFTRv9p1E7s2sJ0bf3PVvMIRignmnqX5Qfe05@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCOEDDIHAA.davidcholmes@aapt.net.au>
Message-ID: <AANLkTin-NgeLoZOiAsmL5hZ0Ja1lV3hobg82vSL6THOI@mail.gmail.com>

Interesting. I didn't think of that.

On Jul 7, 2010 11:52 PM, "David Holmes" <davidcholmes at aapt.net.au> wrote:

 If you have frequent writers and infrequent readers there is little point
in using a ReadWriteLock. The overhead of the RWL has to be amortized across
the additional concurrency it provides for readers. If concurrent readers
are not common then you get more overhead than benefit at which point you
are better off using a regular ReentrantLock.

Of course you need to do the math for a given situation to see where the
switch over point is.

David


>
> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurren...

> Sent: Thursday, 8 July 2010 1:42 PM
> To: concurrency-interest at cs.oswego.edu

> Subject: Re: [concurrency-interest] Inconsistent behavior when running
aReentrantReadWriteLock bet...

> I was talking about un-fair locks. Fair locks have not changed, and the
fair RW locks in Java 5 an...
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100708/181b4334/attachment.html>

From gregg at cytetech.com  Thu Jul  8 12:12:51 2010
From: gregg at cytetech.com (Gregg Wonderly)
Date: Thu, 08 Jul 2010 11:12:51 -0500
Subject: [concurrency-interest] How to schedule important periodic task.
In-Reply-To: <4C358801.1020402@googlemail.com>
References: <NFBBKALFDCPFIDBNKAPCEECGIHAA.davidcholmes@aapt.net.au>
	<4C34DEC3.6080303@cytetech.com>
	<AANLkTilAu0uecd15FsHuT4qDgujtcugF8HRr41MA3hwZ@mail.gmail.com>
	<4C358801.1020402@googlemail.com>
Message-ID: <4C35F903.4070403@cytetech.com>

Holger Hoffst?tte wrote:
> Martin Buchholz wrote:
>> On Wed, Jul 7, 2010 at 13:08, Gregg Wonderly <gregg at cytetech.com> wrote:
>>
>>> I believe that it is possible to add
>>>
>>> try {
>>>        ...
>>> } catch( RuntimeException ex ) {
>>>        log.log( Level.SEVERE, ex.toString(), ex );
>>> }
>>>
>>> into the existing code quite easily, is it not?
>> j.u.c. (and in general, jdk core libraries) don't currently do any logging.
> 
> And that's a good thing, because logging is completely, utterly useless.
> One of the best ways to make software *more* reliable is to *remove* what
> most Java developers call "logging".

Holger, maybe you can share what you consider "logging" verses what you consider 
to be useful information from the execution of a program?  Logging has long been 
a part of UNIX et.al. and many other software systems and operating systems etc.

Even small devices have log capabilities which are used to debug and track 
device behavior for support and product development.

Gregg Wonderly

From ihkris at gmail.com  Thu Jul  8 12:37:20 2010
From: ihkris at gmail.com (HariKrishna)
Date: Thu, 8 Jul 2010 17:37:20 +0100
Subject: [concurrency-interest] Concurrency-interest Digest, Vol 66,
	Issue 12
In-Reply-To: <mailman.1.1278604801.10585.concurrency-interest@cs.oswego.edu>
References: <mailman.1.1278604801.10585.concurrency-interest@cs.oswego.edu>
Message-ID: <AANLkTikUwzG2xk4riOQu1SIMSVzvtvQgldpm2XjhQhaS@mail.gmail.com>

Hi Guys,
    I am in Java for 6 years and worked so far in JEE webapplications. Now i
am more interested in learning Threads,Concurrency, JVM. Can you please let
me know where to start and some best tutorials etc.

Thanks
Hari

On Thu, Jul 8, 2010 at 5:00 PM,
<concurrency-interest-request at cs.oswego.edu>wrote:

> Send Concurrency-interest mailing list submissions to
>        concurrency-interest at cs.oswego.edu
>
> To subscribe or unsubscribe via the World Wide Web, visit
>        http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> or, via email, send a message with subject or body 'help' to
>        concurrency-interest-request at cs.oswego.edu
>
> You can reach the person managing the list at
>        concurrency-interest-owner at cs.oswego.edu
>
> When replying, please edit your Subject line so it is more specific
> than "Re: Contents of Concurrency-interest digest..."
>
>
> Today's Topics:
>
>   1. Re: How to schedule important periodic task. (Holger Hoffst?tte)
>   2. Re: Lock hiding. (David Holmes)
>   3. Re: Inconsistent behavior when running
>      aReentrantReadWriteLock between Java 5 and Java 6. (Victor Grazi)
>
>
> ----------------------------------------------------------------------
>
> Message: 1
> Date: Thu, 08 Jul 2010 10:10:41 +0200
> From: Holger Hoffst?tte <holger.hoffstaette at googlemail.com>
> Subject: Re: [concurrency-interest] How to schedule important periodic
>        task.
> To: concurrency-interest at cs.oswego.edu
> Message-ID: <4C358801.1020402 at googlemail.com>
> Content-Type: text/plain; charset=ISO-8859-1
>
> Martin Buchholz wrote:
> > On Wed, Jul 7, 2010 at 13:08, Gregg Wonderly <gregg at cytetech.com> wrote:
> >
> >> I believe that it is possible to add
> >>
> >> try {
> >>        ...
> >> } catch( RuntimeException ex ) {
> >>        log.log( Level.SEVERE, ex.toString(), ex );
> >> }
> >>
> >> into the existing code quite easily, is it not?
> >
> > j.u.c. (and in general, jdk core libraries) don't currently do any
> logging.
>
> And that's a good thing, because logging is completely, utterly useless.
> One of the best ways to make software *more* reliable is to *remove* what
> most Java developers call "logging".
>
> -h
>
>
> ------------------------------
>
> Message: 2
> Date: Thu, 8 Jul 2010 20:34:30 +1000
> From: "David Holmes" <davidcholmes at aapt.net.au>
> Subject: Re: [concurrency-interest] Lock hiding.
> To: "Paulo Levi" <i30817 at gmail.com>,
>        <concurrency-interest at cs.oswego.edu>
> Message-ID: <NFBBKALFDCPFIDBNKAPCEEDGIHAA.davidcholmes at aapt.net.au>
> Content-Type: text/plain; charset="iso-8859-1"
>
> Paulo,
>
> Sorry for the late reply here ...
>
> There is a cost associated with acquiring an already held lock. What that
> cost is will depend on a number of factors - including whether or not your
> JVM is using biased-locking.
>
> You'd need to benchmark this to quantify the costs in your situation.
>
> David Holmes
>  -----Original Message-----
>  From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Paulo Levi
>  Sent: Saturday, 26 June 2010 3:56 AM
>  To: concurrency-interest at cs.oswego.edu
>  Subject: [concurrency-interest] Lock hiding.
>
>
>  Hi. I have a performance question about some locks. In this case they are
> glazed list library locks, but i am interested in the answer for java locks
> if you can't comment on those.
>
>  I have a simple concurrent data structure that hides its lock internally.
> However i wanted to allow a user to compose the public operations
> exclusively for those cases.
>
>  I cooked up this function:
>  public void withLock(Runable computation), that just acquires the lock,
> runs the computation and releases the lock.
>
>  My question is: if the lock is already acquired, do the hidden lock
> operations inside the the other functions of the public interface delay the
> computation as if the outer lock wasn't acquired?
>
>  Or put another way : Is the delay short-circuited in some way or do the
> inner lock operations delay the computation even with the lack of
> concurrent
> access?
> -------------- next part --------------
> An HTML attachment was scrubbed...
> URL: <
> http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100708/67249ac4/attachment-0001.html
> >
>
> ------------------------------
>
> Message: 3
> Date: Thu, 8 Jul 2010 07:21:32 -0400
> From: Victor Grazi <vgrazi at gmail.com>
> Subject: Re: [concurrency-interest] Inconsistent behavior when running
>        aReentrantReadWriteLock between Java 5 and Java 6.
> To: dholmes at ieee.org
> Cc: concurrency-interest at cs.oswego.edu
> Message-ID:
>        <AANLkTin-NgeLoZOiAsmL5hZ0Ja1lV3hobg82vSL6THOI at mail.gmail.com>
> Content-Type: text/plain; charset="iso-8859-1"
>
> Interesting. I didn't think of that.
>
> On Jul 7, 2010 11:52 PM, "David Holmes" <davidcholmes at aapt.net.au> wrote:
>
>  If you have frequent writers and infrequent readers there is little point
> in using a ReadWriteLock. The overhead of the RWL has to be amortized
> across
> the additional concurrency it provides for readers. If concurrent readers
> are not common then you get more overhead than benefit at which point you
> are better off using a regular ReentrantLock.
>
> Of course you need to do the math for a given situation to see where the
> switch over point is.
>
> David
>
>
> >
> > -----Original Message-----
> > From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurren...
>
> > Sent: Thursday, 8 July 2010 1:42 PM
> > To: concurrency-interest at cs.oswego.edu
>
> > Subject: Re: [concurrency-interest] Inconsistent behavior when running
> aReentrantReadWriteLock bet...
>
> > I was talking about un-fair locks. Fair locks have not changed, and the
> fair RW locks in Java 5 an...
> -------------- next part --------------
> An HTML attachment was scrubbed...
> URL: <
> http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100708/181b4334/attachment-0001.html
> >
>
> ------------------------------
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> End of Concurrency-interest Digest, Vol 66, Issue 12
> ****************************************************
>



-- 
Regards,
HariKrishna
07404367902
02085524594
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100708/12f8923a/attachment.html>

From holger.hoffstaette at googlemail.com  Thu Jul  8 12:38:25 2010
From: holger.hoffstaette at googlemail.com (=?ISO-8859-1?Q?Holger_Hoffst=E4tte?=)
Date: Thu, 08 Jul 2010 18:38:25 +0200
Subject: [concurrency-interest] How to schedule important periodic task.
In-Reply-To: <4C35F903.4070403@cytetech.com>
References: <NFBBKALFDCPFIDBNKAPCEECGIHAA.davidcholmes@aapt.net.au>
	<4C34DEC3.6080303@cytetech.com>
	<AANLkTilAu0uecd15FsHuT4qDgujtcugF8HRr41MA3hwZ@mail.gmail.com>
	<4C358801.1020402@googlemail.com> <4C35F903.4070403@cytetech.com>
Message-ID: <4C35FF01.8060301@googlemail.com>

Gregg Wonderly wrote:
> Holger, maybe you can share what you consider "logging" verses what you consider 
> to be useful information from the execution of a program?  Logging has long been 

Not on this list (and I'm not in the mood for off-list either).
Maybe the single shortest answer I can give you is "arbitrary randomly
ordered human-readable full-text open to subjective guessing" vs.
machine-readable replayable event traces with drillable execution context.
Or, barring the latter, maybe just doing the right thing instead of
chickening out.

-h


From joe.bowbeer at gmail.com  Thu Jul  8 14:04:21 2010
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Thu, 8 Jul 2010 11:04:21 -0700
Subject: [concurrency-interest] Learning Threads, Concurrency,
	JVM (Was: Concurrency-interest Digest, Vol 66, Issue 12)
Message-ID: <AANLkTikssyCOTPmpLA_ni6eeSJkJXgArDnL3hDJF6dvS@mail.gmail.com>

Welcome!

Most people on this list have read the book
http://www.javaconcurrencyinpractice.com/

You can also find presentations from JavaOne online.

For the briefest of intros, there's a short but effective chapter in
"Effective Java".

More details in "Concurrent Programming in Java" and "The Art of
Multiprocessor Programming".

"The Java Programming Language" does a surprisingly good job considering how
much material it has to cover.

This looks like it could be fun:

http://sourceforge.net/projects/javaconcurrenta/

Joe

On Thu, Jul 8, 2010 at 9:37 AM, HariKrishna wrote:

> Hi Guys,
>     I am in Java for 6 years and worked so far in JEE webapplications. Now
> i am more interested in learning Threads,Concurrency, JVM. Can you please
> let me know where to start and some best tutorials etc.
>
> Thanks
> Hari
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100708/c8ae4d1a/attachment.html>

From tomas.mikula at gmail.com  Sun Jul 11 18:05:37 2010
From: tomas.mikula at gmail.com (Tomas Mikula)
Date: Mon, 12 Jul 2010 00:05:37 +0200
Subject: [concurrency-interest] Is it legal to complete() a running task
	from itself?
Message-ID: <AANLkTimzFYFaTGxNNpanmPWUIQUBUAKpLeT_f9pzrlga@mail.gmail.com>

Hello!

I have a question whether it is legal to call complete() from a
running task on itself. To put it into the context, I would like to
use it in the following code:

void compute() {
    if(canProceed()) {
        doActualComputation();
    } else {
        helpQuiesce();
    }
}

where if canProceed() returns false, it is guaranteed that this task
will be complete()d from some other task (possibly invoked from
helpQuiesce(), i.e. from itself).

Alternatively, I can imagine ForkJoinTask having a method that would
prevent the return from the compute() method causing a completion of
the task. Explicit call to complete() would be required to complete
the task instead. With such a method (let's call it
requireExplicitCompletion()), I could write

void compute() {
    if(canProceed()) {
        doActualComputation();
    } else {
        requireExplicitCompletion();
    }
}

But AFAIK, there is no way to return from the task's computation
method without causing the completion of the task.

If anyone is curious, I will gladly describe the use case where I
would use such code.

Regards,
Tomas

From dl at cs.oswego.edu  Mon Jul 12 08:35:53 2010
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon, 12 Jul 2010 08:35:53 -0400
Subject: [concurrency-interest] Is it legal to complete() a running task
 from itself?
In-Reply-To: <AANLkTimzFYFaTGxNNpanmPWUIQUBUAKpLeT_f9pzrlga@mail.gmail.com>
References: <AANLkTimzFYFaTGxNNpanmPWUIQUBUAKpLeT_f9pzrlga@mail.gmail.com>
Message-ID: <4C3B0C29.9050607@cs.oswego.edu>

On 07/11/10 18:05, Tomas Mikula wrote:

> I have a question whether it is legal to call complete() from a
> running task on itself.

It is legal but probably never what you want.

>
> Alternatively, I can imagine ForkJoinTask having a method that would
> prevent the return from the compute() method causing a completion of
> the task. Explicit call to complete() would be required to complete
> the task instead.

It does. Execution control is actually based on abstract
method exec(). You can create a subclass in which it
returns false unless completed by some other thread.
The predefined task classes (RecursiveAction
and RecursiveTask) hardwire exec() to return true
upon return from compute(), so you cannot you those
but must create a different direct ForkJoinTask subclass.
If you do this, you also need to define getRawResult and
setRawResult, probably to:
  public final Void getRawResult() { return null; }
  protected final void setRawResult(Void mustBeNull) { }

-Doug

From tomas.mikula at gmail.com  Mon Jul 12 10:59:17 2010
From: tomas.mikula at gmail.com (Tomas Mikula)
Date: Mon, 12 Jul 2010 16:59:17 +0200
Subject: [concurrency-interest] Is it legal to complete() a running task
	from itself?
In-Reply-To: <4C3B0C29.9050607@cs.oswego.edu>
References: <AANLkTimzFYFaTGxNNpanmPWUIQUBUAKpLeT_f9pzrlga@mail.gmail.com> 
	<4C3B0C29.9050607@cs.oswego.edu>
Message-ID: <AANLkTimTYCauILAThXG9xkNe2m1CPAixZKG1nS8AUUfb@mail.gmail.com>

On Mon, Jul 12, 2010 at 2:35 PM, Doug Lea <dl at cs.oswego.edu> wrote:
> On 07/11/10 18:05, Tomas Mikula wrote:
>>
>> Alternatively, I can imagine ForkJoinTask having a method that would
>> prevent the return from the compute() method causing a completion of
>> the task. Explicit call to complete() would be required to complete
>> the task instead.
>
> It does. Execution control is actually based on abstract
> method exec(). You can create a subclass in which it
> returns false unless completed by some other thread.
> The predefined task classes (RecursiveAction
> and RecursiveTask) hardwire exec() to return true
> upon return from compute(), so you cannot you those
> but must create a different direct ForkJoinTask subclass.
> If you do this, you also need to define getRawResult and
> setRawResult, probably to:
> ?public final Void getRawResult() { return null; }
> ?protected final void setRawResult(Void mustBeNull) { }

Thank you, this solves the problem. I overlooked the detailed
documentation of exec().

Tomas


From vblagoje at redhat.com  Wed Jul 14 11:55:18 2010
From: vblagoje at redhat.com (Vladimir Blagojevic)
Date: Wed, 14 Jul 2010 11:55:18 -0400
Subject: [concurrency-interest] ConcurrentHashMap and eviction
Message-ID: <710F1C15-4E87-44F4-9255-EA560E056C3A@redhat.com>

Hi,

Has anyone investigated efficient approaches for element eviction from data containers? The project I work on needed efficient eviction from a Map like structure. We started from a premise that no matter what eviction algorithm is utilized, if eviction itself is not implemented in a scalable, low lock contention approach, it can seriously degrade overall system performance. In order to do any meaningful selection of entries for eviction we have to lock collection container until appropriate eviction entries are selected. Having such a lock protected container in turn causes high lock contention offsetting any eviction precision gained by sophisticated eviction algorithms. In order to get superior throughput while retaining high eviction precision we need both low lock contention container and high precision eviction algorithm implementation  ? a seemingly impossible feat. Instead of making a trade-off between the high precision eviction algorithm and the low lock contention there is a third approach: we keep lock protected collection container but we amortize locking cost through batching updates without affecting the accuracy of the eviction algorithm.

There are more details about the approach here [1] and the source code is here [2].

I would appreciate your feedback if this is a subject that interests you.

Best regards,
Vladimir

[1] http://infinispan.blogspot.com/2010/03/infinispan-eviction-batching-updates.html
[2] http://fisheye.jboss.org/browse/Infinispan/trunk/core/src/main/java/org/infinispan/util/concurrent/BoundedConcurrentHashMap.java?r=HEAD






From eshioji at gmail.com  Sat Jul 17 03:51:53 2010
From: eshioji at gmail.com (Enno Shioji)
Date: Sat, 17 Jul 2010 16:51:53 +0900
Subject: [concurrency-interest] Can object instantiation be considered as
	synchronization facility?
Message-ID: <AANLkTil8NUQEy5btSA0rnIBgPG1-QFKqlm3kmsYdERwc@mail.gmail.com>

Hi,


In the case below,

class Test {
    Integer stuff = 1;

    public static void main(String[] args){
        final Test test = new Test();

        new Thread(){
            public void run(){
                System.out.println(test.stuff);
            }
        }.start();


    }
 }

My understanding is that all access to test.stuff happens after (in
the JMM sense) the construction of the test object even if the member
is declared non-volatile, and thus that the new thread can never
observe test.stuff = null. But then I was told that this is not the
case, and that it only works if stuff is declared final.

If that were true though, we should not be able to use for example a
read-only HashMap like this, right?
class Test {
    final HashMap map = someHashMap();

    public static void main(String[] args){
        final Test test = new Test();

        new Thread(){
            public void run(){
                System.out.println(test.map.get("A");
            }
        }.start();
    }
 }

Since HashMap has internally non-volatile members. Or does final
declaration on the map field somehow enforce some happens before
relationship??





Regards,
Enno

From davidcholmes at aapt.net.au  Sat Jul 17 05:13:01 2010
From: davidcholmes at aapt.net.au (David Holmes)
Date: Sat, 17 Jul 2010 19:13:01 +1000
Subject: [concurrency-interest] Can object instantiation be considered
	assynchronization facility?
In-Reply-To: <AANLkTil8NUQEy5btSA0rnIBgPG1-QFKqlm3kmsYdERwc@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCGEFHIHAA.davidcholmes@aapt.net.au>

Enno,

Enno Shioji writes:
> In the case below,
>
> class Test {
>     Integer stuff = 1;
>
>     public static void main(String[] args){
>         final Test test = new Test();
>
>         new Thread(){
>             public void run(){
>                 System.out.println(test.stuff);
>             }
>         }.start();
>     }
>  }
>
> My understanding is that all access to test.stuff happens after (in
> the JMM sense) the construction of the test object even if the member
> is declared non-volatile, and thus that the new thread can never
> observe test.stuff = null. But then I was told that this is not the
> case, and that it only works if stuff is declared final.

Your understanding is correct. Everything that occurs before a thread is
started happens-before the first instruction of that Thread executes (in the
run() method). There are a number of synchronization actions that establish
happens-before relationships - starting a thread is one of them.

So the access to test.stuff in the started thread is guaranteed to see the
properly constructed value.

> If that were true though, we should not be able to use for example a
> read-only HashMap like this, right?
> class Test {
>     final HashMap map = someHashMap();
>
>     public static void main(String[] args){
>         final Test test = new Test();
>
>         new Thread(){
>             public void run(){
>                 System.out.println(test.map.get("A");
>             }
>         }.start();
>     }
>  }
>
> Since HashMap has internally non-volatile members. Or does final
> declaration on the map field somehow enforce some happens before
> relationship??

Yes - there are special rules for final fields - see JLS 17.5.1. But this
example also establishes happens-before due to the thread starting.

David Holmes


From eshioji at gmail.com  Sat Jul 17 06:22:31 2010
From: eshioji at gmail.com (Enno Shioji)
Date: Sat, 17 Jul 2010 19:22:31 +0900
Subject: [concurrency-interest] Can object instantiation be considered
	assynchronization facility?
In-Reply-To: <NFBBKALFDCPFIDBNKAPCGEFHIHAA.davidcholmes@aapt.net.au>
References: <AANLkTil8NUQEy5btSA0rnIBgPG1-QFKqlm3kmsYdERwc@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCGEFHIHAA.davidcholmes@aapt.net.au>
Message-ID: <AANLkTimmt3Bddbxyr-h40-6pzV8T52VSno0lqxgEDBWt@mail.gmail.com>

Hi David,


Thanks as always:)

So if the example's correctness depends on the .start() method being
called, what if the threads are already started; in other words, let's
say I try to use an optimization similar to String.hashCode() like
this:


class Clever {
    Map map;

    Map getMap(){
        if(map==null){
            map = deriveMap();        }
        return map;
    }
}

And let's say the assumption is that deriveMap() returns an instance
of HashMap that is equal to each other (hence the attempted
optimization).

Now, t1 checks map and it is not null so it returns it. Is it possible
that this map is partially constructed? I guess the compiler has to
reorder in a way that it allocates heap, assigns map and then
continues construction of it.. Is that conceivable from the JMM spec?



Regards,
Enno

On 7/17/10, David Holmes <davidcholmes at aapt.net.au> wrote:
> Enno,
>
> Enno Shioji writes:
>> In the case below,
>>
>> class Test {
>>     Integer stuff = 1;
>>
>>     public static void main(String[] args){
>>         final Test test = new Test();
>>
>>         new Thread(){
>>             public void run(){
>>                 System.out.println(test.stuff);
>>             }
>>         }.start();
>>     }
>>  }
>>
>> My understanding is that all access to test.stuff happens after (in
>> the JMM sense) the construction of the test object even if the member
>> is declared non-volatile, and thus that the new thread can never
>> observe test.stuff = null. But then I was told that this is not the
>> case, and that it only works if stuff is declared final.
>
> Your understanding is correct. Everything that occurs before a thread is
> started happens-before the first instruction of that Thread executes (in the
> run() method). There are a number of synchronization actions that establish
> happens-before relationships - starting a thread is one of them.
>
> So the access to test.stuff in the started thread is guaranteed to see the
> properly constructed value.
>
>> If that were true though, we should not be able to use for example a
>> read-only HashMap like this, right?
>> class Test {
>>     final HashMap map = someHashMap();
>>
>>     public static void main(String[] args){
>>         final Test test = new Test();
>>
>>         new Thread(){
>>             public void run(){
>>                 System.out.println(test.map.get("A");
>>             }
>>         }.start();
>>     }
>>  }
>>
>> Since HashMap has internally non-volatile members. Or does final
>> declaration on the map field somehow enforce some happens before
>> relationship??
>
> Yes - there are special rules for final fields - see JLS 17.5.1. But this
> example also establishes happens-before due to the thread starting.
>
> David Holmes
>
>

From davidcholmes at aapt.net.au  Sat Jul 17 20:54:28 2010
From: davidcholmes at aapt.net.au (David Holmes)
Date: Sun, 18 Jul 2010 10:54:28 +1000
Subject: [concurrency-interest] Can object instantiation be considered
	assynchronization facility?
In-Reply-To: <AANLkTimmt3Bddbxyr-h40-6pzV8T52VSno0lqxgEDBWt@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCCEFIIHAA.davidcholmes@aapt.net.au>

Enno Shioji writes:
> So if the example's correctness depends on the .start() method being
> called, what if the threads are already started; in other words, let's
> say I try to use an optimization similar to String.hashCode() like
> this:
>
>
> class Clever {
>     Map map;
>
>     Map getMap(){
>         if(map==null){
>             map = deriveMap();        }
>         return map;
>     }
> }

Now you have no happens-before orderings between the thread that sets map to
non-null and any thread that sees it as non-null. So the JMM says that it is
possible to observe partially constructed objects.

Whether or not you ever will in practice is a different matter, but not one
you should consider when writing correct and portable code.

David
-----


> And let's say the assumption is that deriveMap() returns an instance
> of HashMap that is equal to each other (hence the attempted
> optimization).
>
> Now, t1 checks map and it is not null so it returns it. Is it possible
> that this map is partially constructed? I guess the compiler has to
> reorder in a way that it allocates heap, assigns map and then
> continues construction of it.. Is that conceivable from the JMM spec?
>
>
>
> Regards,
> Enno
>
> On 7/17/10, David Holmes <davidcholmes at aapt.net.au> wrote:
> > Enno,
> >
> > Enno Shioji writes:
> >> In the case below,
> >>
> >> class Test {
> >>     Integer stuff = 1;
> >>
> >>     public static void main(String[] args){
> >>         final Test test = new Test();
> >>
> >>         new Thread(){
> >>             public void run(){
> >>                 System.out.println(test.stuff);
> >>             }
> >>         }.start();
> >>     }
> >>  }
> >>
> >> My understanding is that all access to test.stuff happens after (in
> >> the JMM sense) the construction of the test object even if the member
> >> is declared non-volatile, and thus that the new thread can never
> >> observe test.stuff = null. But then I was told that this is not the
> >> case, and that it only works if stuff is declared final.
> >
> > Your understanding is correct. Everything that occurs before a thread is
> > started happens-before the first instruction of that Thread
> executes (in the
> > run() method). There are a number of synchronization actions
> that establish
> > happens-before relationships - starting a thread is one of them.
> >
> > So the access to test.stuff in the started thread is guaranteed
> to see the
> > properly constructed value.
> >
> >> If that were true though, we should not be able to use for example a
> >> read-only HashMap like this, right?
> >> class Test {
> >>     final HashMap map = someHashMap();
> >>
> >>     public static void main(String[] args){
> >>         final Test test = new Test();
> >>
> >>         new Thread(){
> >>             public void run(){
> >>                 System.out.println(test.map.get("A");
> >>             }
> >>         }.start();
> >>     }
> >>  }
> >>
> >> Since HashMap has internally non-volatile members. Or does final
> >> declaration on the map field somehow enforce some happens before
> >> relationship??
> >
> > Yes - there are special rules for final fields - see JLS
> 17.5.1. But this
> > example also establishes happens-before due to the thread starting.
> >
> > David Holmes
> >
> >


From eshioji at gmail.com  Sun Jul 18 00:10:25 2010
From: eshioji at gmail.com (Enno Shioji)
Date: Sun, 18 Jul 2010 13:10:25 +0900
Subject: [concurrency-interest] Can object instantiation be considered
	assynchronization facility?
In-Reply-To: <NFBBKALFDCPFIDBNKAPCCEFIIHAA.davidcholmes@aapt.net.au>
References: <AANLkTimmt3Bddbxyr-h40-6pzV8T52VSno0lqxgEDBWt@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCCEFIIHAA.davidcholmes@aapt.net.au>
Message-ID: <AANLkTilnhAVl8fsJyx9R_9odedJ54GgXKfmZwHN3W9xL@mail.gmail.com>

Hi David,


Thanks!
Now if this is the case, why does the optimization in String.hashCode() work?

(Taken from http://www.java2s.com/Open-Source/Java-Document/6.0-JDK-Core/lang/java/lang/String.java.htm)
0120            /** Cache the hash code for the string */
0121            private int hash; // Default to 0


1509          public int hashCode() {
1510                int h = hash;
1511                if (h == 0) {
1512                    int off = offset;
1513                    char val[] = value;
1514                    int len = count;
1515
1516                    for (int i = 0; i < len; i++) {
1517                        h = 31 * h + val[off++];
1518                    }
1519                    hash = h;
1520                }
1521                return h;
1522            }


Is it because hash is primitive?




Regards,
Enno




On Sun, Jul 18, 2010 at 9:54 AM, David Holmes <davidcholmes at aapt.net.au> wrote:
> Enno Shioji writes:
>> So if the example's correctness depends on the .start() method being
>> called, what if the threads are already started; in other words, let's
>> say I try to use an optimization similar to String.hashCode() like
>> this:
>>
>>
>> class Clever {
>> ? ? Map map;
>>
>> ? ? Map getMap(){
>> ? ? ? ? if(map==null){
>> ? ? ? ? ? ? map = deriveMap(); ? ? ? ?}
>> ? ? ? ? return map;
>> ? ? }
>> }
>
> Now you have no happens-before orderings between the thread that sets map to
> non-null and any thread that sees it as non-null. So the JMM says that it is
> possible to observe partially constructed objects.
>
> Whether or not you ever will in practice is a different matter, but not one
> you should consider when writing correct and portable code.
>
> David
> -----
>
>
>> And let's say the assumption is that deriveMap() returns an instance
>> of HashMap that is equal to each other (hence the attempted
>> optimization).
>>
>> Now, t1 checks map and it is not null so it returns it. Is it possible
>> that this map is partially constructed? I guess the compiler has to
>> reorder in a way that it allocates heap, assigns map and then
>> continues construction of it.. Is that conceivable from the JMM spec?
>>
>>
>>
>> Regards,
>> Enno
>>
>> On 7/17/10, David Holmes <davidcholmes at aapt.net.au> wrote:
>> > Enno,
>> >
>> > Enno Shioji writes:
>> >> In the case below,
>> >>
>> >> class Test {
>> >> ? ? Integer stuff = 1;
>> >>
>> >> ? ? public static void main(String[] args){
>> >> ? ? ? ? final Test test = new Test();
>> >>
>> >> ? ? ? ? new Thread(){
>> >> ? ? ? ? ? ? public void run(){
>> >> ? ? ? ? ? ? ? ? System.out.println(test.stuff);
>> >> ? ? ? ? ? ? }
>> >> ? ? ? ? }.start();
>> >> ? ? }
>> >> ?}
>> >>
>> >> My understanding is that all access to test.stuff happens after (in
>> >> the JMM sense) the construction of the test object even if the member
>> >> is declared non-volatile, and thus that the new thread can never
>> >> observe test.stuff = null. But then I was told that this is not the
>> >> case, and that it only works if stuff is declared final.
>> >
>> > Your understanding is correct. Everything that occurs before a thread is
>> > started happens-before the first instruction of that Thread
>> executes (in the
>> > run() method). There are a number of synchronization actions
>> that establish
>> > happens-before relationships - starting a thread is one of them.
>> >
>> > So the access to test.stuff in the started thread is guaranteed
>> to see the
>> > properly constructed value.
>> >
>> >> If that were true though, we should not be able to use for example a
>> >> read-only HashMap like this, right?
>> >> class Test {
>> >> ? ? final HashMap map = someHashMap();
>> >>
>> >> ? ? public static void main(String[] args){
>> >> ? ? ? ? final Test test = new Test();
>> >>
>> >> ? ? ? ? new Thread(){
>> >> ? ? ? ? ? ? public void run(){
>> >> ? ? ? ? ? ? ? ? System.out.println(test.map.get("A");
>> >> ? ? ? ? ? ? }
>> >> ? ? ? ? }.start();
>> >> ? ? }
>> >> ?}
>> >>
>> >> Since HashMap has internally non-volatile members. Or does final
>> >> declaration on the map field somehow enforce some happens before
>> >> relationship??
>> >
>> > Yes - there are special rules for final fields - see JLS
>> 17.5.1. But this
>> > example also establishes happens-before due to the thread starting.
>> >
>> > David Holmes
>> >
>> >
>
>


From joe.bowbeer at gmail.com  Sun Jul 18 00:34:22 2010
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Sat, 17 Jul 2010 21:34:22 -0700
Subject: [concurrency-interest] Can object instantiation be considered
	assynchronization facility?
In-Reply-To: <AANLkTilnhAVl8fsJyx9R_9odedJ54GgXKfmZwHN3W9xL@mail.gmail.com>
References: <AANLkTimmt3Bddbxyr-h40-6pzV8T52VSno0lqxgEDBWt@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCCEFIIHAA.davidcholmes@aapt.net.au>
	<AANLkTilnhAVl8fsJyx9R_9odedJ54GgXKfmZwHN3W9xL@mail.gmail.com>
Message-ID: <AANLkTinnO2pcp3mCJ3usGpUJqFSjrFdNsQ16JypQp7P7@mail.gmail.com>

String.hashCode is betting that (rare) redundant updates are cheaper than
synchronization.

It works for three reasons:

1. Even if several threads compute the hash concurrently, they will all
compute the same value.
2. The value is atomically written (unlike long), so no half-written
updates.
3. The value is primitive rather than a reference, so no unsafe publication.

On Sat, Jul 17, 2010 at 9:10 PM, Enno Shioji wrote:

>
> Now if this is the case, why does the optimization in String.hashCode()
> work?
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100717/83132642/attachment.html>

From eshioji at gmail.com  Sun Jul 18 03:06:10 2010
From: eshioji at gmail.com (Enno Shioji)
Date: Sun, 18 Jul 2010 16:06:10 +0900
Subject: [concurrency-interest] Can object instantiation be considered
	assynchronization facility?
In-Reply-To: <AANLkTinnO2pcp3mCJ3usGpUJqFSjrFdNsQ16JypQp7P7@mail.gmail.com>
References: <AANLkTimmt3Bddbxyr-h40-6pzV8T52VSno0lqxgEDBWt@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCCEFIIHAA.davidcholmes@aapt.net.au>
	<AANLkTilnhAVl8fsJyx9R_9odedJ54GgXKfmZwHN3W9xL@mail.gmail.com>
	<AANLkTinnO2pcp3mCJ3usGpUJqFSjrFdNsQ16JypQp7P7@mail.gmail.com>
Message-ID: <AANLkTimSB7rQUyEIfFBoMr7BKP0Y9I-1P8tm7hrx_v9W@mail.gmail.com>

Hi Joe, David,



Thank you very much as always!



Regards,
Enno




On Sun, Jul 18, 2010 at 1:34 PM, Joe Bowbeer <joe.bowbeer at gmail.com> wrote:
> String.hashCode is betting that (rare) redundant updates are cheaper than
> synchronization.
> It works for three reasons:
> 1. Even if several threads compute the hash concurrently, they will all
> compute the same value.
> 2. The value is atomically written (unlike long), so no half-written
> updates.
> 3. The value is primitive rather than a reference, so no unsafe publication.
>
> On Sat, Jul 17, 2010 at 9:10 PM, Enno Shioji wrote:
>>
>> Now if this is the case, why does the optimization in String.hashCode()
>> work?
>>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>

From brian at briangoetz.com  Sun Jul 18 15:54:21 2010
From: brian at briangoetz.com (Brian Goetz)
Date: Sun, 18 Jul 2010 15:54:21 -0400
Subject: [concurrency-interest] Can object instantiation be considered
 assynchronization facility?
In-Reply-To: <AANLkTinnO2pcp3mCJ3usGpUJqFSjrFdNsQ16JypQp7P7@mail.gmail.com>
References: <AANLkTimmt3Bddbxyr-h40-6pzV8T52VSno0lqxgEDBWt@mail.gmail.com>	<NFBBKALFDCPFIDBNKAPCCEFIIHAA.davidcholmes@aapt.net.au>	<AANLkTilnhAVl8fsJyx9R_9odedJ54GgXKfmZwHN3W9xL@mail.gmail.com>
	<AANLkTinnO2pcp3mCJ3usGpUJqFSjrFdNsQ16JypQp7P7@mail.gmail.com>
Message-ID: <4C435BED.4000003@briangoetz.com>

And (4), the variable can take on only one non-default value.  Therefore it is 
a data race, but a benign one.  Such tricks are ugly but for classes used as 
frequently as String, sometimes one is willing to resort to ugly tricks.  But 
it is definitely a bad idea to start out with the ugly tricks.

On 7/18/2010 12:34 AM, Joe Bowbeer wrote:
> String.hashCode is betting that (rare) redundant updates are cheaper
> than synchronization.
>
> It works for three reasons:
>
> 1. Even if several threads compute the hash concurrently, they will all
> compute the same value.
> 2. The value is atomically written (unlike long), so no half-written
> updates.
> 3. The value is primitive rather than a reference, so no unsafe publication.
>
> On Sat, Jul 17, 2010 at 9:10 PM, Enno Shioji wrote:
>
>
>     Now if this is the case, why does the optimization in
>     String.hashCode() work?
>
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

From paulo.silveira at caelum.com.br  Sun Jul 18 20:42:29 2010
From: paulo.silveira at caelum.com.br (Paulo Silveira - Caelum)
Date: Sun, 18 Jul 2010 21:42:29 -0300
Subject: [concurrency-interest] Can object instantiation be considered
	assynchronization facility?
In-Reply-To: <4C435BED.4000003@briangoetz.com>
References: <AANLkTimmt3Bddbxyr-h40-6pzV8T52VSno0lqxgEDBWt@mail.gmail.com> 
	<NFBBKALFDCPFIDBNKAPCCEFIIHAA.davidcholmes@aapt.net.au>
	<AANLkTilnhAVl8fsJyx9R_9odedJ54GgXKfmZwHN3W9xL@mail.gmail.com> 
	<AANLkTinnO2pcp3mCJ3usGpUJqFSjrFdNsQ16JypQp7P7@mail.gmail.com> 
	<4C435BED.4000003@briangoetz.com>
Message-ID: <AANLkTil9grHYDS89PwbHt_A-za9FnClp9wowx3SouQO2@mail.gmail.com>

On Sun, Jul 18, 2010 at 4:54 PM, Brian Goetz <brian at briangoetz.com> wrote:
> ?But it is definitely a bad idea to start out with the ugly tricks.
Brian, how would you do this lazy hashCode evaluation without using
the ugly tricks and avoiding the rare recalculations?

Paulo

>
> On 7/18/2010 12:34 AM, Joe Bowbeer wrote:
>>
>> String.hashCode is betting that (rare) redundant updates are cheaper
>> than synchronization.
>>
>> It works for three reasons:
>>
>> 1. Even if several threads compute the hash concurrently, they will all
>> compute the same value.
>> 2. The value is atomically written (unlike long), so no half-written
>> updates.
>> 3. The value is primitive rather than a reference, so no unsafe
>> publication.
>>
>> On Sat, Jul 17, 2010 at 9:10 PM, Enno Shioji wrote:
>>
>>
>> ? ?Now if this is the case, why does the optimization in
>> ? ?String.hashCode() work?
>>
>>
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From gregg at cytetech.com  Mon Jul 19 15:37:40 2010
From: gregg at cytetech.com (Gregg Wonderly)
Date: Mon, 19 Jul 2010 14:37:40 -0500
Subject: [concurrency-interest] How to schedule important periodic task.
In-Reply-To: <4C35FF01.8060301@googlemail.com>
References: <NFBBKALFDCPFIDBNKAPCEECGIHAA.davidcholmes@aapt.net.au>
	<4C34DEC3.6080303@cytetech.com>
	<AANLkTilAu0uecd15FsHuT4qDgujtcugF8HRr41MA3hwZ@mail.gmail.com>
	<4C358801.1020402@googlemail.com> <4C35F903.4070403@cytetech.com>
	<4C35FF01.8060301@googlemail.com>
Message-ID: <4C44A984.5010606@cytetech.com>

Sorry for the delayed response to this.  I really think this is an important 
detail which many people don't give due consideration due to the focus on other 
details which are more important to them.

What I've learned over the years, is there is a lot of things about software and 
other peoples software in particular, that I am not in control of.  Over the 
history of Java, we've had JVM bugs, java core system bugs, and numerous things 
(including concurrency bugs) which made it possible for wrong answers to be 
calculated in software systems which had tested to be "correct".  In Java, the 
try {} catch() {} construct provides the ability for software to "wrest control" 
from external users of that software and control the flow of execution in 
unexpected ways.

If we can't agree on the viability of logging for many reasons, I'd at least 
like to find some way to make logging of caught exceptions happen always.  The 
change of Thread to 'log' RuntimeException made it possible for many people to 
really understand more about where 'strange' failures occurred.

If you are going to catch exceptions, I think it's vital that you log what you 
caught so that there is some report of the problem visible as soon as possible, 
event if you are going to just rethrow.

And on a related note, it would be really great if Throwable.printStackTrace(), 
could insert an '->' indicator in the stack trace to show where the 'printing' 
thread execution is at to help people understand exactly where the stack trace 
was reported at, provided it was within the execution stack.

Gregg Wonderly

Holger Hoffst?tte wrote:
> Gregg Wonderly wrote:
>> Holger, maybe you can share what you consider "logging" verses what you consider 
>> to be useful information from the execution of a program?  Logging has long been 
> 
> Not on this list (and I'm not in the mood for off-list either).
> Maybe the single shortest answer I can give you is "arbitrary randomly
> ordered human-readable full-text open to subjective guessing" vs.
> machine-readable replayable event traces with drillable execution context.
> Or, barring the latter, maybe just doing the right thing instead of
> chickening out.
> 
> -h
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> 


From brian at briangoetz.com  Tue Jul 20 11:32:56 2010
From: brian at briangoetz.com (Brian Goetz)
Date: Tue, 20 Jul 2010 11:32:56 -0400
Subject: [concurrency-interest] Can object instantiation be considered
 assynchronization facility?
In-Reply-To: <AANLkTil9grHYDS89PwbHt_A-za9FnClp9wowx3SouQO2@mail.gmail.com>
References: <AANLkTimmt3Bddbxyr-h40-6pzV8T52VSno0lqxgEDBWt@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCCEFIIHAA.davidcholmes@aapt.net.au>
	<AANLkTilnhAVl8fsJyx9R_9odedJ54GgXKfmZwHN3W9xL@mail.gmail.com>
	<AANLkTinnO2pcp3mCJ3usGpUJqFSjrFdNsQ16JypQp7P7@mail.gmail.com>
	<4C435BED.4000003@briangoetz.com>
	<AANLkTil9grHYDS89PwbHt_A-za9FnClp9wowx3SouQO2@mail.gmail.com>
Message-ID: <4C45C1A8.3060007@briangoetz.com>

Well, your first choice should always be:

   private int hashCode;
   public synchronized int hashCode() {
     if (hashCode == 0)
       hashCode = computeHash();
     return hashCode;
   }

This is simple and correct.  As performance becomes more critical (as it is 
with something so widely-used as String), and you know more about the problem, 
you can consider other options.

Josh recommends double-checked locking in this situation.  I still do not like 
this idiom, but there are cases where it can be justified.

If you know something about the distribution of requests (will they come 
concurrently?  Will the "not yet initialized" case be the common case or the 
uncommon case?  Is the update idempotent?) you can consider alternate options.

You can use the "cheap read-write lock trick" where you use synchronized to 
guard the updates and volatile to ensure readability.  (See 
http://www.ibm.com/developerworks/java/library/j-jtp06197.html.)

You can use an AtomicInteger:

   private AtomicInteger ai = new AtomicInteger();
   public int hashCode() {
     int cur = ai.get();
     if (cur == 0) {
       cur = computeHashCode();
       // One of the few situations where you don't use CAS in a loop
       if (!ai.compareAndSet(0, cur))
         cur = ai.get();
     }
     return cur;
   }

There are others too.  It depends on the situation.  I would always start with 
the dumb synchronized approach.

On 7/18/2010 8:42 PM, Paulo Silveira - Caelum wrote:
> On Sun, Jul 18, 2010 at 4:54 PM, Brian Goetz<brian at briangoetz.com>  wrote:
>>   But it is definitely a bad idea to start out with the ugly tricks.
> Brian, how would you do this lazy hashCode evaluation without using
> the ugly tricks and avoiding the rare recalculations?
>
> Paulo
>
>>
>> On 7/18/2010 12:34 AM, Joe Bowbeer wrote:
>>>
>>> String.hashCode is betting that (rare) redundant updates are cheaper
>>> than synchronization.
>>>
>>> It works for three reasons:
>>>
>>> 1. Even if several threads compute the hash concurrently, they will all
>>> compute the same value.
>>> 2. The value is atomically written (unlike long), so no half-written
>>> updates.
>>> 3. The value is primitive rather than a reference, so no unsafe
>>> publication.
>>>
>>> On Sat, Jul 17, 2010 at 9:10 PM, Enno Shioji wrote:
>>>
>>>
>>>     Now if this is the case, why does the optimization in
>>>     String.hashCode() work?
>>>
>>>
>>>
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>

From gdenys at yahoo.com  Fri Jul 23 07:19:26 2010
From: gdenys at yahoo.com (Geert Denys)
Date: Fri, 23 Jul 2010 04:19:26 -0700 (PDT)
Subject: [concurrency-interest] Join does not throw CancellationException on
	ForkJoinPool.shutdownNow
Message-ID: <158632.96631.qm@web51406.mail.re2.yahoo.com>

When performing FJPool.shutdownNow immediately after submitting a task on the FJ 
pool, the join of the task does not  throw a CancellationException  and seems to 
finish normally. When cancelling the task directly, this does not happen. 


Note that the task is a long running one and it is not finished by the time the 
join returns.

Is this expected?
   
 

sample code:
   CountingTask countingTask = new CountingTask(0, 0);
   forkJoinPool.execute(countingTask);

    //Thread.sleep(100);
    fForkJoinPool.shutdownNow();
    
    countingTask.join();

Thanks,
Geert.


      

From dl at cs.oswego.edu  Fri Jul 23 11:00:45 2010
From: dl at cs.oswego.edu (Doug Lea)
Date: Fri, 23 Jul 2010 11:00:45 -0400
Subject: [concurrency-interest] Join does not throw
 CancellationException on	ForkJoinPool.shutdownNow
In-Reply-To: <158632.96631.qm@web51406.mail.re2.yahoo.com>
References: <158632.96631.qm@web51406.mail.re2.yahoo.com>
Message-ID: <4C49AE9D.3080100@cs.oswego.edu>

On 07/23/10 07:19, Geert Denys wrote:
> When performing FJPool.shutdownNow immediately after submitting a task on the FJ
> pool, the join of the task does not  throw a CancellationException  and seems to
> finish normally. When cancelling the task directly, this does not happen.
>

Thanks to Geert for some off-list checking that this was an
unforeseen consequence of the bypass for self-submits mentioned in
"Upcoming FJ Simplifications" posting last month:
>
> 2. Recast ForkJoinPool.invoke to be equivalent to ForkJoinTask.invoke
> when issued by a task in the current pool.

A new update (jsr166y versions only) rolls this back.
We will have to rethink whether and how to do these bypasses.

The update also has a bunch of other small improvements, so
you probably want to get a copy, at the usual place:


# API specs:  http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166ydocs/
# jar file: http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166y.jar (compiled using 
Java6 javac).
# Browsable CVS sources: 
http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/jsr166y/

-Doug

From dl at cs.oswego.edu  Sat Jul 24 16:34:17 2010
From: dl at cs.oswego.edu (Doug Lea)
Date: Sat, 24 Jul 2010 16:34:17 -0400
Subject: [concurrency-interest] Join does not throw
 CancellationException on	ForkJoinPool.shutdownNow
In-Reply-To: <4C49AE9D.3080100@cs.oswego.edu>
References: <158632.96631.qm@web51406.mail.re2.yahoo.com>
	<4C49AE9D.3080100@cs.oswego.edu>
Message-ID: <4C4B4E49.9080501@cs.oswego.edu>

On 07/23/10 11:00, Doug Lea wrote:
>
> A new update (jsr166y versions only) rolls this back.

Oops; sorry; not quite. But now re-updated:

> The update also has a bunch of other small improvements, so
> you probably want to get a copy, at the usual place:
>
>
> # API specs: http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166ydocs/
> # jar file: http://gee.cs.oswego.edu/dl/jsr166/dist/jsr166y.jar (compiled using
> Java6 javac).
> # Browsable CVS sources:
> http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/jsr166y/
>

From kendall8688 at gmail.com  Sun Jul 25 02:14:16 2010
From: kendall8688 at gmail.com (Kendall Moore)
Date: Sun, 25 Jul 2010 02:14:16 -0400
Subject: [concurrency-interest] Monitoring Tool
Message-ID: <AANLkTimgZzjAgrxhb5ua5MR9zzq0o-U9uvt-p=AjmvkN@mail.gmail.com>

Greetings all,

Is there a common consensus on which monitoring tools are best to use when
writing parallel apps?  To be more specific, I would like to be able to know
how many times a given thread has to try to CAS before succeeding.   Also,
the ability to see how long a thread waits to acquire a lock would be useful
as well.  The end goal would, in my particular case, would be to compare
these in order to determine if a non-blocking approach would be more
effective in a given situation than a lock-based approach.  Any help would
be much appreciated!

-- 
Kendall Moore
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100725/9ec63789/attachment.html>

From davidcholmes at aapt.net.au  Sun Jul 25 06:43:25 2010
From: davidcholmes at aapt.net.au (David Holmes)
Date: Sun, 25 Jul 2010 20:43:25 +1000
Subject: [concurrency-interest] Monitoring Tool
In-Reply-To: <AANLkTimgZzjAgrxhb5ua5MR9zzq0o-U9uvt-p=AjmvkN@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCMEHOIHAA.davidcholmes@aapt.net.au>

Kendall,

In my opinion a monitoring tool looking at the lock acquisition time, or CAS
attempts, won't give you much insight into whether to use a blocking or
non-blocking approach. You need to measure the performance of your
application logic as a whole, utilising the two different approaches.
Afterall how can you compare locking times with number of CAS attempts in
general?

David Holmes
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Kendall
Moore
  Sent: Sunday, 25 July 2010 4:14 PM
  To: concurrency-interest at cs.oswego.edu
  Subject: [concurrency-interest] Monitoring Tool


  Greetings all,


  Is there a common consensus on which monitoring tools are best to use when
writing parallel apps?  To be more specific, I would like to be able to know
how many times a given thread has to try to CAS before succeeding.   Also,
the ability to see how long a thread waits to acquire a lock would be useful
as well.  The end goal would, in my particular case, would be to compare
these in order to determine if a non-blocking approach would be more
effective in a given situation than a lock-based approach.  Any help would
be much appreciated!

  --
  Kendall Moore
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100725/013c12e2/attachment.html>

From fslzdd at gmail.com  Mon Jul 26 01:22:16 2010
From: fslzdd at gmail.com (=?GB2312?B?tO/X0w==?=)
Date: Mon, 26 Jul 2010 13:22:16 +0800
Subject: [concurrency-interest] Monitoring Tool
In-Reply-To: <AANLkTimgZzjAgrxhb5ua5MR9zzq0o-U9uvt-p=AjmvkN@mail.gmail.com>
References: <AANLkTimgZzjAgrxhb5ua5MR9zzq0o-U9uvt-p=AjmvkN@mail.gmail.com>
Message-ID: <AANLkTinkL0xU9hRDFxeziZv95V80JGsNMZXyPmf34Kmp@mail.gmail.com>

Kendall,

MulticoreSDK (http://www.alphaworks.ibm.com/tech/msdk) contains a Java lock
profiler tool capable of capturing Java synchronization monitor and j.u.c
lock contention data. You can download it from
here<https://www6.software.ibm.com/sdfdl/1v2/regs2/awadmin/msdk/Xa.2/Xb.NoLfkPV-k67E8GJ3k8pK2d3Uplpgbio-T9mcu3F8Hw/Xc.msdk/DCInstall.zip/Xd./Xf.LPr.U1ay/Xg.5558720/Xi.AW-0WA/XY.regsrvs/XZ.He6t6jNcnhprWp3qc1gIg49AGxo/DCInstall.zip>.
Hope it can help in your case. Thanks!

Regards,
Daniel

2010/7/25 Kendall Moore <kendall8688 at gmail.com>

> Greetings all,
>
> Is there a common consensus on which monitoring tools are best to use when
> writing parallel apps?  To be more specific, I would like to be able to know
> how many times a given thread has to try to CAS before succeeding.   Also,
> the ability to see how long a thread waits to acquire a lock would be useful
> as well.  The end goal would, in my particular case, would be to compare
> these in order to determine if a non-blocking approach would be more
> effective in a given situation than a lock-based approach.  Any help would
> be much appreciated!
>
> --
> Kendall Moore
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100726/e9051d71/attachment.html>

From gergg at cox.net  Mon Jul 26 15:13:01 2010
From: gergg at cox.net (Gregg Wonderly)
Date: Mon, 26 Jul 2010 14:13:01 -0500
Subject: [concurrency-interest] Monitoring Tool
In-Reply-To: <NFBBKALFDCPFIDBNKAPCMEHOIHAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCMEHOIHAA.davidcholmes@aapt.net.au>
Message-ID: <4C4DDE3D.8040508@cox.net>

Per thread latency measurements with 1, 2, 10 and 100 threads will often tell 
you a lot about how contention is affecting the execution time.  When you get to 
100, a thread dump will often reveal where everyone is standing in line...

Gregg Wonderly

David Holmes wrote:
> Kendall,
>  
> In my opinion a monitoring tool looking at the lock acquisition time, or 
> CAS attempts, won't give you much insight into whether to use a blocking 
> or non-blocking approach. You need to measure the performance of your 
> application logic as a whole, utilising the two different approaches. 
> Afterall how can you compare locking times with number of CAS attempts 
> in general?
>  
> David Holmes
> 
>     -----Original Message-----
>     *From:* concurrency-interest-bounces at cs.oswego.edu
>     [mailto:concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of
>     *Kendall Moore
>     *Sent:* Sunday, 25 July 2010 4:14 PM
>     *To:* concurrency-interest at cs.oswego.edu
>     *Subject:* [concurrency-interest] Monitoring Tool
> 
>     Greetings all,
> 
>     Is there a common consensus on which monitoring tools are best to
>     use when writing parallel apps?  To be more specific, I would like
>     to be able to know how many times a given thread has to try to CAS
>     before succeeding.   Also, the ability to see how long a thread
>     waits to acquire a lock would be useful as well.  The end goal
>     would, in my particular case, would be to compare these in order to
>     determine if a non-blocking approach would be more effective in a
>     given situation than a lock-based approach.  Any help would be much
>     appreciated!
> 
>     -- 
>     Kendall Moore
> 
> 
> ------------------------------------------------------------------------
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From william.louth at jinspired.com  Mon Jul 26 16:00:47 2010
From: william.louth at jinspired.com (William Louth (JINSPIRED.COM))
Date: Mon, 26 Jul 2010 22:00:47 +0200
Subject: [concurrency-interest] Monitoring Tool
In-Reply-To: <4C4DDE3D.8040508@cox.net>
References: <NFBBKALFDCPFIDBNKAPCMEHOIHAA.davidcholmes@aapt.net.au>
	<4C4DDE3D.8040508@cox.net>
Message-ID: <4C4DE96F.2070600@jinspired.com>

  JINSPIRED'S OpenCore (http://opencore.jinspired.com) metering & 
metrics runtime has built in meters for thread contention metering 
(activity based) @see blocking.time and blocking.count.

You can easily extend it with your own custom counters or resource 
measures mapped to meters. And you don't always need to measure time: 
http://williamlouth.wordpress.com/2010/06/11/no-latency-application-performance-analysis-when-wall-clock-time-is-simply-too-slow/

Queuing can also be aggregated at various namespace levels: 
http://williamlouth.wordpress.com/2010/05/20/metered-software-service-queues/

More related articles: http://williamlouth.wordpress.com/category/profiling/

On 26/07/2010 21:13, Gregg Wonderly wrote:
> Per thread latency measurements with 1, 2, 10 and 100 threads will 
> often tell you a lot about how contention is affecting the execution 
> time.  When you get to 100, a thread dump will often reveal where 
> everyone is standing in line...
>
> Gregg Wonderly
>
> David Holmes wrote:
>> Kendall,
>>
>> In my opinion a monitoring tool looking at the lock acquisition time, 
>> or CAS attempts, won't give you much insight into whether to use a 
>> blocking or non-blocking approach. You need to measure the 
>> performance of your application logic as a whole, utilising the two 
>> different approaches. Afterall how can you compare locking times with 
>> number of CAS attempts in general?
>>
>> David Holmes
>>
>>     -----Original Message-----
>>     *From:* concurrency-interest-bounces at cs.oswego.edu
>>     [mailto:concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of
>>     *Kendall Moore
>>     *Sent:* Sunday, 25 July 2010 4:14 PM
>>     *To:* concurrency-interest at cs.oswego.edu
>>     *Subject:* [concurrency-interest] Monitoring Tool
>>
>>     Greetings all,
>>
>>     Is there a common consensus on which monitoring tools are best to
>>     use when writing parallel apps?  To be more specific, I would like
>>     to be able to know how many times a given thread has to try to CAS
>>     before succeeding.   Also, the ability to see how long a thread
>>     waits to acquire a lock would be useful as well.  The end goal
>>     would, in my particular case, would be to compare these in order to
>>     determine if a non-blocking approach would be more effective in a
>>     given situation than a lock-based approach.  Any help would be much
>>     appreciated!
>>
>>     --     Kendall Moore
>>
>>
>> ------------------------------------------------------------------------
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>

From gregg at cytetech.com  Mon Jul 26 17:56:36 2010
From: gregg at cytetech.com (Gregg Wonderly)
Date: Mon, 26 Jul 2010 16:56:36 -0500
Subject: [concurrency-interest] Monitoring Tool
In-Reply-To: <4C4DE96F.2070600@jinspired.com>
References: <NFBBKALFDCPFIDBNKAPCMEHOIHAA.davidcholmes@aapt.net.au>
	<4C4DDE3D.8040508@cox.net> <4C4DE96F.2070600@jinspired.com>
Message-ID: <4C4E0494.4050309@cytetech.com>

This doesn't really point at concurrency issues, only in execution time of 
compute bound execution, and perhaps some simple linear scaling of complexity.

When you have 1000s of instructions and multiple code paths, which include 
synchronization, and random latency injection, I am not sure that you can see 
how threads are "waiting in line" until you see the quadratic change in 
execution time that is typically visible when high contention occurs.

Maybe you can point out where this tool provides for the ability to monitor lock 
contention and count stalled threads and otherwise see the real contention that 
develops over time as load increases on a highly contended code segment?

If you could "time" the execution interval through all code paths, and then look 
at the "percentage" of threads and time through each path, and then see that the 
"largest" latency block was in a very common code path, you could perhaps then 
say this was the area to look at.  All of that analysis might not really be 
possible though, because of the exponential potential complexity of code path 
coverage.

Gregg Wonderly

William Louth (JINSPIRED.COM) wrote:
>  JINSPIRED'S OpenCore (http://opencore.jinspired.com) metering & metrics 
> runtime has built in meters for thread contention metering (activity 
> based) @see blocking.time and blocking.count.
> 
> You can easily extend it with your own custom counters or resource 
> measures mapped to meters. And you don't always need to measure time: 
> http://williamlouth.wordpress.com/2010/06/11/no-latency-application-performance-analysis-when-wall-clock-time-is-simply-too-slow/ 
> 
> 
> Queuing can also be aggregated at various namespace levels: 
> http://williamlouth.wordpress.com/2010/05/20/metered-software-service-queues/ 
> 
> 
> More related articles: 
> http://williamlouth.wordpress.com/category/profiling/
> 
> On 26/07/2010 21:13, Gregg Wonderly wrote:
>> Per thread latency measurements with 1, 2, 10 and 100 threads will 
>> often tell you a lot about how contention is affecting the execution 
>> time.  When you get to 100, a thread dump will often reveal where 
>> everyone is standing in line...
>>
>> Gregg Wonderly
>>
>> David Holmes wrote:
>>> Kendall,
>>>
>>> In my opinion a monitoring tool looking at the lock acquisition time, 
>>> or CAS attempts, won't give you much insight into whether to use a 
>>> blocking or non-blocking approach. You need to measure the 
>>> performance of your application logic as a whole, utilising the two 
>>> different approaches. Afterall how can you compare locking times with 
>>> number of CAS attempts in general?
>>>
>>> David Holmes
>>>
>>>     -----Original Message-----
>>>     *From:* concurrency-interest-bounces at cs.oswego.edu
>>>     [mailto:concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of
>>>     *Kendall Moore
>>>     *Sent:* Sunday, 25 July 2010 4:14 PM
>>>     *To:* concurrency-interest at cs.oswego.edu
>>>     *Subject:* [concurrency-interest] Monitoring Tool
>>>
>>>     Greetings all,
>>>
>>>     Is there a common consensus on which monitoring tools are best to
>>>     use when writing parallel apps?  To be more specific, I would like
>>>     to be able to know how many times a given thread has to try to CAS
>>>     before succeeding.   Also, the ability to see how long a thread
>>>     waits to acquire a lock would be useful as well.  The end goal
>>>     would, in my particular case, would be to compare these in order to
>>>     determine if a non-blocking approach would be more effective in a
>>>     given situation than a lock-based approach.  Any help would be much
>>>     appreciated!
>>>
>>>     --     Kendall Moore
>>>
>>>
>>> ------------------------------------------------------------------------
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> 


From william.louth at jinspired.com  Mon Jul 26 18:18:32 2010
From: william.louth at jinspired.com (William Louth (JINSPIRED.COM))
Date: Tue, 27 Jul 2010 00:18:32 +0200
Subject: [concurrency-interest] Monitoring Tool
In-Reply-To: <4C4E0494.4050309@cytetech.com>
References: <NFBBKALFDCPFIDBNKAPCMEHOIHAA.davidcholmes@aapt.net.au>
	<4C4DDE3D.8040508@cox.net> <4C4DE96F.2070600@jinspired.com>
	<4C4E0494.4050309@cytetech.com>
Message-ID: <4C4E09B8.6010007@jinspired.com>

  It can do this and more but maybe I should have factor in the 
likelihood that you would have only skimmed over the links I email - 
time is money. Sorry for not directly pointing this out but as the 
architect of both the Probes and Metrics Open API's I thought this would 
be rather obvious.

OpenCore supports many meters some of which are contention related: 
http://opencore.jinspired.com/?page_id=981#p:core A lot of these are not 
time related.

OpenCore's Probes metering can be automatically mapped to Metrics to 
correlate across multiple threads with other measurements not metered based.
http://opencore.jinspired.com/?page_id=377

OpenCore supports the reporting of metering at thread and probes level. 
You indicate transaction points to the runtime via config and then see 
which particular probes (activities) and meters (resources: monitors) 
contributed the most. See transaction probes provider under Guides.
http://opencore.jinspired.com/?page_id=772

On 26/07/2010 23:56, Gregg Wonderly wrote:
> This doesn't really point at concurrency issues, only in execution 
> time of compute bound execution, and perhaps some simple linear 
> scaling of complexity.
>
> When you have 1000s of instructions and multiple code paths, which 
> include synchronization, and random latency injection, I am not sure 
> that you can see how threads are "waiting in line" until you see the 
> quadratic change in execution time that is typically visible when high 
> contention occurs.
>
> Maybe you can point out where this tool provides for the ability to 
> monitor lock contention and count stalled threads and otherwise see 
> the real contention that develops over time as load increases on a 
> highly contended code segment?
>
> If you could "time" the execution interval through all code paths, and 
> then look at the "percentage" of threads and time through each path, 
> and then see that the "largest" latency block was in a very common 
> code path, you could perhaps then say this was the area to look at.  
> All of that analysis might not really be possible though, because of 
> the exponential potential complexity of code path coverage.
>
> Gregg Wonderly
>
> William Louth (JINSPIRED.COM) wrote:
>>  JINSPIRED'S OpenCore (http://opencore.jinspired.com) metering & 
>> metrics runtime has built in meters for thread contention metering 
>> (activity based) @see blocking.time and blocking.count.
>>
>> You can easily extend it with your own custom counters or resource 
>> measures mapped to meters. And you don't always need to measure time: 
>> http://williamlouth.wordpress.com/2010/06/11/no-latency-application-performance-analysis-when-wall-clock-time-is-simply-too-slow/ 
>>
>>
>> Queuing can also be aggregated at various namespace levels: 
>> http://williamlouth.wordpress.com/2010/05/20/metered-software-service-queues/ 
>>
>>
>> More related articles: 
>> http://williamlouth.wordpress.com/category/profiling/
>>
>> On 26/07/2010 21:13, Gregg Wonderly wrote:
>>> Per thread latency measurements with 1, 2, 10 and 100 threads will 
>>> often tell you a lot about how contention is affecting the execution 
>>> time.  When you get to 100, a thread dump will often reveal where 
>>> everyone is standing in line...
>>>
>>> Gregg Wonderly
>>>
>>> David Holmes wrote:
>>>> Kendall,
>>>>
>>>> In my opinion a monitoring tool looking at the lock acquisition 
>>>> time, or CAS attempts, won't give you much insight into whether to 
>>>> use a blocking or non-blocking approach. You need to measure the 
>>>> performance of your application logic as a whole, utilising the two 
>>>> different approaches. Afterall how can you compare locking times 
>>>> with number of CAS attempts in general?
>>>>
>>>> David Holmes
>>>>
>>>>     -----Original Message-----
>>>>     *From:* concurrency-interest-bounces at cs.oswego.edu
>>>>     [mailto:concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of
>>>>     *Kendall Moore
>>>>     *Sent:* Sunday, 25 July 2010 4:14 PM
>>>>     *To:* concurrency-interest at cs.oswego.edu
>>>>     *Subject:* [concurrency-interest] Monitoring Tool
>>>>
>>>>     Greetings all,
>>>>
>>>>     Is there a common consensus on which monitoring tools are best to
>>>>     use when writing parallel apps?  To be more specific, I would like
>>>>     to be able to know how many times a given thread has to try to CAS
>>>>     before succeeding.   Also, the ability to see how long a thread
>>>>     waits to acquire a lock would be useful as well.  The end goal
>>>>     would, in my particular case, would be to compare these in 
>>>> order to
>>>>     determine if a non-blocking approach would be more effective in a
>>>>     given situation than a lock-based approach.  Any help would be 
>>>> much
>>>>     appreciated!
>>>>
>>>>     --     Kendall Moore
>>>>
>>>>
>>>> ------------------------------------------------------------------------ 
>>>>
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
>
>

From gregg at cytetech.com  Thu Jul 29 15:02:04 2010
From: gregg at cytetech.com (Gregg Wonderly)
Date: Thu, 29 Jul 2010 14:02:04 -0500
Subject: [concurrency-interest] Monitoring Tool
In-Reply-To: <4C4E09B8.6010007@jinspired.com>
References: <NFBBKALFDCPFIDBNKAPCMEHOIHAA.davidcholmes@aapt.net.au>
	<4C4DDE3D.8040508@cox.net> <4C4DE96F.2070600@jinspired.com>
	<4C4E0494.4050309@cytetech.com> <4C4E09B8.6010007@jinspired.com>
Message-ID: <4C51D02C.3050807@cytetech.com>

Again, one of the big issues for me, is that typically, such metering requires 
atomic counting, which, itself, is a form of contention.  This contention can 
get injected into previously uncontended blocks and redistribute the timing 
through particular parts of the code which then don't reveal the actual behavior 
in normal execution.

Where can I see more information about how issues like this are dealt with?

Gregg Wonderly

William Louth (JINSPIRED.COM) wrote:
>  It can do this and more but maybe I should have factor in the 
> likelihood that you would have only skimmed over the links I email - 
> time is money. Sorry for not directly pointing this out but as the 
> architect of both the Probes and Metrics Open API's I thought this would 
> be rather obvious.
> 
> OpenCore supports many meters some of which are contention related: 
> http://opencore.jinspired.com/?page_id=981#p:core A lot of these are not 
> time related.
> 
> OpenCore's Probes metering can be automatically mapped to Metrics to 
> correlate across multiple threads with other measurements not metered 
> based.
> http://opencore.jinspired.com/?page_id=377
> 
> OpenCore supports the reporting of metering at thread and probes level. 
> You indicate transaction points to the runtime via config and then see 
> which particular probes (activities) and meters (resources: monitors) 
> contributed the most. See transaction probes provider under Guides.
> http://opencore.jinspired.com/?page_id=772
> 
> On 26/07/2010 23:56, Gregg Wonderly wrote:
>> This doesn't really point at concurrency issues, only in execution 
>> time of compute bound execution, and perhaps some simple linear 
>> scaling of complexity.
>>
>> When you have 1000s of instructions and multiple code paths, which 
>> include synchronization, and random latency injection, I am not sure 
>> that you can see how threads are "waiting in line" until you see the 
>> quadratic change in execution time that is typically visible when high 
>> contention occurs.
>>
>> Maybe you can point out where this tool provides for the ability to 
>> monitor lock contention and count stalled threads and otherwise see 
>> the real contention that develops over time as load increases on a 
>> highly contended code segment?
>>
>> If you could "time" the execution interval through all code paths, and 
>> then look at the "percentage" of threads and time through each path, 
>> and then see that the "largest" latency block was in a very common 
>> code path, you could perhaps then say this was the area to look at.  
>> All of that analysis might not really be possible though, because of 
>> the exponential potential complexity of code path coverage.
>>
>> Gregg Wonderly
>>
>> William Louth (JINSPIRED.COM) wrote:
>>>  JINSPIRED'S OpenCore (http://opencore.jinspired.com) metering & 
>>> metrics runtime has built in meters for thread contention metering 
>>> (activity based) @see blocking.time and blocking.count.
>>>
>>> You can easily extend it with your own custom counters or resource 
>>> measures mapped to meters. And you don't always need to measure time: 
>>> http://williamlouth.wordpress.com/2010/06/11/no-latency-application-performance-analysis-when-wall-clock-time-is-simply-too-slow/ 
>>>
>>>
>>> Queuing can also be aggregated at various namespace levels: 
>>> http://williamlouth.wordpress.com/2010/05/20/metered-software-service-queues/ 
>>>
>>>
>>> More related articles: 
>>> http://williamlouth.wordpress.com/category/profiling/
>>>
>>> On 26/07/2010 21:13, Gregg Wonderly wrote:
>>>> Per thread latency measurements with 1, 2, 10 and 100 threads will 
>>>> often tell you a lot about how contention is affecting the execution 
>>>> time.  When you get to 100, a thread dump will often reveal where 
>>>> everyone is standing in line...
>>>>
>>>> Gregg Wonderly
>>>>
>>>> David Holmes wrote:
>>>>> Kendall,
>>>>>
>>>>> In my opinion a monitoring tool looking at the lock acquisition 
>>>>> time, or CAS attempts, won't give you much insight into whether to 
>>>>> use a blocking or non-blocking approach. You need to measure the 
>>>>> performance of your application logic as a whole, utilising the two 
>>>>> different approaches. Afterall how can you compare locking times 
>>>>> with number of CAS attempts in general?
>>>>>
>>>>> David Holmes
>>>>>
>>>>>     -----Original Message-----
>>>>>     *From:* concurrency-interest-bounces at cs.oswego.edu
>>>>>     [mailto:concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of
>>>>>     *Kendall Moore
>>>>>     *Sent:* Sunday, 25 July 2010 4:14 PM
>>>>>     *To:* concurrency-interest at cs.oswego.edu
>>>>>     *Subject:* [concurrency-interest] Monitoring Tool
>>>>>
>>>>>     Greetings all,
>>>>>
>>>>>     Is there a common consensus on which monitoring tools are best to
>>>>>     use when writing parallel apps?  To be more specific, I would like
>>>>>     to be able to know how many times a given thread has to try to CAS
>>>>>     before succeeding.   Also, the ability to see how long a thread
>>>>>     waits to acquire a lock would be useful as well.  The end goal
>>>>>     would, in my particular case, would be to compare these in 
>>>>> order to
>>>>>     determine if a non-blocking approach would be more effective in a
>>>>>     given situation than a lock-based approach.  Any help would be 
>>>>> much
>>>>>     appreciated!
>>>>>
>>>>>     --     Kendall Moore
>>>>>
>>>>>
>>>>> ------------------------------------------------------------------------ 
>>>>>
>>>>>
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at cs.oswego.edu
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>
>>
>>
> 
> 


From ben_manes at yahoo.com  Thu Jul 29 16:18:17 2010
From: ben_manes at yahoo.com (Ben Manes)
Date: Thu, 29 Jul 2010 13:18:17 -0700 (PDT)
Subject: [concurrency-interest] Monitoring Tool
In-Reply-To: <4C51D02C.3050807@cytetech.com>
References: <NFBBKALFDCPFIDBNKAPCMEHOIHAA.davidcholmes@aapt.net.au>
	<4C4DDE3D.8040508@cox.net> <4C4DE96F.2070600@jinspired.com>
	<4C4E0494.4050309@cytetech.com> <4C4E09B8.6010007@jinspired.com>
	<4C51D02C.3050807@cytetech.com>
Message-ID: <879050.54032.qm@web38804.mail.mud.yahoo.com>

Most likely their using an inverted read/write lock approach where the count is 
kept thread-local and aggregated by a monitoring thread. I haven't looked at 
their implementation, but that's a standard idiom for avoiding contention for 
capturing statistical information.




________________________________
From: Gregg Wonderly <gregg at cytetech.com>
To: William Louth (JINSPIRED.COM) <william.louth at jinspired.com>
Cc: concurrency-interest at cs.oswego.edu; gregg.wonderly at pobox.com
Sent: Thu, July 29, 2010 12:02:04 PM
Subject: Re: [concurrency-interest] Monitoring Tool

Again, one of the big issues for me, is that typically, such metering requires 
atomic counting, which, itself, is a form of contention.  This contention can 
get injected into previously uncontended blocks and redistribute the timing 
through particular parts of the code which then don't reveal the actual behavior 
in normal execution.

Where can I see more information about how issues like this are dealt with?

Gregg Wonderly

William Louth (JINSPIRED.COM) wrote:
>  It can do this and more but maybe I should have factor in the likelihood that 
>you would have only skimmed over the links I email - time is money. Sorry for 
>not directly pointing this out but as the architect of both the Probes and 
>Metrics Open API's I thought this would be rather obvious.
> 
> OpenCore supports many meters some of which are contention related: 
>http://opencore.jinspired.com/?page_id=981#p:core A lot of these are not time 
>related.
> 
> OpenCore's Probes metering can be automatically mapped to Metrics to correlate 
>across multiple threads with other measurements not metered based.
> http://opencore.jinspired.com/?page_id=377
> 
> OpenCore supports the reporting of metering at thread and probes level. You 
>indicate transaction points to the runtime via config and then see which 
>particular probes (activities) and meters (resources: monitors) contributed the 
>most. See transaction probes provider under Guides.
> http://opencore.jinspired.com/?page_id=772
> 
> On 26/07/2010 23:56, Gregg Wonderly wrote:
>> This doesn't really point at concurrency issues, only in execution time of 
>>compute bound execution, and perhaps some simple linear scaling of complexity.
>> 
>> When you have 1000s of instructions and multiple code paths, which include 
>>synchronization, and random latency injection, I am not sure that you can see 
>>how threads are "waiting in line" until you see the quadratic change in 
>>execution time that is typically visible when high contention occurs.
>> 
>> Maybe you can point out where this tool provides for the ability to monitor 
>>lock contention and count stalled threads and otherwise see the real contention 
>>that develops over time as load increases on a highly contended code segment?
>> 
>> If you could "time" the execution interval through all code paths, and then 
>>look at the "percentage" of threads and time through each path, and then see 
>>that the "largest" latency block was in a very common code path, you could 
>>perhaps then say this was the area to look at.  All of that analysis might not 
>>really be possible though, because of the exponential potential complexity of 
>>code path coverage.
>> 
>> Gregg Wonderly
>> 
>> William Louth (JINSPIRED.COM) wrote:
>>>  JINSPIRED'S OpenCore (http://opencore.jinspired.com) metering & metrics 
>>>runtime has built in meters for thread contention metering (activity based) @see 
>>>blocking.time and blocking.count.
>>> 
>>> You can easily extend it with your own custom counters or resource measures 
>>>mapped to meters. And you don't always need to measure time: 
>>>http://williamlouth.wordpress.com/2010/06/11/no-latency-application-performance-analysis-when-wall-clock-time-is-simply-too-slow/
>>> 
>>> 
>>> Queuing can also be aggregated at various namespace levels: 
>>>http://williamlouth.wordpress.com/2010/05/20/metered-software-service-queues/ 
>>>
>>> 
>>> More related articles: http://williamlouth.wordpress.com/category/profiling/
>>> 
>>> On 26/07/2010 21:13, Gregg Wonderly wrote:
>>>> Per thread latency measurements with 1, 2, 10 and 100 threads will often tell 
>>>>you a lot about how contention is affecting the execution time.  When you get to 
>>>>100, a thread dump will often reveal where everyone is standing in line...
>>>> 
>>>> Gregg Wonderly
>>>> 
>>>> David Holmes wrote:
>>>>> Kendall,
>>>>> 
>>>>> In my opinion a monitoring tool looking at the lock acquisition time, or CAS 
>>>>>attempts, won't give you much insight into whether to use a blocking or 
>>>>>non-blocking approach. You need to measure the performance of your application 
>>>>>logic as a whole, utilising the two different approaches. Afterall how can you 
>>>>>compare locking times with number of CAS attempts in general?
>>>>> 
>>>>> David Holmes
>>>>> 
>>>>>     -----Original Message-----
>>>>>     *From:* concurrency-interest-bounces at cs.oswego.edu
>>>>>     [mailto:concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of
>>>>>     *Kendall Moore
>>>>>     *Sent:* Sunday, 25 July 2010 4:14 PM
>>>>>     *To:* concurrency-interest at cs.oswego.edu
>>>>>     *Subject:* [concurrency-interest] Monitoring Tool
>>>>> 
>>>>>     Greetings all,
>>>>> 
>>>>>     Is there a common consensus on which monitoring tools are best to
>>>>>     use when writing parallel apps?  To be more specific, I would like
>>>>>     to be able to know how many times a given thread has to try to CAS
>>>>>     before succeeding.   Also, the ability to see how long a thread
>>>>>     waits to acquire a lock would be useful as well.  The end goal
>>>>>     would, in my particular case, would be to compare these in order to
>>>>>     determine if a non-blocking approach would be more effective in a
>>>>>     given situation than a lock-based approach.  Any help would be much
>>>>>     appreciated!
>>>>> 
>>>>>     --     Kendall Moore
>>>>> 
>>>>> 
>>>>> ------------------------------------------------------------------------ 
>>>>> 
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at cs.oswego.edu
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>> 
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>> 
>>>> 
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>> 
>>> 
>> 
>> 
>> 
> 
> 

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest



      
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20100729/4578d9fd/attachment.html>

From kkr at trifork.com  Fri Jul 30 13:05:48 2010
From: kkr at trifork.com (Karl Krukow)
Date: Fri, 30 Jul 2010 19:05:48 +0200
Subject: [concurrency-interest] Safety of initializations in Servlet init
Message-ID: <992148B0-2668-4535-8253-15A23D285D93@trifork.com>

Hello,
I've been wondering about the safety of performing (non-volatile) instance variable initializations in the Servlet or Portlet init methods. The servlet/portlet interface provides an init method that containers are required to call before putting the servlet into service, calling the service method upon each request. 

A common pattern is to perform initialization of costly objects in the init method, and to store those objects in instance variables of the Servlet/Portlet. Since it may be different threads calling the init and service methods and neither init nor service are synchronized what guarantees do we have that accessing the instance variable in the service method isn't a race condition?

As far as I can see, the servlet and portlet specifications don't even mention this problem?

This ibm article (and the fact that it is a common pattern) seems to imply that it is in-fact safe, so I am sure I am missing something :)

http://www.ibm.com/developerworks/websphere/library/bestpractices/using_httpservlet_method.html

"By definition, the init() method is thread-safe. The results of operations in the HttpServlet.init() method can be cached safely in servlet instance variables, which become read-only in the servlet service method."

(although saying that init() is thread-safe doesn't make sense to me).

Thanks,
Karl. 

From kschneider at gmail.com  Fri Jul 30 14:15:49 2010
From: kschneider at gmail.com (Kris Schneider)
Date: Fri, 30 Jul 2010 14:15:49 -0400
Subject: [concurrency-interest] Safety of initializations in Servlet init
In-Reply-To: <992148B0-2668-4535-8253-15A23D285D93@trifork.com>
References: <992148B0-2668-4535-8253-15A23D285D93@trifork.com>
Message-ID: <AANLkTimeFWNepVTPQkXJtZkx02Q=XRhcrbJ6Gf6PVnjn@mail.gmail.com>

On Fri, Jul 30, 2010 at 1:05 PM, Karl Krukow <kkr at trifork.com> wrote:
> Hello,
> I've been wondering about the safety of performing (non-volatile) instance variable initializations in the Servlet or Portlet init methods. The servlet/portlet interface provides an init method that containers are required to call before putting the servlet into service, calling the service method upon each request.
>
> A common pattern is to perform initialization of costly objects in the init method, and to store those objects in instance variables of the Servlet/Portlet. Since it may be different threads calling the init and service methods and neither init nor service are synchronized what guarantees do we have that accessing the instance variable in the service method isn't a race condition?

The article you reference below is almost 10 years old and common (or
at least better) practice has moved on to use ServletContextListener.

> As far as I can see, the servlet and portlet specifications don't even mention this problem?
>
> This ibm article (and the fact that it is a common pattern) seems to imply that it is in-fact safe, so I am sure I am missing something :)
>
> http://www.ibm.com/developerworks/websphere/library/bestpractices/using_httpservlet_method.html
>
> "By definition, the init() method is thread-safe. The results of operations in the HttpServlet.init() method can be cached safely in servlet instance variables, which become read-only in the servlet service method."
>
> (although saying that init() is thread-safe doesn't make sense to me).

Although I don't recall that Servlet.init is specifically covered,
this is a more recent and much more informative article on state and
concurrency in the context of servlet containers:

http://www.ibm.com/developerworks/library/j-jtp09238.html

> Thanks,
> Karl.

-- 
Kris Schneider


From kkr at trifork.com  Sat Jul 31 01:19:20 2010
From: kkr at trifork.com (Karl Krukow)
Date: Sat, 31 Jul 2010 07:19:20 +0200
Subject: [concurrency-interest] Safety of initializations in Servlet init
In-Reply-To: <AANLkTimeFWNepVTPQkXJtZkx02Q=XRhcrbJ6Gf6PVnjn@mail.gmail.com>
References: <992148B0-2668-4535-8253-15A23D285D93@trifork.com>
	<AANLkTimeFWNepVTPQkXJtZkx02Q=XRhcrbJ6Gf6PVnjn@mail.gmail.com>
Message-ID: <4E7C4E9E-4D1D-4A22-8A37-D504A04AC018@trifork.com>


On 30/07/2010, at 20.15, Kris Schneider wrote:

> On Fri, Jul 30, 2010 at 1:05 PM, Karl Krukow <kkr at trifork.com> wrote:
[snip]

> The article you reference below is almost 10 years old and common (or
> at least better) practice has moved on to use ServletContextListener.

Even so, the question is still valid: why is it safe to do an initialization of an instance variable in one thread (in the init method) and read the variable in another thread (in the service method) without synchronization. There must be two scenarios: (i) either it's isn't really safe and the common pattern is really broken; or (ii) app servers perform some form of synchronization under the hood to ensure a happens-before relation between the write in the init and the read in the service method.

Hopefully it is not (i), but the problem with (ii) is that as far as I can see, it isn't mentioned in the servlet or portlet specifications so if coding strictly to the spec the programmer would have to synchronize. 

Regarding: ServletContextListener: I didn't know that using it is common best practice, and the servlet spec still writes about the init method (servlet 2.5, SRV.2.3.2):

  >>After the servlet object is instantiated, the container must initialize the servlet before it can handle requests from clients. Initialization is provided so that a servlet can read persistent configuration data, initialize costly resources (such as JDBCTM API- based connections), and perform other one-time activities. The container initializes the servlet instance by calling the init method of the Servlet interface with a unique (per servlet declaration) object implementing the ServletConfig interface.<<

In either case, in the servlet context listener, the same question applies, but Brian's article you mention below shows that it is up to the user to synchronize if storing an object in the servlet context.

[snip]

> 
> Although I don't recall that Servlet.init is specifically covered,
> this is a more recent and much more informative article on state and
> concurrency in the context of servlet containers:
> 
> http://www.ibm.com/developerworks/library/j-jtp09238.html

Yes, thanks for that (good) reference. I remember reading that some time ago. It doesn't mention the init method.

[snip..]

Thanks for taking the time,
- Karl

