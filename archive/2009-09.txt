From thejo at kote.in  Tue Sep  1 15:14:56 2009
From: thejo at kote.in (Thejo Kote)
Date: Tue, 1 Sep 2009 12:14:56 -0700
Subject: [concurrency-interest] Concurrent Puts on HashMap on different
Message-ID: <3a5ad2140909011214t7ae51141vb6ed55dfa3a47f82@mail.gmail.com>

Here is an interesting analysis of a race condition -
http://mailinator.blogspot.com/2009/06/beautiful-race-condition.html

Thejo

On Tue, Sep 1, 2009 at 9:00 AM,
<concurrency-interest-request at cs.oswego.edu>wrote:

> Send Concurrency-interest mailing list submissions to
>        concurrency-interest at cs.oswego.edu
>
> To subscribe or unsubscribe via the World Wide Web, visit
>        http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> or, via email, send a message with subject or body 'help' to
>        concurrency-interest-request at cs.oswego.edu
>
> You can reach the person managing the list at
>        concurrency-interest-owner at cs.oswego.edu
>
> When replying, please edit your Subject line so it is more specific
> than "Re: Contents of Concurrency-interest digest..."
>
> Today's Topics:
>
>   1. Re: Concurrent Puts on HashMap on different       key sets. (James
> Gan)
>
>
> ---------- Forwarded message ----------
> From: James Gan <ganzhi at gmail.com>
> To: Elias Ross <genman at noderunner.net>
> Date: Tue, 1 Sep 2009 08:31:48 +0800
> Subject: Re: [concurrency-interest] Concurrent Puts on HashMap on different
> key sets.
> The worst thing in my mind is data race. If it happens, your customer might
> get wrong result without any prompt. It takes days to find such kind of bugs
> without an adequate tool.
>
> On Sat, Aug 29, 2009 at 1:57 AM, Elias Ross <genman at noderunner.net> wrote:
>
>>
>> On Fri, Aug 28, 2009 at 7:07 AM, Tim Peierls <tim at peierls.net> wrote:
>>
>>> No, you cannot. HashMap is not at all thread-safe. Examples of what might
>>> go wrong, not in any way exhaustive: Two keys from different key sets might
>>> hash to the same bucket, or the bucket array might be resized in one thread
>>> while you're putting a value in another.
>>>
>>
>> I have also experienced one thread hanging in an infinite loop on a put.
>> An infinite loop is probably the worst possible thing to happen to your
>> application, as it not only hangs one thread but it will use most of the CPU
>> resources.
>>
>> java.util.Hashtable is not safe to iterate over without locking. A college
>> said that it was, since you won't see ConcurrentModificationException thrown
>> from the Enumerator when iterating over the keys and values, but I can't
>> imagine that it is.
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
>
> --
> Best Regards
> James Gan
> Current Project: Concurrent Building Block at
> http://amino-cbbs.sourceforge.net/
> Blog: http://ganzhi.blogspot.com
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090901/b8eb62a5/attachment.html>

From ashwin.jayaprakash at gmail.com  Fri Sep  4 02:12:46 2009
From: ashwin.jayaprakash at gmail.com (Ashwin Jayaprakash)
Date: Thu, 3 Sep 2009 23:12:46 -0700
Subject: [concurrency-interest] Object allocations in different threads &
	Thread local allocation buffers
Message-ID: <837c23d40909032312g2febf590qcf1b8d99bc9423de@mail.gmail.com>

Hi, I know this forum is probably not the right place to ask these
questions, but since most of the Concurrency experts read this forum I'm
going to post my question anyway.

We all know that the newer versions of JVMs (Java 5+) have Thread local
allocation buffers. What I'm curious to know is what happens to the objects
when the following happens:
- Thread 1 allocates a bunch of objects
- These object references are handed over to Thread 2, let's say via a
BlockingQueue
- Thread 2 then starts modifying those objects

1) So, when Object mutations cross such Thread boundaries, does the TLAB get
fragmented because it cannot be reclaimed completely?
2) Does this affect performance a lot? Doesn't the new Fork-Join or any such
thread-pooling logic make heavy use of such allocations?
3) If it does affect performance, is there a better way to do things? Now,
this might be a pointless question - you might say "don't use threads
then".. but I'm just thinking aloud

Thanks!
Ashwin (http://www.ashwinjayaprakash.com)
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090903/9896076b/attachment.html>

From andrew.trick at gmail.com  Wed Sep  9 13:02:37 2009
From: andrew.trick at gmail.com (Andrew Trick)
Date: Wed, 9 Sep 2009 10:02:37 -0700
Subject: [concurrency-interest] Object allocations in different threads
	& Thread local allocation buffers
In-Reply-To: <837c23d40909032312g2febf590qcf1b8d99bc9423de@mail.gmail.com>
References: <837c23d40909032312g2febf590qcf1b8d99bc9423de@mail.gmail.com>
Message-ID: <16a4b1030909091002va1d1488r39f4cb4ebc88da1b@mail.gmail.com>

JVMs typically implement precise garbage collection. This allows the GC
thread to move objects while mutator threads hold reference (in registers or
thread stack). TLABs are small, so in that case a JVM would probably stop
the mutator threads, move ALL objects in the TLAB, update each thread's root
set (registers/stack values), then restart mutators.

TLAB objects are locally allocated... they are often not thread-local
objects.
-Andy

On Thu, Sep 3, 2009 at 11:12 PM, Ashwin Jayaprakash <
ashwin.jayaprakash at gmail.com> wrote:

> Hi, I know this forum is probably not the right place to ask these
> questions, but since most of the Concurrency experts read this forum I'm
> going to post my question anyway.
>
> We all know that the newer versions of JVMs (Java 5+) have Thread local
> allocation buffers. What I'm curious to know is what happens to the objects
> when the following happens:
> - Thread 1 allocates a bunch of objects
> - These object references are handed over to Thread 2, let's say via a
> BlockingQueue
> - Thread 2 then starts modifying those objects
>
> 1) So, when Object mutations cross such Thread boundaries, does the TLAB
> get fragmented because it cannot be reclaimed completely?
> 2) Does this affect performance a lot? Doesn't the new Fork-Join or any
> such thread-pooling logic make heavy use of such allocations?
> 3) If it does affect performance, is there a better way to do things? Now,
> this might be a pointless question - you might say "don't use threads
> then".. but I'm just thinking aloud
>
> Thanks!
> Ashwin (http://www.ashwinjayaprakash.com)
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090909/73f49223/attachment.html>

From hans.boehm at hp.com  Wed Sep  9 13:43:50 2009
From: hans.boehm at hp.com (Boehm, Hans)
Date: Wed, 9 Sep 2009 17:43:50 +0000
Subject: [concurrency-interest] Object allocations in different
	threads	& Thread local allocation buffers
In-Reply-To: <16a4b1030909091002va1d1488r39f4cb4ebc88da1b@mail.gmail.com>
References: <837c23d40909032312g2febf590qcf1b8d99bc9423de@mail.gmail.com>
	<16a4b1030909091002va1d1488r39f4cb4ebc88da1b@mail.gmail.com>
Message-ID: <238A96A773B3934685A7269CC8A8D042577AF1C715@GVW0436EXB.americas.hpqcorp.net>

Andy's last observation here is the crucial one; these objects are typically allocated essentially by allocating a large block and then dividing it.  This avoids locking on the allocation path.  The resulting objects are usually not in any sense thread-local.

I'm not sure how the details are handled in Hotspot.  We in fact do something bery similar in our conservative non-moving collector, though the "TLAB" there is not even a contiguous piece of memory.  A thread can grab a bunch of objects of a given size (linked together in a free-list) at once with a single lock acquisition, and then parcel them out as needed without further locking.  Though this isn't quite as fast as the contiguous case, it still seems to get most of the benefit.

Hans

________________________________
From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Andrew Trick
Sent: Wednesday, September 09, 2009 10:03 AM
To: Ashwin Jayaprakash
Cc: concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] Object allocations in different threads & Thread local allocation buffers

JVMs typically implement precise garbage collection. This allows the GC thread to move objects while mutator threads hold reference (in registers or thread stack). TLABs are small, so in that case a JVM would probably stop the mutator threads, move ALL objects in the TLAB, update each thread's root set (registers/stack values), then restart mutators.

TLAB objects are locally allocated... they are often not thread-local objects.
-Andy

On Thu, Sep 3, 2009 at 11:12 PM, Ashwin Jayaprakash <ashwin.jayaprakash at gmail.com<mailto:ashwin.jayaprakash at gmail.com>> wrote:
Hi, I know this forum is probably not the right place to ask these questions, but since most of the Concurrency experts read this forum I'm going to post my question anyway.

We all know that the newer versions of JVMs (Java 5+) have Thread local allocation buffers. What I'm curious to know is what happens to the objects when the following happens:
- Thread 1 allocates a bunch of objects
- These object references are handed over to Thread 2, let's say via a BlockingQueue
- Thread 2 then starts modifying those objects

1) So, when Object mutations cross such Thread boundaries, does the TLAB get fragmented because it cannot be reclaimed completely?
2) Does this affect performance a lot? Doesn't the new Fork-Join or any such thread-pooling logic make heavy use of such allocations?
3) If it does affect performance, is there a better way to do things? Now, this might be a pointless question - you might say "don't use threads then".. but I'm just thinking aloud

Thanks!
Ashwin (http://www.ashwinjayaprakash.com)

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu<mailto:Concurrency-interest at cs.oswego.edu>
http://cs.oswego.edu/mailman/listinfo/concurrency-interest


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090909/0e43dc4d/attachment.html>

From ashwin.jayaprakash at gmail.com  Wed Sep  9 13:54:24 2009
From: ashwin.jayaprakash at gmail.com (Ashwin Jayaprakash)
Date: Wed, 9 Sep 2009 10:54:24 -0700
Subject: [concurrency-interest] Object allocations in different threads
	& Thread local allocation buffers
In-Reply-To: <238A96A773B3934685A7269CC8A8D042577AF1C715@GVW0436EXB.americas.hpqcorp.net>
References: <837c23d40909032312g2febf590qcf1b8d99bc9423de@mail.gmail.com>
	<16a4b1030909091002va1d1488r39f4cb4ebc88da1b@mail.gmail.com>
	<238A96A773B3934685A7269CC8A8D042577AF1C715@GVW0436EXB.americas.hpqcorp.net>
Message-ID: <837c23d40909091054l2f068f1dp1a11e723ddfc3ad3@mail.gmail.com>

Thanks for explaining.

Ashwin.


On Wed, Sep 9, 2009 at 10:43 AM, Boehm, Hans <hans.boehm at hp.com> wrote:

>  Andy's last observation here is the crucial one; these objects are
> typically allocated essentially by allocating a large block and then
> dividing it.  This avoids locking on the allocation path.  The resulting
> objects are usually not in any sense thread-local.
>
> I'm not sure how the details are handled in Hotspot.  We in fact do
> something bery similar in our conservative non-moving collector, though the
> "TLAB" there is not even a contiguous piece of memory.  A thread can grab a
> bunch of objects of a given size (linked together in a free-list) at once
> with a single lock acquisition, and then parcel them out as needed without
> further locking.  Though this isn't quite as fast as the contiguous case, it
> still seems to get most of the benefit.
>
> Hans
>
>  ------------------------------
> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
> concurrency-interest-bounces at cs.oswego.edu] *On Behalf Of *Andrew Trick
> *Sent:* Wednesday, September 09, 2009 10:03 AM
> *To:* Ashwin Jayaprakash
> *Cc:* concurrency-interest at cs.oswego.edu
> *Subject:* Re: [concurrency-interest] Object allocations in different
> threads & Thread local allocation buffers
>
> JVMs typically implement precise garbage collection. This allows the GC
> thread to move objects while mutator threads hold reference (in registers or
> thread stack). TLABs are small, so in that case a JVM would probably stop
> the mutator threads, move ALL objects in the TLAB, update each thread's root
> set (registers/stack values), then restart mutators.
>
> TLAB objects are locally allocated... they are often not thread-local
> objects.
> -Andy
>
> On Thu, Sep 3, 2009 at 11:12 PM, Ashwin Jayaprakash <
> ashwin.jayaprakash at gmail.com> wrote:
>
>> Hi, I know this forum is probably not the right place to ask these
>> questions, but since most of the Concurrency experts read this forum I'm
>> going to post my question anyway.
>>
>> We all know that the newer versions of JVMs (Java 5+) have Thread local
>> allocation buffers. What I'm curious to know is what happens to the objects
>> when the following happens:
>> - Thread 1 allocates a bunch of objects
>> - These object references are handed over to Thread 2, let's say via a
>> BlockingQueue
>> - Thread 2 then starts modifying those objects
>>
>> 1) So, when Object mutations cross such Thread boundaries, does the TLAB
>> get fragmented because it cannot be reclaimed completely?
>> 2) Does this affect performance a lot? Doesn't the new Fork-Join or any
>> such thread-pooling logic make heavy use of such allocations?
>> 3) If it does affect performance, is there a better way to do things? Now,
>> this might be a pointless question - you might say "don't use threads
>> then".. but I'm just thinking aloud
>>
>> Thanks!
>> Ashwin (http://www.ashwinjayaprakash.com)
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090909/c9fd197e/attachment.html>

From jdmarshall at gmail.com  Wed Sep 16 16:45:39 2009
From: jdmarshall at gmail.com (jason marshall)
Date: Wed, 16 Sep 2009 13:45:39 -0700
Subject: [concurrency-interest] STM and Spinning
In-Reply-To: <1466c1d60908231350o7d234d12ma02b566bbcade35e@mail.gmail.com>
References: <1466c1d60908231350o7d234d12ma02b566bbcade35e@mail.gmail.com>
Message-ID: <3cf41bb90909161345k1ca6ae4epd6d999ddb37b6a35@mail.gmail.com>

I think in one of the Peyton-Jones interview videos, (if not in one of their
papers), he mentions another option, which is to track what read operations
lead to the abort, and not to restart the transaction until a commit
succeeds on one of those fields.



On Sun, Aug 23, 2009 at 1:50 PM, Peter Veentjer <alarmnummer at gmail.com>wrote:

> Hi Guys,
>
> I'm working on a software transactional memory implementation and one
> of the things I want to pick up is spinning and I'm looking for
> advice/ideas.
>
> With my main STM implementation if can happen that a load is done for
> a specific version (the data loaded should have a version equal or
> smaller than the load version) even though the write has not happened
> yet. When the load detects that it can't determine if the version the
> transaction is looking for has been written, it aborts the transaction
> and the transaction is retried. If this problem is not detected, the
> STM could suffer from isolation problems (not seeing changes made by
> transactions completed before it) and we don't want that. With
> databases this particular problem is called the lost update.
>
> The problem of aborting is that a lot of work has been executed for
> nothing. To lower the chance that a transaction aborts with this
> failure, I want to add spinning: just keep repeat reading until the
> load can determine if the current version is the correct one (or not).
> Since this ambiguous period is very short, spinning could be a light
> weight solution.
>
> The simplest approach that comes to mind is some form of bounded
> spinning. But what kind of value should be used for the number of
> iterations? 10, 100, 1000?
>
> Or should it be more flexible like adaptive spinning used on the
> intrinsic lock? If yes, how to design such a component? And how to
> prevent it causing overhead?
>
> And is there no better way? What about the (deprecated) Thread.yield
> method? The advantage is that other transactions have more cpu-room,
> so are more likely to complete sooner. And the sooner other
> transactions complete, the sooner the load can complete.
>
> ps:
> using a lock in combination with a wait set is to expensive here.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



-- 
- Jason
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090916/a8603537/attachment.html>

From Henry.Qin at pnl.gov  Wed Sep 16 18:57:09 2009
From: Henry.Qin at pnl.gov (Qin, Henry)
Date: Wed, 16 Sep 2009 15:57:09 -0700
Subject: [concurrency-interest] ForkJoinTasks seem to quit without throwing,
	on larger tasks.
Message-ID: <5FD617D5BE1D114CB1F10F4358D109F501255A06@EMAIL02.pnl.gov>

Hi,

In the following example, it appears that forkjoin simply quits without
completing its task. 
If you change Integer.MAX_VALUE to 1000, the task is completed
correctly.

import jsr166y.*;

public class ForkPlay extends RecursiveAction{

    int myInt;

    public ForkPlay(int i){ myInt = i;}
    public void compute(){
        try{
//            if (myInt > 1000) return;
            (new ForkPlay(myInt+1)).fork();
            System.out.println(myInt);
            int tcount = 0;
            while (tcount < Integer.MAX_VALUE)
                tcount++;
        }
        catch (Exception e){
            e.printStackTrace();
        }
    }
    static ForkJoinPool pool;

    public static void main(String[] args) throws Exception{
            pool = new ForkJoinPool(7);
            pool.invoke(new ForkPlay(1));

    }
}

If someone has come across the behavior before, could they please share
a work-around?
If this is indeed a hard limitation on the size of tasks that
ForkJoinTasks can accomplish, perhaps it should be documented? The
current documentation speaks of an ideal ForkJoinTask as being between
100 and 10000 computations, but does not mention that the framework will
simply quit with larger tasks.
Note that I've tried to extend ForkJoinTask directly as well, and not
just RecursiveAction.


Thanks,
~Henry
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090916/a9baa154/attachment.html>

From davidcholmes at aapt.net.au  Wed Sep 16 19:09:13 2009
From: davidcholmes at aapt.net.au (David Holmes)
Date: Thu, 17 Sep 2009 09:09:13 +1000
Subject: [concurrency-interest] ForkJoinTasks seem to quit without
	throwing, on larger tasks.
In-Reply-To: <5FD617D5BE1D114CB1F10F4358D109F501255A06@EMAIL02.pnl.gov>
Message-ID: <NFBBKALFDCPFIDBNKAPCGEHIIDAA.davidcholmes@aapt.net.au>

ForkJoinTasks seem to quit without throwing, on larger tasks.Does it also
quit at, say, 10000?

I think you are seeing the JIT compiler at work. The whole loop does nothing
visible so can simply be elided.

> int tcount = 0;
> while (tcount < Integer.MAX_VALUE)
>   tcount++;

The JIT threshhold for the loop is around 1500 if I recall correctly. Try
making tcount a field instead and refer to it elsewhere. Though even then,
the loop could be replaced with a simple assignment (not sure the JIT will
see that though :) )

David Holmes
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Qin, Henry
  Sent: Thursday, 17 September 2009 8:57 AM
  To: concurrency-interest at cs.oswego.edu
  Subject: [concurrency-interest] ForkJoinTasks seem to quit without
throwing,on larger tasks.


  Hi,

  In the following example, it appears that forkjoin simply quits without
completing its task.

  If you change Integer.MAX_VALUE to 1000, the task is completed correctly.


  import jsr166y.*;

  public class ForkPlay extends RecursiveAction{

      int myInt;

      public ForkPlay(int i){ myInt = i;}

      public void compute(){

          try{

  //            if (myInt > 1000) return;

              (new ForkPlay(myInt+1)).fork();

              System.out.println(myInt);

              int tcount = 0;

              while (tcount < Integer.MAX_VALUE)

                  tcount++;

          }

          catch (Exception e){

              e.printStackTrace();

          }

      }

      static ForkJoinPool pool;

      public static void main(String[] args) throws Exception{

              pool = new ForkJoinPool(7);

              pool.invoke(new ForkPlay(1));

      }

  }

  If someone has come across the behavior before, could they please share a
work-around?

  If this is indeed a hard limitation on the size of tasks that
ForkJoinTasks can accomplish, perhaps it should be documented? The current
documentation speaks of an ideal ForkJoinTask as being between 100 and 10000
computations, but does not mention that the framework will simply quit with
larger tasks.

  Note that I've tried to extend ForkJoinTask directly as well, and not just
RecursiveAction.



  Thanks,

  ~Henry
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090917/d8e3511f/attachment-0001.html>

From Henry.Qin at pnl.gov  Wed Sep 16 19:20:51 2009
From: Henry.Qin at pnl.gov (Qin, Henry)
Date: Wed, 16 Sep 2009 16:20:51 -0700
Subject: [concurrency-interest] ForkJoinTasks seem to quit without
	throwing, on larger tasks.
In-Reply-To: <NFBBKALFDCPFIDBNKAPCGEHIIDAA.davidcholmes@aapt.net.au>
References: <5FD617D5BE1D114CB1F10F4358D109F501255A06@EMAIL02.pnl.gov>
	<NFBBKALFDCPFIDBNKAPCGEHIIDAA.davidcholmes@aapt.net.au>
Message-ID: <5FD617D5BE1D114CB1F10F4358D109F501255A1C@EMAIL02.pnl.gov>

The problem is not that the loop is not looping. What happens is that
the print statement in the code (which is not in the loop) doesn't print
all the numbers from 1 to 1000, as it would when you change
Integer.MAX_VALUE to 1000 or even 2000.

 

That is, it is only printing some of those numbers (it's not always the
same number of numbers, but always less than the full 1000). That leads
me to believe that forkjoin is simply giving up when the task grows too
large (I have no idea why it might do this, but that's what it looks
like).

 

Thanks,

~Henry

 

From: David Holmes [mailto:davidcholmes at aapt.net.au] 
Sent: Wednesday, September 16, 2009 4:09 PM
To: Qin, Henry; concurrency-interest at cs.oswego.edu
Subject: RE: [concurrency-interest] ForkJoinTasks seem to quit without
throwing,on larger tasks.

 

Does it also quit at, say, 10000?

 

I think you are seeing the JIT compiler at work. The whole loop does
nothing visible so can simply be elided.

 

> int tcount = 0;  

> while (tcount < Integer.MAX_VALUE)   

>   tcount++;

 

The JIT threshhold for the loop is around 1500 if I recall correctly.
Try making tcount a field instead and refer to it elsewhere. Though even
then, the loop could be replaced with a simple assignment (not sure the
JIT will see that though :) )

 

David Holmes

	-----Original Message-----
	From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Qin,
Henry
	Sent: Thursday, 17 September 2009 8:57 AM
	To: concurrency-interest at cs.oswego.edu
	Subject: [concurrency-interest] ForkJoinTasks seem to quit
without throwing,on larger tasks.

	Hi,

	In the following example, it appears that forkjoin simply quits
without completing its task. 

	If you change Integer.MAX_VALUE to 1000, the task is completed
correctly.

	import jsr166y.*;

	public class ForkPlay extends RecursiveAction{

	    int myInt;

	    public ForkPlay(int i){ myInt = i;}

	    public void compute(){

	        try{

	//            if (myInt > 1000) return;

	            (new ForkPlay(myInt+1)).fork();

	            System.out.println(myInt);

	            int tcount = 0;

	            while (tcount < Integer.MAX_VALUE)

	                tcount++;

	        }

	        catch (Exception e){

	            e.printStackTrace();

	        }

	    }

	    static ForkJoinPool pool;

	    public static void main(String[] args) throws Exception{

	            pool = new ForkJoinPool(7);

	            pool.invoke(new ForkPlay(1));

	    }

	}

	If someone has come across the behavior before, could they
please share a work-around?

	If this is indeed a hard limitation on the size of tasks that
ForkJoinTasks can accomplish, perhaps it should be documented? The
current documentation speaks of an ideal ForkJoinTask as being between
100 and 10000 computations, but does not mention that the framework will
simply quit with larger tasks.

	Note that I've tried to extend ForkJoinTask directly as well,
and not just RecursiveAction.

	 

	Thanks,

	~Henry

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090916/c0b2ab24/attachment.html>

From davidcholmes at aapt.net.au  Wed Sep 16 19:27:47 2009
From: davidcholmes at aapt.net.au (David Holmes)
Date: Thu, 17 Sep 2009 09:27:47 +1000
Subject: [concurrency-interest] ForkJoinTasks seem to quit without
	throwing, on larger tasks.
In-Reply-To: <5FD617D5BE1D114CB1F10F4358D109F501255A1C@EMAIL02.pnl.gov>
Message-ID: <NFBBKALFDCPFIDBNKAPCOEHIIDAA.davidcholmes@aapt.net.au>

ForkJoinTasks seem to quit without throwing, on larger tasks.Henry,

Please clarify. The only occurrence of Integer.MAX_VALUE is in the loop that
I quoted so I don't see what affect changing it can have on the println.

Another possibility is that you're hitting an exception (an Error not
Exception) - such as OutOfMemoryError - and so you fail silently. I'm not
sure what happens with uncaught exceptions in the FJ framework.

(I don't have jsr166y installed locally so can't try this directly right
now.)

David Holmes
  -----Original Message-----
  From: Qin, Henry [mailto:Henry.Qin at pnl.gov]
  Sent: Thursday, 17 September 2009 9:21 AM
  To: dholmes at ieee.org; concurrency-interest at cs.oswego.edu
  Subject: RE: [concurrency-interest] ForkJoinTasks seem to quit without
throwing,on larger tasks.


  The problem is not that the loop is not looping. What happens is that the
print statement in the code (which is not in the loop) doesn't print all the
numbers from 1 to 1000, as it would when you change Integer.MAX_VALUE to
1000 or even 2000.



  That is, it is only printing some of those numbers (it's not always the
same number of numbers, but always less than the full 1000). That leads me
to believe that forkjoin is simply giving up when the task grows too large
(I have no idea why it might do this, but that's what it looks like).



  Thanks,

  ~Henry



  From: David Holmes [mailto:davidcholmes at aapt.net.au]
  Sent: Wednesday, September 16, 2009 4:09 PM
  To: Qin, Henry; concurrency-interest at cs.oswego.edu
  Subject: RE: [concurrency-interest] ForkJoinTasks seem to quit without
throwing,on larger tasks.



  Does it also quit at, say, 10000?



  I think you are seeing the JIT compiler at work. The whole loop does
nothing visible so can simply be elided.



  > int tcount = 0;

  > while (tcount < Integer.MAX_VALUE)

  >   tcount++;



  The JIT threshhold for the loop is around 1500 if I recall correctly. Try
making tcount a field instead and refer to it elsewhere. Though even then,
the loop could be replaced with a simple assignment (not sure the JIT will
see that though :) )



  David Holmes

    -----Original Message-----
    From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Qin, Henry
    Sent: Thursday, 17 September 2009 8:57 AM
    To: concurrency-interest at cs.oswego.edu
    Subject: [concurrency-interest] ForkJoinTasks seem to quit without
throwing,on larger tasks.

    Hi,

    In the following example, it appears that forkjoin simply quits without
completing its task.

    If you change Integer.MAX_VALUE to 1000, the task is completed
correctly.

    import jsr166y.*;

    public class ForkPlay extends RecursiveAction{

        int myInt;

        public ForkPlay(int i){ myInt = i;}

        public void compute(){

            try{

    //            if (myInt > 1000) return;

                (new ForkPlay(myInt+1)).fork();

                System.out.println(myInt);

                int tcount = 0;

                while (tcount < Integer.MAX_VALUE)

                    tcount++;

            }

            catch (Exception e){

                e.printStackTrace();

            }

        }

        static ForkJoinPool pool;

        public static void main(String[] args) throws Exception{

                pool = new ForkJoinPool(7);

                pool.invoke(new ForkPlay(1));

        }

    }

    If someone has come across the behavior before, could they please share
a work-around?

    If this is indeed a hard limitation on the size of tasks that
ForkJoinTasks can accomplish, perhaps it should be documented? The current
documentation speaks of an ideal ForkJoinTask as being between 100 and 10000
computations, but does not mention that the framework will simply quit with
larger tasks.

    Note that I've tried to extend ForkJoinTask directly as well, and not
just RecursiveAction.



    Thanks,

    ~Henry
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090917/49821a9d/attachment-0001.html>

From Henry.Qin at pnl.gov  Wed Sep 16 19:38:33 2009
From: Henry.Qin at pnl.gov (Qin, Henry)
Date: Wed, 16 Sep 2009 16:38:33 -0700
Subject: [concurrency-interest] ForkJoinTasks seem to quit without
	throwing, on larger tasks.
In-Reply-To: <NFBBKALFDCPFIDBNKAPCOEHIIDAA.davidcholmes@aapt.net.au>
References: <5FD617D5BE1D114CB1F10F4358D109F501255A1C@EMAIL02.pnl.gov>
	<NFBBKALFDCPFIDBNKAPCOEHIIDAA.davidcholmes@aapt.net.au>
Message-ID: <5FD617D5BE1D114CB1F10F4358D109F501255A2D@EMAIL02.pnl.gov>

First, I just realized that the code below had an if statement commented
out. That was a mistake that was not in the code I was compiling and
testing. That limits the amount of tasks created.

 

It's exactly as you said,  the only occurrence of Integer.MAX_VALUE is
in the loop, and the println appears to be completely separate in the
code.

What I observe is that when I change that value (the place where
integer.MAX_VALUE currently is), it affects whether or not all the
prints are executed. 

 

The expected behavior of the code is to print all the numbers from 1 to
1000, although not necessarily in that order. The code as it is below
(without the conditional commented out) only prints some of the numbers
from 1 to 1000. However, when I change Integer.MAX_VALUE to 1000 or
2000, then the code behaves as expected, printing all the numbers from 1
to 1000.

 

For clarity, here is the code again, without the incorrect comment:

 

import jsr166y.*;

public class ForkPlay extends RecursiveAction{

    int myInt;

    public ForkPlay(int i){ myInt = i;}

    public void compute(){

        try{

          if (myInt > 1000) return;

            (new ForkPlay(myInt+1)).fork();

            System.out.println(myInt);

            int tcount = 0;

            while (tcount < Integer.MAX_VALUE)

                tcount++;

        }

        catch (Exception e){

            e.printStackTrace();

        }

    }

    static ForkJoinPool pool;

    public static void main(String[] args) throws Exception{

            pool = new ForkJoinPool(7);

            pool.invoke(new ForkPlay(1));

    }

}

 

 

From: David Holmes [mailto:davidcholmes at aapt.net.au] 
Sent: Wednesday, September 16, 2009 4:28 PM
To: Qin, Henry; concurrency-interest at cs.oswego.edu
Subject: RE: [concurrency-interest] ForkJoinTasks seem to quit without
throwing,on larger tasks.

 

Henry,

 

Please clarify. The only occurrence of Integer.MAX_VALUE is in the loop
that I quoted so I don't see what affect changing it can have on the
println.

 

Another possibility is that you're hitting an exception (an Error not
Exception) - such as OutOfMemoryError - and so you fail silently. I'm
not sure what happens with uncaught exceptions in the FJ framework.

 

(I don't have jsr166y installed locally so can't try this directly right
now.)

 

David Holmes

	-----Original Message-----
	From: Qin, Henry [mailto:Henry.Qin at pnl.gov]
	Sent: Thursday, 17 September 2009 9:21 AM
	To: dholmes at ieee.org; concurrency-interest at cs.oswego.edu
	Subject: RE: [concurrency-interest] ForkJoinTasks seem to quit
without throwing,on larger tasks.

	The problem is not that the loop is not looping. What happens is
that the print statement in the code (which is not in the loop) doesn't
print all the numbers from 1 to 1000, as it would when you change
Integer.MAX_VALUE to 1000 or even 2000.

	 

	That is, it is only printing some of those numbers (it's not
always the same number of numbers, but always less than the full 1000).
That leads me to believe that forkjoin is simply giving up when the task
grows too large (I have no idea why it might do this, but that's what it
looks like).

	 

	Thanks,

	~Henry

	 

	From: David Holmes [mailto:davidcholmes at aapt.net.au] 
	Sent: Wednesday, September 16, 2009 4:09 PM
	To: Qin, Henry; concurrency-interest at cs.oswego.edu
	Subject: RE: [concurrency-interest] ForkJoinTasks seem to quit
without throwing,on larger tasks.

	 

	Does it also quit at, say, 10000?

	 

	I think you are seeing the JIT compiler at work. The whole loop
does nothing visible so can simply be elided.

	 

	> int tcount = 0;  

	> while (tcount < Integer.MAX_VALUE)   

	>   tcount++;

	 

	The JIT threshhold for the loop is around 1500 if I recall
correctly. Try making tcount a field instead and refer to it elsewhere.
Though even then, the loop could be replaced with a simple assignment
(not sure the JIT will see that though :) )

	 

	David Holmes

		-----Original Message-----
		From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Qin,
Henry
		Sent: Thursday, 17 September 2009 8:57 AM
		To: concurrency-interest at cs.oswego.edu
		Subject: [concurrency-interest] ForkJoinTasks seem to
quit without throwing,on larger tasks.

		Hi,

		In the following example, it appears that forkjoin
simply quits without completing its task. 

		If you change Integer.MAX_VALUE to 1000, the task is
completed correctly.

		import jsr166y.*;

		public class ForkPlay extends RecursiveAction{

		    int myInt;

		    public ForkPlay(int i){ myInt = i;}

		    public void compute(){

		        try{

		      //     if (myInt > 1000) return;

		            (new ForkPlay(myInt+1)).fork();

		            System.out.println(myInt);

		            int tcount = 0;

		            while (tcount < Integer.MAX_VALUE)

		                tcount++;

		        }

		        catch (Exception e){

		            e.printStackTrace();

		        }

		    }

		    static ForkJoinPool pool;

		    public static void main(String[] args) throws
Exception{

		            pool = new ForkJoinPool(7);

		            pool.invoke(new ForkPlay(1));

		    }

		}

		If someone has come across the behavior before, could
they please share a work-around?

		If this is indeed a hard limitation on the size of tasks
that ForkJoinTasks can accomplish, perhaps it should be documented? The
current documentation speaks of an ideal ForkJoinTask as being between
100 and 10000 computations, but does not mention that the framework will
simply quit with larger tasks.

		Note that I've tried to extend ForkJoinTask directly as
well, and not just RecursiveAction.

		 

		Thanks,

		~Henry

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090916/6da59aca/attachment.html>

From davidcholmes at aapt.net.au  Wed Sep 16 19:44:52 2009
From: davidcholmes at aapt.net.au (David Holmes)
Date: Thu, 17 Sep 2009 09:44:52 +1000
Subject: [concurrency-interest] ForkJoinTasks seem to quit without
	throwing, on larger tasks.
In-Reply-To: <5FD617D5BE1D114CB1F10F4358D109F501255A2D@EMAIL02.pnl.gov>
Message-ID: <NFBBKALFDCPFIDBNKAPCGEHJIDAA.davidcholmes@aapt.net.au>

ForkJoinTasks seem to quit without throwing, on larger tasks.Henry,

Did you check for OutOfMemoryError?

The loop (assuming it isn't compiled away) affects the lifetime of the task
and hence the amount of memory needed.

I'll see if I can run this on another system ... not sure if FJ has been
pushed into JDK7 yet.

David
  -----Original Message-----
  From: Qin, Henry [mailto:Henry.Qin at pnl.gov]
  Sent: Thursday, 17 September 2009 9:39 AM
  To: dholmes at ieee.org; concurrency-interest at cs.oswego.edu
  Subject: RE: [concurrency-interest] ForkJoinTasks seem to quit without
throwing,on larger tasks.


  First, I just realized that the code below had an if statement commented
out. That was a mistake that was not in the code I was compiling and
testing. That limits the amount of tasks created.



  It's exactly as you said,  the only occurrence of Integer.MAX_VALUE is in
the loop, and the println appears to be completely separate in the code.

  What I observe is that when I change that value (the place where
integer.MAX_VALUE currently is), it affects whether or not all the prints
are executed.



  The expected behavior of the code is to print all the numbers from 1 to
1000, although not necessarily in that order. The code as it is below
(without the conditional commented out) only prints some of the numbers from
1 to 1000. However, when I change Integer.MAX_VALUE to 1000 or 2000, then
the code behaves as expected, printing all the numbers from 1 to 1000.



  For clarity, here is the code again, without the incorrect comment:



  import jsr166y.*;

  public class ForkPlay extends RecursiveAction{

      int myInt;

      public ForkPlay(int i){ myInt = i;}

      public void compute(){

          try{

            if (myInt > 1000) return;

              (new ForkPlay(myInt+1)).fork();

              System.out.println(myInt);

              int tcount = 0;

              while (tcount < Integer.MAX_VALUE)

                  tcount++;

          }

          catch (Exception e){

              e.printStackTrace();

          }

      }

      static ForkJoinPool pool;

      public static void main(String[] args) throws Exception{

              pool = new ForkJoinPool(7);

              pool.invoke(new ForkPlay(1));

      }

  }





  From: David Holmes [mailto:davidcholmes at aapt.net.au]
  Sent: Wednesday, September 16, 2009 4:28 PM
  To: Qin, Henry; concurrency-interest at cs.oswego.edu
  Subject: RE: [concurrency-interest] ForkJoinTasks seem to quit without
throwing,on larger tasks.



  Henry,



  Please clarify. The only occurrence of Integer.MAX_VALUE is in the loop
that I quoted so I don't see what affect changing it can have on the
println.



  Another possibility is that you're hitting an exception (an Error not
Exception) - such as OutOfMemoryError - and so you fail silently. I'm not
sure what happens with uncaught exceptions in the FJ framework.



  (I don't have jsr166y installed locally so can't try this directly right
now.)



  David Holmes

    -----Original Message-----
    From: Qin, Henry [mailto:Henry.Qin at pnl.gov]
    Sent: Thursday, 17 September 2009 9:21 AM
    To: dholmes at ieee.org; concurrency-interest at cs.oswego.edu
    Subject: RE: [concurrency-interest] ForkJoinTasks seem to quit without
throwing,on larger tasks.

    The problem is not that the loop is not looping. What happens is that
the print statement in the code (which is not in the loop) doesn't print all
the numbers from 1 to 1000, as it would when you change Integer.MAX_VALUE to
1000 or even 2000.



    That is, it is only printing some of those numbers (it's not always the
same number of numbers, but always less than the full 1000). That leads me
to believe that forkjoin is simply giving up when the task grows too large
(I have no idea why it might do this, but that's what it looks like).



    Thanks,

    ~Henry



    From: David Holmes [mailto:davidcholmes at aapt.net.au]
    Sent: Wednesday, September 16, 2009 4:09 PM
    To: Qin, Henry; concurrency-interest at cs.oswego.edu
    Subject: RE: [concurrency-interest] ForkJoinTasks seem to quit without
throwing,on larger tasks.



    Does it also quit at, say, 10000?



    I think you are seeing the JIT compiler at work. The whole loop does
nothing visible so can simply be elided.



    > int tcount = 0;

    > while (tcount < Integer.MAX_VALUE)

    >   tcount++;



    The JIT threshhold for the loop is around 1500 if I recall correctly.
Try making tcount a field instead and refer to it elsewhere. Though even
then, the loop could be replaced with a simple assignment (not sure the JIT
will see that though :) )



    David Holmes

      -----Original Message-----
      From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Qin, Henry
      Sent: Thursday, 17 September 2009 8:57 AM
      To: concurrency-interest at cs.oswego.edu
      Subject: [concurrency-interest] ForkJoinTasks seem to quit without
throwing,on larger tasks.

      Hi,

      In the following example, it appears that forkjoin simply quits
without completing its task.

      If you change Integer.MAX_VALUE to 1000, the task is completed
correctly.

      import jsr166y.*;

      public class ForkPlay extends RecursiveAction{

          int myInt;

          public ForkPlay(int i){ myInt = i;}

          public void compute(){

              try{

            //     if (myInt > 1000) return;

                  (new ForkPlay(myInt+1)).fork();

                  System.out.println(myInt);

                  int tcount = 0;

                  while (tcount < Integer.MAX_VALUE)

                      tcount++;

              }

              catch (Exception e){

                  e.printStackTrace();

              }

          }

          static ForkJoinPool pool;

          public static void main(String[] args) throws Exception{

                  pool = new ForkJoinPool(7);

                  pool.invoke(new ForkPlay(1));

          }

      }

      If someone has come across the behavior before, could they please
share a work-around?

      If this is indeed a hard limitation on the size of tasks that
ForkJoinTasks can accomplish, perhaps it should be documented? The current
documentation speaks of an ideal ForkJoinTask as being between 100 and 10000
computations, but does not mention that the framework will simply quit with
larger tasks.

      Note that I've tried to extend ForkJoinTask directly as well, and not
just RecursiveAction.



      Thanks,

      ~Henry
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090917/7171f439/attachment-0001.html>

From davidcholmes at aapt.net.au  Wed Sep 16 20:53:29 2009
From: davidcholmes at aapt.net.au (David Holmes)
Date: Thu, 17 Sep 2009 10:53:29 +1000
Subject: [concurrency-interest] ForkJoinTasks seem to quit without
	throwing, on larger tasks.
In-Reply-To: <5FD617D5BE1D114CB1F10F4358D109F501255A2D@EMAIL02.pnl.gov>
Message-ID: <NFBBKALFDCPFIDBNKAPCMEHJIDAA.davidcholmes@aapt.net.au>

ForkJoinTasks seem to quit without throwing, on larger tasks.Henry,

ForkJoinPool uses daemon threads by default so your application is
terminating before the task is done.

David
  -----Original Message-----
  From: Qin, Henry [mailto:Henry.Qin at pnl.gov]
  Sent: Thursday, 17 September 2009 9:39 AM
  To: dholmes at ieee.org; concurrency-interest at cs.oswego.edu
  Subject: RE: [concurrency-interest] ForkJoinTasks seem to quit without
throwing,on larger tasks.


  First, I just realized that the code below had an if statement commented
out. That was a mistake that was not in the code I was compiling and
testing. That limits the amount of tasks created.



  It's exactly as you said,  the only occurrence of Integer.MAX_VALUE is in
the loop, and the println appears to be completely separate in the code.

  What I observe is that when I change that value (the place where
integer.MAX_VALUE currently is), it affects whether or not all the prints
are executed.



  The expected behavior of the code is to print all the numbers from 1 to
1000, although not necessarily in that order. The code as it is below
(without the conditional commented out) only prints some of the numbers from
1 to 1000. However, when I change Integer.MAX_VALUE to 1000 or 2000, then
the code behaves as expected, printing all the numbers from 1 to 1000.



  For clarity, here is the code again, without the incorrect comment:



  import jsr166y.*;

  public class ForkPlay extends RecursiveAction{

      int myInt;

      public ForkPlay(int i){ myInt = i;}

      public void compute(){

          try{

            if (myInt > 1000) return;

              (new ForkPlay(myInt+1)).fork();

              System.out.println(myInt);

              int tcount = 0;

              while (tcount < Integer.MAX_VALUE)

                  tcount++;

          }

          catch (Exception e){

              e.printStackTrace();

          }

      }

      static ForkJoinPool pool;

      public static void main(String[] args) throws Exception{

              pool = new ForkJoinPool(7);

              pool.invoke(new ForkPlay(1));

      }

  }





  From: David Holmes [mailto:davidcholmes at aapt.net.au]
  Sent: Wednesday, September 16, 2009 4:28 PM
  To: Qin, Henry; concurrency-interest at cs.oswego.edu
  Subject: RE: [concurrency-interest] ForkJoinTasks seem to quit without
throwing,on larger tasks.



  Henry,



  Please clarify. The only occurrence of Integer.MAX_VALUE is in the loop
that I quoted so I don't see what affect changing it can have on the
println.



  Another possibility is that you're hitting an exception (an Error not
Exception) - such as OutOfMemoryError - and so you fail silently. I'm not
sure what happens with uncaught exceptions in the FJ framework.



  (I don't have jsr166y installed locally so can't try this directly right
now.)



  David Holmes

    -----Original Message-----
    From: Qin, Henry [mailto:Henry.Qin at pnl.gov]
    Sent: Thursday, 17 September 2009 9:21 AM
    To: dholmes at ieee.org; concurrency-interest at cs.oswego.edu
    Subject: RE: [concurrency-interest] ForkJoinTasks seem to quit without
throwing,on larger tasks.

    The problem is not that the loop is not looping. What happens is that
the print statement in the code (which is not in the loop) doesn't print all
the numbers from 1 to 1000, as it would when you change Integer.MAX_VALUE to
1000 or even 2000.



    That is, it is only printing some of those numbers (it's not always the
same number of numbers, but always less than the full 1000). That leads me
to believe that forkjoin is simply giving up when the task grows too large
(I have no idea why it might do this, but that's what it looks like).



    Thanks,

    ~Henry



    From: David Holmes [mailto:davidcholmes at aapt.net.au]
    Sent: Wednesday, September 16, 2009 4:09 PM
    To: Qin, Henry; concurrency-interest at cs.oswego.edu
    Subject: RE: [concurrency-interest] ForkJoinTasks seem to quit without
throwing,on larger tasks.



    Does it also quit at, say, 10000?



    I think you are seeing the JIT compiler at work. The whole loop does
nothing visible so can simply be elided.



    > int tcount = 0;

    > while (tcount < Integer.MAX_VALUE)

    >   tcount++;



    The JIT threshhold for the loop is around 1500 if I recall correctly.
Try making tcount a field instead and refer to it elsewhere. Though even
then, the loop could be replaced with a simple assignment (not sure the JIT
will see that though :) )



    David Holmes

      -----Original Message-----
      From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Qin, Henry
      Sent: Thursday, 17 September 2009 8:57 AM
      To: concurrency-interest at cs.oswego.edu
      Subject: [concurrency-interest] ForkJoinTasks seem to quit without
throwing,on larger tasks.

      Hi,

      In the following example, it appears that forkjoin simply quits
without completing its task.

      If you change Integer.MAX_VALUE to 1000, the task is completed
correctly.

      import jsr166y.*;

      public class ForkPlay extends RecursiveAction{

          int myInt;

          public ForkPlay(int i){ myInt = i;}

          public void compute(){

              try{

            //     if (myInt > 1000) return;

                  (new ForkPlay(myInt+1)).fork();

                  System.out.println(myInt);

                  int tcount = 0;

                  while (tcount < Integer.MAX_VALUE)

                      tcount++;

              }

              catch (Exception e){

                  e.printStackTrace();

              }

          }

          static ForkJoinPool pool;

          public static void main(String[] args) throws Exception{

                  pool = new ForkJoinPool(7);

                  pool.invoke(new ForkPlay(1));

          }

      }

      If someone has come across the behavior before, could they please
share a work-around?

      If this is indeed a hard limitation on the size of tasks that
ForkJoinTasks can accomplish, perhaps it should be documented? The current
documentation speaks of an ideal ForkJoinTask as being between 100 and 10000
computations, but does not mention that the framework will simply quit with
larger tasks.

      Note that I've tried to extend ForkJoinTask directly as well, and not
just RecursiveAction.



      Thanks,

      ~Henry
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090917/f6bfe0a3/attachment-0001.html>

From dl at cs.oswego.edu  Thu Sep 17 07:19:49 2009
From: dl at cs.oswego.edu (Doug Lea)
Date: Thu, 17 Sep 2009 07:19:49 -0400
Subject: [concurrency-interest] ForkJoinTasks seem to quit without
 throwing, on larger tasks.
In-Reply-To: <5FD617D5BE1D114CB1F10F4358D109F501255A06@EMAIL02.pnl.gov>
References: <5FD617D5BE1D114CB1F10F4358D109F501255A06@EMAIL02.pnl.gov>
Message-ID: <4AB21B55.5050207@cs.oswego.edu>

Qin, Henry wrote:
> Hi,
> 
> In the following example, it appears that forkjoin simply quits without 
> completing its task.
> 

Your tasks never join their subtasks, so the call to
   pool.invoke(new ForkPlay(1))
will return as soon as ForkPlay(1) forks a single
task, prints, and then exits a loop that will
terminate as soon as it is optimized. So,
as correctly guessed by David, the number of printed lines you see
depends on how soon the compiler optimizes ForkPlay.compute.

You don't see the other tasks output because your
main() terminates upon return of ForkPlay(1).

-Doug


> If you change Integer.MAX_VALUE to 1000, the task is completed correctly.
> 
> import jsr166y.*;
> 
> public class ForkPlay extends RecursiveAction{
> 
>     int myInt;
> 
>     public ForkPlay(int i){ myInt = i;}
> 
>     public void compute(){
> 
>         try{
> 
> //            if (myInt > 1000) return;
> 
>             (new ForkPlay(myInt+1)).fork();
> 
>             System.out.println(myInt);
> 
>             int tcount = 0;
> 
>             while (tcount < Integer.MAX_VALUE)
> 
>                 tcount++;
> 
>         }
> 
>         catch (Exception e){
> 
>             e.printStackTrace();
> 
>         }
> 
>     }
> 
>     static ForkJoinPool pool;
> 
>     public static void main(String[] args) throws Exception{
> 
>             pool = new ForkJoinPool(7);
> 
>             pool.invoke(new ForkPlay(1));
> 
>     }
> 
> }
> 
> If someone has come across the behavior before, could they please share 
> a work-around?
> 
> If this is indeed a hard limitation on the size of tasks that 
> ForkJoinTasks can accomplish, perhaps it should be documented? The 
> current documentation speaks of an ideal ForkJoinTask as being between 
> 100 and 10000 computations, but does not mention that the framework will 
> simply quit with larger tasks.
> 
> Note that I?ve tried to extend ForkJoinTask directly as well, and not 
> just RecursiveAction.
> 
> 
> Thanks,
> 
> ~Henry
> 
> 
> ------------------------------------------------------------------------
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest




From Henry.Qin at pnl.gov  Thu Sep 24 18:32:20 2009
From: Henry.Qin at pnl.gov (Qin, Henry)
Date: Thu, 24 Sep 2009 15:32:20 -0700
Subject: [concurrency-interest] ForkJoinTasks seem to make java ImageIO.read
	crash with IOExceptions.
Message-ID: <5FD617D5BE1D114CB1F10F4358D109F5012BD4F6@EMAIL02.pnl.gov>

Hi,


I have the following code:

public class ParallelPusher{
    static Queue<byte[]> theQueue;
    public void compute(){
        while (!theQueue.isEmpty()){
            byte[] bytes = theQueue.poll();
            if (bytes == null) return; //just a safety check

            ByteArrayInputStream bais = null;

            try{
                bais = new ByteArrayInputStream(bytes);
                SimpleHistogram sht = new SimpleHistogram();
                sht.compute(bais);
                ShannonEntropy se = new ShannonEntropy();
                BufferedImage img = se.compute(sht);
            }
            catch(IOException ioe){
                System.err.println("An IOException has occured.");
                ioe.printStackTrace();
            }


        }
    }
}

Inside the SimpleHistogram.compute:
  
public void compute(File file) throws IOException {
    BufferedImage bufImage = ImageIO.read(file);
    loadBufferedImage(bufImage);
  }

When I call the compute method directly, from a new instance in main:

(new ParallelPusher()).compute();

It works fine with no errors.
However, when I execute it from a ForkJoinPool, with Parallism set to 1,
and running only one task, ImageIO crashes:

    ForkJoinPool pool = new ForkJoinPool(1);

   pool.execute(new ParallelPusher());

ImageIO crashes with the following stacktrace:
java.io.IOException: closed
        at
javax.imageio.stream.ImageInputStreamImpl.checkClosed(ImageInputStreamIm
pl.java:96)
        at
javax.imageio.stream.ImageInputStreamImpl.close(ImageInputStreamImpl.jav
a:843)
        at
javax.imageio.stream.FileCacheImageInputStream.close(FileCacheImageInput
Stream.java:230)
        at javax.imageio.ImageIO.read(ImageIO.java:1425)
        at javax.imageio.ImageIO.read(ImageIO.java:1326)
        at
gov.pnnl.dici.image.SimpleHistogram.compute(SimpleHistogram.java:62)
        at
gov.pnnl.dici.image.ParallelPusher.compute(ParallelPusher.java:24)
        at jsr166y.RecursiveAction.exec(RecursiveAction.java:147)
        at jsr166y.ForkJoinTask.quietlyExec(ForkJoinTask.java:459)
        at
jsr166y.ForkJoinWorkerThread.mainLoop(ForkJoinWorkerThread.java:356)
        at
jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:341)

I believe the error has to do with the fact that ForkJoin doesn't play
well with blocking IO, but that is inconsistent with an earlier result I
got, where I had ForkJoin threads writing a lot of files to disk. I am
curious about why the exact same code should crash ImageIO under
ForkJoin, but not normally, and whether there is a fix for this.

Thanks,
~Henry



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090924/6c34122d/attachment.html>

From Henry.Qin at pnl.gov  Thu Sep 24 19:25:50 2009
From: Henry.Qin at pnl.gov (Qin, Henry)
Date: Thu, 24 Sep 2009 16:25:50 -0700
Subject: [concurrency-interest] ForkJoinTasks seem to make java
	ImageIO.read crash with IOExceptions.
Message-ID: <5FD617D5BE1D114CB1F10F4358D109F5012BD54C@EMAIL02.pnl.gov>

Please ignore the previous message. Apparently, the problem was that I
had an isQuiescentCheck() running before the pool became active, and
after I called pool.execute(ParallelPusher).

Apologies,
~Henry 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090924/1c99571e/attachment.html>

From bomba at cboe.com  Mon Sep 28 15:31:21 2009
From: bomba at cboe.com (Bomba, Craig)
Date: Mon, 28 Sep 2009 14:31:21 -0500
Subject: [concurrency-interest] GlobalServer Info from this morning
Message-ID: <4BD174404CF4A34C98322DC926CF862B116787A1@MSMAIL.cboent.cboe.com>

Hi Connie,

Here is some info regarding GlobalServer from this morning.

First of all, any GCs larger than a second all occurred prior to 5a.m.

Between 8:30 and 8:45 this morning CPU is at least 40% idle or more, so
that is also not an issue.  In addition, there don't appear to be any
memory pressures. 
Same story for 9:50 to 10:00a.m.

In addition, I looked at the POA queue instrumentor for global server.
The queues were only 1 or 2 deep and 4 deep at most.

Hope this helps.

Thanks,
Craig 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20090928/41e2ed11/attachment.html>

From bomba at cboe.com  Mon Sep 28 15:34:05 2009
From: bomba at cboe.com (Bomba, Craig)
Date: Mon, 28 Sep 2009 14:34:05 -0500
Subject: [concurrency-interest] Recall: GlobalServer Info from this morning
Message-ID: <4BD174404CF4A34C98322DC926CF862B116787A3@MSMAIL.cboent.cboe.com>

Bomba, Craig would like to recall the message, "GlobalServer Info from this morning".


From galder.zamarreno at redhat.com  Wed Sep 30 11:26:11 2009
From: galder.zamarreno at redhat.com (Galder Zamarreno)
Date: Wed, 30 Sep 2009 17:26:11 +0200
Subject: [concurrency-interest] Problem in customizing the
 Executor	framework behaviour
In-Reply-To: <357218.55539.qm@web54502.mail.re2.yahoo.com>
References: <932330.75771.qm@web54506.mail.re2.yahoo.com>	<e0563fd80907150201k49527e72l758f96b4beb463cb@mail.gmail.com>
	<357218.55539.qm@web54502.mail.re2.yahoo.com>
Message-ID: <4AC37893.6020602@redhat.com>

Where's the response Erkin sent? Did he reply to you direcly?
If so, could you share the email with us?

On 07/15/2009 12:11 PM, A Shahkar wrote:
> Thanks Erkin.
>
> I was told to take a similar approach when I posted the problem in Sun forums. IMHO, it has a few drawbacks.
>
> Let's suppose N=1,000. We create a fixed thread pool with the size of 10, and instantiate BlindRequest to send 100 requests. We submit the BlindRequestTask 10 times to the thread pool. Now, for any reason, sending the forth request in the second thread takes a long time. Other threads likely finish their work far before the second thread does while it still has a few ten requests to send. Other threads won't help the second one finish its job, and that's not the way thread pool executors are intended to act it. Actually it does not make such difference (specially in performance) if we just run 10 threads instead of a pool, each one executing BlindTaskRequest. Am I right?
>
> Thanks
>
>
>
>
>
>
> ------------------------------------------------------------------------
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-- 
Galder Zamarre?o
Sr. Software Engineer
Infinispan, JBoss Cache

From galder.zamarreno at redhat.com  Wed Sep 30 11:26:30 2009
From: galder.zamarreno at redhat.com (Galder Zamarreno)
Date: Wed, 30 Sep 2009 17:26:30 +0200
Subject: [concurrency-interest] Problem in customizing the
 Executor	framework behaviour
In-Reply-To: <396375.48468.qm@web54504.mail.re2.yahoo.com>
References: <932330.75771.qm@web54506.mail.re2.yahoo.com>	<e0563fd80907150201k49527e72l758f96b4beb463cb@mail.gmail.com>	<357218.55539.qm@web54502.mail.re2.yahoo.com>	<e0563fd80907150355xb01c88cre60e3b7c66c85f7f@mail.gmail.com>
	<396375.48468.qm@web54504.mail.re2.yahoo.com>
Message-ID: <4AC378A6.4090708@redhat.com>

Same here, Erkin's replies do not appear in the archive:
http://cs.oswego.edu/pipermail/concurrency-interest/2009-July/thread.html

On 07/15/2009 02:25 PM, A Shahkar wrote:
> Thanks Erkin. Your idea about supplying the delay to the task's
> constructor solved one aspect of the problem. Delays are no more
> considered a dark point. Nice idea!
>
>
>> So you can create your ThreadPoolExecutor first then pause, submit all
>> the tasks and unpause the the threadPoolExecutor. So when test starts,
>> you will not observe any delay because of request production.
>
> I think actually now you can really understand what I was talking about in my original post.
>
> As I told before, total number of requests (N) can be a very large number. Submitting a few thousand tasks makes the executor create a huge list of identical items. Not only this approach is extremely inefficient, but it also does not allow the application to make N equal to infinity. What is wrong with a user code which wants the framework's library to keep sending requests in a few threads until shut down?
>
> As you might remember, I told about a customized ArrayBlockingQueue given to the executor in my original post. I thought that such a construct could return one task for N times, solving the huge list problem. It can hold an internal parameter which is equal to N and decrement it each time a thread fetches a task to execute. Or it can ignore the parameter if total number of requests is not known. The problem is, I don't know really how I can effectively implement such a construct which does not harm performance. I don't know which methods should be overriden and in what way. This somehow requires a deep understanding of how an executor works with its BlockingQueue.
>
> Can you help me to implement the customized ArrayBlockingQueue?
>
> Again, thank you for your idea on solving the delays problem.
>
>
>
>
>
>
> ------------------------------------------------------------------------
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-- 
Galder Zamarre?o
Sr. Software Engineer
Infinispan, JBoss Cache

