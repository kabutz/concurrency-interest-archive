From radhakrishnan.mohan at gmail.com  Wed Jun  1 03:49:31 2005
From: radhakrishnan.mohan at gmail.com (Mohan Radhakrishnan)
Date: Wed Jun  1 07:06:40 2005
Subject: [concurrency-interest] What type of queue to use ?
Message-ID: <loom.20050601T094416-760@post.gmane.org>

Hi,

    We have a Thread framework that queues request to access external 
WebServices . Sometimes the services might be slow or down. Each request uses 
a thread from a pool.

    What type of queue should I use ? I am looking at these two.
LinkedQueue 
An unbounded linked-list-based queue. This is usually the best choice for a 
general-purpose queue. 
BoundedLinkedQueue 
A linked queue with a capacity bound 

Any ideas would help. This is supposed to have good throughput because it is 
an commerce site.

Mohan

    

From dl at cs.oswego.edu  Wed Jun  1 07:12:06 2005
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed Jun  1 07:12:08 2005
Subject: [concurrency-interest] SCOOL
Message-ID: <429D9806.2030409@cs.oswego.edu>

We don't usually post CFPs here, but this one is likely
to be of interest to the researchers on this list...



                   Synchronization and Concurrency in

                   Object-Oriented Languages (SCOOL)


            OOPSLA 2005 Workshop, San Diego, California, USA

                        Sunday October 16, 2005



Call for papers

   As mainstream hardware moves to multicore processors, programmers
   will be forced to write multithreaded programs in order to achieve
   high performance.  One thing seems clear: mainstream programmers
   cannot use today's abstractions of locks, condition variables,
   semaphores, and barriers to develop scalable parallel software
   effectively.  This workshop addresses the problem of how best to
   express synchronization and concurrency in object-oriented
   multithreaded programming environments.

   This workshop will bring together researchers working on different
   parts of this problem, including: frameworks and libraries for
   concurrent object-oriented programming, patterns in concurrent
   software, tools for detecting concurrency-related bugs, new
   programming abstractions, and new directions for low-level support
   from the operating system and hardware.

   We hope to build on the success of last year's workshop on
   Concurrency and Synchronization in Java Programs (CSJP) to
   understand how these pieces fit together, and to define the research
   challenges that will adapt today's object-oriented programming
   languages for tomorrow's hardware.

   Papers will be selected for their intrinsic interest and timeliness
   of the work.  Authors are encouraged to submit polished descriptions
   of work in progress as well as papers describing completed projects.

   In case of queries about the workshop, please contact
   scool05@microsoft.com.

Specific topics of interest include, but are not limited to:

   - Analysis, assurance, testing and verification techniques.
   - Case studies.
   - Compiler transformations.
   - Concurrent data structure implementations.
   - Contention management.
   - Expression of concurrency-related design intent.
   - Hard synchronization problems without adequate solutions.
   - Hardware and operating system support for concurrency abstractions.
   - Interactions between garbage collection and synchronization.
   - Languages and semantics.
   - Libraries to support concurrency, e.g., JSR166.
   - Memory models for concurrent object-oriented languages.
   - Nonblocking synchronization.
   - Performance and scalability techniques and studies.
   - Synchronization abstractions such as transactional memory and
monitors.

Paper submission:

   Accepted papers will be available online before the workshop rather
   than in printed form.  They will be placed in a permanent collection
   at the University of Rochester's online digital archive
   (http://urresearch.rochester.edu) for future citation.  Papers
   accepted to the workshop may subsequently be submitted to more
   formal publication venues if this is allowed by the rules of those
   venues. A special journal issue associated with the workshop is
   being considered.  Papers should be submitted online in PDF or
   PostScript format formatted with an 11 point font on up to 10 pages
   of US "letter" paper.  Instructions will be available from
   http://research.microsoft.com/~tharris/scool05/

Important dates:

   Submissions due: 29 July 2005
   Notification: 5 September 2005
   Revisions due: 3 October 2005
   Workshop: 16 October 2005

Co-organizers:

   Tim Harris (Microsoft Research)
   Doug Lea (State University of New York)

Program committee:

   David F. Bacon (IBM Research)
   Keir Fraser (University of Cambridge)
   Aaron Greenhouse (Software Engineering Institute, CMU)
   Bradley C. Kuszmaul (MIT)
   Maurice Herlihy (Brown University)
   Michael Hicks (University of Maryland)
   Tony Hosking (Purdue University)
   Gary Lindstrom (University of Utah)
   Victor Luchangco (Sun Microsystems Labs)
   John Potter (University of New South Wales)
   Ravi Rajwar (Intel)
   Michael L. Scott (University of Rochester)
From dl at cs.oswego.edu  Wed Jun  1 07:35:31 2005
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed Jun  1 07:35:33 2005
Subject: [concurrency-interest] JTRES
Message-ID: <429D9D83.2070605@cs.oswego.edu>


The JTRES workshop will also be held at OOPSLA this year,
the day after the SCOOL workshop. If you are working on
concurrency and synchronization issues that apply to real-time and
embedded systems, you might consider submitting and attending.

See: http://www.cs.purdue.edu/homes/jv/JTRES05/

Pasting in the CFP....


The 3rd Workshop on Java Technologies for Real-time and Embedded Systems
OOPSLA 2005
17 October 2005
San Diego, California, USA

::Motivation::

Over 90 percent of all microprocessors are now used for real-time and 
embedded applications, and the behavior of many of these applications is 
constrained by the physical world. Designing real-time and embedded 
systems that implement their required capabilities, are dependable and 
predictable, and are parsimonious in their use of limited computing 
resources is hard; building them on time and within budget is even 
harder. It is therefore essential that the production of real-time 
embedded systems can take advantage of languages, tools, and methods 
that enable higher software productivity.

Ideally, developers should use a programming language that shields them 
from many complexities, such as type errors, memory management and that 
allow them to express the desired application structure in a convenient 
way -- such as supporting periodic tasks or sporadic events in the 
language. The Java programming language has become an attractive choice 
because of advantages such as safety, productivity, low maintenance 
costs, and availability of well trained developers. But, to meet 
real-time constraints, issues such as under-specification of thread 
scheduling and predictability of the runtime system must be addressed. A 
number of real-time extensions to Java have been proposed, the main two 
being the Real-Time Specification for Java (RTSJ) and the J-Consortium 
Real-Time Core Extension (RTCore). The intent of these specifications is 
to ease the development of real-time applications by providing several 
additions such as extending the memory management model, providing 
stronger semantics in thread scheduling, and so on.

::Goal::

There is an increasingly growing interest in Real-Time object 
technologies in both the research community and the industry. The goal 
of the workshop is to gather researchers working on real-time and 
embedded Java, and related languages, to identify the challenging 
problems that still need to be properly solved, and to report on 
research results and practical experience. The topics of interest are 
not limited to particular variants of real-time Java, we are looking for 
novel ideas and techniques in the following research areas:

     * New real-time programming paradigms and language features
     * Industrial experience and practitioner reports
     * Real-time design patterns and programming idioms
     * Formal models of real-time computation
     * Extensions to RTSJ and RTCore
     * Virtual machines and execution environments
     * Memory management and Real-time Garbage collection
     * Compiler analysis and implementation techniques
     * Distributed real-time Java
     * Scheduling frameworks, feasibility analysis, and timing analysis

::Important dates::

Submissions due: August 22, 2005
Notification: September 19, 2005

From radhakrishnan.mohan at gmail.com  Tue Jun  7 02:00:01 2005
From: radhakrishnan.mohan at gmail.com (Mohan Radhakrishnan)
Date: Tue Jun  7 08:11:12 2005
Subject: [concurrency-interest] leaders/followers pattern 
Message-ID: <loom.20050607T075747-8@post.gmane.org>

Hi,
     I am looking for some ideas about the comparison between the 
leaders/followers pattern and the Worker thread pattern. In our framework a 
Worker thread picks up tasks and assigns them to a thread from a pool. I read 
that this pattern reduces context switching.

Thanks,
Mohan

From dholmes at dltech.com.au  Tue Jun  7 21:13:06 2005
From: dholmes at dltech.com.au (David Holmes)
Date: Tue Jun  7 21:13:32 2005
Subject: [concurrency-interest] leaders/followers pattern 
In-Reply-To: <loom.20050607T075747-8@post.gmane.org>
Message-ID: <NFBBKALFDCPFIDBNKAPCIECFFNAA.dholmes@dltech.com.au>

Mohan,

>      I am looking for some ideas about the comparison between the
> leaders/followers pattern and the Worker thread pattern. In our
> framework a
> Worker thread picks up tasks and assigns them to a thread from a
> pool. I read that this pattern reduces context switching.

I'm not clear on what the "leaders/followers" pattern actually is. But from
your description instead of having multiple workers pull from a work queue,
you instead have a single worker that pulls from the queue and then
hands-off to a thread from a "pool".

>From an abstract perspective, assuming sufficient CPU's to handle all
incoming tasks, then the design you describe has more context switches due
to the hand off from the single worker to the pool thread - you can cut out
the "middle man". That single worker can also become the bottleneck.

A decent pool will only wake up at most one thread in the pool for each item
of work that arrives - but a badly written pool might do a notifyAll and
wake them all up. So perhaps what you read was reflecting experience with a
concrete implementation, rather than an abstract analysis of the pattern
itself.

Cheers,
David Holmes



From matthias.ernst at coremedia.com  Wed Jun  8 06:11:22 2005
From: matthias.ernst at coremedia.com (Ernst, Matthias)
Date: Wed Jun  8 06:11:31 2005
Subject: [concurrency-interest] Shared Memory ?
Message-ID: <F34C8A704C489B46B9E9FBDBD1B91D5FC0452F@MARS.coremedia.com>

Hi,

I'm musing about java-to-java/java-to-native, shared memory,
inter-process communication. Is stuff like that on the list of things
you see as a possible Java feature?

I imagine an API for
* creating/deleting shared-memory descriptors
* attaching to a shared-memory descriptor, yielding a DirectByteBuffer
* an extension of j.u.c.atomic to support atomic instructions on such
areas
* basic IPC abstractions on top

I guess a small JNI library + sun.misc.Unsafe might already go a long
way. Well, I just see there's no intrinsic like
sun.misc.Unsafe.compareAndSwap*(long address, expectedValue, newValue)
(the existing methods all take an object). Or is there?

What do you think? I'm thinking of some sort of crash-resilient, shared
area for a set of VMs. Well, ideally that area would be a
garbage-collected heap but let's stay realistic. Gemstone's been
offering something like that for a while.

I see this might be complementary to the isolate effort.

Matthias

From radhakrishnan.mohan at gmail.com  Wed Jun  8 00:21:05 2005
From: radhakrishnan.mohan at gmail.com (Mohan Radhakrishnan)
Date: Wed Jun  8 06:39:24 2005
Subject: [concurrency-interest] Re: leaders/followers pattern
References: <loom.20050607T075747-8@post.gmane.org>
	<NFBBKALFDCPFIDBNKAPCIECFFNAA.dholmes@dltech.com.au>
Message-ID: <loom.20050608T061752-336@post.gmane.org>

Hi,

  Actually I meant the following pattern.

1. Leader thread waits for event on any handle in its handle set
2. When event is detected, a follower is promoted to leader
3. Uses event handler to process event.Rejoins the thread pool when processing 
finishes (as either leader or follower)

So instead of giving the task to another thread in the pool, the leader 
handles it.

This is supposed to reduce context switching. Are there any implementations of 
this ? I am looking at java.util.concurrent now. Can we use this if we want to 
reduce context switching ?


Mohan


From dawidk at mathcs.emory.edu  Wed Jun  8 09:54:59 2005
From: dawidk at mathcs.emory.edu (Dawid Kurzyniec)
Date: Wed Jun  8 09:55:28 2005
Subject: [concurrency-interest] Shared Memory ?
In-Reply-To: <F34C8A704C489B46B9E9FBDBD1B91D5FC0452F@MARS.coremedia.com>
References: <F34C8A704C489B46B9E9FBDBD1B91D5FC0452F@MARS.coremedia.com>
Message-ID: <42A6F8B3.1030208@mathcs.emory.edu>

Ernst, Matthias wrote:

>Hi,
>
>I'm musing about java-to-java/java-to-native, shared memory,
>inter-process communication. Is stuff like that on the list of things
>you see as a possible Java feature?
>
>I imagine an API for
>* creating/deleting shared-memory descriptors
>* attaching to a shared-memory descriptor, yielding a DirectByteBuffer
>* an extension of j.u.c.atomic to support atomic instructions on such
>areas
>* basic IPC abstractions on top
>
>  
>
This sounds like a small and nice library that you could write yourself 
using the JNI functions for byte buffers. You probably already have some 
prototype code written. My understanding is that you could only share 
buffers, not objects, due to reachability (gc) / mutability issues. I 
don't think there is any JSR that deals with this at the moment, but the 
beauty of the JSR process is that you could always propose one, I guess...

It seems that except for atomic functions, this sort of library could be 
implemented w/o changes to the Java platform code (atomicity via JNI 
would probably be too expensive).

Regards,
Dawid

From Pete.Soper at Sun.COM  Wed Jun  8 14:12:17 2005
From: Pete.Soper at Sun.COM (Pete Soper)
Date: Wed Jun  8 14:12:32 2005
Subject: [concurrency-interest] Shared Memory ?
In-Reply-To: <F34C8A704C489B46B9E9FBDBD1B91D5FC0452F@MARS.coremedia.com>
References: <F34C8A704C489B46B9E9FBDBD1B91D5FC0452F@MARS.coremedia.com>
Message-ID: <42A73501.1070505@Sun.COM>

Ernst, Matthias wrote:
> Hi,
> 
> I'm musing about java-to-java/java-to-native, shared memory,
> inter-process communication. Is stuff like that on the list of things
> you see as a possible Java feature?
> 
> I imagine an API for
> * creating/deleting shared-memory descriptors
> * attaching to a shared-memory descriptor, yielding a DirectByteBuffer
> * an extension of j.u.c.atomic to support atomic instructions on such
> areas
> * basic IPC abstractions on top
> 
> I guess a small JNI library + sun.misc.Unsafe might already go a long
> way. Well, I just see there's no intrinsic like
> sun.misc.Unsafe.compareAndSwap*(long address, expectedValue, newValue)
> (the existing methods all take an object). Or is there?
> 
> What do you think? I'm thinking of some sort of crash-resilient, shared
> area for a set of VMs. Well, ideally that area would be a
> garbage-collected heap but let's stay realistic. Gemstone's been
> offering something like that for a while.
> 
> I see this might be complementary to the isolate effort.
> 

Multiple 1:1 style isolation prototypes (where each isolate is a JRE 
instance) exploited shared memory. One set made the entire aggregate 
state (all isolates having a common ancestor) in shared memory. Actually 
the design made provisions for almost any shareable object with basic 
lock semantics, such as a disk file. A vastly superior, feature complete 
implementation (albeit with heavy J2SE dependencies) that got close to 
inclusion in Tiger held the shared state to an absolute minimum (a pair 
of words, as I recall). The aim was robustness in the face of arbitrary 
failures of individual isolates. I have no idea what the plans are for 
the 121 RI (I had to move on to other things last December) but suspect 
shared memory and synchronization will be involved if the RI is 1:1 
style. Actually if it's not 1:1 it will still involve shared memory, but 
perhaps not the type you're writing about!

I personally think that hoisting (more) raw (bytes vs objects) shared 
memory manipulation up into a public Java API intended for applications 
would be at least as compromising to Java's safety guarantees as JSR'ing 
Unsafe and making it available to application classes (i.e. not 
requiring users to be on the bootclass path, which at least tends to 
involve debugged code). To be complementary to the spirit of isolates 
such APIs would have to be both OO and neutral with respect to 
implementation styles while doing no harm with respect to the safety of 
the runtime environment. I'm in no position to be a wet blanket about 
this: just my two cents as they say.

-Pete
From dholmes at dltech.com.au  Wed Jun  8 19:34:14 2005
From: dholmes at dltech.com.au (David Holmes)
Date: Wed Jun  8 19:34:24 2005
Subject: [concurrency-interest] Re: leaders/followers pattern
In-Reply-To: <loom.20050608T061752-336@post.gmane.org>
Message-ID: <NFBBKALFDCPFIDBNKAPCOEDFFNAA.dholmes@dltech.com.au>

Mohan,

>   Actually I meant the following pattern.
>
> 1. Leader thread waits for event on any handle in its handle set
> 2. When event is detected, a follower is promoted to leader
> 3. Uses event handler to process event.Rejoins the thread pool
> when processing finishes (as either leader or follower)
>
> So instead of giving the task to another thread in the pool, the leader
> handles it.
>
> This is supposed to reduce context switching. Are there any
> implementations of this ? I am looking at java.util.concurrent now. Can we
use this
> if we want to reduce context switching ?

I don't see any advantage of the above "pattern" with the more usual/common
design where a single thread handles the event demultiplexing and then
"pushes" the event into a work queue for processing by a thread pool - ie
the event is submitted to the pool as a task. In abstract terms the number
of context switches are the same - both designs require the event
demultiplexing thread to cause another thread to wake: either to become the
new demultiplexor; or to process the event.

The advantage of the pool based design is that all the bits are present in
java.util.concurrent, and all thread pool management is handled for you. In
the leader/follower pattern your "pool" is simpler in that all threads just
wait on a given object until notified, but then you have the responsibility
for all thread management: how many to create, when to create, do they die
when idle, what happens when you run out of threads etc etc. All that
functionality is built in to the existing executor services: you just need
to decide on what policy you want. Of course you could use an executor in
leader/follower too: the leader submits a task that simply waits for the
next event then submits a similar task.

Personally I like to keep thread roles distinct: an event demultiplexor or
an event processor.

David Holmes

From dawidk at mathcs.emory.edu  Wed Jun  8 23:54:38 2005
From: dawidk at mathcs.emory.edu (Dawid Kurzyniec)
Date: Wed Jun  8 23:54:44 2005
Subject: [concurrency-interest] Re: leaders/followers pattern
In-Reply-To: <NFBBKALFDCPFIDBNKAPCOEDFFNAA.dholmes@dltech.com.au>
References: <NFBBKALFDCPFIDBNKAPCOEDFFNAA.dholmes@dltech.com.au>
Message-ID: <42A7BD7E.20302@mathcs.emory.edu>

David Holmes wrote:

>Mohan,
>
>  
>
>>  Actually I meant the following pattern.
>>
>>1. Leader thread waits for event on any handle in its handle set
>>2. When event is detected, a follower is promoted to leader
>>3. Uses event handler to process event.Rejoins the thread pool
>>when processing finishes (as either leader or follower)
>>
>>So instead of giving the task to another thread in the pool, the leader
>>handles it.
>>
>>This is supposed to reduce context switching. Are there any
>>implementations of this ? I am looking at java.util.concurrent now. Can we
>>    
>>
>use this
>  
>
>>if we want to reduce context switching ?
>>    
>>
>
>I don't see any advantage of the above "pattern" with the more usual/common
>design where a single thread handles the event demultiplexing and then
>"pushes" the event into a work queue for processing by a thread pool - ie
>the event is submitted to the pool as a task. In abstract terms the number
>of context switches are the same - both designs require the event
>demultiplexing thread to cause another thread to wake: either to become the
>new demultiplexor; or to process the event.
>  
>
Actually I am with Mohan on this one. The point is to reduce response 
latency, i.e. eliminate the context switch on the path between receiving 
request (usually on the socket) and sending back the response. 
Underlying assumption is that the requests are relatively infrequent 
(i.e. typically less frequent than preemptive context switches) but 
non-uniform in the required processing time. Some may take long time, so 
you want to have concurrency. On the other hand, many are very short, 
e.g. of the order of microseconds, and for those you want to minimize 
latency. Context switching overhead in Java on Linux with 2.4 kernel, on 
2.4 GHz P4 machine is about 100 microseconds, so it can significantly 
contribute to the latency in this case.

Regards,
Dawid


From dholmes at dltech.com.au  Thu Jun  9 00:07:40 2005
From: dholmes at dltech.com.au (David Holmes)
Date: Thu Jun  9 00:07:49 2005
Subject: [concurrency-interest] Re: leaders/followers pattern
In-Reply-To: <42A7BD7E.20302@mathcs.emory.edu>
Message-ID: <NFBBKALFDCPFIDBNKAPCGEDMFNAA.dholmes@dltech.com.au>

> Dawid Kurzyniec writes
> Actually I am with Mohan on this one. The point is to reduce response
> latency, i.e. eliminate the context switch on the path between receiving
> request (usually on the socket) and sending back the response.

Ah! -  you  are assuming that the leader can signal a new leader without
actually causing a context switch to that new leader prior to the processing
of the current event? That is certainly possible, but not guaranteed by any
means.

Both designs cause the same *number* of context switches. But the *timing*
of those context switches depends on many things. Certainly in the
leader/follower pattern it is possible that a request may get responded to
without an intervening context switch, while with the hand-off to the worker
thread the request can not be responded to without the worker getting
switched in. So yes - the leader/follower pattern may provide better
response time, but not by reducing the number of context switches, but by
having them occur off the "critical path".

For a given system, with a regular event arrival distribution and processing
load, you could profile the two designs to see if one gives better overall
response time to individual requests.

Cheers,
David Holmes

From dholmes at dltech.com.au  Thu Jun  9 00:24:35 2005
From: dholmes at dltech.com.au (David Holmes)
Date: Thu Jun  9 00:24:42 2005
Subject: [concurrency-interest] Re: leaders/followers pattern
In-Reply-To: <NFBBKALFDCPFIDBNKAPCGEDMFNAA.dholmes@dltech.com.au>
Message-ID: <NFBBKALFDCPFIDBNKAPCOEDMFNAA.dholmes@dltech.com.au>

I wrote:
> Ah! -  you  are assuming that the leader can signal a new leader without
> actually causing a context switch to that new leader prior to the
> processing of the current event? That is certainly possible, but not
> guaranteed by any means.

Sorry - getting my wires crossed. On a SMP with short processing
requirements it is of course quite likely that the leader can process the
request after signalling a follower without getting hit by a context switch.

Cheers,
David Holmes

From radhakrishnan.mohan at gmail.com  Thu Jun  9 01:49:04 2005
From: radhakrishnan.mohan at gmail.com (Mohan Radhakrishnan)
Date: Thu Jun  9 06:42:01 2005
Subject: [concurrency-interest] leaders/followers pattern
Message-ID: <loom.20050609T074841-25@post.gmane.org>

Hi,

    The other reason is the overall context switching overhead. Not sure about 
this because I am not a threading expert. I am going to deploy this in an app. 
server where there are other threads. It is an e-commerce site. Lots of users 
will be connecting.

    
Mohan

From dl at cs.oswego.edu  Thu Jun  9 07:13:05 2005
From: dl at cs.oswego.edu (Doug Lea)
Date: Thu Jun  9 07:13:08 2005
Subject: [concurrency-interest] leaders/followers pattern
In-Reply-To: <loom.20050609T074841-25@post.gmane.org>
References: <loom.20050609T074841-25@post.gmane.org>
Message-ID: <42A82441.3070104@cs.oswego.edu>


Nearly all java.util.concurrent queues, locks, etc., are designed
to by default deliver high throughput -- you have to specifically
disable this using fairness settings.

High throughput almost always means low context switching.
The techniques uesed here are related to Leaders/followers
(which is a form of directed handoff), but usually perform
even better.

There are some kinds of IO-related applications where you
may be able to do better still. It's too bad that JSR203
is still on hold (current plans seem to be to postpone
until J2SE7) -- this probably will include an async IO
framework. Until then, if you are doing this sort of thing,
you might consider using Coconut AIO, that is built on top
of java.util.concurrent
http://coconut.codehaus.org/Coconut+AIO

-Doug



From jean.morissette666 at videotron.ca  Thu Jun  9 09:40:16 2005
From: jean.morissette666 at videotron.ca (Jean Morissette)
Date: Thu Jun  9 09:40:37 2005
Subject: [concurrency-interest] Context-switching VS Continuation
Message-ID: <200506090940.16661.jean.morissette666@videotron.ca>

Hi all,
	I would be happy to have your comments on this...

First, we are building a SEDA-based framework, http://jcyclone.sf.net, which 
is designed to support massive degrees of concurrency.  Our framework is a 
little bit like a graph of executors (Stages) linked by queues.  

Actually, we are working on an innovative (or weird) continuation-based 
scheduler that eliminate context-switch.  The idea is to instrument 
user-provided classes (event handlers) in such a way that the thread context 
(call stack, etc) is saved in a continuation object when a thread encounters 
a wait(), and can be restored later.  When notify() is called on the monitor, 
a waiting continuation is put in the ready queue of the scheduler.

So, by creating as much threads as cpus, there is no context-switch at all.  
However, the cost of saving the thread context can be significant if the 
stack is very deep.  To overcome this, our scheduler use some statistiques to 
schedule threads in such a way that "continuation-switchs" occurs very 
rarely.

We believe that this design will outperform our the conventional 
"thread-pool / OS scheduler" design because we can use high-level 
informations (stages stats) that the OS scheduler don't have.  I'm very 
interested to have your opinions about this continuation-style design.

Thanks,
-Jean
From dawidk at mathcs.emory.edu  Thu Jun  9 10:10:08 2005
From: dawidk at mathcs.emory.edu (Dawid Kurzyniec)
Date: Thu Jun  9 10:10:18 2005
Subject: [concurrency-interest] Context-switching VS Continuation
In-Reply-To: <200506090940.16661.jean.morissette666@videotron.ca>
References: <200506090940.16661.jean.morissette666@videotron.ca>
Message-ID: <42A84DC0.6070008@mathcs.emory.edu>

Jean Morissette wrote:

>Hi all,
>	I would be happy to have your comments on this...
>
>First, we are building a SEDA-based framework, http://jcyclone.sf.net, which 
>is designed to support massive degrees of concurrency.  Our framework is a 
>little bit like a graph of executors (Stages) linked by queues.  
>
>Actually, we are working on an innovative (or weird) continuation-based 
>scheduler that eliminate context-switch.  The idea is to instrument 
>user-provided classes (event handlers) in such a way that the thread context 
>(call stack, etc) is saved in a continuation object when a thread encounters 
>a wait(), and can be restored later.  When notify() is called on the monitor, 
>a waiting continuation is put in the ready queue of the scheduler.
>
>  
>
Somehow I doubt that it could outperform normal context switching. After 
all, the "continuation switching" is a lot like context switching 
implemented at the user level. Why would it be faster than switching 
performed close to the hardware, where the amount of state is smaller 
and also you can take advantage of kernel-mode, fine-tuned assembly 
language, etc? Also, one of performance hits related to context 
switching is caused by cache flushing. You will not avoid it anyway.

Linux prior to 2.6 had poor thread switching performance, AFAIK mostly 
because there was no dedicated abstraction of a thread at the low level 
(threads were essentially processes). MS Windows and Solaris were 
actually much better in that respect, since they supported threads 
natively. But Linux kernel 2.6 has changed the game - all the threading 
stuff has been reimplemented, and it is much faster now. I can't give 
exact numbers, but I would suggest you first run some benchmarks to 
measure the overheads that you are trying to beat.

Regards,
Dawid

From tim at peierls.net  Thu Jun  9 11:19:23 2005
From: tim at peierls.net (Tim Peierls)
Date: Thu Jun  9 11:19:34 2005
Subject: [concurrency-interest] Context-switching VS Continuation (long
	reply)
In-Reply-To: <200506090940.16661.jean.morissette666@videotron.ca>
References: <200506090940.16661.jean.morissette666@videotron.ca>
Message-ID: <42A85DFB.10008@peierls.net>

I like this approach a lot, and I've been working off and on to build
something that sounds very similar. I was prompted to attempt this after
reading, on Doug Lea's recommendation (how else does anything happen?), the
Larus and Parkes paper on cohort scheduling:

  http://research.microsoft.com/users/larus/Papers/usenix02_cohort.pdf

As with your approach, #threads == #cpus, and there is no context-switching
(as long as you stick to the rules).

In my formulation, a Stage is an extension of AbstractExecutorService
that overrides the submit methods to covariantly return AwaitableFuture<T>,
and Continuation is a special Callable whose additional invoke() method
arranges to schedule the Continuation when a given list of AwaitableFutures
have all completed.

You can either extend Stage or, as in this (uncompiled!) sketch, wrap a
Stage instance with the stage logic. This example assumes a pipeline where
URL first resolves to Image, then Image is rendered into ImageBuf.

   class ImageResolver {
       public AwaitableFuture<Image> resolve(final URL url) {
           return stage.submit(new Callable<Image>() {
               public Image call() throws Exception { ... }
           });
       }
       private final Stage stage = ...;
   }

   class ImageRenderer {
       public AwaitableFuture<ImageBuf> renderToBuffer(final URL url) {
           return stage.submit(new Callable<ImageBuf>() {
               public ImageBuf call() throws Exception {
                   AwaitableFuture<Image> futureImage =
                       imageResolver.resolve(url);
                   return new Continuation<ImageBuf>(stage, futureImage) {
                       public ImageBuf call() throws Exception {
                           return reallyRender(futureImage.get());
                       }
                   }.invoke();
               }
           });
       }
       private ImageBuf reallyRender(Image image) {...}
       private final Stage stage = ...;
       private final ImageResolver imageResolver = ...;
   }

   // some caller
   AwaitableFuture<ImageBuf> futureBuf =
       imageRenderer.renderToBuffer(url);
   ...
   // in a continuation:
       Graphics g = ...;
       ImageBuf imageBuf = futureBuf.get();
       display(g, imageBuf);

There is some trickiness and performance optimization involving that
Continuation.invoke() method. If the AwaitableFutures are ready by the time
invoke() is called, the continuation just runs immediately and invoke()
returns the result value normally. If not, the invoke() method returns
null, and the Stage machinery returns an AwaitableFuture that will be ready
when the Continuation is eventually run.

As Larus and Parkes describe, each thread has a per-thread stack (an
ArrayDeque) for local invocations and a per-thread queue for sending work
to other threads. The stack is drained first, then the queue.

Stages can be "partitioned" so that method invocations are dispatched to
specific threads based on some key, as in Larus and Parkes. This can be
used, for example, to eliminate the need for locking partitioned data
structures.

Just for comparison, here's what the plain ol' context-switching version
might look like:

   class ImageResolver {
       public Future<Image> resolve(final URL url) {
           return exec.submit(new Callable<Image>() {
               public Image call() throws Exception { ... }
           });
       }
   }

   class ImageRenderer {
       public Future<ImageBuf> renderToBuffer(final URL url) {
           return exec.submit(new Callable<ImageBuf>() {
               public ImageBuf call() throws Exception {
                   Future<Image> futureImage =
                       imageResolver.resolve(url);
                   return reallyRender(futureImage.get());
               }
           });
       }
       private ImageBuf reallyRender(Image image) {...}
   }

   // some caller
   Future<ImageBuf> futureBuf =
       imageRenderer.renderToBuffer(url);
   ...
   Graphics g = ...;
   ImageBuf imageBuf = futureBuf.get();
   display(g, imageBuf);


Dawid Kurzyniec wrote:
> Somehow I doubt that it could outperform normal context switching. After
> all, the "continuation switching" is a lot like context switching
> implemented at the user level. Why would it be faster than switching
> performed close to the hardware, where the amount of state is smaller
> and also you can take advantage of kernel-mode, fine-tuned assembly
> language, etc? Also, one of performance hits related to context
> switching is caused by cache flushing. You will not avoid it anyway.

You could be right about that, but see if the Larus and Parkes paper makes
you feel differently. I don't have enough experience with this framework
yet to say anything conclusive.

--tim


Jean Morissette wrote:
> Hi all, I would be happy to have your comments on this...
> 
> First, we are building a SEDA-based framework, http://jcyclone.sf.net,
> which is designed to support massive degrees of concurrency.  Our
> framework is a little bit like a graph of executors (Stages) linked by
> queues.
> 
> Actually, we are working on an innovative (or weird) continuation-based
>  scheduler that eliminate context-switch.  The idea is to instrument 
> user-provided classes (event handlers) in such a way that the thread
> context (call stack, etc) is saved in a continuation object when a
> thread encounters a wait(), and can be restored later.  When notify() is
> called on the monitor, a waiting continuation is put in the ready queue
> of the scheduler.
> 
> So, by creating as much threads as cpus, there is no context-switch at
> all. However, the cost of saving the thread context can be significant
> if the stack is very deep.  To overcome this, our scheduler use some
> statistiques to schedule threads in such a way that
> "continuation-switchs" occurs very rarely.
> 
> We believe that this design will outperform our the conventional 
> "thread-pool / OS scheduler" design because we can use high-level 
> informations (stages stats) that the OS scheduler don't have.  I'm very
>  interested to have your opinions about this continuation-style design.
> 
> Thanks, -Jean





From hans.boehm at hp.com  Thu Jun  9 17:03:42 2005
From: hans.boehm at hp.com (Boehm, Hans)
Date: Thu Jun  9 17:06:16 2005
Subject: [concurrency-interest] Context-switching VS Continuation
Message-ID: <65953E8166311641A685BDF71D8658262B701B@cacexc12.americas.cpqcorp.net>

> -----Original Message-----
> From: Dawid Kurzyniec
> Linux prior to 2.6 had poor thread switching performance, 
> AFAIK mostly 
> because there was no dedicated abstraction of a thread at the 
> low level 
> (threads were essentially processes). MS Windows and Solaris were 
> actually much better in that respect, since they supported threads 
> natively. But Linux kernel 2.6 has changed the game - all the 
> threading 
> stuff has been reimplemented, and it is much faster now. I can't give 
> exact numbers, but I would suggest you first run some benchmarks to 
> measure the overheads that you are trying to beat.
> 
My impression is that the changes relevant to thread switch performance
in Linux around this time were:

- Futexes, which presumably eliminated the use of signals to
wake up blocked processes.
- A different scheduler.

I don't think this had much to do with better kernel understanding of
threads,
though that helps for Posix-compliance.  Unlike older versions of
Solaris,
Linux thread switches have always been done in the kernel.

Hans

From dawidk at mathcs.emory.edu  Sat Jun 11 00:21:08 2005
From: dawidk at mathcs.emory.edu (Dawid Kurzyniec)
Date: Sat Jun 11 00:21:20 2005
Subject: [concurrency-interest] is "volatile read / synchronized write"
	legal?
Message-ID: <42AA66B4.3010103@mathcs.emory.edu>

I am trying to optimize some algorithms in the backport-util-concurrent. 
Currently, backported atomics are written defensively:

public class AtomicInteger {
    int val;
    synchronized int get() { return val; }
    synchronized void set(int newVal) { val = newVal; }
    synchronized int getAndIncrement() { return value++; }
    synchronized int compareAndSet(int expect, int update) {
        boolean success = (expect == value);
        if (success) value = update;
        return success;
    }
    // etc.
}

But the following is tempting. Is it valid and safe to use in Java <= 
1.4? And if not, why:

public class AtomicInteger {
    volatile int val;
    int get() { return val; } // not synchronized
    synchronized void set(int newVal) { val = newVal; }
    synchronized void lazySet(int newVal) { val = newVal; }
    synchronized int getAndIncrement() { return value++; }
    synchronized int compareAndSet(int expect, int update) {
        boolean success = (expect == value);
        if (success) value = update;
        return success;
    }
    // etc.
}


And then, is the above legal (I guess not, since read-writes still need 
to be atomic with respect to lazySet):

public class AtomicInteger {
    volatile int val;
    int get() { return val; } // not synchronized
    synchronized void set(int newVal) { val = newVal; }
    void lazySet(int newVal) { val = newVal; } // not synchronized
    synchronized int getAndIncrement() { return value++; }
    synchronized int compareAndSet(int expect, int update) {
        boolean success = (expect == value);
        if (success) value = update;
        return success;
    }
    // etc.
}


Of course I realize that these hacks are definitely not allowed for 
longs and non-primitives.

Regards,
Dawid

From dl at cs.oswego.edu  Sat Jun 11 06:34:06 2005
From: dl at cs.oswego.edu (Doug Lea)
Date: Sat Jun 11 06:34:08 2005
Subject: [concurrency-interest] is "volatile read / synchronized write"
	legal?
In-Reply-To: <42AA66B4.3010103@mathcs.emory.edu>
References: <42AA66B4.3010103@mathcs.emory.edu>
Message-ID: <42AABE1E.9020806@cs.oswego.edu>

Dawid Kurzyniec wrote:
> I am trying to optimize some algorithms in the backport-util-concurrent. 

> 
> But the following is tempting. Is it valid and safe to use in Java <= 
> 1.4? And if not, why:
>
> public class AtomicInteger {
>     volatile int val;
>     int get() { return val; } // not synchronized
>     synchronized void set(int newVal) { val = newVal; }
>     void lazySet(int newVal) { val = newVal; } // not synchronized
>     synchronized int getAndIncrement() { return value++; }
>     synchronized int compareAndSet(int expect, int update) {
>         boolean success = (expect == value);
>         if (success) value = update;
>         return success;
>     }
>     // etc.
> }
> 

This would be OK on JVMs that correctly implement the JMM
volatile spec. No pre-J2SE5 JVM promises to do so, but in
practice Sun 1.4+ JVMs correctly handle this particular usage.
"Volatile" was not implemented at all on
IBM JVMs until, I think, 1.4.1 (I could be wrong).
Similarly, I think, for jrockit.
And I don't know about others.

So, in general, this has a chance of being OK on most 1.4.1+
JVMs. Definitely not on most earlier ones. All in all,
probably not worth doing.

-Doug
From pugh at cs.umd.edu  Sat Jun 11 06:33:55 2005
From: pugh at cs.umd.edu (Bill Pugh)
Date: Sat Jun 11 06:34:33 2005
Subject: [concurrency-interest] is "volatile read / synchronized write"
	legal?
In-Reply-To: <42AA66B4.3010103@mathcs.emory.edu>
References: <42AA66B4.3010103@mathcs.emory.edu>
Message-ID: <5BE7C126-2CCB-4CA4-9646-EF3507E28392@cs.umd.edu>

The first implementation is legal.
It should even work fine for longs and primitives
in Sun's 1.4 JVM.

The one with the unsynchronized lazySet is broken. Don't use it.

     Bill Pugh



On Jun 11, 2005, at 12:21 AM, Dawid Kurzyniec wrote:
> But the following is tempting. Is it valid and safe to use in Java  
> <= 1.4? And if not, why:
>
> public class AtomicInteger {
>    volatile int val;
>    int get() { return val; } // not synchronized
>    synchronized void set(int newVal) { val = newVal; }
>    synchronized void lazySet(int newVal) { val = newVal; }
>    synchronized int getAndIncrement() { return value++; }
>    synchronized int compareAndSet(int expect, int update) {
>        boolean success = (expect == value);
>        if (success) value = update;
>        return success;
>    }
>    // etc.
> }
>
> And then, is the above legal (I guess not, since read-writes still  
> need to be atomic with respect to lazySet):
>
> public class AtomicInteger {
>    volatile int val;
>    int get() { return val; } // not synchronized
>    synchronized void set(int newVal) { val = newVal; }
>    void lazySet(int newVal) { val = newVal; } // not synchronized
>    synchronized int getAndIncrement() { return value++; }
>    synchronized int compareAndSet(int expect, int update) {
>        boolean success = (expect == value);
>        if (success) value = update;
>        return success;
>    }
>    // etc.
> }
>
>
> Of course I realize that these hacks are definitely not allowed for  
> longs and non-primitives.
>
> Regards,
> Dawid
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest@altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>

From unmesh_joshi at hotmail.com  Sun Jun 12 21:21:13 2005
From: unmesh_joshi at hotmail.com (Unmesh joshi)
Date: Sun Jun 12 21:21:23 2005
Subject: [concurrency-interest] Test Driven Development 
In-Reply-To: <200503181700.j2IH09L0002802@altair.cs.oswego.edu>
Message-ID: <BAY21-F794E8C92F06F6A1F5F0ACEFF00@phx.gbl>

Hi,

I have been reading about Test Driven Development recently and trying to 
apply it. My first impression is that its great thind and leads to a better 
OO design.
What is experience of you all? Can any type of development be done test 
first? Have you been using TDD while developing core libraries?

Thanks,
Unmesh

_________________________________________________________________
Test Your Memory and Win Amazing Prizes! 
http://adfarm.mediaplex.com/ad/ck/4686-26272-10936-429?ck=BrainTeaser DVD 
Players, Digicams & more!

From bnewport at us.ibm.com  Mon Jun 13 13:45:29 2005
From: bnewport at us.ibm.com (Billy Newport)
Date: Mon Jun 13 13:45:44 2005
Subject: [concurrency-interest] Billy Newport/Rochester/IBM is out of the
	office.
Message-ID: <OF09536A07.E50F9AF3-ON8625701F.00618C48-8625701F.00618C4E@us.ibm.com>

I will be out of the office starting  06/13/2005 and will not return until
06/14/2005.

I have no or very limited access to email. Please try my cell but I'll have
infrequent access to that also.

From MSmith at netspoke.com  Mon Jun 13 15:12:54 2005
From: MSmith at netspoke.com (Michael Smith)
Date: Mon Jun 13 15:12:36 2005
Subject: [concurrency-interest] Re: synchronized vs ReentrantLock semantic
Message-ID: <F466C071FE519D49B1A51FCD134C6CC5036B23AD@smtp.netspoke.net>

(Resurrecting a thread from a few weeks ago -- I have not previously
been on this list and didn't see it, otherwise I would have responded
then)
 
On Monday May 23, 2005, Doug Lea wrote:
> Jean Morissette wrote:
> > Hi all,
> >  I would like to know whether replacing "synchronized" statement by 
> > ReentrantLock instances has the same semantic and meaning for the
jvm?  By 
> > exemple, I'm wondering whether ReentrantLock would allow reliable 
> > transmission of values or sets of values from one thread to another
through 
> > shared variables?
> >
> 
> yes. See the javadoc for the Lock interface, that says:
> 
> Memory Synchronization
> 
> All Lock implementations must enforce the same memory synchronization 
> semantics as provided by the built-in monitor lock:
> 
>      * A successful lock operation acts like a successful monitorEnter

> action
>      * A successful unlock operation acts like a successful
monitorExit 
> action

A coworker of mine brought up this exact question recently and while we
found this quoted documentation in the Lock API, we're not entirely
convinced that the implementation actually performs the requisite memory
synchronization required by the semantics defined for monitorEnter and
monitorExit.  ReentrantReadWriteLock uses AbstractQueuedSychronizer to
manage the locking but that class explicitly states that it does not
perform any memory synchronization other than the "state" that it
manages ("only the atomically updated int  value manipulated using
methods getState(), setState(int) and compareAndSetState(int, int) is
tracked with respect to synchronization").   
 
Thus, the following class will properly ensure the thread execution
semantics implied by the locks (cannot call getValue while another
thread is calling setValue, vice-versa, and multiple calls to getValue
are allowed simultanously), however it does not appear that it would
have proper memory synchronization semantics such that threads would see
the same value since there's no memory barriers that would copy a new
value from main memory to the thread's local memory.
 
public class Test {
  private Data data = new Data();
  private ReentrantReadWriteLock myLock =
    new ReentrantReadWriteLock(true);
 
  public setValue(int value) {
    myLock.writeLock().lock();
    try {
      data.setValue(value);
    }
    finally {
      myLock.writeLock.unlock();
    }
  }
 
  public getValue() {
    myLock.readLock.lock();
    try {
      return data.getValue();
    }
    finally {
      myLock.readLock.unlock();
    }
  }
}

Since this seems like the fundamental use-case for this API, we can only
assume that we are blind and just aren't seeing something in the code.
Can someone provide more details about how and where thread memory and
main memory are synchronized when using ReentrantReadWriteLock's locks?
 
Note:  My coworker started a thread on this question here:
http://forum.java.sun.com/thread.jspa?forumID=31&threadID=631014
 
 
regards,
Michael
 
---
Michael Smith
msmith@netspoke.com <mailto:msmith@netspoke.com> 
Software Engineering Manager
Netspoke, Inc.
5001 Centre Ave., 2nd Floor
Pittsburgh, PA 15213
t: (412) 605-1508
f: (412) 687-4615

 
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20050613/bb1b5ec5/attachment.htm
From dl at cs.oswego.edu  Mon Jun 13 15:25:02 2005
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon Jun 13 15:25:04 2005
Subject: [concurrency-interest] Re: synchronized vs ReentrantLock semantic
In-Reply-To: <F466C071FE519D49B1A51FCD134C6CC5036B23AD@smtp.netspoke.net>
References: <F466C071FE519D49B1A51FCD134C6CC5036B23AD@smtp.netspoke.net>
Message-ID: <42ADDD8E.6050904@cs.oswego.edu>


>   ReentrantReadWriteLock uses AbstractQueuedSychronizer to 
> manage the locking but that class explicitly states that it does not 
> perform any memory synchronization other than the "state" that it 
> manages 

Which means that if any subclass performs any operation
with memory effects equivalent to a volatile-read
in tryAcquire, and volatile-write in tryRelease, then proper JMM
memory semantics hold. Which they all do.
(It could be ensured in other ways, but this is
the most straightforward way.)

-Doug

From gregg at cytetech.com  Mon Jun 13 16:23:55 2005
From: gregg at cytetech.com (Gregg Wonderly)
Date: Mon Jun 13 16:24:05 2005
Subject: [concurrency-interest] Re: synchronized vs ReentrantLock semantic
In-Reply-To: <F466C071FE519D49B1A51FCD134C6CC5036B23AD@smtp.netspoke.net>
References: <F466C071FE519D49B1A51FCD134C6CC5036B23AD@smtp.netspoke.net>
Message-ID: <42ADEB5B.30907@cytetech.com>



Michael Smith wrote:
> Since this seems like the fundamental use-case for this API, we can only 
> assume that we are blind and just aren't seeing something in the code.  
> Can someone provide more details about how and where thread memory and 
> main memory are synchronized when using ReentrantReadWriteLock's locks?

The purpose of this type of lock it to optimize read access.  Without 
these new locks, you'd have to use synchronized(), and then you'd be 
flushing cache/TLB on every read!

So, this lock provides guards that will prohibit read while a write is 
in progress, but otherwise let reads go through unfettered.

When you use this lock to protect reading and writing of other values, 
those values must be volatile if you want their changed state to 
propagate between threads/processors etc.

The JMM has not changed.  synchronized flushes the value of everything 
referenced inside of it.  This is expensive.  volatile writes are always 
flushed.  However, volatile does not produce atomic operations (which 
synchronized also provides) that you need this type of lock to create.

So, the programming method has changed slightly, and more responsibility 
is on you to make sure you are using volatile.  By using volatile, and 
this type of lock, you get atomic code for read/write access, but 
without cache/TLB flush on read.

If you are doing this type of stuff and your read/write mix is close to 
10%/90%, you will probably not see much of a gain, because of the 
flushes that will be happening  on the writes no matter what.  But, if 
the mix is slanted the other way, then you'll win due to all the flushes 
that aren't happening on reads...

Gregg Wonderly
From dawidk at mathcs.emory.edu  Mon Jun 13 18:28:29 2005
From: dawidk at mathcs.emory.edu (Dawid Kurzyniec)
Date: Mon Jun 13 18:32:46 2005
Subject: [concurrency-interest] Re: synchronized vs ReentrantLock semantic
In-Reply-To: <42ADEB5B.30907@cytetech.com>
References: <F466C071FE519D49B1A51FCD134C6CC5036B23AD@smtp.netspoke.net>
	<42ADEB5B.30907@cytetech.com>
Message-ID: <42AE088D.2020108@mathcs.emory.edu>

Gregg Wonderly wrote:

>
>
> Michael Smith wrote:
>
>> Since this seems like the fundamental use-case for this API, we can 
>> only assume that we are blind and just aren't seeing something in the 
>> code.  Can someone provide more details about how and where thread 
>> memory and main memory are synchronized when using 
>> ReentrantReadWriteLock's locks?
>
>
> When you use this lock to protect reading and writing of other values, 
> those values must be volatile if you want their changed state to 
> propagate between threads/processors etc.
>
Gregg,

I am not sure if you're right on this (I hope you aren't), since it 
would indirectly imply that all shared state protected by 
java.util.concurrent.locks.* need to be volatile, including all the 
fields of shared objects deep in the graph. This conclusion follows from 
the fact that in the ReentrantReadWriteLock, the same synchronizer 
instance is used by the write lock and the read lock. Either of the 
following must be true: 1) the synchronizer ensures visibility by 
flushing caches, or 2) it does not. If 2) is true, it is also true for 
all other locks based on AbstractQueuedSynchronizer, in particular for 
the ReentrantLock. This would necessitate making all the shared state 
volatile when protecting it by ReentrantLock. But it is not done in 
internal j.u.c. classes, e.g. LinkedBlockingQueue. It seems that the 
only logical conclusion is that AbstractQueuedSynchronizer does provide 
visibility semantics, although I agree with Michael Smith that it does 
not follow from the javadoc at all.

 I think that a scenario that you had in mind is synchronized write and 
lock-free volatile read, kind of like the "volatile read / synchronized 
write" pattern that I inquired about a few e-mails back.

It seems that the original question posed by Michael Smith has not yet 
been answered; let me reformulate it and ask again:

We all know that the AbstractQueuedSynchronizer provides atomicity. But 
does it also ensure visibility?

(The contract of monitorExit ensures visibility, so I would expect the 
answer to be yes based on the Lock javadoc, but it does not follow from 
the AbstractQueuedSynchronizer javadoc, or the source code, as far as I 
can see).


In particular, is it safe to replace:

synchronized (x) {
   ...
}

by:

lock.lock();
try {
   ...
}
finally {
   lock.unlock();
}


regardless of what the "..." is?

Regards,
Dawid


From dl at cs.oswego.edu  Mon Jun 13 19:08:48 2005
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon Jun 13 19:08:49 2005
Subject: [concurrency-interest] Re: synchronized vs ReentrantLock semantic
In-Reply-To: <42AE088D.2020108@mathcs.emory.edu>
References: <F466C071FE519D49B1A51FCD134C6CC5036B23AD@smtp.netspoke.net>	<42ADEB5B.30907@cytetech.com>
	<42AE088D.2020108@mathcs.emory.edu>
Message-ID: <42AE1200.2000904@cs.oswego.edu>


> We all know that the AbstractQueuedSynchronizer provides atomicity.
> But does it also ensure visibility?
> 
> 
> In particular, is it safe to replace:
> 
> synchronized (x) { ... }
> 
> by:
> 
> lock.lock(); try { ... } finally { lock.unlock(); }
> 
> 

Yes! That's what the Lock spec says. Rely on it.



> (The contract of monitorExit ensures visibility, so I would expect
> the answer to be yes based on the Lock javadoc, but it does not
> follow from the AbstractQueuedSynchronizer javadoc, or the source
> code, as far as I can see).

It does follow from the javadoc spec. See the Java
memory model (in the 3rd edition of JLS
http://java.sun.com/docs/books/jls/index.html)
to help decode it and to see how getState/setState/compareAndSetState
suffice to obtain lock memory semantics.


-Doug
From dawidk at mathcs.emory.edu  Mon Jun 13 19:54:56 2005
From: dawidk at mathcs.emory.edu (Dawid Kurzyniec)
Date: Mon Jun 13 19:55:05 2005
Subject: [concurrency-interest] Re: synchronized vs ReentrantLock semantic
In-Reply-To: <42AE1200.2000904@cs.oswego.edu>
References: <F466C071FE519D49B1A51FCD134C6CC5036B23AD@smtp.netspoke.net>	<42	ADEB5B.30907@cytetech.com><42AE088D.2020108@mathcs.emory.edu>
	<42AE1200.2000904@cs.oswego.edu>
Message-ID: <42AE1CD0.8080705@mathcs.emory.edu>

Doug Lea wrote:

>
>> We all know that the AbstractQueuedSynchronizer provides atomicity.
>> But does it also ensure visibility?
>>
>>
>> In particular, is it safe to replace:
>>
>> synchronized (x) { ... }
>>
>> by:
>>
>> lock.lock(); try { ... } finally { lock.unlock(); }
>>
>>
>
> Yes! That's what the Lock spec says. Rely on it.
>
Great! So, what's the final answer on ReentrantReadWriteLock? Is it OK 
to use it to share non-volatiles then? Technically, RWLock is not a 
lock, so the spec does not explicitly apply, and RWLock doc does not say 
anything about memory synchronization. But since read and write locks 
share a synchronizer?...

BTW. Being too lazy to analyze the new JMM semantics just now, whilst 
still curious, let me ask: is it that the volatile read/write, performed 
by the AbstractQueuedSynchronizer, has implicit memory synchronization 
effects?

Thanks,
Dawid


From dl at cs.oswego.edu  Mon Jun 13 19:59:51 2005
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon Jun 13 19:59:54 2005
Subject: [concurrency-interest] Re: synchronized vs ReentrantLock semantic
In-Reply-To: <42AE1CD0.8080705@mathcs.emory.edu>
References: <F466C071FE519D49B1A51FCD134C6CC5036B23AD@smtp.netspoke.net>	<42	ADEB5B.30907@cytetech.com><42AE088D.2020108@mathcs.emory.edu>
	<42AE1200.2000904@cs.oswego.edu>
	<42AE1CD0.8080705@mathcs.emory.edu>
Message-ID: <42AE1DF7.3010904@cs.oswego.edu>

Dawid Kurzyniec wrote:

> Great! So, what's the final answer on ReentrantReadWriteLock? Is it OK 
> to use it to share non-volatiles then? Technically, RWLock is not a 
> lock, 

Yes, it is (rrwl.readLock() returns a Lock), and yes it does.


> BTW. Being too lazy to analyze the new JMM semantics just now, whilst 
> still curious, let me ask: is it that the volatile read/write, performed 
> by the AbstractQueuedSynchronizer, has implicit memory synchronization 
> effects?

Well, it has explicit memory effects :-)

In general, a volatile read acts in the same was as entry into a
synchronized block with respect to the memory model (NOT with respect to
exclusion). And a volatile acts in same way as exit from block.



-Doug
From dholmes at dltech.com.au  Mon Jun 13 21:36:44 2005
From: dholmes at dltech.com.au (David Holmes)
Date: Mon Jun 13 21:37:05 2005
Subject: [concurrency-interest] Re: synchronized vs ReentrantLock semantic
In-Reply-To: <42ADEB5B.30907@cytetech.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCAEHGFNAA.dholmes@dltech.com.au>

> Gregg Wonderly wrote:
> When you use this lock to protect reading and writing of other values,
> those values must be volatile if you want their changed state to
> propagate between threads/processors etc.

Absolutely NOT! You do not need to have the shared data that is protected by
a Lock - any kind - be declared volatile. The Lock implementations take care
of all the required memory model semantics.

Sorry Gregg but I have to make sure this misconception is entirely squashed
(both here are on the referenced forum).

Hopefully it is clear now that the lock implementations use AQS and its
getState, setState and compareAndSetState methods are specified to have the
same memory model semantics as a volatile read, write, read/write -
respectively. So if you use those methods in a class like ReentrantLock then
you automatically get the required memory model semantics


I think we (JSR-166 EG) do need to clarify the ReadWriteLock interface
documentation, because at present while both readLock() and writeLock()
returns Lock instances - and so have the specified memory model semantics
relating to lock() and unlock() - what is missing is any statement that the
two Lock instances act as if they read/write the *same* volatile. ie there
is nothing that indicates that a writeLock().unlock() has the right memory
effects related to a following readLock().lock(). The implementation
certainly does, but the spec for the interface doesn't make that clear.

David Holmes

From dawidk at mathcs.emory.edu  Mon Jun 13 22:32:15 2005
From: dawidk at mathcs.emory.edu (Dawid Kurzyniec)
Date: Mon Jun 13 22:32:21 2005
Subject: [concurrency-interest] Re: synchronized vs ReentrantLock semantic
In-Reply-To: <NFBBKALFDCPFIDBNKAPCAEHGFNAA.dholmes@dltech.com.au>
References: <NFBBKALFDCPFIDBNKAPCAEHGFNAA.dholmes@dltech.com.au>
Message-ID: <42AE41AF.2080900@mathcs.emory.edu>

David Holmes wrote:

>I think we (JSR-166 EG) do need to clarify the ReadWriteLock interface
>documentation, because at present while both readLock() and writeLock()
>returns Lock instances - and so have the specified memory model semantics
>relating to lock() and unlock() - what is missing is any statement that the
>two Lock instances act as if they read/write the *same* volatile. 
>
Which will be important not only because it will document the current 
implementation, but also because it will refine and strenghten the 
ReadWriteLock interface contract.

Regards,
Dawid


From gregg at cytetech.com  Mon Jun 13 23:16:40 2005
From: gregg at cytetech.com (Gregg Wonderly)
Date: Mon Jun 13 23:16:49 2005
Subject: [concurrency-interest] Re: synchronized vs ReentrantLock semantic
In-Reply-To: <NFBBKALFDCPFIDBNKAPCAEHGFNAA.dholmes@dltech.com.au>
References: <NFBBKALFDCPFIDBNKAPCAEHGFNAA.dholmes@dltech.com.au>
Message-ID: <42AE4C18.2010603@cytetech.com>



David Holmes wrote:
>>Gregg Wonderly wrote:
>>When you use this lock to protect reading and writing of other values,
>>those values must be volatile if you want their changed state to
>>propagate between threads/processors etc.
> 
> 
> Absolutely NOT! You do not need to have the shared data that is protected by
> a Lock - any kind - be declared volatile. The Lock implementations take care
> of all the required memory model semantics.
> 
> Sorry Gregg but I have to make sure this misconception is entirely squashed
> (both here are on the referenced forum).

Okay, then this is sure not clear to me either.  I guess I'm really 
confused now.  I would suggest there should be some even more explicit 
wording about all the specific steps and the specific ties into the 
memory model.

I guess I need to go back and reread some things to glean what I seem to 
have forgot or overlooked...

My profuse appologies for making such a misstatement...

Gregg
From dholmes at dltech.com.au  Mon Jun 13 23:38:21 2005
From: dholmes at dltech.com.au (David Holmes)
Date: Mon Jun 13 23:38:31 2005
Subject: [concurrency-interest] Re: synchronized vs ReentrantLock semantic
In-Reply-To: <42AE4C18.2010603@cytetech.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCEEHJFNAA.dholmes@dltech.com.au>

Gregg Wonderly wrote:
> Okay, then this is sure not clear to me either.  I guess I'm really
> confused now.  I would suggest there should be some even more explicit
> wording about all the specific steps and the specific ties into the
> memory model.

Where has this confusion arisen?

Locks are intended as a direct alternative for use of synchronized
blocks/methods. Hence the Lock interface specifies:

"All Lock implementations must enforce the same memory synchronization
semantics as provided by the built-in monitor lock:

- A successful lock operation acts like a successful monitorEnter action
- A successful unlock operation acts like a successful monitorExit action "


Has the confusion arisen by trying to figure out how this is actually
achieved for a given Lock implementation? Or is the whole JMM so confusing
that anything related to it causes confusion?

This aspect of things was meant to be a "no-brainer" - you shouldn't be
worrying about the details. Some of the comments in that forum thread are
scary! - people are delving into unneccessary detail far too much.

Cheers,
David Holmes

From gregg at cytetech.com  Tue Jun 14 00:16:01 2005
From: gregg at cytetech.com (Gregg Wonderly)
Date: Tue Jun 14 00:16:05 2005
Subject: [concurrency-interest] Re: synchronized vs ReentrantLock semantic
In-Reply-To: <NFBBKALFDCPFIDBNKAPCEEHJFNAA.dholmes@dltech.com.au>
References: <NFBBKALFDCPFIDBNKAPCEEHJFNAA.dholmes@dltech.com.au>
Message-ID: <42AE5A01.8000207@cytetech.com>



David Holmes wrote:
> Gregg Wonderly wrote:
> 
>>Okay, then this is sure not clear to me either.  I guess I'm really
>>confused now.  I would suggest there should be some even more explicit
>>wording about all the specific steps and the specific ties into the
>>memory model.
> 
> Where has this confusion arisen?

I guess that I haven't remembered enough of what I've read, or perhaps I 
haven't read what I needed.  For some reason, I was under the impression 
that lock semantics were better optimizations then just the ability to 
create synchronized pieces of code that were not Java language blocks.
Clearly, I was day dreaming or something...  When I've used the 
monitorEnter and monitorExit JNI methods, I've thought about having more 
flexible control as Lock provides.  I guess I just throught that perhaps 
read would be cheaper so that I could lock operations down, without 
flushing caches and such.  But, now that I think longer and harder, I do 
know that the read must use synchronized() semantics, and that, in and 
of itself will create the side effects that are being discussed here.

private Object countLock = new Object();
private int count;
public int getCount() {
	synchronized( countLock ) {
		return count;
	}
}

public int addCount( int val ) {
	synchronized( countLock ) {
		count += val;
	}
}

is the same thing as doing

private Lock countLock = new ReentrantReadWriteLock();
private int count;
public int getCount() {
	Lock l = countLock.readLock();
	try {
		l.lock();
		return count;
	} finally {
		l.unlock();
	}
}

public int addCount( int val ) {
	Lock l = countLock.writeLock();
	try {
		l.lock();
		count += val;
	} finally {
		l.unlock();
	}
}

which, in and of itself doesn't look very exciting as a replacement. 
But, there are some more interesting opportunities for managing the 
state around the Locks with all of the associated methods on Lock as 
well as the expanded methods on these two types of Locks.

> Locks are intended as a direct alternative for use of synchronized
> blocks/methods. Hence the Lock interface specifies:

I guess I just didn't see this as the literal statement that it is 
intended to be.

> "All Lock implementations must enforce the same memory synchronization
> semantics as provided by the built-in monitor lock:
> 
> - A successful lock operation acts like a successful monitorEnter action
> - A successful unlock operation acts like a successful monitorExit action "

In my opinion, having this level of implementation detail in the 
documentation adds some complications because it doesn't tell the casual 
user anything.  Nothing in the language or API has those words in it. 
Only the underlying implementation details in JNI have such things.  I 
think this should say.

  - A successful lock operation is equivalent to beginning a 
synchronized section of code.

  - A successful unlock operation is equivalent to exiting the 
previously entered synchronized section of code.

This makes it clear that everything we depended on happening before 
still happens.

One might be tempted to add phrases like:

On lock entry, all Java Memory Model semantics for synchronized are met 
including...

But I think that just makes it even more likely that people will read 
stuff that is not important to them and become confused by their 
interpretation of the words and phrases therein.

> Has the confusion arisen by trying to figure out how this is actually
> achieved for a given Lock implementation? Or is the whole JMM so confusing
> that anything related to it causes confusion?

Certainly, I don't have any issues with understanding, now, what was 
intended.  The concepts of the JMM are complex, but not confusing to me. 
  The scary string of comments on that forum thread, to me, just go to 
show how many people still think that the Java language and the Java 
platform as implemented in the JVM are two completely separate things 
and that you have to look under the covers to know what will happen.

For me, the javadocs should always be enough for a method's description.

The problem here maybe related to the fact that this work took the 
semantics of a keyword and made those visible in an interface with 
program structure issues and documentation semantic issues that are 
apparently confusing.

> This aspect of things was meant to be a "no-brainer" - you shouldn't be
> worrying about the details. Some of the comments in that forum thread are
> scary! - people are delving into unneccessary detail far too much.

And I am one to ignore the JLS and JMM until I can't explain what I see. 
  Then I have to look at what is intended to be happening based on the 
specs.

Apparently I just completely missed the boat on understanding these 
changes.  I should have gone back and reread and thought about it first. 
  When I was composing that email I was thinking that the JMM equivalent 
synchronized flush would not be done implicitly by the lock() call. 
Otherwise it would have been called something else, besides lock(). If I 
would have thought a little longer and read the docs again, I wouldn't 
have volunteered such useless information...

Please just swat me up the side of the head and I'll go sit in the corner...

Gregg Wonderly
From dawidk at mathcs.emory.edu  Tue Jun 14 00:33:27 2005
From: dawidk at mathcs.emory.edu (Dawid Kurzyniec)
Date: Tue Jun 14 00:33:42 2005
Subject: [concurrency-interest] Re: synchronized vs ReentrantLock semantic
In-Reply-To: <NFBBKALFDCPFIDBNKAPCEEHJFNAA.dholmes@dltech.com.au>
References: <NFBBKALFDCPFIDBNKAPCEEHJFNAA.dholmes@dltech.com.au>
Message-ID: <42AE5E17.1000100@mathcs.emory.edu>

David Holmes wrote:

>Gregg Wonderly wrote:
>  
>
>>Okay, then this is sure not clear to me either.  I guess I'm really
>>confused now.  I would suggest there should be some even more explicit
>>wording about all the specific steps and the specific ties into the
>>memory model.
>>    
>>
>
>Where has this confusion arisen?
>
>Locks are intended as a direct alternative for use of synchronized
>blocks/methods. Hence the Lock interface specifies:
>
>"All Lock implementations must enforce the same memory synchronization
>semantics as provided by the built-in monitor lock:
>
>- A successful lock operation acts like a successful monitorEnter action
>- A successful unlock operation acts like a successful monitorExit action "
>
>
>  
>
I agree with Greg that perhaps the javadoc should be more verbose on 
this. Including the spec of AQS. As it is, it is perfectly accurate but 
requires the reader to have read and understood the JVM spec, in 
particular the semantics of monitorEnter/monitorExit and the JMM. I 
think it would be beneficial for the average API user if you briefly 
reiterate some crucial properties here, without implicitly referring to 
the JVM spec.

I think that some people may not even immediately understand, or be sure 
about, the connection between phrases "built-in monitor lock, 
monitorEnter, monitorExit" and "synchronized() {}". Maybe just a simple 
example could remove the confusion.

Regards,
Dawid

From dholmes at dltech.com.au  Tue Jun 14 00:50:03 2005
From: dholmes at dltech.com.au (David Holmes)
Date: Tue Jun 14 00:50:10 2005
Subject: [concurrency-interest] Re: synchronized vs ReentrantLock semantic
In-Reply-To: <42AE5E17.1000100@mathcs.emory.edu>
Message-ID: <NFBBKALFDCPFIDBNKAPCGEHLFNAA.dholmes@dltech.com.au>

> Dawid Kurzyniec writes:
> I agree with Greg that perhaps the javadoc should be more verbose on
> this. Including the spec of AQS. As it is, it is perfectly accurate but
> requires the reader to have read and understood the JVM spec, in
> particular the semantics of monitorEnter/monitorExit and the JMM.

I don't think there is any problem with the AQS docs - it is written in
terms of behaviour of volatiles and to understand a volatile you have to
understand the JMM - JLS 8.3.1.4 is quite clear on that.

I do agree about Lock however. The use of "monitorEnter" and "monitorExit"
is quite unfamiliar to most language users. Even the JLS only talks about
monitors in Chapter 17; it doesn't mention them at all in the discussion of
the synchronized statement in 14.19 !

And anytime the docs mention memory semantics there should really be a
reference to JLS Chapter 17. We should have a standard phrase like:

"In terms of the Java Memory Model (see "The Java Language Specification",
Chapter 17) the memory synchronization effect of this method is ..."

Cheers,
David Holmes

From dholmes at dltech.com.au  Tue Jun 14 00:58:00 2005
From: dholmes at dltech.com.au (David Holmes)
Date: Tue Jun 14 00:58:16 2005
Subject: [concurrency-interest] Re: synchronized vs ReentrantLock semantic
In-Reply-To: <42AE5A01.8000207@cytetech.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCAEHMFNAA.dholmes@dltech.com.au>

Gregg,

> private Object countLock = new Object();
> private int count;
> public int getCount() {
> 	synchronized( countLock ) {
> 		return count;
> 	}
> }
> public int addCount( int val ) {
> 	synchronized( countLock ) {
> 		count += val;
> 	}
> }
>
> is the same thing as doing
>
> private Lock countLock = new ReentrantReadWriteLock();
> private int count;
> public int getCount() {
> 	Lock l = countLock.readLock();
> 	try {
> 		l.lock();
> 		return count;
> 	} finally {
> 		l.unlock();
> 	}
> }
>
> public int addCount( int val ) {
> 	Lock l = countLock.writeLock();
> 	try {
> 		l.lock();
> 		count += val;
> 	} finally {
> 		l.unlock();
> 	}
> }

Aside: the lock() occurs before the try block, otherwise you may attempt to
unlock a lock you never acquired.

> which, in and of itself doesn't look very exciting as a replacement.

I think you are misunderstanding the purpose of a ReadWriteLock. RWL allows
for additional concurrency by allowing multiple concurrent readers. For that
to actually occur "read" operations should be frequent and relatively
lengthy compared to "write" operations that should be infrequent.

Naively changing existing synchronized code to use a readLock for a "getter"
method, and a writeLock for a "setter" method, will generally result in
poorer performance due to the additional overhead of a ReadWriteLock.

Cheers,
David Holmes

From dl at cs.oswego.edu  Tue Jun 14 08:49:00 2005
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue Jun 14 08:49:03 2005
Subject: [concurrency-interest] Re: synchronized vs ReentrantLock semantic
In-Reply-To: <NFBBKALFDCPFIDBNKAPCGEHLFNAA.dholmes@dltech.com.au>
References: <NFBBKALFDCPFIDBNKAPCGEHLFNAA.dholmes@dltech.com.au>
Message-ID: <42AED23C.708@cs.oswego.edu>

David Holmes wrote:

> And anytime the docs mention memory semantics there should really be a
> reference to JLS Chapter 17. We should have a standard phrase like:
> 
> "In terms of the Java Memory Model (see "The Java Language Specification",
> Chapter 17) the memory synchronization effect of this method is ..."
> 

We couldn't do this for initial J2SE5 release, because
the revised JLS didn't even exist yet. But we should
be able to do this for Mustang updates. (David will
help me put these in :-)

-Doug
From gregg at cytetech.com  Tue Jun 14 09:36:32 2005
From: gregg at cytetech.com (Gregg Wonderly)
Date: Tue Jun 14 09:36:39 2005
Subject: [concurrency-interest] Re: synchronized vs ReentrantLock semantic
In-Reply-To: <NFBBKALFDCPFIDBNKAPCAEHMFNAA.dholmes@dltech.com.au>
References: <NFBBKALFDCPFIDBNKAPCAEHMFNAA.dholmes@dltech.com.au>
Message-ID: <42AEDD60.7040003@cytetech.com>

David Holmes wrote:
>>private Lock countLock = new ReentrantReadWriteLock();
>>private int count;
>>public int getCount() {
>>	Lock l = countLock.readLock();
>>	try {
>>		l.lock();
>>		return count;
>>	} finally {
>>		l.unlock();
>>	}
>>}
>>
>>public int addCount( int val ) {
>>	Lock l = countLock.writeLock();
>>	try {
>>		l.lock();
>>		count += val;
>>	} finally {
>>		l.unlock();
>>	}
>>}
> 
> 
> Aside: the lock() occurs before the try block, otherwise you may attempt to
> unlock a lock you never acquired.

Yes, typically, one should put the 'entry' event for anything undone in 
finally, outside of the try.  In this case, the only failure mode is 'l' 
being null which would also fail in 'finally' and not attempt anything 
ugly.  This is a good point to mention to people...

> I think you are misunderstanding the purpose of a ReadWriteLock. RWL allows
> for additional concurrency by allowing multiple concurrent readers. For that
> to actually occur "read" operations should be frequent and relatively
> lengthy compared to "write" operations that should be infrequent.

This is the point that I was trying to convince myself of.  I didn't 
express the concurrency that is not possible in my above example 
explicitly.  I was going back after the cache effects of synchronized. 
In my original misstatement I was drawing the wrong conclusions about 
the effects of read lock entry related to cache effects because I didn't 
remember what had to happen for read lock entry to work.

> Naively changing existing synchronized code to use a readLock for a "getter"
> method, and a writeLock for a "setter" method, will generally result in
> poorer performance due to the additional overhead of a ReadWriteLock.

Yes, there has to be a significantly larger percentage of read 
operations before you will see the effects of the concurrency benefits 
designed into the locks...

Gregg Wonderly
From jozart at blarg.net  Sun Jun 19 23:17:58 2005
From: jozart at blarg.net (Joe Bowbeer)
Date: Sun Jun 19 23:20:33 2005
Subject: [concurrency-interest] Swing Threads article updated
References: <98cf1af15b7fb5ebcc8c895a1102afb5@blargmail.com>
	<42913916.7010109@cytetech.com>
Message-ID: <00be01c57546$a8e5a530$0200a8c0@REPLICANT2>

On May 22, Gregg Wonderly wrote:

> Please have a look at [...] swingutil.dev.java.net

Sorry to respond so late.  I've been away from my station for a few weeks.

I just added "swingutil.dev.java.net" to my watch list.


I would also like to point you to "swingworker.dev.java.net"

This is a backport of the SwingWorker that will be included in JDK 1.6 (!)


----- Original Message ----- 
From: "Gregg Wonderly" <gregg@cytetech.com>
To: "Joe Bowbeer" <jozart@blarg.net>
Cc: <concurrency-interest@altair.cs.oswego.edu>
Sent: Sunday, May 22, 2005 6:59 PM
Subject: Re: [concurrency-interest] Swing Threads article updated


Joe Bowbeer wrote:
> Now featured at javadesktop.org:
>
> Proving that "the last word" never really is, Joseph Bowbeer has just 
> updated
> this Swing Connection article to reflect the java.util.concurrent package 
> that
> debuted in JDK 5.0. The article features SwingWorker, a form of which will 
> be
> included in JDK 6.0 if RFE 4681682 is approved.

Please have a look at the ComponentUpdateThread class in
swingutil.dev.java.net for an expansion of SwingWorker that includes
some helpful, additional features for UIs dealing with remote or long
running operations.

Gregg Wonderly

From jheintz at gmail.com  Tue Jun 28 14:17:15 2005
From: jheintz at gmail.com (John D. Heintz)
Date: Tue Jun 28 14:49:32 2005
Subject: [concurrency-interest] AtomicBoolean and Double-Checking Locking?
	Is this Singleton safe?
Message-ID: <c8204543050628111752af4443@mail.gmail.com>

Hello all,

I've been waiting for a bunch of new Double-Checked locking articles
to come out since the release of the java.util.concurrent.atomic
package, but I've seen very little ;-)

The most recent discussion I could find was
http://altair.cs.oswego.edu/pipermail/concurrency-interest/2005-May/001472.html,
but of course I may have missed something.

I have a simple question: can an AtomicBoolean check _safely_ replace
the '== null' from yester-years un-safe Double-Checked Locking
Singleton idiom?

Here's an example Singleton implementation to show what I mean.
================================================
import java.util.concurrent.atomic.AtomicBoolean;

public class Singleton {
	static Singleton _instance;
	static AtomicBoolean _instanceFlag = new AtomicBoolean(false);
	
	public Singleton() {
		
	}
	
	static Singleton getInstance() {
		if (_instanceFlag.get() != true) {

			synchronized (_instanceFlag) { // block others until finished constructing
				// double check
				if (_instanceFlag.get() != true) {
					_instance = new Singleton();
					
					if (!_instanceFlag.compareAndSet(false, true)) {
						throw new IllegalStateException("Ack! Can't fail this call!!!");
					}
				}
			}
			
		}
		return _instance;
	}
}
===============================================


Thanks all,
John

-- 
John D. Heintz
Software Craftsman
Austin, TX
(512) 633-1198

jheintz@pobox.com
http://johnheintz.homeip.net

From jmanson at cs.purdue.edu  Tue Jun 28 15:02:23 2005
From: jmanson at cs.purdue.edu (Jeremy Manson)
Date: Tue Jun 28 15:02:30 2005
Subject: [concurrency-interest] AtomicBoolean and Double-Checking Locking?
	Is this Singleton safe?
In-Reply-To: <c8204543050628111752af4443@mail.gmail.com>
References: <c8204543050628111752af4443@mail.gmail.com>
Message-ID: <20050628190223.GA22443@zardoz.cs.purdue.edu>

Hi John,

This is safe, in the sense that AtomicBooleans can be used for volatiles.
It would probably make somewhat more sense to use AtomicReference
rather than AtomicBoolean.

Having said that, I am not sure this is a good idea.  Not all platforms are 
guaranteed
to support efficient AtomicXXXs.  The code is uglier than the simple use
of volatile, which is designed to work correctly in this case.  
Where is the win here?

					Jeremy

On Tue, Jun 28, 2005 at 01:17:15PM -0500, John D. Heintz wrote:
> Hello all,
> 
> I've been waiting for a bunch of new Double-Checked locking articles
> to come out since the release of the java.util.concurrent.atomic
> package, but I've seen very little ;-)
> 
> The most recent discussion I could find was
> http://altair.cs.oswego.edu/pipermail/concurrency-interest/2005-May/001472.html,
> but of course I may have missed something.
> 
> I have a simple question: can an AtomicBoolean check _safely_ replace
> the '== null' from yester-years un-safe Double-Checked Locking
> Singleton idiom?
> 
> Here's an example Singleton implementation to show what I mean.
> ================================================
> import java.util.concurrent.atomic.AtomicBoolean;
> 
> public class Singleton {
> 	static Singleton _instance;
> 	static AtomicBoolean _instanceFlag = new AtomicBoolean(false);
> 	
> 	public Singleton() {
> 		
> 	}
> 	
> 	static Singleton getInstance() {
> 		if (_instanceFlag.get() != true) {
> 
> 			synchronized (_instanceFlag) { // block others until finished constructing
> 				// double check
> 				if (_instanceFlag.get() != true) {
> 					_instance = new Singleton();
> 					
> 					if (!_instanceFlag.compareAndSet(false, true)) {
> 						throw new IllegalStateException("Ack! Can't fail this call!!!");
> 					}
> 				}
> 			}
> 			
> 		}
> 		return _instance;
> 	}
> }
> ===============================================
> 
> 
> Thanks all,
> John
> 
> -- 
> John D. Heintz
> Software Craftsman
> Austin, TX
> (512) 633-1198
> 
> jheintz@pobox.com
> http://johnheintz.homeip.net
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest@altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
From jheintz at gmail.com  Tue Jun 28 15:36:27 2005
From: jheintz at gmail.com (John D. Heintz)
Date: Tue Jun 28 15:39:09 2005
Subject: [concurrency-interest] AtomicBoolean and Double-Checking Locking?
	Is this Singleton safe?
In-Reply-To: <20050628190223.GA22443@zardoz.cs.purdue.edu>
References: <c8204543050628111752af4443@mail.gmail.com>
	<20050628190223.GA22443@zardoz.cs.purdue.edu>
Message-ID: <c82045430506281236234d369c@mail.gmail.com>

Thanks Jeremy for verifying that assumption.

I used Singleton as a clear/obvious example of what I'm really trying
to do. You're right, either volatile or using the
"Initialize-On-Demand Holder Class idiom" would both be more
straightforward and rational for a Singleton.

I want to use Annotations to support lazy loading "services" from an
IOC container, but to do so in an efficient and thread-safe way. I
basically want "write once" fields (I think they're sometimes called
logic variables).

Now that my assumptions for AtomicBoolean are verified I can throw
together a simple proof of concept. I'll basically need an
AtomicBoolean for each field (per instance or per class for statics)
to check for the "already initialized" condition. I'm starting with
AspectJ for this, but am seeing that AspectJ doesn't generalize over
fields as well as I would like. It looks like I'll need to have a per
instance (or per class for statics) ConcurrentHashMap to lookup the
_right_ AtomicBoolean object for each field for each instance.

For example:
class FooService {
  @Obtain("emailService")
  private IEmailService _emailService;

  @Obtain("jndi:dataSourceXYZ")
  private DataSource _dataSource;

  public void doSomething() {
    _dataSource.getConnection()......

    _emailService.send("", "", ...);
  }
}

The two fields here should be lazy-loadable, thread-safe, and efficient. 

I picked up this style from PEAK (a python component app framework
with very sophisticated composition mechanisms). See
http://peak.telecommunity.com/DevCenter/IntroToPeak_2fLessonOne for
more details, but this is an example of some PEAK python service code:

class Foo:
    service = binding.Obtain(PropertyName('helloworld.service'))

    def doSomething(self, args):
        self.service(args)
        .....

The idea is that each component declares it's dependencies (here with
a PropertyName but anything can be a key: JNDI name, Interface type,
...)  Components are then bound together in a hierarchical context
where these bindings are resolved. Python has a global interpreter
lock so threads don't run concurrently. That's great everywhere
(simplifies a bunch of things) but SMP machines of course.

Thanks for the verification! If anyone's interested I'll let you know
how it turns out.
John

On 6/28/05, Jeremy Manson <jmanson@cs.purdue.edu> wrote:
> Hi John,
> 
> This is safe, in the sense that AtomicBooleans can be used for volatiles.
> It would probably make somewhat more sense to use AtomicReference
> rather than AtomicBoolean.
> 
> Having said that, I am not sure this is a good idea.  Not all platforms are
> guaranteed
> to support efficient AtomicXXXs.  The code is uglier than the simple use
> of volatile, which is designed to work correctly in this case.
> Where is the win here?
> 
>                                         Jeremy
> 
> On Tue, Jun 28, 2005 at 01:17:15PM -0500, John D. Heintz wrote:
> > Hello all,
> >
> > I've been waiting for a bunch of new Double-Checked locking articles
> > to come out since the release of the java.util.concurrent.atomic
> > package, but I've seen very little ;-)
> >
> > The most recent discussion I could find was
> > http://altair.cs.oswego.edu/pipermail/concurrency-interest/2005-May/001472.html,
> > but of course I may have missed something.
> >
> > I have a simple question: can an AtomicBoolean check _safely_ replace
> > the '== null' from yester-years un-safe Double-Checked Locking
> > Singleton idiom?
> >
> > Here's an example Singleton implementation to show what I mean.
> > ================================================
> > import java.util.concurrent.atomic.AtomicBoolean;
> >
> > public class Singleton {
> >       static Singleton _instance;
> >       static AtomicBoolean _instanceFlag = new AtomicBoolean(false);
> >
> >       public Singleton() {
> >
> >       }
> >
> >       static Singleton getInstance() {
> >               if (_instanceFlag.get() != true) {
> >
> >                       synchronized (_instanceFlag) { // block others until finished constructing
> >                               // double check
> >                               if (_instanceFlag.get() != true) {
> >                                       _instance = new Singleton();
> >
> >                                       if (!_instanceFlag.compareAndSet(false, true)) {
> >                                               throw new IllegalStateException("Ack! Can't fail this call!!!");
> >                                       }
> >                               }
> >                       }
> >
> >               }
> >               return _instance;
> >       }
> > }
> > ===============================================
> >
> >
> > Thanks all,
> > John
> >
> > --
> > John D. Heintz
> > Software Craftsman
> > Austin, TX
> > (512) 633-1198
> >
> > jheintz@pobox.com
> > http://johnheintz.homeip.net
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest@altair.cs.oswego.edu
> > http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> 


-- 
John D. Heintz
Software Craftsman
Austin, TX
(512) 633-1198

jheintz@pobox.com
http://johnheintz.homeip.net

From nikolai at ifad.dk  Wed Jun 29 10:38:26 2005
From: nikolai at ifad.dk (Nikolai V. Chr.)
Date: Wed Jun 29 10:56:22 2005
Subject: [concurrency-interest] Synchronizing on AtomicBoolean safe?
Message-ID: <42C2B262.7020601@ifad.dk>

Is this sane? If not, how could I do it? I used to use getLock() on 
Lea's old SynchronizedBoolean.

AtomicBoolean protocolSupportEnabled = new AtomicBoolean(false);

synchronized(protocolSupportEnabled) {
   old = protocolSupportEnabled.getAndSet(enable);
   if(enable) {
      
propertyChangeMulticaster.addPropertyChangeListenerIfAbsent(protocolListener);
   } else {
      
propertyChangeMulticaster.removePropertyChangeListener(protocolListener);
   }
   logProtocolSupportEnabled(enable);
}

-- 
Nikolai V. Christensen, Computer Engineer,
Simulation and Training department
IFAD TS A/S, ?stre stationsvej 43 2.tv, DK-5000 Odense C
Denmark, EU
Phone: +45 63 11 02 11  Fax: +45 65 93 29 99
WWWeb: http://www.ifad.dk
e-mail: Nikolai.V.Christensen@ifad.dk
--


From dholmes at dltech.com.au  Wed Jun 29 12:19:16 2005
From: dholmes at dltech.com.au (David Holmes)
Date: Wed Jun 29 12:19:24 2005
Subject: [concurrency-interest] Synchronizing on AtomicBoolean safe?
In-Reply-To: <42C2B262.7020601@ifad.dk>
Message-ID: <NFBBKALFDCPFIDBNKAPCOEFGFOAA.dholmes@dltech.com.au>

Nikolai,

An AtomicBoolean is just an Object and you can synchronize on it just as you
can any other object.

That said it seems somewhat obscure to use the lock of the AtomicBoolean
this way - if you need a lock object then just declare one:
   Object lock = new Object();
and synchronize on that. It is also unclear from the code snippet why you
want an AtomicBoolean when you are using synchronized blocks anyway??

Cheers,
David Holmes

> -----Original Message-----
> From: concurrency-interest-bounces@cs.oswego.edu
> [mailto:concurrency-interest-bounces@cs.oswego.edu]On Behalf Of Nikolai
> V. Chr.
> Sent: Thursday, 30 June 2005 12:38 AM
> To: concurrency-interest@altair.cs.oswego.edu
> Subject: [concurrency-interest] Synchronizing on AtomicBoolean safe?
>
>
> Is this sane? If not, how could I do it? I used to use getLock() on
> Lea's old SynchronizedBoolean.
>
> AtomicBoolean protocolSupportEnabled = new AtomicBoolean(false);
>
> synchronized(protocolSupportEnabled) {
>    old = protocolSupportEnabled.getAndSet(enable);
>    if(enable) {
>
> propertyChangeMulticaster.addPropertyChangeListenerIfAbsent(protoc
olListener);
>    } else {
>
> propertyChangeMulticaster.removePropertyChangeListener(protocolListener);
>    }
>    logProtocolSupportEnabled(enable);
> }
>
> --
> Nikolai V. Christensen, Computer Engineer,
> Simulation and Training department
> IFAD TS A/S, ?stre stationsvej 43 2.tv, DK-5000 Odense C
> Denmark, EU
> Phone: +45 63 11 02 11  Fax: +45 65 93 29 99
> WWWeb: http://www.ifad.dk
> e-mail: Nikolai.V.Christensen@ifad.dk
> --
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest@altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>

From tim at peierls.net  Wed Jun 29 12:30:28 2005
From: tim at peierls.net (Tim Peierls)
Date: Wed Jun 29 12:30:47 2005
Subject: [concurrency-interest] Synchronizing on AtomicBoolean safe?
In-Reply-To: <42C2B262.7020601@ifad.dk>
References: <42C2B262.7020601@ifad.dk>
Message-ID: <42C2CCA4.8080000@peierls.net>

This will work, but why use AtomicBoolean at all? There is no intrinsic 
relationship between the monitor lock associated with an object and that 
object's state. It's simpler to use a plain boolean guarded by the 
containing object's monitor lock:

   /**
    * Class invariant:
    * protocolSupportEnabled is true iff propertyChangeMulticaster
    * contains protocolListener on its list of listeners.
    */
   public synchronized boolean enableProtocolSupport(boolean enable) {
       if (enable)
           ... add protocolListener ...
       else
           ... remove protocolListener ...
       boolean old = protocolSupportEnabled;
       protocolSupportEnabled = enable;
       return old;
   }

   public synchronized boolean isProtocolSupportEnabled() {
       return protocolSupportEnabled;
   }

   /** Guarded by this object's monitor lock */
   private boolean protocolSupportEnabled = false;

   /** Listener list state guarded by this object's monitor lock */
   private final PropertyChangeMulticaster propertyChangeMulticaster;

If for some reason you can't (or don't want to) use the containing object's 
monitor lock, you could use instead, for example:

  synchronized (protocolListener) { ... }

instead of synchronized methods.

--tim


Nikolai V. Chr. wrote:
> Is this sane? If not, how could I do it? I used to use getLock() on 
> Lea's old SynchronizedBoolean.
> 
> AtomicBoolean protocolSupportEnabled = new AtomicBoolean(false);
> 
> synchronized(protocolSupportEnabled) {
>   old = protocolSupportEnabled.getAndSet(enable);
>   if(enable) {
>      propertyChangeMulticaster.addPropertyChangeListenerIfAbsent(protocolListener); 
>   } else {
>      propertyChangeMulticaster.removePropertyChangeListener(protocolListener);
>   }
>   logProtocolSupportEnabled(enable);
> }


From david at walend.net  Wed Jun 29 12:31:54 2005
From: david at walend.net (David Walend)
Date: Wed Jun 29 12:32:15 2005
Subject: [concurrency-interest] Talk about the juc backport,
	retroweaver and somnifugiJMS today
Message-ID: <D1F094C0-1031-406C-AFCD-420848AAF02C@walend.net>

I'm giving a quick talk on how I used the juc backport and  
retroweaver at JavaOne in the java.net booth on the pavilion floor at  
5 pm today.

If some of you who work on the backport are here, I'd be grateful for  
some help fielding difficult questions about it.

Thanks,

Dave

David Walend

From Arne.Burmeister at infopark.de  Thu Jun 30 06:28:55 2005
From: Arne.Burmeister at infopark.de (Arne Burmeister)
Date: Thu Jun 30 06:53:24 2005
Subject: [concurrency-interest] ThreadPoolExecutor with
	PriorityBlockingQueue throws ClassCastException
Message-ID: <42C3C967.3000803@infopark.de>

I got in trouble using a ThreadPoolExecutor with a PriorityBlockingQueue.
This seems not to work due to the FutureTask is wrapped around the Callable
passed to submit(), invokeAll() or invokeAny(). The FutureTask is queued but
it is not Comparable. So a ClassCastException is thrown.

Ok, i tried a Comparator set to the PriorityBlockingQueue, seeing the 
FutureTask
is queued. But there is no chance to get the wrapped Callable from the 
FutureTask
to use its Comparable interface. This seems to be a general problem of 
the Java 1.5
implementation. So if i do not have a great misunderstanding of using 
the framework
this is a bug or lack of functionality in the implementation.

Having a look at the JSR166 implementation CVS and also the backport
(Thanks Dawid!) i see at AbstractExecutorService a new factory method
newTaskFor() to create the Future so i may override by one creating a 
Comparable
Future.

Is this the recommended way to do so? Or should FutureTask implement 
Comparable
delegating to the wrapped task? Or should FutureTask offer the wrapped 
task and i use
a Comparator for the queue?

Thanks in advance,
  Arne Burmeister
From mph at cs.brown.edu  Thu Jun 30 06:59:10 2005
From: mph at cs.brown.edu (Maurice Herlihy)
Date: Thu Jun 30 06:59:16 2005
Subject: [concurrency-interest] Open-source Software Transactional memory
	package available
Message-ID: <42C3D07E.604@cs.brown.edu>

Announcing the availability of SXM, an open-source software 
transactional memory package for C#. This code is made public in the 
hope it will be helpful to researchers in our community.

A novel aspect of SXM is support for customizable atomic object 
``factories'' that automatically add transactional synchronization to 
sequential objects. For more information: 
http://www.cs.brown.edu/~mph/SXM/README.doc. Download: 
http://www.cs.brown.edu/~mph/
From jozart at blarg.net  Thu Jun 30 08:23:23 2005
From: jozart at blarg.net (Joe Bowbeer)
Date: Thu Jun 30 08:23:38 2005
Subject: [concurrency-interest] ThreadPoolExecutor
	with	PriorityBlockingQueue throws ClassCastException
Message-ID: <f73c06589332d2f851b2bf30891a612b@blargmail.com>

Is there a reason that you're using executorService.submit rather than executor.execute?

The most direct way to accomplish what you want, I believe, would be to create your own ComparableFutureTasks and "execute" them directly -- as executor.execute operates on your FutureTasks directly, without any rewrapping.


On Thu, Jun 30, 2005 at 3:55am Arne Burmeister <Arne.Burmeister@infopark.de> wrote:
> I got in trouble using a ThreadPoolExecutor with a PriorityBlockingQueue.
> This seems not to work due to the FutureTask is wrapped around the Callable
> passed to submit(), invokeAll() or invokeAny(). The FutureTask is queued but
> it is not Comparable. So a ClassCastException is thrown.
> 
> Ok, i tried a Comparator set to the PriorityBlockingQueue, seeing the 
> FutureTask
> is queued. But there is no chance to get the wrapped Callable from the 
> FutureTask
> to use its Comparable interface. This seems to be a general problem of 
> the Java 1.5
> implementation. So if i do not have a great misunderstanding of using 
> the framework
> this is a bug or lack of functionality in the implementation.
> 
> Having a look at the JSR166 implementation CVS and also the backport
> (Thanks Dawid!) i see at AbstractExecutorService a new factory method
> newTaskFor() to create the Future so i may override by one creating a 
> Comparable
> Future.
> 
> Is this the recommended way to do so? Or should FutureTask implement 
> Comparable
> delegating to the wrapped task? Or should FutureTask offer the wrapped 
> task and i use
> a Comparator for the queue?
> 
> Thanks in advance,
>   Arne Burmeister

From nikolai at ifad.dk  Thu Jun 30 09:57:25 2005
From: nikolai at ifad.dk (Nikolai V. Chr.)
Date: Thu Jun 30 10:00:42 2005
Subject: [concurrency-interest] Synchronizing on AtomicBoolean safe?
In-Reply-To: <42C2CCA4.8080000@peierls.net>
References: <42C2B262.7020601@ifad.dk> <42C2CCA4.8080000@peierls.net>
Message-ID: <42C3FA45.6040806@ifad.dk>

Tim Peierls wrote:

> This will work, but why use AtomicBoolean at all?


Sometimes I wish to just set a value on it, using set(). And I was 
hoping that while I was synchronizing on the AtomicBoolean, I was 
synchronizing on the same same object that AtomicBoolean was 
synchronizing on.

Just like doing this would do:

SynchronizedBoolean protocolSupportEnabled = new 
SynchronizedBoolean(false);

synchronized(protocolSupportEnabled.getLock()) {
  old = protocolSupportEnabled.set(enable);
  if(enable) {
     
propertyChangeMulticaster.addPropertyChangeListenerIfAbsent(protocolListener); 

  } else {
     
propertyChangeMulticaster.removePropertyChangeListener(protocolListener);
  }
  logProtocolSupportEnabled(enable);
}

And someshwere else I would do this at the same time:

protocolSupportEnabled.set(true);

And then expect them to lock on the same Object. The question is, will 
the previous posted code do the same?


-- 
Nikolai V. Christensen, Computer Engineer,
Simulation and Training department
IFAD TS A/S, ?stre stationsvej 43 2.tv, DK-5000 Odense C
Denmark, EU
Phone: +45 63 11 02 11  Fax: +45 65 93 29 99
WWWeb: http://www.ifad.dk
e-mail: Nikolai.V.Christensen@ifad.dk
--


From tim at peierls.net  Thu Jun 30 11:00:00 2005
From: tim at peierls.net (Tim Peierls)
Date: Thu Jun 30 11:00:11 2005
Subject: [concurrency-interest] Synchronizing on AtomicBoolean safe?
In-Reply-To: <42C3FA45.6040806@ifad.dk>
References: <42C2B262.7020601@ifad.dk> <42C2CCA4.8080000@peierls.net>
	<42C3FA45.6040806@ifad.dk>
Message-ID: <42C408F0.4050201@peierls.net>

Nikolai V. Chr. wrote:
> Sometimes I wish to just set a value on it, using set(). And I was 
> hoping that while I was synchronizing on the AtomicBoolean, I was 
> synchronizing on the same same object that AtomicBoolean was 
> synchronizing on.

AtomicBoolean doesn't synchronize on *any* value. The set() method executes 
atomically without obtaining a lock. That's a feature, not a bug. :-)


> Just like doing this would do:
> 
> SynchronizedBoolean protocolSupportEnabled = new SynchronizedBoolean(false);
> 
> synchronized(protocolSupportEnabled.getLock()) {
>  old = protocolSupportEnabled.set(enable);
>  if(enable) {
>     propertyChangeMulticaster.addPropertyChangeListenerIfAbsent(protocolListener); 
>  } else {
>     propertyChangeMulticaster.removePropertyChangeListener(protocolListener);
>  }
>  logProtocolSupportEnabled(enable);
> }
> 
> And somewhere else I would do this at the same time:
> 
> protocolSupportEnabled.set(true);
> 
> And then expect them to lock on the same Object. The question is, will 
> the previous posted code do the same?

No. I'm not sure what behavior you are trying to support, but you cannot 
simply replace SynchronizedBoolean with AtomicBoolean and synchronize on 
the AtomicBoolean instance itself instead of SynchronizedBoolean.getLock().

It's worth repeating: There is no intrinsic relationship between the 
monitor lock associated with an object and that object's state.

Use the AtomicX classes when their state is independent of the rest of the 
containing class's state.

I suggested using plain boolean and synchronization because I was assuming 
a class invariant that tied the value of protocolSupportEnabled to whether 
propertyChangeMulticaster contained protocolListener.

But since you say you can set protocolSupportEnabled's value independently, 
that invalidates my assumption, and now I don't see why you need *any* 
synchronization if protocolSupportEnabled is AtomicBoolean. 
(PropertyChangeMulticaster uses CopyOnWriteArrayList, so it's safe.)

--tim

From abaker at c-cor.com  Thu Jun 30 11:17:53 2005
From: abaker at c-cor.com (Baker, Anthony)
Date: Thu Jun 30 11:18:00 2005
Subject: [concurrency-interest] Performance of LinkedBlockingQueue.poll()
Message-ID: <85EC2A2F2D64DB4AB2F796CEBD7E2539E375@beomail1.NTSCD.C-COR.com>

Hi,

During a recent profiling session, I was surprised to discover a perfomance hotspot identified as LinkedBlockingQueue.poll().  The profiler recorded about 39k invocations of poll() as accummulating 139s of cpu time (that's an avg of about 3.5ms per invocation).  Is this a known issue or perhaps a profiling artifact?  Switching back to Doug Lea's BoundedLinkedQueue removed this hotspot.

The test was executed on a 2-cpu Solaris8 server using jdk 1.5.0_03-b07 (64bit).  Our usage looks something like this:

  public void run() {
    while (true) {
      Transaction t = _queuedWrites.poll(1000, TimeUnit.MILLISECONDS);
      if (t == null) {
        flush();

      } else {
        write(t);
      }
    }
  }

There is a single producer thread and a single consumer thread.

Thanks,
Anthony

-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20050630/88428972/attachment.htm
From dl at cs.oswego.edu  Thu Jun 30 11:27:37 2005
From: dl at cs.oswego.edu (Doug Lea)
Date: Thu Jun 30 11:27:39 2005
Subject: [concurrency-interest] Performance of LinkedBlockingQueue.poll()
In-Reply-To: <85EC2A2F2D64DB4AB2F796CEBD7E2539E375@beomail1.NTSCD.C-COR.com>
References: <85EC2A2F2D64DB4AB2F796CEBD7E2539E375@beomail1.NTSCD.C-COR.com>
Message-ID: <42C40F69.6020205@cs.oswego.edu>

Baker, Anthony wrote:
> Hi,
> 
> During a recent profiling session, I was surprised to discover a 
> perfomance hotspot identified as LinkedBlockingQueue.poll().  The 
> profiler recorded about 39k invocations of poll() as accummulating 139s 
> of cpu time (that's an avg of about 3.5ms per invocation).  Is this a 
> known issue or perhaps a profiling artifact?  Switching back to Doug 
> Lea's BoundedLinkedQueue removed this hotspot.

LinkedBlockingQueue is faster than old BoundedLinkedQueue on
every test I have. If you can put something together that
we can use to replicate your results, we'd be happy to try to
help diagnose and further improve it.


-Doug
From nikolai at ifad.dk  Thu Jun 30 11:43:54 2005
From: nikolai at ifad.dk (Nikolai V. Chr.)
Date: Thu Jun 30 11:48:47 2005
Subject: [concurrency-interest] Synchronizing on AtomicBoolean safe?
In-Reply-To: <42C408F0.4050201@peierls.net>
References: <42C2B262.7020601@ifad.dk> <42C2CCA4.8080000@peierls.net>
	<42C3FA45.6040806@ifad.dk> <42C408F0.4050201@peierls.net>
Message-ID: <42C4133A.9000508@ifad.dk>

Tim Peierls wrote:

> Nikolai V. Chr. wrote:
>
>> Sometimes I wish to just set a value on it, using set(). And I was 
>> hoping that while I was synchronizing on the AtomicBoolean, I was 
>> synchronizing on the same same object that AtomicBoolean was 
>> synchronizing on.
>
>
> AtomicBoolean doesn't synchronize on *any* value. The set() method 
> executes atomically without obtaining a lock. That's a feature, not a 
> bug. :-)
>
>
>> Just like doing this would do:
>>
>> SynchronizedBoolean protocolSupportEnabled = new 
>> SynchronizedBoolean(false);
>>
>> synchronized(protocolSupportEnabled.getLock()) {
>>  old = protocolSupportEnabled.set(enable);
>>  if(enable) {
>>     
>> propertyChangeMulticaster.addPropertyChangeListenerIfAbsent(protocolListener); 
>>  } else {
>>     
>> propertyChangeMulticaster.removePropertyChangeListener(protocolListener); 
>>
>>  }
>>  logProtocolSupportEnabled(enable);
>> }
>>
>> And somewhere else I would do this at the same time:
>>
>> protocolSupportEnabled.set(true);
>>
>> And then expect them to lock on the same Object. The question is, 
>> will the previous posted code do the same?
>
>
> No. I'm not sure what behavior you are trying to support, but you 
> cannot simply replace SynchronizedBoolean with AtomicBoolean and 
> synchronize on the AtomicBoolean instance itself instead of 
> SynchronizedBoolean.getLock().
>
> It's worth repeating: There is no intrinsic relationship between the 
> monitor lock associated with an object and that object's state.
>
> Use the AtomicX classes when their state is independent of the rest of 
> the containing class's state.
>
> I suggested using plain boolean and synchronization because I was 
> assuming a class invariant that tied the value of 
> protocolSupportEnabled to whether propertyChangeMulticaster contained 
> protocolListener.
>
> But since you say you can set protocolSupportEnabled's value 
> independently, that invalidates my assumption, and now I don't see why 
> you need *any* synchronization if protocolSupportEnabled is 
> AtomicBoolean. (PropertyChangeMulticaster uses CopyOnWriteArrayList, 
> so it's safe.) 


Okay, thanks for the answers. (allthough I did not understand everything..)

-- 
Nikolai V. Christensen, Computer Engineer,
Simulation and Training department
IFAD TS A/S, ?stre stationsvej 43 2.tv, DK-5000 Odense C
Denmark, EU
Phone: +45 63 11 02 11  Fax: +45 65 93 29 99
WWWeb: http://www.ifad.dk
e-mail: Nikolai.V.Christensen@ifad.dk
--


From nikolai at ifad.dk  Thu Jun 30 11:48:14 2005
From: nikolai at ifad.dk (Nikolai V. Chr.)
Date: Thu Jun 30 11:48:49 2005
Subject: [concurrency-interest] How do I replicate these in
	java.util.concurrent
Message-ID: <42C4143E.2050007@ifad.dk>

I have taken over maintaning alot of code using Doug Lea's utils. It is 
very complex and big and I not not understand much of it.

I am in the process of converting it all to the concurrent library in 
Java 1.5.

My question is, how do I convert these classes:

ReentrantReaderPreferenceReadWriteLock
ReentrantWriterPreferenceReadWriteLock
ConcurrentReaderHashMap
PropertyChangeMulticaster

I could not find their replacements in java.util.concurrent..

Thanks

-- 
Nikolai V. Christensen, Computer Engineer,
Simulation and Training department
IFAD TS A/S, ?stre stationsvej 43 2.tv, DK-5000 Odense C
Denmark, EU
Phone: +45 63 11 02 11  Fax: +45 65 93 29 99
WWWeb: http://www.ifad.dk
e-mail: Nikolai.V.Christensen@ifad.dk
--


From dl at cs.oswego.edu  Thu Jun 30 12:42:22 2005
From: dl at cs.oswego.edu (Doug Lea)
Date: Thu Jun 30 12:42:24 2005
Subject: [concurrency-interest] How do I replicate these
	in	java.util.concurrent
In-Reply-To: <42C4143E.2050007@ifad.dk>
References: <42C4143E.2050007@ifad.dk>
Message-ID: <42C420EE.1040307@cs.oswego.edu>

Nikolai V. Chr. wrote:
> 
> My question is, how do I convert these classes:
> 
> ReentrantReaderPreferenceReadWriteLock
> ReentrantWriterPreferenceReadWriteLock

Unless you have a specific need for r/w preference (which I bet you
don't), just use ReentrantReadWriteLock.

> ConcurrentReaderHashMap

Use ConcurrentHashMap with concurrency level of 1.

> PropertyChangeMulticaster

As of J2SE5.0/Tiger, the JDK version of this class
(java.beans.PropertyChangeSupport) uses
something close to a copy-on-write, so there's no longer
a need for this.

-Doug

From tim at peierls.net  Thu Jun 30 13:09:31 2005
From: tim at peierls.net (Tim Peierls)
Date: Thu Jun 30 13:09:45 2005
Subject: [concurrency-interest] Synchronizing on AtomicBoolean safe?
In-Reply-To: <42C4133A.9000508@ifad.dk>
References: <42C2B262.7020601@ifad.dk> <42C2CCA4.8080000@peierls.net>
	<42C3FA45.6040806@ifad.dk> <42C408F0.4050201@peierls.net>
	<42C4133A.9000508@ifad.dk>
Message-ID: <42C4274B.4000305@peierls.net>

Nikolai V. Chr. wrote:
> Okay, thanks for the answers. (allthough I did not understand everything..)

Sorry I wasn't clear. The answer to your original question is no: 
AtomicBoolean is not a drop-in replacement for SynchronizedBoolean and it 
doesn't have anything corresponding to getLock().

But could you explain why you think you need that synchronized block in the 
first place?

--tim

From tim at peierls.net  Thu Jun 30 13:22:25 2005
From: tim at peierls.net (Tim Peierls)
Date: Thu Jun 30 13:23:25 2005
Subject: [concurrency-interest] ThreadPoolExecutor	with
	PriorityBlockingQueue throws ClassCastException
In-Reply-To: <f73c06589332d2f851b2bf30891a612b@blargmail.com>
References: <f73c06589332d2f851b2bf30891a612b@blargmail.com>
Message-ID: <42C42A51.4060503@peierls.net>

Arne Burmeister wrote:
>> ...i see at AbstractExecutorService a new factory method newTaskFor() 
>> to create the Future so i may override by one creating a Comparable 
>> Future.
>> 
>> Is this the recommended way to do so? Or should FutureTask implement 
>> Comparable delegating to the wrapped task? Or should FutureTask offer 
>> the wrapped task and i use a Comparator for the queue?

Joe Bowbeer wrote:
> The most direct way to accomplish what you want, I believe, would be to 
> create your own ComparableFutureTasks and "execute" them directly -- as 
> executor.execute operates on your FutureTasks directly, without any 
> rewrapping.

Two cases where you can't use the "direct" approach:

1. ExecutorCompletionService.submit
2. ExecutorService.invokeAll/invokeAny

In case #1, since the underlying Runnable that is constructed by the submit
method is hardwired and not overridable, there is not much you can do. I
hadn't noticed this until your email provoked me to look.

[Doug, is it too late to fix this for Mustang?]

In case #2, under Mustang, you will be able to use the newTaskFor approach. 
Pre-Mustang, you can override the submit(Callable) method of 
ThreadPoolExecutor.

--tim


From ggagne at westminstercollege.edu  Thu Jun 30 17:40:02 2005
From: ggagne at westminstercollege.edu (Greg Gagne)
Date: Thu Jun 30 17:40:39 2005
Subject: [concurrency-interest] Locks with try/finally
Message-ID: <78D7F9D8-F705-496B-9914-1E34E465717D@westminstercollege.edu>

Hello all -

I bet this has already been covered (many times) before, but in the  
API for ReentrantLock, it recommends the strategy of placing the code  
following the lock in a try-finally block, i.e.

Lock lock = new ....

lock.lock();
try {
     ... code that is run with the lock held
}
finally {
     lock.unlock();
}

I understand why we want the unlock() should be in the finally clause  
(to ensure we release the lock), but why not put the lock() in the  
try as well? i.e.

Lock lock = new ....

try {
     lock.lock();
     ... code that is run with the lock held
}
finally {
     lock.unlock();
}

Much thanks in advance for anyone who can enlighten me on this.
From dholmes at dltech.com.au  Thu Jun 30 17:58:41 2005
From: dholmes at dltech.com.au (David Holmes)
Date: Thu Jun 30 17:58:52 2005
Subject: [concurrency-interest] Locks with try/finally
In-Reply-To: <78D7F9D8-F705-496B-9914-1E34E465717D@westminstercollege.edu>
Message-ID: <NFBBKALFDCPFIDBNKAPCCEGLFOAA.dholmes@dltech.com.au>

Greg,

Yes it has been covered :)

Because if the lock() throws an exception you don't want to try to unlock()
a lock you don't own - which will probably throw a different exception that
replaces the first.

While it should be rare for a lock() to throw it may well be possible - such
as OutOfMemoryError. Imagine if the lock() were inside the try. Once in a
blue moon you run out of memory temporarily and start to throw
OutOfMemoryError. Your finally clause does an unlock() which (may) results
in  IllegalMonitorStateException being thrown** which replaces the OOME. You
thern try to figure out why once in a blue moon you are unlocking a lock you
don't own, oblivious to the underlying out-of-memory condition.

I actually made explicit mention of this at our (Joe Bowbeer and myself)
JavaOne talk today, as I've been asked about this by a couple of people.

** Hindsight: I'm no longer of the view that "borrowing" this exception was
the right choice to make. Ah well.

Cheers,
David Holmes

> -----Original Message-----
> From: concurrency-interest-bounces@cs.oswego.edu
> [mailto:concurrency-interest-bounces@cs.oswego.edu]On Behalf Of Greg
> Gagne
> Sent: Friday, 1 July 2005 7:40 AM
> To: concurrency-interest@altair.cs.oswego.edu
> Subject: [concurrency-interest] Locks with try/finally
>
>
> Hello all -
>
> I bet this has already been covered (many times) before, but in the
> API for ReentrantLock, it recommends the strategy of placing the code
> following the lock in a try-finally block, i.e.
>
> Lock lock = new ....
>
> lock.lock();
> try {
>      ... code that is run with the lock held
> }
> finally {
>      lock.unlock();
> }
>
> I understand why we want the unlock() should be in the finally clause
> (to ensure we release the lock), but why not put the lock() in the
> try as well? i.e.
>
> Lock lock = new ....
>
> try {
>      lock.lock();
>      ... code that is run with the lock held
> }
> finally {
>      lock.unlock();
> }
>
> Much thanks in advance for anyone who can enlighten me on this.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest@altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>

From dawidk at mathcs.emory.edu  Thu Jun 30 21:06:01 2005
From: dawidk at mathcs.emory.edu (Dawid Kurzyniec)
Date: Thu Jun 30 21:06:13 2005
Subject: [concurrency-interest] Locks with try/finally
In-Reply-To: <NFBBKALFDCPFIDBNKAPCCEGLFOAA.dholmes@dltech.com.au>
References: <NFBBKALFDCPFIDBNKAPCCEGLFOAA.dholmes@dltech.com.au>
Message-ID: <42C496F9.5090005@mathcs.emory.edu>

David Holmes wrote:

>Greg,
>
>Yes it has been covered :)
>
>Because if the lock() throws an exception you don't want to try to unlock()
>a lock you don't own - which will probably throw a different exception that
>replaces the first.
>
>While it should be rare for a lock() to throw it may well be possible - such
>as OutOfMemoryError. 
>
To throw in my 2c:

It gets a bit more important with tryLock(timeout) and 
lockInterruptibly(), since they are likely and expected to throw 
exceptions. And the consequences may be more serious than just exception 
masking for lock implementations other than ReentrantLock, which may 
have more sophisticated notion of ownership than per-thread, so that you 
may actually succeed in unlocking what you have not locked, and get in 
trouble much later.

Regards,
Dawid


