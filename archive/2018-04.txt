From heinz at javaspecialists.eu  Mon Apr  2 03:21:19 2018
From: heinz at javaspecialists.eu (Dr Heinz M. Kabutz)
Date: Mon, 02 Apr 2018 10:21:19 +0300
Subject: [concurrency-interest] CountDownLatch vs Phaser Article
Message-ID: <5AC1D9EF.9060201@javaspecialists.eu>

The newsletter is a bit basic for this audience, but perhaps you know 
someone who has never heard of Phaser who could benefit?

https://www.javaspecialists.eu/archive/Issue257.html

Regards

Heinz
-- 
Dr Heinz M. Kabutz (PhD CompSci)
Author of "The Java™ Specialists' Newsletter" - www.javaspecialists.eu
Java Champion - www.javachampions.org
Oracle Developer Champion - www.twitter.com/dev_champions
JavaOne Rock Star - www.oracle.com/javaone/rock-star-wall-of-fame.html
Tel: +30 69 75 595 262
Skype: kabutz


From james at lightbend.com  Mon Apr  2 22:05:44 2018
From: james at lightbend.com (James Roper)
Date: Tue, 3 Apr 2018 12:05:44 +1000
Subject: [concurrency-interest] Reactive Streams Utility API
In-Reply-To: <49ab6d99-3cfa-a792-4697-cd0502ea920a@cs.oswego.edu>
References: <CABY0rKMNfKWgi6i=z71Xdcppp59UU1LkxtrziuBx84LrtS25Ew@mail.gmail.com>
 <49ab6d99-3cfa-a792-4697-cd0502ea920a@cs.oswego.edu>
Message-ID: <CABY0rKOxRypKyJY0=RS+MedpcMcVNP6VhbHv-PH-C7URr5=gVQ@mail.gmail.com>

On 30 March 2018 at 23:19, Doug Lea via Concurrency-interest <
concurrency-interest at cs.oswego.edu> wrote:

> On 02/28/2018 10:37 PM, James Roper via Concurrency-interest wrote:
>
> > We (Lightbend) would like to put forward a proposal for a Reactive
> > Streams utility API for building instances of juc.Flow interfaces. The
> > rationale, goals and non goals for this, along with our proposed
> > approach to the API, and an actual (incomplete) API proposal, TCK, with
> > implementation examples both in Akka Streams and RxJava, can be found
> here:
> >
> > https://github.com/lightbend/reactive-streams-utils
> >
>
> I initially sat this out hoping that more people using Flow and
> reactive-streams would comment. But not many. And I'm reminded by
> my last post about my main reservation, that history shows that
> fluent APIs always grow to cover all combinations of all capabilities.
> Which is an explicit non-goal of your proposal, but seems
> inevitable anyway.


Perhaps relevant here would be to get some feedback from the
java.util.stream API developers/maintainers. Have they found it inevitable
that that API needs to grow to cover all combinations of all capabilities?
Have they found any major issues preventing adoption due to the current
capabilities (or lack there of) offered?

I haven't seen anything much out there saying that java.util.stream is
incomplete and needs to be extended (aside from people saying that an
equivalent is needed for asynchronous streams, ie, this proposal). Assuming
that my assessment is correct, why haven't they found that their API hasn't
needed to grow to cover all combinations of all capabilities? Does it
already cover all combinations of all capabilities? Is there anything about
asynchronous streaming that makes it fundamentally different to in memory
collection streaming that would lead us to have a different experience when
designing such an API?

This is a little scary from the point of view
> creating JDK APIs. A much less ambitious first step would be
> to create some static utilities (in class Flow) for map(),
> collect() and a few others that could be used by layered
> third-party frameworks. A few of these were initially present
> in jsr166 prerelease versions but we killed them out of even
> more cowardice.
>
> To decide among these and other options, we do need some input
> from current and prospective users!
>
> -Doug
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



-- 
*James Roper*
*Senior Octonaut*

Lightbend <https://www.lightbend.com/> – Build reactive apps!
Twitter: @jroper <https://twitter.com/jroper>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20180403/4de2a492/attachment.html>

From viktor.klang at gmail.com  Tue Apr  3 03:53:12 2018
From: viktor.klang at gmail.com (Viktor Klang)
Date: Tue, 3 Apr 2018 09:53:12 +0200
Subject: [concurrency-interest] Reactive Streams Utility API
In-Reply-To: <49ab6d99-3cfa-a792-4697-cd0502ea920a@cs.oswego.edu>
References: <CABY0rKMNfKWgi6i=z71Xdcppp59UU1LkxtrziuBx84LrtS25Ew@mail.gmail.com>
 <49ab6d99-3cfa-a792-4697-cd0502ea920a@cs.oswego.edu>
Message-ID: <CANPzfU-BwHyvvRr11sd28WQBfuDY3Y+V4J818oWmSARh+FvMtQ@mail.gmail.com>

On Fri, Mar 30, 2018 at 2:19 PM, Doug Lea via Concurrency-interest <
concurrency-interest at cs.oswego.edu> wrote:

> On 02/28/2018 10:37 PM, James Roper via Concurrency-interest wrote:
>
> > We (Lightbend) would like to put forward a proposal for a Reactive
> > Streams utility API for building instances of juc.Flow interfaces. The
> > rationale, goals and non goals for this, along with our proposed
> > approach to the API, and an actual (incomplete) API proposal, TCK, with
> > implementation examples both in Akka Streams and RxJava, can be found
> here:
> >
> > https://github.com/lightbend/reactive-streams-utils
> >
>
> I initially sat this out hoping that more people using Flow and
> reactive-streams would comment.


This is most likely not the right channel to elicit feedback from Flow/RS
*end users*—may I suggest that if feedback from end-users is desired that
we announce some sort of poll, or otherwise, on a different media such as,
but not limited to, Twitter?


> But not many. And I'm reminded by
> my last post about my main reservation, that history shows that
> fluent APIs always grow to cover all combinations of all capabilities.
> Which is an explicit non-goal of your proposal, but seems
> inevitable anyway. This is a little scary from the point of view
> creating JDK APIs. A much less ambitious first step would be
> to create some static utilities (in class Flow) for map(),
> collect() and a few others that could be used by layered
> third-party frameworks. A few of these were initially present
> in jsr166 prerelease versions but we killed them out of even
> more cowardice.
>
> To decide among these and other options, we do need some input
> from current and prospective users!
>
> -Doug
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



-- 
Cheers,
√
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20180403/b78a5cb2/attachment.html>

From akarnokd at gmail.com  Tue Apr  3 05:27:13 2018
From: akarnokd at gmail.com (=?UTF-8?Q?D=C3=A1vid_Karnok?=)
Date: Tue, 03 Apr 2018 09:27:13 +0000
Subject: [concurrency-interest] When does ForkJoinPool.commonPool run on the
	caller thread?
Message-ID: <CAAWwtm-nxe8p4PycgfnsBo1Hp29pe0FHqbS88MMm84=-WG7jCQ@mail.gmail.com>

Hi,

I can remember reading somewhere that ForkJoinPool can decide to run the
task on the caller thread under some circumstances. I probably encountered
this effect in form of test hangs on Travis CI (i.e., submitting two tasks
to it, one waiting for the other to count down a latch which then never
happens). However, when I run code that prints the current thread, I get
one of the worker thread names printed, never "main".

The secondary reason I'm asking this is that I've seen benchmarks using
ForkJoinPool as the async-provider showing super performance compared to an
ExecutorService, often 10x, which is often the difference when I run some
code with or without asynchrony. So my guess is that the benchmark has a
utilization pattern that makes it run on the caller thread thus not
thread-hopping like with an ExecutorService.submit.


So is there a way to predict if ForkJoinPool will/has run the task on the
caller thread?


-- 
Best regards,
David Karnok
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20180403/05998bd5/attachment.html>

From davidcholmes at aapt.net.au  Tue Apr  3 06:07:06 2018
From: davidcholmes at aapt.net.au (David Holmes)
Date: Tue, 3 Apr 2018 20:07:06 +1000
Subject: [concurrency-interest] When does ForkJoinPool.commonPool run on
	the	caller thread?
In-Reply-To: <CAAWwtm-nxe8p4PycgfnsBo1Hp29pe0FHqbS88MMm84=-WG7jCQ@mail.gmail.com>
References: <CAAWwtm-nxe8p4PycgfnsBo1Hp29pe0FHqbS88MMm84=-WG7jCQ@mail.gmail.com>
Message-ID: <030601d3cb33$85b7c420$91274c60$@aapt.net.au>

Hi David,

 

As far as I can see the only time a submission to a FJP runs on the current thread is if the current thread is a worker thread in that FJP. See FJP.externalSubmit.

 

Cheers,

David H.

 

From: Concurrency-interest <concurrency-interest-bounces at cs.oswego.edu> On Behalf Of Dávid Karnok via Concurrency-interest
Sent: Tuesday, April 3, 2018 7:27 PM
To: concurrency-interest <concurrency-interest at cs.oswego.edu>
Subject: [concurrency-interest] When does ForkJoinPool.commonPool run on the caller thread?

 

Hi,

 

I can remember reading somewhere that ForkJoinPool can decide to run the task on the caller thread under some circumstances. I probably encountered this effect in form of test hangs on Travis CI (i.e., submitting two tasks to it, one waiting for the other to count down a latch which then never happens). However, when I run code that prints the current thread, I get one of the worker thread names printed, never "main".

 

The secondary reason I'm asking this is that I've seen benchmarks using ForkJoinPool as the async-provider showing super performance compared to an ExecutorService, often 10x, which is often the difference when I run some code with or without asynchrony. So my guess is that the benchmark has a utilization pattern that makes it run on the caller thread thus not thread-hopping like with an ExecutorService.submit. 

 

 

So is there a way to predict if ForkJoinPool will/has run the task on the caller thread?

 

 

-- 

Best regards,

David Karnok

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20180403/11262822/attachment-0001.html>

From viktor.klang at gmail.com  Tue Apr  3 11:30:16 2018
From: viktor.klang at gmail.com (Viktor Klang)
Date: Tue, 3 Apr 2018 17:30:16 +0200
Subject: [concurrency-interest] When does ForkJoinPool.commonPool run on
 the caller thread?
In-Reply-To: <030601d3cb33$85b7c420$91274c60$@aapt.net.au>
References: <CAAWwtm-nxe8p4PycgfnsBo1Hp29pe0FHqbS88MMm84=-WG7jCQ@mail.gmail.com>
 <030601d3cb33$85b7c420$91274c60$@aapt.net.au>
Message-ID: <CANPzfU_N7W0Pgqg1eVSOcQxxvkrX49QbYter6Vevmd=jNrZoZQ@mail.gmail.com>

David,

I'd only expect this to occur in FIFO mode. Do you have a link to the
version of the source you're referring to?

On Tue, Apr 3, 2018 at 12:07 PM, David Holmes via Concurrency-interest <
concurrency-interest at cs.oswego.edu> wrote:

> Hi David,
>
>
>
> As far as I can see the only time a submission to a FJP runs on the
> current thread is if the current thread is a worker thread in that FJP. See
> FJP.externalSubmit.
>
>
>
> Cheers,
>
> David H.
>
>
>
> *From:* Concurrency-interest <concurrency-interest-bounces at cs.oswego.edu> *On
> Behalf Of *Dávid Karnok via Concurrency-interest
> *Sent:* Tuesday, April 3, 2018 7:27 PM
> *To:* concurrency-interest <concurrency-interest at cs.oswego.edu>
> *Subject:* [concurrency-interest] When does ForkJoinPool.commonPool run
> on the caller thread?
>
>
>
> Hi,
>
>
>
> I can remember reading somewhere that ForkJoinPool can decide to run the
> task on the caller thread under some circumstances. I probably encountered
> this effect in form of test hangs on Travis CI (i.e., submitting two tasks
> to it, one waiting for the other to count down a latch which then never
> happens). However, when I run code that prints the current thread, I get
> one of the worker thread names printed, never "main".
>
>
>
> The secondary reason I'm asking this is that I've seen benchmarks using
> ForkJoinPool as the async-provider showing super performance compared to an
> ExecutorService, often 10x, which is often the difference when I run some
> code with or without asynchrony. So my guess is that the benchmark has a
> utilization pattern that makes it run on the caller thread thus not
> thread-hopping like with an ExecutorService.submit.
>
>
>
>
>
> So is there a way to predict if ForkJoinPool will/has run the task on the
> caller thread?
>
>
>
>
>
> --
>
> Best regards,
>
> David Karnok
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


-- 
Cheers,
√
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20180403/874aec5c/attachment.html>

From akarnokd at gmail.com  Tue Apr  3 11:46:50 2018
From: akarnokd at gmail.com (=?UTF-8?Q?D=C3=A1vid_Karnok?=)
Date: Tue, 03 Apr 2018 15:46:50 +0000
Subject: [concurrency-interest] When does ForkJoinPool.commonPool run on
 the caller thread?
In-Reply-To: <CANPzfU_N7W0Pgqg1eVSOcQxxvkrX49QbYter6Vevmd=jNrZoZQ@mail.gmail.com>
References: <CAAWwtm-nxe8p4PycgfnsBo1Hp29pe0FHqbS88MMm84=-WG7jCQ@mail.gmail.com>
 <030601d3cb33$85b7c420$91274c60$@aapt.net.au>
 <CANPzfU_N7W0Pgqg1eVSOcQxxvkrX49QbYter6Vevmd=jNrZoZQ@mail.gmail.com>
Message-ID: <CAAWwtm8Kurf0x8Zra+=JjNwGi6wyRTxOBPHwA0PuxRg0_6=Gmg@mail.gmail.com>

Sure:

https://github.com/reactive-streams/reactive-streams-jvm/blob/master/flow-adapters/src/test/java/org/reactivestreams/SubmissionPublisherTckTest.java#L31

When I first pushed pushed this, I used
`ForkJoinPool.commonPool().submit()` instead of the current `new Thread`.
It worked locally but hung on Travis with FJP, not sure why exactly.

Viktor Klang <viktor.klang at gmail.com> ezt írta (időpont: 2018. ápr. 3., K,
17:30):

> David,
>
> I'd only expect this to occur in FIFO mode. Do you have a link to the
> version of the source you're referring to?
>
> On Tue, Apr 3, 2018 at 12:07 PM, David Holmes via Concurrency-interest <
> concurrency-interest at cs.oswego.edu> wrote:
>
>> Hi David,
>>
>>
>>
>> As far as I can see the only time a submission to a FJP runs on the
>> current thread is if the current thread is a worker thread in that FJP. See
>> FJP.externalSubmit.
>>
>>
>>
>> Cheers,
>>
>> David H.
>>
>>
>>
>> *From:* Concurrency-interest <concurrency-interest-bounces at cs.oswego.edu>
>> *On Behalf Of *Dávid Karnok via Concurrency-interest
>> *Sent:* Tuesday, April 3, 2018 7:27 PM
>> *To:* concurrency-interest <concurrency-interest at cs.oswego.edu>
>> *Subject:* [concurrency-interest] When does ForkJoinPool.commonPool run
>> on the caller thread?
>>
>>
>>
>> Hi,
>>
>>
>>
>> I can remember reading somewhere that ForkJoinPool can decide to run the
>> task on the caller thread under some circumstances. I probably encountered
>> this effect in form of test hangs on Travis CI (i.e., submitting two tasks
>> to it, one waiting for the other to count down a latch which then never
>> happens). However, when I run code that prints the current thread, I get
>> one of the worker thread names printed, never "main".
>>
>>
>>
>> The secondary reason I'm asking this is that I've seen benchmarks using
>> ForkJoinPool as the async-provider showing super performance compared to an
>> ExecutorService, often 10x, which is often the difference when I run some
>> code with or without asynchrony. So my guess is that the benchmark has a
>> utilization pattern that makes it run on the caller thread thus not
>> thread-hopping like with an ExecutorService.submit.
>>
>>
>>
>>
>>
>> So is there a way to predict if ForkJoinPool will/has run the task on the
>> caller thread?
>>
>>
>>
>>
>>
>> --
>>
>> Best regards,
>>
>> David Karnok
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
>
> --
> Cheers,
> √
>


-- 
Best regards,
David Karnok
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20180403/a3de5de9/attachment.html>

From smaldini at pivotal.io  Tue Apr  3 12:10:45 2018
From: smaldini at pivotal.io (Stephane Maldini)
Date: Tue, 3 Apr 2018 09:10:45 -0700
Subject: [concurrency-interest] Reactive Streams Utility API
In-Reply-To: <CANPzfU-BwHyvvRr11sd28WQBfuDY3Y+V4J818oWmSARh+FvMtQ@mail.gmail.com>
References: <CABY0rKMNfKWgi6i=z71Xdcppp59UU1LkxtrziuBx84LrtS25Ew@mail.gmail.com>
 <49ab6d99-3cfa-a792-4697-cd0502ea920a@cs.oswego.edu>
 <CANPzfU-BwHyvvRr11sd28WQBfuDY3Y+V4J818oWmSARh+FvMtQ@mail.gmail.com>
Message-ID: <CAFm0joF1VWC_FE-sr1FetjNOoY-d3sNL6qjLc6fgs0PRLmQhgQ@mail.gmail.com>

I thought these RS API questions in the JDK were already past us since we
created RS and argued about already. My take is that we already have 3
different flavors of using the RS and many more different implementations:

- As a pure reactive-stream library: rxjava, akka streams, reactor...
  -- Note that rx/reactor and akka differ in design decisions, with the
formers using Publisher at all stage with Subscriber lifting, and the later
doing "materialization" and Publisher API at the edge.
- As an adaptable API: spring framework, spring boot, jakarta  EE(?),
factory based libs...
- As an edge API (drivers...): jdk http client, jetty, tomcat, vert.x,
ratpack...

In these 3 categories, the last 2 ones don't really need anything more than
RS utils, often opinionated, and the JDK can build on top of those for
various low level concerns (IO). The reactive streams "pure" operational
libraries are only chosen to experience some transformation pipelining
while connecting two or more reactive-streams ready framework, drivers or
Publisher-ready JDK apis. Note that I already conclude about using
Publisher based API since the implementation burden is mostly located in
them. However some could argue about having Subscriber factories which I
consider dangerous given the spec rules around its non reusability and
state isolation via Subscription.
So the point here is that the spec shines for its interop protocol, even
inspiring other tech such as JS or .Net, but its variety of flavors would
discourage any opinionated take from the JDK.

To date we still have updates to the scope, fixes and optimizations to work
on (at least in rx and reactor apis/impls), and we have been implementing
RS for already 4 years. Some Reactive Streams libs have added exclusive
features such as operator fusion and contextual transportation
(thread-local equivalent). Breaking the flow with JDK provided operators
with their own opinion on the matter would be an interesting discussion to
have. Generally speaking, I wouldn't see that full effort or experience
translated into the JDK maintenance cycle.

Finally I am not a fan of the Stream API or design and I could sidetrack
quite a bit on it (especially on the state design or the parallelism api),
so the argument about it as a prime example doesn't really resonate. We
even have rxjava and reactor users only using our APIs in a synchronous
fashion because they just prefer them and because they microbench well.
Even with the best common denominator of API, let's say a Monad with
filter, map and flatMap, we would still compete between larger
implementations and the core JDK. It would then cause many wrapping to
bridge between them, loosing context and optimizations each of those
libraries try to provide to the user with.

At the end of the day, IMHO, it's better to expose JDK based Publisher apis
as factories, or dedicated edge API for IO and other time-bound data
processing.

On Tue, Apr 3, 2018 at 12:53 AM, Viktor Klang via Concurrency-interest <
concurrency-interest at cs.oswego.edu> wrote:

>
>
> On Fri, Mar 30, 2018 at 2:19 PM, Doug Lea via Concurrency-interest <
> concurrency-interest at cs.oswego.edu> wrote:
>
>> On 02/28/2018 10:37 PM, James Roper via Concurrency-interest wrote:
>>
>> > We (Lightbend) would like to put forward a proposal for a Reactive
>> > Streams utility API for building instances of juc.Flow interfaces. The
>> > rationale, goals and non goals for this, along with our proposed
>> > approach to the API, and an actual (incomplete) API proposal, TCK, with
>> > implementation examples both in Akka Streams and RxJava, can be found
>> here:
>> >
>> > https://github.com/lightbend/reactive-streams-utils
>> >
>>
>> I initially sat this out hoping that more people using Flow and
>> reactive-streams would comment.
>
>
> This is most likely not the right channel to elicit feedback from Flow/RS
> *end users*—may I suggest that if feedback from end-users is desired that
> we announce some sort of poll, or otherwise, on a different media such as,
> but not limited to, Twitter?
>
>
>> But not many. And I'm reminded by
>> my last post about my main reservation, that history shows that
>> fluent APIs always grow to cover all combinations of all capabilities.
>> Which is an explicit non-goal of your proposal, but seems
>> inevitable anyway. This is a little scary from the point of view
>> creating JDK APIs. A much less ambitious first step would be
>> to create some static utilities (in class Flow) for map(),
>> collect() and a few others that could be used by layered
>> third-party frameworks. A few of these were initially present
>> in jsr166 prerelease versions but we killed them out of even
>> more cowardice.
>>
>> To decide among these and other options, we do need some input
>> from current and prospective users!
>>
>> -Doug
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
>
> --
> Cheers,
> √
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


-- 
Stephane Maldini | Project Reactor Lead, Spring Engineering | San Francisco |
Pivotal
W: pivotal.io - projectreactor.io | T: @smaldini
<https://twitter.com/smaldini>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20180403/6b0b511d/attachment.html>

From edharned at gmail.com  Tue Apr  3 14:03:59 2018
From: edharned at gmail.com (Edward Harned)
Date: Tue, 3 Apr 2018 14:03:59 -0400
Subject: [concurrency-interest] When does ForkJoinPool.commonPool run on
 the caller thread?
In-Reply-To: <CAAWwtm-nxe8p4PycgfnsBo1Hp29pe0FHqbS88MMm84=-WG7jCQ@mail.gmail.com>
References: <CAAWwtm-nxe8p4PycgfnsBo1Hp29pe0FHqbS88MMm84=-WG7jCQ@mail.gmail.com>
Message-ID: <CALNbAKyUD2dk=waoP+9cycD3ZVSpmab-uSeNux9cha=iZiGeHA@mail.gmail.com>

See the internal documentation under Work Queue in FJP:

* WorkQueues are also used in a similar way for tasks submitted
 * to the pool. We cannot mix these tasks in the same queues used
 * by workers. Instead, we randomly associate submission queues
 * with submitting threads, using a form of hashing.  The
 * ThreadLocalRandom probe value serves as a hash code for
 * choosing existing queues, and may be randomly repositioned upon
 * contention with other submitters.  In essence, submitters act
 * like workers except that they are restricted to executing local
 * tasks that they submitted.

ed

On Tue, Apr 3, 2018 at 5:27 AM, Dávid Karnok via Concurrency-interest <
concurrency-interest at cs.oswego.edu> wrote:

> Hi,
>
> I can remember reading somewhere that ForkJoinPool can decide to run the
> task on the caller thread under some circumstances. I probably encountered
> this effect in form of test hangs on Travis CI (i.e., submitting two tasks
> to it, one waiting for the other to count down a latch which then never
> happens). However, when I run code that prints the current thread, I get
> one of the worker thread names printed, never "main".
>
> The secondary reason I'm asking this is that I've seen benchmarks using
> ForkJoinPool as the async-provider showing super performance compared to an
> ExecutorService, often 10x, which is often the difference when I run some
> code with or without asynchrony. So my guess is that the benchmark has a
> utilization pattern that makes it run on the caller thread thus not
> thread-hopping like with an ExecutorService.submit.
>
>
> So is there a way to predict if ForkJoinPool will/has run the task on the
> caller thread?
>
>
> --
> Best regards,
> David Karnok
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20180403/b2459f73/attachment.html>

From davidcholmes at aapt.net.au  Tue Apr  3 17:00:56 2018
From: davidcholmes at aapt.net.au (David Holmes)
Date: Wed, 4 Apr 2018 07:00:56 +1000
Subject: [concurrency-interest] When does ForkJoinPool.commonPool run on
	the caller thread?
In-Reply-To: <CANPzfU_N7W0Pgqg1eVSOcQxxvkrX49QbYter6Vevmd=jNrZoZQ@mail.gmail.com>
References: <CAAWwtm-nxe8p4PycgfnsBo1Hp29pe0FHqbS88MMm84=-WG7jCQ@mail.gmail.com>
 <030601d3cb33$85b7c420$91274c60$@aapt.net.au>
 <CANPzfU_N7W0Pgqg1eVSOcQxxvkrX49QbYter6Vevmd=jNrZoZQ@mail.gmail.com>
Message-ID: <033201d3cb8e$dc9408c0$95bc1a40$@aapt.net.au>

It’s just the current JDK sources.

 

http://hg.openjdk.java.net/jdk/jdk/file/a387ee36e5e0/src/java.base/share/classes/java/util/concurrent/ForkJoinPool.java#l1949

 

But I also noted this in the comments:

 

* When external threads submit to the common pool, they can

* perform subtask processing (see externalHelpComplete and

* related methods) upon joins.  

 

Perhaps it is this form of current-thread-runs that David Karnok was thinking of.

 

David H.

 

From: Viktor Klang <viktor.klang at gmail.com> 
Sent: Wednesday, April 4, 2018 1:30 AM
To: David Holmes <dholmes at ieee.org>
Cc: Dávid Karnok <akarnokd at gmail.com>; David Holmes <davidcholmes at aapt.net.au>; concurrency-interest <concurrency-interest at cs.oswego.edu>
Subject: Re: [concurrency-interest] When does ForkJoinPool.commonPool run on the caller thread?

 

David,

 

I'd only expect this to occur in FIFO mode. Do you have a link to the version of the source you're referring to?

 

On Tue, Apr 3, 2018 at 12:07 PM, David Holmes via Concurrency-interest <concurrency-interest at cs.oswego.edu <mailto:concurrency-interest at cs.oswego.edu> > wrote:

Hi David,

 

As far as I can see the only time a submission to a FJP runs on the current thread is if the current thread is a worker thread in that FJP. See FJP.externalSubmit.

 

Cheers,

David H.

 

From: Concurrency-interest <concurrency-interest-bounces at cs.oswego.edu <mailto:concurrency-interest-bounces at cs.oswego.edu> > On Behalf Of Dávid Karnok via Concurrency-interest
Sent: Tuesday, April 3, 2018 7:27 PM
To: concurrency-interest <concurrency-interest at cs.oswego.edu <mailto:concurrency-interest at cs.oswego.edu> >
Subject: [concurrency-interest] When does ForkJoinPool.commonPool run on the caller thread?

 

Hi,

 

I can remember reading somewhere that ForkJoinPool can decide to run the task on the caller thread under some circumstances. I probably encountered this effect in form of test hangs on Travis CI (i.e., submitting two tasks to it, one waiting for the other to count down a latch which then never happens). However, when I run code that prints the current thread, I get one of the worker thread names printed, never "main".

 

The secondary reason I'm asking this is that I've seen benchmarks using ForkJoinPool as the async-provider showing super performance compared to an ExecutorService, often 10x, which is often the difference when I run some code with or without asynchrony. So my guess is that the benchmark has a utilization pattern that makes it run on the caller thread thus not thread-hopping like with an ExecutorService.submit. 

 

 

So is there a way to predict if ForkJoinPool will/has run the task on the caller thread?

 

 

-- 

Best regards,

David Karnok


_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu <mailto:Concurrency-interest at cs.oswego.edu> 
http://cs.oswego.edu/mailman/listinfo/concurrency-interest





 

-- 

Cheers,

√

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20180404/805be2a4/attachment-0001.html>

From jamesbtobin at gmail.com  Tue Apr  3 22:23:30 2018
From: jamesbtobin at gmail.com (James Tobin)
Date: Wed, 4 Apr 2018 03:23:30 +0100
Subject: [concurrency-interest] JOB | Permanent Java Web Developer (New York)
Message-ID: <CAMyPCTSYoDcSs2r42E8ZfsvkhpyKuL_O-n42xLvZeSBbXz8wUA@mail.gmail.com>

Hello, I'm working with an employer that is looking to hire a
permanent web developer (for their New York office) that has
significant java client side, multi-threading, application development
experience.  Consequently I had hoped that some members of this
mailing list may like to discuss further; off-list.  I can be reached
using "JamesBTobin (at) Gmail (dot) Com".  Kind regards, James

From peter.levart at gmail.com  Wed Apr  4 04:58:20 2018
From: peter.levart at gmail.com (Peter Levart)
Date: Wed, 4 Apr 2018 10:58:20 +0200
Subject: [concurrency-interest] VolatileArray
Message-ID: <bf00d676-94f1-eaad-4853-94b48bc49c3b@gmail.com>

Hi,

We know that in Java, reads from and writes to array slots are not 
volatile. We can use AtomicXXXArray to get volatile access but for the 
price of a wrapper dereference. Suppose that one does not need total 
ordering when publishing elements via array slots and just wants to be 
sure that on the reading side, the reads from array slots are not 
hoisted out of loops so that they do not appear to return stale values 
for indefinite amount of time.

Would something like that help?

public class VolatileArray {

     private volatile Object[] array;

     public VolatileArray(int length) {
         array = new Object[length];
     }

     public Object get(int i) {
         Object[] a;
         do {
             a = array; // read reference to array
         } while (a == null);
         return a[i];   // read array slot
     }

     public void set(int i, Object o) {
         Object[] a;
         do { a = array; } while (a == null);
         a[i] = o;  // write array slot
         array = a; // write reference to array
     }
}

Note that this is not a utility class to be used in code 
(AtomicReferenceArray could be used instead). It just represents an idea 
of how to publish references (via data race of course) but still ensure 
that the reading side will see published references, using only basic 
Java (no VarHandle(s) or Unsafe).

Does JMM guarantee this to work or not?

Regards, Peter


From akarnokd at gmail.com  Wed Apr  4 05:17:59 2018
From: akarnokd at gmail.com (=?UTF-8?Q?D=C3=A1vid_Karnok?=)
Date: Wed, 04 Apr 2018 09:17:59 +0000
Subject: [concurrency-interest] VolatileArray
In-Reply-To: <bf00d676-94f1-eaad-4853-94b48bc49c3b@gmail.com>
References: <bf00d676-94f1-eaad-4853-94b48bc49c3b@gmail.com>
Message-ID: <CAAWwtm_Jf7hC1C2ZSydquqROU9MD7DUBcPH9bm1zUOyUD6fnow@mail.gmail.com>

Hi Peter,

I'm not a JMM expert but that pattern looks fragile to me.

If you are on Java 9 or later, you can use `setRelease` [1] on the
AtomicReferenceArray and simply use plain `set` for the last element to
flush all before. Note that Java 9 allows you to manipulate plain arrays
via VarHandles so there might be no need for AtomicReferenceArray
indirection after all.

If you are on Java 6-8, you can still use `lazySet` [2] for the same
purpose and `set` for the last element.



[1]
https://docs.oracle.com/javase/9/docs/api/java/util/concurrent/atomic/AtomicReferenceArray.html#setRelease-int-E-
[2]
https://docs.oracle.com/javase/6/docs/api/java/util/concurrent/atomic/AtomicReferenceArray.html#lazySet(int,%20E)


Peter Levart via Concurrency-interest <concurrency-interest at cs.oswego.edu>
ezt írta (időpont: 2018. ápr. 4., Sze, 10:59):

> Hi,
>
> We know that in Java, reads from and writes to array slots are not
> volatile. We can use AtomicXXXArray to get volatile access but for the
> price of a wrapper dereference. Suppose that one does not need total
> ordering when publishing elements via array slots and just wants to be
> sure that on the reading side, the reads from array slots are not
> hoisted out of loops so that they do not appear to return stale values
> for indefinite amount of time.
>
> Would something like that help?
>
> public class VolatileArray {
>
>      private volatile Object[] array;
>
>      public VolatileArray(int length) {
>          array = new Object[length];
>      }
>
>      public Object get(int i) {
>          Object[] a;
>          do {
>              a = array; // read reference to array
>          } while (a == null);
>          return a[i];   // read array slot
>      }
>
>      public void set(int i, Object o) {
>          Object[] a;
>          do { a = array; } while (a == null);
>          a[i] = o;  // write array slot
>          array = a; // write reference to array
>      }
> }
>
> Note that this is not a utility class to be used in code
> (AtomicReferenceArray could be used instead). It just represents an idea
> of how to publish references (via data race of course) but still ensure
> that the reading side will see published references, using only basic
> Java (no VarHandle(s) or Unsafe).
>
> Does JMM guarantee this to work or not?
>
> Regards, Peter
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


-- 
Best regards,
David Karnok
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20180404/50a53b64/attachment.html>

From peter.levart at gmail.com  Wed Apr  4 06:03:12 2018
From: peter.levart at gmail.com (Peter Levart)
Date: Wed, 4 Apr 2018 12:03:12 +0200
Subject: [concurrency-interest] VolatileArray
In-Reply-To: <CAAWwtm_Jf7hC1C2ZSydquqROU9MD7DUBcPH9bm1zUOyUD6fnow@mail.gmail.com>
References: <bf00d676-94f1-eaad-4853-94b48bc49c3b@gmail.com>
 <CAAWwtm_Jf7hC1C2ZSydquqROU9MD7DUBcPH9bm1zUOyUD6fnow@mail.gmail.com>
Message-ID: <091241de-3d34-e191-c30d-e568d5c32a13@gmail.com>

Hi David,

On 04/04/2018 11:17 AM, Dávid Karnok wrote:
> Hi Peter,
>
> I'm not a JMM expert but that pattern looks fragile to me.
>
> If you are on Java 9 or later, you can use `setRelease` [1] on the  
> AtomicReferenceArray and simply use plain `set` for the last element 
> to flush all before. Note that Java 9 allows you to manipulate plain 
> arrays via VarHandles so there might be no need for 
> AtomicReferenceArray indirection after all.
>
> If you are on Java 6-8, you can still use `lazySet` [2] for the same 
> purpose and `set` for the last element.

Yes, I know, but I specifically don't want (or can't) use any of these 
utilities. Let's assume that I want to use an array to publish (and it's 
ok to publish via data race) references to objects that contain just 
final fields. I just want to ensure that published objects are seen by 
readers that are accessing them in a loop. I want to prevent hoisting 
these reads out of loop. Specifically I want to (eventually) see new 
reference of some object published in the same array slot as some other 
old object. The state transitions in individual slots are not only null 
-> nonnull, but also nonnull -> nonnull and nonnull -> null. So I'm not 
implementing a compute-once-and-keep-forever cache. The contents of 
array changes.

Regards, Peter

>
>
>
> [1] 
> https://docs.oracle.com/javase/9/docs/api/java/util/concurrent/atomic/AtomicReferenceArray.html#setRelease-int-E-
> [2] 
> https://docs.oracle.com/javase/6/docs/api/java/util/concurrent/atomic/AtomicReferenceArray.html#lazySet(int,%20E) 
> <https://docs.oracle.com/javase/6/docs/api/java/util/concurrent/atomic/AtomicReferenceArray.html#lazySet%28int,%20E%29>
>
>
> Peter Levart via Concurrency-interest 
> <concurrency-interest at cs.oswego.edu 
> <mailto:concurrency-interest at cs.oswego.edu>> ezt írta (időpont: 2018. 
> ápr. 4., Sze, 10:59):
>
>     Hi,
>
>     We know that in Java, reads from and writes to array slots are not
>     volatile. We can use AtomicXXXArray to get volatile access but for the
>     price of a wrapper dereference. Suppose that one does not need total
>     ordering when publishing elements via array slots and just wants to be
>     sure that on the reading side, the reads from array slots are not
>     hoisted out of loops so that they do not appear to return stale values
>     for indefinite amount of time.
>
>     Would something like that help?
>
>     public class VolatileArray {
>
>          private volatile Object[] array;
>
>          public VolatileArray(int length) {
>              array = new Object[length];
>          }
>
>          public Object get(int i) {
>              Object[] a;
>              do {
>                  a = array; // read reference to array
>              } while (a == null);
>              return a[i];   // read array slot
>          }
>
>          public void set(int i, Object o) {
>              Object[] a;
>              do { a = array; } while (a == null);
>              a[i] = o;  // write array slot
>              array = a; // write reference to array
>          }
>     }
>
>     Note that this is not a utility class to be used in code
>     (AtomicReferenceArray could be used instead). It just represents
>     an idea
>     of how to publish references (via data race of course) but still
>     ensure
>     that the reading side will see published references, using only basic
>     Java (no VarHandle(s) or Unsafe).
>
>     Does JMM guarantee this to work or not?
>
>     Regards, Peter
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> -- 
> Best regards,
> David Karnok

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20180404/b95dab79/attachment-0001.html>

From shade at redhat.com  Wed Apr  4 06:20:59 2018
From: shade at redhat.com (Aleksey Shipilev)
Date: Wed, 4 Apr 2018 12:20:59 +0200
Subject: [concurrency-interest] VolatileArray
In-Reply-To: <bf00d676-94f1-eaad-4853-94b48bc49c3b@gmail.com>
References: <bf00d676-94f1-eaad-4853-94b48bc49c3b@gmail.com>
Message-ID: <48b7b908-ef30-fe1b-2888-63bbccf8d7ce@redhat.com>

On 04/04/2018 10:58 AM, Peter Levart via Concurrency-interest wrote:
> We know that in Java, reads from and writes to array slots are not volatile. We can use
> AtomicXXXArray to get volatile access but for the price of a wrapper dereference. Suppose that one
> does not need total ordering when publishing elements via array slots and just wants to be sure that
> on the reading side, the reads from array slots are not hoisted out of loops so that they do not
> appear to return stale values for indefinite amount of time.
> 
> Would something like that help?
> 
> public class VolatileArray {
> 
>     private volatile Object[] array;
> 
>     public VolatileArray(int length) {
>         array = new Object[length];
>     }
> 
>     public Object get(int i) {
>         Object[] a;
>         do {
>             a = array; // read reference to array
>         } while (a == null);
>         return a[i];   // read array slot
>     }
> 
>     public void set(int i, Object o) {
>         Object[] a;
>         do { a = array; } while (a == null);
>         a[i] = o;  // write array slot
>         array = a; // write reference to array
>     }
> }
>
> Note that this is not a utility class to be used in code (AtomicReferenceArray could be used
> instead). It just represents an idea of how to publish references (via data race of course) but
> still ensure that the reading side will see published references, using only basic Java (no
> VarHandle(s) or Unsafe).
> 
> Does JMM guarantee this to work or not?

I don't think it does.

Trivial and intuitive failure mode is "premature publication": In set(), writes to "o" happen
unordered to the store to a[i] -- volatile-acquire for $array not preventing much of that -- which
means the object could be available in a[i] in incomplete state, and get() reads that incomplete
state. It is too late to deal with that race in get() at that point.

In other words, you want the releasing store *to* a[i], and acquiring *from* a[i] -- that is, given
a[i] is exposed already, you need to order operations relative to a[i] load/stores themselves. That
is what VarHandles and AtomicXXXArray do.

In yet another words, given that a[i] is exposed already, you are facing seeing the racy update of
a[i] *after* you have stored/loaded the $array itself:

 Thread 1:                                  Thread 2:

 // --------------- Good case ------------------------------------------------------
   a[i] = o1;
   o1.x = 42; // unordered store
   array = a; // (*)
                                              a = array;
                                              o = a[i];
                                              assert (o.x == 42); // passes

 // --------------- Bad case ------------------------------------------------------

   a[i] = o2;
                                              a = array; // (**)
                                              o = a[i];
                                              assert (o.x == 43); // fails!
   o2.x = 43; // unordered store
   array = a;

Oops. The problem is, you might think that at (*) you are reading the store at (**), and that
provides you ordering guarantees, while in reality that gives you nothing in bad case, because the
racy update is happening nevertheless.

The way out without VarHandles or AtomicXXXArray-s is to "version" the array, increment the version
before and after updating the a[i], and and check if version had changed while we were reading the
a[i] in another thread, but that reinvents sequence locks. Which might be the answer for you:
optimistic StampedLock in get(), maybe?

HTHS,
-Aleksey

From peter.levart at gmail.com  Wed Apr  4 07:04:38 2018
From: peter.levart at gmail.com (Peter Levart)
Date: Wed, 4 Apr 2018 13:04:38 +0200
Subject: [concurrency-interest] VolatileArray
In-Reply-To: <48b7b908-ef30-fe1b-2888-63bbccf8d7ce@redhat.com>
References: <bf00d676-94f1-eaad-4853-94b48bc49c3b@gmail.com>
 <48b7b908-ef30-fe1b-2888-63bbccf8d7ce@redhat.com>
Message-ID: <c1518b51-0b9d-833c-7e54-889b2b80b9c8@gmail.com>

Hi Aleksey,

On 04/04/2018 12:20 PM, Aleksey Shipilev wrote:
> On 04/04/2018 10:58 AM, Peter Levart via Concurrency-interest wrote:
>> We know that in Java, reads from and writes to array slots are not volatile. We can use
>> AtomicXXXArray to get volatile access but for the price of a wrapper dereference. Suppose that one
>> does not need total ordering when publishing elements via array slots and just wants to be sure that
>> on the reading side, the reads from array slots are not hoisted out of loops so that they do not
>> appear to return stale values for indefinite amount of time.
>>
>> Would something like that help?
>>
>> public class VolatileArray {
>>
>>      private volatile Object[] array;
>>
>>      public VolatileArray(int length) {
>>          array = new Object[length];
>>      }
>>
>>      public Object get(int i) {
>>          Object[] a;
>>          do {
>>              a = array; // read reference to array
>>          } while (a == null);
>>          return a[i];   // read array slot
>>      }
>>
>>      public void set(int i, Object o) {
>>          Object[] a;
>>          do { a = array; } while (a == null);
>>          a[i] = o;  // write array slot
>>          array = a; // write reference to array
>>      }
>> }
>>
>> Note that this is not a utility class to be used in code (AtomicReferenceArray could be used
>> instead). It just represents an idea of how to publish references (via data race of course) but
>> still ensure that the reading side will see published references, using only basic Java (no
>> VarHandle(s) or Unsafe).
>>
>> Does JMM guarantee this to work or not?
> I don't think it does.
>
> Trivial and intuitive failure mode is "premature publication": In set(), writes to "o" happen
> unordered to the store to a[i] -- volatile-acquire for $array not preventing much of that -- which
> means the object could be available in a[i] in incomplete state, and get() reads that incomplete
> state. It is too late to deal with that race in get() at that point.
>
> In other words, you want the releasing store *to* a[i], and acquiring *from* a[i] -- that is, given
> a[i] is exposed already, you need to order operations relative to a[i] load/stores themselves. That
> is what VarHandles and AtomicXXXArray do.

Or I can achieve that ordering (of reads and writes beyond o) by 
ensuring o is always an immutable object with final fields. Perhaps I 
would have better demonstrated the idea using an array of primitives 
instead of references...

>
> In yet another words, given that a[i] is exposed already, you are facing seeing the racy update of
> a[i] *after* you have stored/loaded the $array itself:
>
>   Thread 1:                                  Thread 2:
>
>   // --------------- Good case ------------------------------------------------------
>     a[i] = o1;
>     o1.x = 42; // unordered store
>     array = a; // (*)
>                                                a = array;
>                                                o = a[i];
>                                                assert (o.x == 42); // passes
>
>   // --------------- Bad case ------------------------------------------------------
>
>     a[i] = o2;
>                                                a = array; // (**)
>                                                o = a[i];
>                                                assert (o.x == 43); // fails!
>     o2.x = 43; // unordered store
>     array = a;
>
> Oops. The problem is, you might think that at (*) you are reading the store at (**), and that
> provides you ordering guarantees, while in reality that gives you nothing in bad case, because the
> racy update is happening nevertheless.
>
> The way out without VarHandles or AtomicXXXArray-s is to "version" the array, increment the version
> before and after updating the a[i], and and check if version had changed while we were reading the
> a[i] in another thread, but that reinvents sequence locks. Which might be the answer for you:
> optimistic StampedLock in get(), maybe?

I think that using anything more would defeat the purpose of not using 
any utilities (suppose I want to do something in low-level JDK which is 
used by those utilities an I want to prevent bootstrapping issues).

But I think that my example might be flawed nevertheless. I wanted to 
prevent hoisting of array reads out of loops, but then I realized that 
the following hypothetical optimization of some JIT might defeat that 
and may still satisfy JMM (for the purpose of this example, 
cached[A,O,I] are locals hoisted out of loop that calls into get(i) and 
doesn't call into set(i, o)):

     Object[] cachedA = null;
     Object cachedO = new Object();
     int cachedI = 0;

...

     public Object get(int i) {
         Object[] a;
         do {
             a = array; // read reference to array
         } while (a == null);

         if (a == cachedA && i == cachedI) {
             return cachedO;
         } else {
             cachedA = a;
             cachedI = i;
             return cachedO = a[i];   // read array slot
         }
     }

Is such transformation legal according to JMM? Could we argue that 
reading thread did not observe the volatile write of array if that array 
stayed the same and therefore is not obligated to order other accesses 
around such volatile read of array?

Regards,
Peter


From peter.levart at gmail.com  Wed Apr  4 07:09:49 2018
From: peter.levart at gmail.com (Peter Levart)
Date: Wed, 4 Apr 2018 13:09:49 +0200
Subject: [concurrency-interest] VolatileArray
In-Reply-To: <c1518b51-0b9d-833c-7e54-889b2b80b9c8@gmail.com>
References: <bf00d676-94f1-eaad-4853-94b48bc49c3b@gmail.com>
 <48b7b908-ef30-fe1b-2888-63bbccf8d7ce@redhat.com>
 <c1518b51-0b9d-833c-7e54-889b2b80b9c8@gmail.com>
Message-ID: <c8673725-a0c7-f512-f695-a63d59722645@gmail.com>

Ops,

On 04/04/2018 01:04 PM, Peter Levart wrote:
> Object[] cachedA = null;
>     Object cachedO = new Object();
>     int cachedI = 0; 

I wanted to write something like this to pre-initialize the locals (i.e. 
something that will never match in get(i) the 1st time around):

     Object[] cachedA = new Object[0];
     Object cachedO = null;
     int cachedI = 0;


From oleksandr.otenko at gmail.com  Wed Apr  4 09:58:18 2018
From: oleksandr.otenko at gmail.com (Alex Otenko)
Date: Wed, 4 Apr 2018 14:58:18 +0100
Subject: [concurrency-interest] VolatileArray
In-Reply-To: <c1518b51-0b9d-833c-7e54-889b2b80b9c8@gmail.com>
References: <bf00d676-94f1-eaad-4853-94b48bc49c3b@gmail.com>
 <48b7b908-ef30-fe1b-2888-63bbccf8d7ce@redhat.com>
 <c1518b51-0b9d-833c-7e54-889b2b80b9c8@gmail.com>
Message-ID: <5E6337E0-C719-426A-A1D2-9156A57D8E21@gmail.com>

It’s hard to see what you have in mind. In simple terms a non-volatile load always has to be issued after volatile loads, so the code won’t behave like it’s “caching”.

In JMM terms the non-volatile load observes at least the value stored before the volatile store preceding the volatile load in synchronization order. (But is allowed to observe any other non-volatile stores that appear after the volatile store in program order)

So if you imagine one thread producing:

a[i] = o1; // non-volatile store
array = a; // volatile store
a[i] = o2;
array = a;
…

and the other thread producing

a = array;
o1 = a[i];
a = array;
o2 = a[i];
…

the second thread cannot behave as if it saw a non-null value o1, and kept looping. On the other hand, if this thread observes o1 is null, the JVM has to place the corresponding a=array in the synchronization order before any volatile store array=a that follows a store of a non-null value to a[i].

Alex


> On 4 Apr 2018, at 12:04, Peter Levart via Concurrency-interest <concurrency-interest at cs.oswego.edu> wrote:
> 
> Hi Aleksey,
> 
> On 04/04/2018 12:20 PM, Aleksey Shipilev wrote:
>> On 04/04/2018 10:58 AM, Peter Levart via Concurrency-interest wrote:
>>> We know that in Java, reads from and writes to array slots are not volatile. We can use
>>> AtomicXXXArray to get volatile access but for the price of a wrapper dereference. Suppose that one
>>> does not need total ordering when publishing elements via array slots and just wants to be sure that
>>> on the reading side, the reads from array slots are not hoisted out of loops so that they do not
>>> appear to return stale values for indefinite amount of time.
>>> 
>>> Would something like that help?
>>> 
>>> public class VolatileArray {
>>> 
>>>     private volatile Object[] array;
>>> 
>>>     public VolatileArray(int length) {
>>>         array = new Object[length];
>>>     }
>>> 
>>>     public Object get(int i) {
>>>         Object[] a;
>>>         do {
>>>             a = array; // read reference to array
>>>         } while (a == null);
>>>         return a[i];   // read array slot
>>>     }
>>> 
>>>     public void set(int i, Object o) {
>>>         Object[] a;
>>>         do { a = array; } while (a == null);
>>>         a[i] = o;  // write array slot
>>>         array = a; // write reference to array
>>>     }
>>> }
>>> 
>>> Note that this is not a utility class to be used in code (AtomicReferenceArray could be used
>>> instead). It just represents an idea of how to publish references (via data race of course) but
>>> still ensure that the reading side will see published references, using only basic Java (no
>>> VarHandle(s) or Unsafe).
>>> 
>>> Does JMM guarantee this to work or not?
>> I don't think it does.
>> 
>> Trivial and intuitive failure mode is "premature publication": In set(), writes to "o" happen
>> unordered to the store to a[i] -- volatile-acquire for $array not preventing much of that -- which
>> means the object could be available in a[i] in incomplete state, and get() reads that incomplete
>> state. It is too late to deal with that race in get() at that point.
>> 
>> In other words, you want the releasing store *to* a[i], and acquiring *from* a[i] -- that is, given
>> a[i] is exposed already, you need to order operations relative to a[i] load/stores themselves. That
>> is what VarHandles and AtomicXXXArray do.
> 
> Or I can achieve that ordering (of reads and writes beyond o) by ensuring o is always an immutable object with final fields. Perhaps I would have better demonstrated the idea using an array of primitives instead of references...
> 
>> 
>> In yet another words, given that a[i] is exposed already, you are facing seeing the racy update of
>> a[i] *after* you have stored/loaded the $array itself:
>> 
>>  Thread 1:                                  Thread 2:
>> 
>>  // --------------- Good case ------------------------------------------------------
>>    a[i] = o1;
>>    o1.x = 42; // unordered store
>>    array = a; // (*)
>>                                               a = array;
>>                                               o = a[i];
>>                                               assert (o.x == 42); // passes
>> 
>>  // --------------- Bad case ------------------------------------------------------
>> 
>>    a[i] = o2;
>>                                               a = array; // (**)
>>                                               o = a[i];
>>                                               assert (o.x == 43); // fails!
>>    o2.x = 43; // unordered store
>>    array = a;
>> 
>> Oops. The problem is, you might think that at (*) you are reading the store at (**), and that
>> provides you ordering guarantees, while in reality that gives you nothing in bad case, because the
>> racy update is happening nevertheless.
>> 
>> The way out without VarHandles or AtomicXXXArray-s is to "version" the array, increment the version
>> before and after updating the a[i], and and check if version had changed while we were reading the
>> a[i] in another thread, but that reinvents sequence locks. Which might be the answer for you:
>> optimistic StampedLock in get(), maybe?
> 
> I think that using anything more would defeat the purpose of not using any utilities (suppose I want to do something in low-level JDK which is used by those utilities an I want to prevent bootstrapping issues).
> 
> But I think that my example might be flawed nevertheless. I wanted to prevent hoisting of array reads out of loops, but then I realized that the following hypothetical optimization of some JIT might defeat that and may still satisfy JMM (for the purpose of this example, cached[A,O,I] are locals hoisted out of loop that calls into get(i) and doesn't call into set(i, o)):
> 
>     Object[] cachedA = null;
>     Object cachedO = new Object();
>     int cachedI = 0;
> 
> ...
> 
>     public Object get(int i) {
>         Object[] a;
>         do {
>             a = array; // read reference to array
>         } while (a == null);
> 
>         if (a == cachedA && i == cachedI) {
>             return cachedO;
>         } else {
>             cachedA = a;
>             cachedI = i;
>             return cachedO = a[i];   // read array slot
>         }
>     }
> 
> Is such transformation legal according to JMM? Could we argue that reading thread did not observe the volatile write of array if that array stayed the same and therefore is not obligated to order other accesses around such volatile read of array?
> 
> Regards,
> Peter
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu <mailto:Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20180404/d8623158/attachment-0001.html>

From shade at redhat.com  Wed Apr  4 11:22:47 2018
From: shade at redhat.com (Aleksey Shipilev)
Date: Wed, 4 Apr 2018 17:22:47 +0200
Subject: [concurrency-interest] VolatileArray
In-Reply-To: <c1518b51-0b9d-833c-7e54-889b2b80b9c8@gmail.com>
References: <bf00d676-94f1-eaad-4853-94b48bc49c3b@gmail.com>
 <48b7b908-ef30-fe1b-2888-63bbccf8d7ce@redhat.com>
 <c1518b51-0b9d-833c-7e54-889b2b80b9c8@gmail.com>
Message-ID: <ba619078-e013-de25-9c8c-bad66fe1fa21@redhat.com>

On 04/04/2018 01:04 PM, Peter Levart wrote:
> On 04/04/2018 12:20 PM, Aleksey Shipilev wrote:
>> On 04/04/2018 10:58 AM, Peter Levart via Concurrency-interest wrote:
>> The way out without VarHandles or AtomicXXXArray-s is to "version" the array, increment the version
>> before and after updating the a[i], and and check if version had changed while we were reading the
>> a[i] in another thread, but that reinvents sequence locks. Which might be the answer for you:
>> optimistic StampedLock in get(), maybe?
> 
> I think that using anything more would defeat the purpose of not using any utilities (suppose I want
> to do something in low-level JDK which is used by those utilities an I want to prevent bootstrapping
> issues).

That means Unsafe is available, and you are golden! Just use whatever j.u.c classes are using. It is
hard to come by the bootstrapping problem that includes Unsafe not being available, given it is
~40-th primordial class loaded during bootstrapping at least in Hotspot.

> But I think that my example might be flawed nevertheless. I wanted to prevent hoisting of array
> reads out of loops, 

Sorry, I missed that.

It is hard to reconcile the requirement for "not being hoisted from the loop" and generic JMM. If we
are looking for the answer in JMM terms, we have to pose the question in JMM terms. The trouble is
that progress guarantees are awkwardly-defined. So, I find it more productive to just use the blocks
that have higher-level semantics, and employ the rule-of-thumb that plain accesses have no progress
guarantees, while everything stronger has. And try not to thing to extend those progress guarantees
beyond the accesses *themselves*: e.g. make array element accesses with stronger ordering, not try
to piggyback on something else.

See the discussion here:
  http://gee.cs.oswego.edu/dl/html/j9mm.html

I suggest using VarHandles (or their relevant Unsafe entry points) over array elements to begin
with, and only then figure out bootstrapping problems that might or might not exist. If the desired
behavior can be relaxed to only progress guarantees (it is arguably safe when arrays in your example
are primitive), then out-of-JMM VarHandles opaque mode (similar to std::atomic mem_ord_relaxed) is
what you are looking for.

On the other hand, doing the super-constrained ultra-low-level work sometimes means doing the
implementation-specific assumptions. Doug does them sometimes in j.u.c. (and some people, weirdly,
take that as the guidance, pour souls). So, in current Hotspot, if Unsafe is not available, it seems
to be enough to make the particular sorts of (Object?) volatile reads / Unsafe calls (and store its
result somewhere on heap?) to break the loop-hoisting optimizations. But it is a super-fragile
assumption, and you should only use that as an absolutely last resort.

Thanks,
-Aleksey

From forax at univ-mlv.fr  Wed Apr  4 11:47:24 2018
From: forax at univ-mlv.fr (Remi Forax)
Date: Wed, 4 Apr 2018 17:47:24 +0200 (CEST)
Subject: [concurrency-interest] VolatileArray
In-Reply-To: <ba619078-e013-de25-9c8c-bad66fe1fa21@redhat.com>
References: <bf00d676-94f1-eaad-4853-94b48bc49c3b@gmail.com>
 <48b7b908-ef30-fe1b-2888-63bbccf8d7ce@redhat.com>
 <c1518b51-0b9d-833c-7e54-889b2b80b9c8@gmail.com>
 <ba619078-e013-de25-9c8c-bad66fe1fa21@redhat.com>
Message-ID: <281981838.1073902.1522856844364.JavaMail.zimbra@u-pem.fr>



----- Mail original -----
> De: "Aleksey Shipilev via Concurrency-interest" <concurrency-interest at cs.oswego.edu>
> À: "Peter Levart" <peter.levart at gmail.com>, concurrency-interest at cs.oswego.edu
> Envoyé: Mercredi 4 Avril 2018 17:22:47
> Objet: Re: [concurrency-interest] VolatileArray

> On 04/04/2018 01:04 PM, Peter Levart wrote:
>> On 04/04/2018 12:20 PM, Aleksey Shipilev wrote:
>>> On 04/04/2018 10:58 AM, Peter Levart via Concurrency-interest wrote:
>>> The way out without VarHandles or AtomicXXXArray-s is to "version" the array,
>>> increment the version
>>> before and after updating the a[i], and and check if version had changed while
>>> we were reading the
>>> a[i] in another thread, but that reinvents sequence locks. Which might be the
>>> answer for you:
>>> optimistic StampedLock in get(), maybe?
>> 
>> I think that using anything more would defeat the purpose of not using any
>> utilities (suppose I want
>> to do something in low-level JDK which is used by those utilities an I want to
>> prevent bootstrapping
>> issues).
> 
> That means Unsafe is available, and you are golden! Just use whatever j.u.c
> classes are using. It is
> hard to come by the bootstrapping problem that includes Unsafe not being
> available, given it is
> ~40-th primordial class loaded during bootstrapping at least in Hotspot.

Just to be clear for everybody, Aleksey is talking about jdk.internal.misc.Unsafe and not sun.misc.Unsafe.

There are two 'Unsafe' now (since java 9),
- jdk.internal.misc.Unsafe is the one used by j.u.c and loaded early by the VM, but this class is not exported by the JDK, so can only be used inside the JDK,
- sun.misc.Unsafe that had already several methods deprecated and removed, that you can use at your own risk and that will be retired maybe one day.

Rémi

From edharned at gmail.com  Wed Apr  4 14:02:19 2018
From: edharned at gmail.com (Edward Harned)
Date: Wed, 4 Apr 2018 14:02:19 -0400
Subject: [concurrency-interest] When does ForkJoinPool.commonPool run on
 the caller thread?
In-Reply-To: <033201d3cb8e$dc9408c0$95bc1a40$@aapt.net.au>
References: <CAAWwtm-nxe8p4PycgfnsBo1Hp29pe0FHqbS88MMm84=-WG7jCQ@mail.gmail.com>
 <030601d3cb33$85b7c420$91274c60$@aapt.net.au>
 <CANPzfU_N7W0Pgqg1eVSOcQxxvkrX49QbYter6Vevmd=jNrZoZQ@mail.gmail.com>
 <033201d3cb8e$dc9408c0$95bc1a40$@aapt.net.au>
Message-ID: <CALNbAKyafepPAuH35EwaXgUoy1V6R1z2sMX5rUwCagvrbm20Og@mail.gmail.com>

To add an example that uses the submitting thread as a worker, from some
time ago:

public static void main(final String[] a) {
    Stream.of(1, 2, 3, 4, 5, 6, 7, 8).map(i ->
ForkJoinPool.commonPool().submit(new RecursiveAction() {
        protected void compute() {
            System.out.println(Thread.currentThread());
        }
    })).forEach(ForkJoinTask::join);
}

ed

On Tue, Apr 3, 2018 at 5:00 PM, David Holmes via Concurrency-interest <
concurrency-interest at cs.oswego.edu> wrote:

> It’s just the current JDK sources.
>
>
>
> http://hg.openjdk.java.net/jdk/jdk/file/a387ee36e5e0/src/
> java.base/share/classes/java/util/concurrent/ForkJoinPool.java#l1949
>
>
>
> But I also noted this in the comments:
>
>
>
> * When external threads submit to the common pool, they can
>
> * perform subtask processing (see externalHelpComplete and
>
> * related methods) upon joins.
>
>
>
> Perhaps it is this form of current-thread-runs that David Karnok was
> thinking of.
>
>
>
> David H.
>
>
>
> *From:* Viktor Klang <viktor.klang at gmail.com>
> *Sent:* Wednesday, April 4, 2018 1:30 AM
> *To:* David Holmes <dholmes at ieee.org>
> *Cc:* Dávid Karnok <akarnokd at gmail.com>; David Holmes <
> davidcholmes at aapt.net.au>; concurrency-interest <concurrency-interest at cs.
> oswego.edu>
> *Subject:* Re: [concurrency-interest] When does ForkJoinPool.commonPool
> run on the caller thread?
>
>
>
> David,
>
>
>
> I'd only expect this to occur in FIFO mode. Do you have a link to the
> version of the source you're referring to?
>
>
>
> On Tue, Apr 3, 2018 at 12:07 PM, David Holmes via Concurrency-interest <
> concurrency-interest at cs.oswego.edu> wrote:
>
> Hi David,
>
>
>
> As far as I can see the only time a submission to a FJP runs on the
> current thread is if the current thread is a worker thread in that FJP. See
> FJP.externalSubmit.
>
>
>
> Cheers,
>
> David H.
>
>
>
> *From:* Concurrency-interest <concurrency-interest-bounces at cs.oswego.edu> *On
> Behalf Of *Dávid Karnok via Concurrency-interest
> *Sent:* Tuesday, April 3, 2018 7:27 PM
> *To:* concurrency-interest <concurrency-interest at cs.oswego.edu>
> *Subject:* [concurrency-interest] When does ForkJoinPool.commonPool run
> on the caller thread?
>
>
>
> Hi,
>
>
>
> I can remember reading somewhere that ForkJoinPool can decide to run the
> task on the caller thread under some circumstances. I probably encountered
> this effect in form of test hangs on Travis CI (i.e., submitting two tasks
> to it, one waiting for the other to count down a latch which then never
> happens). However, when I run code that prints the current thread, I get
> one of the worker thread names printed, never "main".
>
>
>
> The secondary reason I'm asking this is that I've seen benchmarks using
> ForkJoinPool as the async-provider showing super performance compared to an
> ExecutorService, often 10x, which is often the difference when I run some
> code with or without asynchrony. So my guess is that the benchmark has a
> utilization pattern that makes it run on the caller thread thus not
> thread-hopping like with an ExecutorService.submit.
>
>
>
>
>
> So is there a way to predict if ForkJoinPool will/has run the task on the
> caller thread?
>
>
>
>
>
> --
>
> Best regards,
>
> David Karnok
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
>
> --
>
> Cheers,
>
> √
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20180404/74d79f7d/attachment.html>

From martinrb at google.com  Wed Apr  4 18:23:24 2018
From: martinrb at google.com (Martin Buchholz)
Date: Wed, 4 Apr 2018 15:23:24 -0700
Subject: [concurrency-interest] VolatileArray
In-Reply-To: <281981838.1073902.1522856844364.JavaMail.zimbra@u-pem.fr>
References: <bf00d676-94f1-eaad-4853-94b48bc49c3b@gmail.com>
 <48b7b908-ef30-fe1b-2888-63bbccf8d7ce@redhat.com>
 <c1518b51-0b9d-833c-7e54-889b2b80b9c8@gmail.com>
 <ba619078-e013-de25-9c8c-bad66fe1fa21@redhat.com>
 <281981838.1073902.1522856844364.JavaMail.zimbra@u-pem.fr>
Message-ID: <CA+kOe0_sscaztae6pRyMpLmifEDR4Su00LiPgBshb3McObmCvw@mail.gmail.com>

Opaque mode is the weakest form of access that promises progress, and is
offered by Unsafe and VarHandle and Atomic*
http://gee.cs.oswego.edu/dl/html/j9mm.html#opaquesec
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20180404/d322ca5e/attachment.html>

From jini at zeus.net.au  Wed Apr  4 21:33:59 2018
From: jini at zeus.net.au (Peter)
Date: Thu, 05 Apr 2018 11:33:59 +1000
Subject: [concurrency-interest] VolatileArray
In-Reply-To: <CA+kOe0_sscaztae6pRyMpLmifEDR4Su00LiPgBshb3McObmCvw@mail.gmail.com>
References: <bf00d676-94f1-eaad-4853-94b48bc49c3b@gmail.com>
 <48b7b908-ef30-fe1b-2888-63bbccf8d7ce@redhat.com>
 <c1518b51-0b9d-833c-7e54-889b2b80b9c8@gmail.com>
 <ba619078-e013-de25-9c8c-bad66fe1fa21@redhat.com>
 <281981838.1073902.1522856844364.JavaMail.zimbra@u-pem.fr>
 <CA+kOe0_sscaztae6pRyMpLmifEDR4Su00LiPgBshb3McObmCvw@mail.gmail.com>
Message-ID: <5AC57D07.2010007@zeus.net.au>

I think an immutable array would be more useful.

How many times do we clone an array, just to ensure we don't expose 
internal state?

If you have a volatile reference and the array is immutable, you can 
guarantee all changes to mutable state is atomic.

Too much mutable state is too hard to reason about, eg what if the 
object you want to reference changes position in a mutable but volatile 
element array?

Just my 20 cents.

Cheers,

Peter.

On 5/04/2018 8:23 AM, Martin Buchholz via Concurrency-interest wrote:
> Opaque mode is the weakest form of access that promises progress, and 
> is offered by Unsafe and VarHandle and Atomic*
> http://gee.cs.oswego.edu/dl/html/j9mm.html#opaquesec
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From viktor.klang at gmail.com  Thu Apr  5 04:13:45 2018
From: viktor.klang at gmail.com (Viktor Klang)
Date: Thu, 5 Apr 2018 10:13:45 +0200
Subject: [concurrency-interest] Reactive Streams Utility API
In-Reply-To: <CAFm0joF1VWC_FE-sr1FetjNOoY-d3sNL6qjLc6fgs0PRLmQhgQ@mail.gmail.com>
References: <CABY0rKMNfKWgi6i=z71Xdcppp59UU1LkxtrziuBx84LrtS25Ew@mail.gmail.com>
 <49ab6d99-3cfa-a792-4697-cd0502ea920a@cs.oswego.edu>
 <CANPzfU-BwHyvvRr11sd28WQBfuDY3Y+V4J818oWmSARh+FvMtQ@mail.gmail.com>
 <CAFm0joF1VWC_FE-sr1FetjNOoY-d3sNL6qjLc6fgs0PRLmQhgQ@mail.gmail.com>
Message-ID: <CANPzfU-wDVbqUP_ZJd-wyTUe_M1ZXUcXTuMLomyH5pzTMUXViA@mail.gmail.com>

Hi Stephane!

Great to hear from you :)

On Tue, Apr 3, 2018 at 6:10 PM, Stephane Maldini <smaldini at pivotal.io>
wrote:

> I thought these RS API questions in the JDK were already past us since we
> created RS and argued about already. My take is that we already have 3
> different flavors of using the RS and many more different implementations:
>
> - As a pure reactive-stream library: rxjava, akka streams, reactor...
>   -- Note that rx/reactor and akka differ in design decisions, with the
> formers using Publisher at all stage with Subscriber lifting, and the later
> doing "materialization" and Publisher API at the edge.
> - As an adaptable API: spring framework, spring boot, jakarta  EE(?),
> factory based libs...
> - As an edge API (drivers...): jdk http client, jetty, tomcat, vert.x,
> ratpack...
>
> In these 3 categories, the last 2 ones don't really need anything more
> than RS utils, often opinionated, and the JDK can build on top of those for
> various low level concerns (IO). The reactive streams "pure" operational
> libraries are only chosen to experience some transformation pipelining
> while connecting two or more reactive-streams ready framework, drivers or
> Publisher-ready JDK apis. Note that I already conclude about using
> Publisher based API since the implementation burden is mostly located in
> them. However some could argue about having Subscriber factories which I
> consider dangerous given the spec rules around its non reusability and
> state isolation via Subscription.
>

Subscriber factories: Supplier<Subscriber<T>>? I think returning a
Subscriber from user code is fine. Taking a Subscriber as a parameter is
more challenging in the sense that there's no typelevel indication that it
is to be used.

No matter what, and I believe we all agree, is that all implementations of
the RS interfaces (and especially those who are exposed to third-party
code) conform to the spec and pass the TCK.


> So the point here is that the spec shines for its interop protocol, even
> inspiring other tech such as JS or .Net, but its variety of flavors would
> discourage any opinionated take from the JDK.
>

I think the JDK could greatly benefit from having an RS "utils"
implementation (even if it turns out to be completely internal to the JDK)
so that more things in the JDK can add streaming support. That said,
without being able to use it—as an application developer—when just having
access to the JDK might turn out to be a challenge.


>
> To date we still have updates to the scope, fixes and optimizations to
> work on (at least in rx and reactor apis/impls), and we have been
> implementing RS for already 4 years.
>

As they say: Software is never done, only discontinued. :)


> Some Reactive Streams libs have added exclusive features such as operator
> fusion
>

AFAIK all "full" implementations have operator fusion. (Reactor, RxJava,
Akka Streams)


> and contextual transportation (thread-local equivalent).
>

This functionality would be nice to generalize into a standalone "async
local" or similar under the java.util.concurrent flag. Less use of
ThreadLocals would be desirable in general. :)


> Breaking the flow with JDK provided operators with their own opinion on
> the matter would be an interesting discussion to have. Generally speaking, I
> wouldn't see that full effort or experience translated into the JDK
> maintenance cycle.
>

Hmmm, could you elaborate?


>
> Finally I am not a fan of the Stream API or design and I could sidetrack
> quite a bit on it (especially on the state design or the parallelism api),
> so the argument about it as a prime example doesn't really resonate.
>

The question (IMO) is: Are Java developers in general happy with the Stream
API?


> We even have rxjava and reactor users only using our APIs in a synchronous
> fashion because they just prefer them and because they microbench well.
> Even with the best common denominator of API, let's say a Monad with
> filter, map and flatMap, we would still compete between larger
> implementations and the core JDK.
>



> It would then cause many wrapping to bridge between them, loosing context
> and optimizations each of those libraries try to provide to the user with.
>

Sure—but viewing RS/Flow as an integration API, it would be expected to not
get cross-connection optimizations magically happening (since that requires
more specific knowledge than simply the RS/Flow interfaces), would it not?


>
> At the end of the day, IMHO, it's better to expose JDK based Publisher
> apis as factories,
>

Isn't Publisher/Processor/Subscriber on a case-by-case basis?


> or dedicated edge API for IO and other time-bound data processing.
>

Having a Flow-enabled Java NIO 3000(tm) would be really nice, the question
is if we can get there step-by-step?


>
> On Tue, Apr 3, 2018 at 12:53 AM, Viktor Klang via Concurrency-interest <
> concurrency-interest at cs.oswego.edu> wrote:
>
>>
>>
>> On Fri, Mar 30, 2018 at 2:19 PM, Doug Lea via Concurrency-interest <
>> concurrency-interest at cs.oswego.edu> wrote:
>>
>>> On 02/28/2018 10:37 PM, James Roper via Concurrency-interest wrote:
>>>
>>> > We (Lightbend) would like to put forward a proposal for a Reactive
>>> > Streams utility API for building instances of juc.Flow interfaces. The
>>> > rationale, goals and non goals for this, along with our proposed
>>> > approach to the API, and an actual (incomplete) API proposal, TCK, with
>>> > implementation examples both in Akka Streams and RxJava, can be found
>>> here:
>>> >
>>> > https://github.com/lightbend/reactive-streams-utils
>>> >
>>>
>>> I initially sat this out hoping that more people using Flow and
>>> reactive-streams would comment.
>>
>>
>> This is most likely not the right channel to elicit feedback from Flow/RS
>> *end users*—may I suggest that if feedback from end-users is desired that
>> we announce some sort of poll, or otherwise, on a different media such as,
>> but not limited to, Twitter?
>>
>>
>>> But not many. And I'm reminded by
>>> my last post about my main reservation, that history shows that
>>> fluent APIs always grow to cover all combinations of all capabilities.
>>> Which is an explicit non-goal of your proposal, but seems
>>> inevitable anyway. This is a little scary from the point of view
>>> creating JDK APIs. A much less ambitious first step would be
>>> to create some static utilities (in class Flow) for map(),
>>> collect() and a few others that could be used by layered
>>> third-party frameworks. A few of these were initially present
>>> in jsr166 prerelease versions but we killed them out of even
>>> more cowardice.
>>>
>>> To decide among these and other options, we do need some input
>>> from current and prospective users!
>>>
>>> -Doug
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>
>>
>>
>> --
>> Cheers,
>> √
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
>
> --
> Stephane Maldini | Project Reactor Lead, Spring Engineering | San
> Francisco | Pivotal
> W: pivotal.io - projectreactor.io | T: @smaldini
> <https://twitter.com/smaldini>
>
>


-- 
Cheers,
√
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20180405/de4a6a2c/attachment-0001.html>

From cpovirk at google.com  Fri Apr  6 14:32:24 2018
From: cpovirk at google.com (Chris Povirk)
Date: Fri, 6 Apr 2018 14:32:24 -0400
Subject: [concurrency-interest] CompletionStage.exceptionallyCompose
In-Reply-To: <f7a662ec-a143-2909-c188-760b407b535e@cs.oswego.edu>
References: <f7a662ec-a143-2909-c188-760b407b535e@cs.oswego.edu>
Message-ID: <CAEvq2nrpqAccfjL5ey6j3oKj84tG=0_aNzc9rRkQXCbO8cyvLA@mail.gmail.com>

FWIW, Guava has both catching()
<http://google.github.io/guava/releases/snapshot-jre/api/docs/com/google/common/util/concurrent/Futures.html#catching-com.google.common.util.concurrent.ListenableFuture-java.lang.Class-com.google.common.base.Function-java.util.concurrent.Executor->
and catchingAsync()
<http://google.github.io/guava/releases/snapshot-jre/api/docs/com/google/common/util/concurrent/Futures.html#catchingAsync-com.google.common.util.concurrent.ListenableFuture-java.lang.Class-com.google.common.util.concurrent.AsyncFunction-java.util.concurrent.Executor->,
which are basically exceptionally() and exceptionallyCompose(). In our
depot, we see about 2 calls to catching() for every 1 call to
catchingAsync(). Naturally, results will vary for different codebases with
different requirements.

(OK, actually, we see about an equal number of calls to both. But oddly, we
had catchingAsync() before we had catching(), so I'm subtracting out the
number of catchingAsync() calls that existed before we added catching().
Since some of those calls might have been deleted (or converted to
catching() calls), that might artificially reduce the number of
catchingAsync() calls. But I feel comfortable saying that our ratio is
between 2:1 and 1:1.)

(Sorry, I haven't read the rest of the thread. I can try to put together
some data+thoughts on allOf() and friends if it would be helpful.)
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20180406/b985802e/attachment.html>

From james at lightbend.com  Fri Apr  6 19:49:15 2018
From: james at lightbend.com (James Roper)
Date: Sat, 7 Apr 2018 09:49:15 +1000
Subject: [concurrency-interest] CompletionStage.exceptionallyCompose
In-Reply-To: <CAEvq2nrpqAccfjL5ey6j3oKj84tG=0_aNzc9rRkQXCbO8cyvLA@mail.gmail.com>
References: <f7a662ec-a143-2909-c188-760b407b535e@cs.oswego.edu>
 <CAEvq2nrpqAccfjL5ey6j3oKj84tG=0_aNzc9rRkQXCbO8cyvLA@mail.gmail.com>
Message-ID: <CABY0rKPUfzN9QFojDD03CVu7PQR6536NwZhq4TCGvipeJD3MLQ@mail.gmail.com>

Just wanted to add my vote to adding exceptionallyCompose, and also
handleCompose would also be good. I've come across numerous times when I've
needed these abstractions on CompletionStage.

On 7 April 2018 at 04:32, Chris Povirk via Concurrency-interest <
concurrency-interest at cs.oswego.edu> wrote:

> FWIW, Guava has both catching()
> <http://google.github.io/guava/releases/snapshot-jre/api/docs/com/google/common/util/concurrent/Futures.html#catching-com.google.common.util.concurrent.ListenableFuture-java.lang.Class-com.google.common.base.Function-java.util.concurrent.Executor->
> and catchingAsync()
> <http://google.github.io/guava/releases/snapshot-jre/api/docs/com/google/common/util/concurrent/Futures.html#catchingAsync-com.google.common.util.concurrent.ListenableFuture-java.lang.Class-com.google.common.util.concurrent.AsyncFunction-java.util.concurrent.Executor->,
> which are basically exceptionally() and exceptionallyCompose(). In our
> depot, we see about 2 calls to catching() for every 1 call to
> catchingAsync(). Naturally, results will vary for different codebases with
> different requirements.
>
> (OK, actually, we see about an equal number of calls to both. But oddly,
> we had catchingAsync() before we had catching(), so I'm subtracting out the
> number of catchingAsync() calls that existed before we added catching().
> Since some of those calls might have been deleted (or converted to
> catching() calls), that might artificially reduce the number of
> catchingAsync() calls. But I feel comfortable saying that our ratio is
> between 2:1 and 1:1.)
>
> (Sorry, I haven't read the rest of the thread. I can try to put together
> some data+thoughts on allOf() and friends if it would be helpful.)
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


-- 
*James Roper*
*Senior Octonaut*

Lightbend <https://www.lightbend.com/> – Build reactive apps!
Twitter: @jroper <https://twitter.com/jroper>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20180407/f2c1cb7d/attachment.html>

From cpovirk at google.com  Mon Apr  9 13:06:59 2018
From: cpovirk at google.com (Chris Povirk)
Date: Mon, 9 Apr 2018 13:06:59 -0400
Subject: [concurrency-interest] CompletionStage.exceptionallyCompose
In-Reply-To: <CAEvq2nrpqAccfjL5ey6j3oKj84tG=0_aNzc9rRkQXCbO8cyvLA@mail.gmail.com>
References: <f7a662ec-a143-2909-c188-760b407b535e@cs.oswego.edu>
 <CAEvq2nrpqAccfjL5ey6j3oKj84tG=0_aNzc9rRkQXCbO8cyvLA@mail.gmail.com>
Message-ID: <CAEvq2noTcu+0V3agpKRie==s6jSCcCoQL_uzMsC4u8CZU9Z98A@mail.gmail.com>

No, sorry, I forgot: The common reason that people use catchingAsync() is
not to return a Future. Instead, they use it to be able to throw a checked
exception from within apply() itself. (Background: For practical and
historical reasons, Guava's catching() uses Function, which can't throw,
but catchingAsync() uses AsyncFunction, which can.)

I had previously calculated
<http://cs.oswego.edu/pipermail/concurrency-interest/2016-February/014946.html>
that
only 15% of our catching()/catchingAsync() users need to return a Future.
But apparently just as many other users *don't* need to return a Future yet
*do* use catchingAsync() so that they can throw a checked exception.

So, for exceptionally() and exceptionallyCompose(), Google's depot suggests
the usage ratio would be more like 6:1 than the 2:1 or 1:1 I claimed above
(again, for "codebases like Google's," whatever that means).
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20180409/6d5502af/attachment.html>

From viktor.klang at gmail.com  Tue Apr 10 08:25:45 2018
From: viktor.klang at gmail.com (Viktor Klang)
Date: Tue, 10 Apr 2018 14:25:45 +0200
Subject: [concurrency-interest] Strange FJP behavior (bug?)
Message-ID: <CANPzfU_wNi-9SJZK8ffgZqEgFyVN0FE6kJqfP_PhrDb7pLTFGg@mail.gmail.com>

Experts,

We've observed, and isolated, a strange behavior with ForkJoinPool in
relation to capped ForkJoinThreadWorkerFactories and ManagedBlocker code
followed by unmanaged blocking.

*Expected behavior:*
Pool scales up to the limit of workers as given by the pool, as a result of
the managed blocking, then either terminates those extra threads or uses
them to help out processing the work.

*Observed behavior:*
Threads are created during the managed blocking, but then get stuck in this
stack frame:

sun.misc.Unsafe.park(Native Method)
java.util.concurrent.ForkJoinPool.awaitWork(ForkJoinPool.java:1824)
java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1693)
java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157)

*And only a single thread continues to process the rest of the workload,
one item at a time!*



*Reproducer code (Foo.java) just compile with javac and then run with java,
no parameters needed:*

package test;

import java.time.Instant;
import java.util.function.Function;
import java.util.stream.Stream;
import java.util.concurrent.ForkJoinPool;
import java.util.concurrent.ForkJoinWorkerThread;
import java.util.concurrent.CompletableFuture;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicInteger;

public final class Foo {

  public final static int MAX_THREADS = 128;
  public final static int MAX_BLOCKERS = 256;
  public final static int CORES =
Runtime.getRuntime().availableProcessors();

  public static final class FJWTF extends AtomicInteger implements
ForkJoinPool.ForkJoinWorkerThreadFactory {
    private final boolean allocate() {
       for(;;) {
         final int current = get();
         if (current == MAX_THREADS || current == Integer.MAX_VALUE) return
false;
         else if (compareAndSet(current, current + 1)) return true;
       }
    }

    private final boolean deallocate() {
      for(;;) {
         final int current = get();
         if (current == 0) return false;
         else if (compareAndSet(current, current - 1)) return true;
       }
    }

    public ForkJoinWorkerThread newThread(final ForkJoinPool fjp) {
      if (allocate()) {
        final ForkJoinWorkerThread worker = new ForkJoinWorkerThread(fjp) {
          @Override public void onTermination(final Throwable cause) {
            try {
              deallocate();
            } finally {
              super.onTermination(cause);
            }
          }
        };
        return worker;
      } else return null;
    }
  };

  static String printAndGetTime(final String prefix, String name) {
    try {
    final String t = Thread.currentThread().toString();
    System.out.println(t + " started " + prefix + " " + name);
    final Instant now = Instant.now();
    Thread.sleep(1000);
    final String res = t + " finished " + prefix + " " + name + " : " + now;
    System.out.println(res);
    return res;
    } catch (final InterruptedException ie) {
      Thread.currentThread().interrupt();
      return "interrupted";
    }
  }

  static CompletableFuture<Void> generate(final int n, Function<String,
CompletableFuture<String>> f) {
    return CompletableFuture.allOf(Stream.iterate(0, i -> i +
1).limit(n).map(String::valueOf).map(f).toArray(CompletableFuture[]::new));
  }

  static CompletableFuture<Void> managedBlockers(final int n, final
ForkJoinPool fjp) {
    return generate(n, (String name) -> {
        return CompletableFuture.completedFuture("").thenApplyAsync(unused
->
            {
              try {
                ForkJoinPool.managedBlock(new ForkJoinPool.ManagedBlocker()
{
                  @Override public boolean block() {
                    printAndGetTime("managed", name);
                    return true;
                  }
                  @Override public boolean isReleasable() { return false; }
                });
              } catch (final InterruptedException ie) {
                Thread.currentThread().interrupt();
              }
            return name;
          }, fjp);
      });
  }

  static CompletableFuture<Void> unmanagedBlockers(final int n, final
ForkJoinPool fjp) {
    return generate(n, (String name) ->
CompletableFuture.completedFuture("").thenApplyAsync(unused ->
printAndGetTime("unmanaged", name), fjp));
  }

  public static void main(final String[] args) throws Exception {
    final ForkJoinPool fjp = new ForkJoinPool(CORES, new FJWTF(), (t, c) ->
c.printStackTrace(System.err),true);
    final int n = CORES + MAX_BLOCKERS;

    CompletableFuture.completedFuture("").
      thenComposeAsync(unused -> managedBlockers(n,fjp), fjp).
      thenComposeAsync(unused -> unmanagedBlockers(n, fjp), fjp).
      get(100, TimeUnit.SECONDS);

    fjp.shutdownNow();
  }

}

*Sample output:*

Thread[ForkJoinPool-1-worker-0,5,main] started managed 2
Thread[ForkJoinPool-1-worker-3,5,main] started managed 1
Thread[ForkJoinPool-1-worker-2,5,main] started managed 0
Thread[ForkJoinPool-1-worker-1,5,main] started managed 3
Thread[ForkJoinPool-1-worker-4,5,main] started managed 4
Thread[ForkJoinPool-1-worker-6,5,main] started managed 5
Thread[ForkJoinPool-1-worker-7,5,main] started managed 6
Thread[ForkJoinPool-1-worker-5,5,main] started managed 7
Thread[ForkJoinPool-1-worker-11,5,main] started managed 8
Thread[ForkJoinPool-1-worker-10,5,main] started managed 9
Thread[ForkJoinPool-1-worker-12,5,main] started managed 10
Thread[ForkJoinPool-1-worker-14,5,main] started managed 11
Thread[ForkJoinPool-1-worker-9,5,main] started managed 12
Thread[ForkJoinPool-1-worker-13,5,main] started managed 13
Thread[ForkJoinPool-1-worker-15,5,main] started managed 14
Thread[ForkJoinPool-1-worker-8,5,main] started managed 15
Thread[ForkJoinPool-1-worker-18,5,main] started managed 16
Thread[ForkJoinPool-1-worker-19,5,main] started managed 17
Thread[ForkJoinPool-1-worker-27,5,main] started managed 18
Thread[ForkJoinPool-1-worker-20,5,main] started managed 19
Thread[ForkJoinPool-1-worker-30,5,main] started managed 20
Thread[ForkJoinPool-1-worker-23,5,main] started managed 21
Thread[ForkJoinPool-1-worker-31,5,main] started managed 22
Thread[ForkJoinPool-1-worker-24,5,main] started managed 23
Thread[ForkJoinPool-1-worker-17,5,main] started managed 24
Thread[ForkJoinPool-1-worker-29,5,main] started managed 25
Thread[ForkJoinPool-1-worker-22,5,main] started managed 26
Thread[ForkJoinPool-1-worker-28,5,main] started managed 27
Thread[ForkJoinPool-1-worker-21,5,main] started managed 28
Thread[ForkJoinPool-1-worker-16,5,main] started managed 29
Thread[ForkJoinPool-1-worker-26,5,main] started managed 30
Thread[ForkJoinPool-1-worker-25,5,main] started managed 31
Thread[ForkJoinPool-1-worker-42,5,main] started managed 32
Thread[ForkJoinPool-1-worker-51,5,main] started managed 33
Thread[ForkJoinPool-1-worker-44,5,main] started managed 34
Thread[ForkJoinPool-1-worker-37,5,main] started managed 35
Thread[ForkJoinPool-1-worker-61,5,main] started managed 36
Thread[ForkJoinPool-1-worker-54,5,main] started managed 37
Thread[ForkJoinPool-1-worker-47,5,main] started managed 38
Thread[ForkJoinPool-1-worker-40,5,main] started managed 39
Thread[ForkJoinPool-1-worker-33,5,main] started managed 40
Thread[ForkJoinPool-1-worker-59,5,main] started managed 41
Thread[ForkJoinPool-1-worker-52,5,main] started managed 42
Thread[ForkJoinPool-1-worker-45,5,main] started managed 43
Thread[ForkJoinPool-1-worker-38,5,main] started managed 44
Thread[ForkJoinPool-1-worker-62,5,main] started managed 45
Thread[ForkJoinPool-1-worker-55,5,main] started managed 46
Thread[ForkJoinPool-1-worker-48,5,main] started managed 47
Thread[ForkJoinPool-1-worker-41,5,main] started managed 48
Thread[ForkJoinPool-1-worker-34,5,main] started managed 49
Thread[ForkJoinPool-1-worker-60,5,main] started managed 50
Thread[ForkJoinPool-1-worker-53,5,main] started managed 51
Thread[ForkJoinPool-1-worker-46,5,main] started managed 52
Thread[ForkJoinPool-1-worker-39,5,main] started managed 53
Thread[ForkJoinPool-1-worker-63,5,main] started managed 54
Thread[ForkJoinPool-1-worker-56,5,main] started managed 55
Thread[ForkJoinPool-1-worker-49,5,main] started managed 56
Thread[ForkJoinPool-1-worker-50,5,main] started managed 57
Thread[ForkJoinPool-1-worker-35,5,main] started managed 58
Thread[ForkJoinPool-1-worker-32,5,main] started managed 59
Thread[ForkJoinPool-1-worker-58,5,main] started managed 60
Thread[ForkJoinPool-1-worker-57,5,main] started managed 61
Thread[ForkJoinPool-1-worker-43,5,main] started managed 62
Thread[ForkJoinPool-1-worker-36,5,main] started managed 63
Thread[ForkJoinPool-1-worker-90,5,main] started managed 64
Thread[ForkJoinPool-1-worker-115,5,main] started managed 65
Thread[ForkJoinPool-1-worker-107,5,main] started managed 66
Thread[ForkJoinPool-1-worker-71,5,main] started managed 126
Thread[ForkJoinPool-1-worker-86,5,main] started managed 125
Thread[ForkJoinPool-1-worker-85,5,main] started managed 124
Thread[ForkJoinPool-1-worker-72,5,main] started managed 123
Thread[ForkJoinPool-1-worker-99,5,main] started managed 122
Thread[ForkJoinPool-1-worker-64,5,main] started managed 121
Thread[ForkJoinPool-1-worker-113,5,main] started managed 120
Thread[ForkJoinPool-1-worker-121,5,main] started managed 119
Thread[ForkJoinPool-1-worker-127,5,main] started managed 118
Thread[ForkJoinPool-1-worker-70,5,main] started managed 117
Thread[ForkJoinPool-1-worker-78,5,main] started managed 116
Thread[ForkJoinPool-1-worker-84,5,main] started managed 115
Thread[ForkJoinPool-1-worker-100,5,main] started managed 114
Thread[ForkJoinPool-1-worker-98,5,main] started managed 113
Thread[ForkJoinPool-1-worker-106,5,main] started managed 112
Thread[ForkJoinPool-1-worker-112,5,main] started managed 111
Thread[ForkJoinPool-1-worker-120,5,main] started managed 110
Thread[ForkJoinPool-1-worker-126,5,main] started managed 109
Thread[ForkJoinPool-1-worker-69,5,main] started managed 108
Thread[ForkJoinPool-1-worker-77,5,main] started managed 107
Thread[ForkJoinPool-1-worker-83,5,main] started managed 106
Thread[ForkJoinPool-1-worker-91,5,main] started managed 105
Thread[ForkJoinPool-1-worker-97,5,main] started managed 104
Thread[ForkJoinPool-1-worker-105,5,main] started managed 103
Thread[ForkJoinPool-1-worker-111,5,main] started managed 102
Thread[ForkJoinPool-1-worker-119,5,main] started managed 101
Thread[ForkJoinPool-1-worker-125,5,main] started managed 100
Thread[ForkJoinPool-1-worker-68,5,main] started managed 99
Thread[ForkJoinPool-1-worker-76,5,main] started managed 98
Thread[ForkJoinPool-1-worker-82,5,main] started managed 97
Thread[ForkJoinPool-1-worker-92,5,main] started managed 96
Thread[ForkJoinPool-1-worker-96,5,main] started managed 95
Thread[ForkJoinPool-1-worker-104,5,main] started managed 94
Thread[ForkJoinPool-1-worker-110,5,main] started managed 93
Thread[ForkJoinPool-1-worker-118,5,main] started managed 92
Thread[ForkJoinPool-1-worker-124,5,main] started managed 91
Thread[ForkJoinPool-1-worker-67,5,main] started managed 90
Thread[ForkJoinPool-1-worker-75,5,main] started managed 89
Thread[ForkJoinPool-1-worker-81,5,main] started managed 88
Thread[ForkJoinPool-1-worker-89,5,main] started managed 87
Thread[ForkJoinPool-1-worker-95,5,main] started managed 86
Thread[ForkJoinPool-1-worker-103,5,main] started managed 85
Thread[ForkJoinPool-1-worker-109,5,main] started managed 84
Thread[ForkJoinPool-1-worker-117,5,main] started managed 83
Thread[ForkJoinPool-1-worker-123,5,main] started managed 82
Thread[ForkJoinPool-1-worker-66,5,main] started managed 81
Thread[ForkJoinPool-1-worker-74,5,main] started managed 80
Thread[ForkJoinPool-1-worker-80,5,main] started managed 79
Thread[ForkJoinPool-1-worker-88,5,main] started managed 78
Thread[ForkJoinPool-1-worker-94,5,main] started managed 77
Thread[ForkJoinPool-1-worker-102,5,main] started managed 76
Thread[ForkJoinPool-1-worker-108,5,main] started managed 75
Thread[ForkJoinPool-1-worker-116,5,main] started managed 74
Thread[ForkJoinPool-1-worker-122,5,main] started managed 73
Thread[ForkJoinPool-1-worker-65,5,main] started managed 72
Thread[ForkJoinPool-1-worker-73,5,main] started managed 71
Thread[ForkJoinPool-1-worker-79,5,main] started managed 70
Thread[ForkJoinPool-1-worker-87,5,main] started managed 69
Thread[ForkJoinPool-1-worker-93,5,main] started managed 68
Thread[ForkJoinPool-1-worker-101,5,main] started managed 67
Thread[ForkJoinPool-1-worker-114,5,main] started managed 127
Thread[ForkJoinPool-1-worker-115,5,main] finished managed 65 :
2018-04-10T12:21:37.324Z
Thread[ForkJoinPool-1-worker-3,5,main] finished managed 1 :
2018-04-10T12:21:37.301Z
Thread[ForkJoinPool-1-worker-48,5,main] finished managed 47 :
2018-04-10T12:21:37.319Z
Thread[ForkJoinPool-1-worker-115,5,main] started managed 128
Thread[ForkJoinPool-1-worker-48,5,main] started managed 129
Thread[ForkJoinPool-1-worker-3,5,main] started managed 130
Thread[ForkJoinPool-1-worker-5,5,main] finished managed 7 :
2018-04-10T12:21:37.303Z
Thread[ForkJoinPool-1-worker-1,5,main] finished managed 3 :
2018-04-10T12:21:37.302Z
Thread[ForkJoinPool-1-worker-1,5,main] started managed 132
Thread[ForkJoinPool-1-worker-6,5,main] finished managed 5 :
2018-04-10T12:21:37.303Z
Thread[ForkJoinPool-1-worker-6,5,main] started managed 133
Thread[ForkJoinPool-1-worker-4,5,main] finished managed 4 :
2018-04-10T12:21:37.302Z
Thread[ForkJoinPool-1-worker-35,5,main] finished managed 58 :
2018-04-10T12:21:37.322Z
Thread[ForkJoinPool-1-worker-11,5,main] finished managed 8 :
2018-04-10T12:21:37.304Z
Thread[ForkJoinPool-1-worker-35,5,main] started managed 135
Thread[ForkJoinPool-1-worker-40,5,main] finished managed 39 :
2018-04-10T12:21:37.313Z
Thread[ForkJoinPool-1-worker-12,5,main] finished managed 10 :
2018-04-10T12:21:37.304Z
Thread[ForkJoinPool-1-worker-10,5,main] finished managed 9 :
2018-04-10T12:21:37.304Z
Thread[ForkJoinPool-1-worker-33,5,main] finished managed 40 :
2018-04-10T12:21:37.313Z
Thread[ForkJoinPool-1-worker-36,5,main] finished managed 63 :
2018-04-10T12:21:37.324Z
Thread[ForkJoinPool-1-worker-7,5,main] finished managed 6 :
2018-04-10T12:21:37.303Z
Thread[ForkJoinPool-1-worker-36,5,main] started managed 141
Thread[ForkJoinPool-1-worker-7,5,main] started managed 142
Thread[ForkJoinPool-1-worker-59,5,main] finished managed 41 :
2018-04-10T12:21:37.313Z
Thread[ForkJoinPool-1-worker-41,5,main] finished managed 48 :
2018-04-10T12:21:37.319Z
Thread[ForkJoinPool-1-worker-61,5,main] finished managed 36 :
2018-04-10T12:21:37.312Z
Thread[ForkJoinPool-1-worker-8,5,main] finished managed 15 :
2018-04-10T12:21:37.306Z
Thread[ForkJoinPool-1-worker-9,5,main] finished managed 12 :
2018-04-10T12:21:37.305Z
Thread[ForkJoinPool-1-worker-8,5,main] started managed 146
Thread[ForkJoinPool-1-worker-33,5,main] started managed 140
Thread[ForkJoinPool-1-worker-18,5,main] finished managed 16 :
2018-04-10T12:21:37.306Z
Thread[ForkJoinPool-1-worker-18,5,main] started managed 148
Thread[ForkJoinPool-1-worker-10,5,main] started managed 139
Thread[ForkJoinPool-1-worker-62,5,main] finished managed 45 :
2018-04-10T12:21:37.318Z
Thread[ForkJoinPool-1-worker-62,5,main] started managed 149
Thread[ForkJoinPool-1-worker-14,5,main] finished managed 11 :
2018-04-10T12:21:37.304Z
Thread[ForkJoinPool-1-worker-14,5,main] started managed 150
Thread[ForkJoinPool-1-worker-19,5,main] finished managed 17 :
2018-04-10T12:21:37.306Z
Thread[ForkJoinPool-1-worker-19,5,main] started managed 151
Thread[ForkJoinPool-1-worker-12,5,main] started managed 138
Thread[ForkJoinPool-1-worker-40,5,main] started managed 137
Thread[ForkJoinPool-1-worker-44,5,main] finished managed 34 :
2018-04-10T12:21:37.311Z
Thread[ForkJoinPool-1-worker-44,5,main] started managed 152
Thread[ForkJoinPool-1-worker-57,5,main] finished managed 61 :
2018-04-10T12:21:37.323Z
Thread[ForkJoinPool-1-worker-23,5,main] finished managed 21 :
2018-04-10T12:21:37.307Z
Thread[ForkJoinPool-1-worker-90,5,main] finished managed 64 :
2018-04-10T12:21:37.324Z
Thread[ForkJoinPool-1-worker-11,5,main] started managed 136
Thread[ForkJoinPool-1-worker-54,5,main] finished managed 37 :
2018-04-10T12:21:37.312Z
Thread[ForkJoinPool-1-worker-31,5,main] finished managed 22 :
2018-04-10T12:21:37.308Z
Thread[ForkJoinPool-1-worker-54,5,main] started managed 156
Thread[ForkJoinPool-1-worker-90,5,main] started managed 155
Thread[ForkJoinPool-1-worker-4,5,main] started managed 134
Thread[ForkJoinPool-1-worker-34,5,main] finished managed 49 :
2018-04-10T12:21:37.319Z
Thread[ForkJoinPool-1-worker-53,5,main] finished managed 51 :
2018-04-10T12:21:37.320Z
Thread[ForkJoinPool-1-worker-53,5,main] started managed 159
Thread[ForkJoinPool-1-worker-2,5,main] finished managed 0 :
2018-04-10T12:21:37.301Z
Thread[ForkJoinPool-1-worker-42,5,main] finished managed 32 :
2018-04-10T12:21:37.311Z
Thread[ForkJoinPool-1-worker-42,5,main] started managed 161
Thread[ForkJoinPool-1-worker-47,5,main] finished managed 38 :
2018-04-10T12:21:37.312Z
Thread[ForkJoinPool-1-worker-49,5,main] finished managed 56 :
2018-04-10T12:21:37.321Z
Thread[ForkJoinPool-1-worker-47,5,main] started managed 162
Thread[ForkJoinPool-1-worker-5,5,main] started managed 131
Thread[ForkJoinPool-1-worker-56,5,main] finished managed 55 :
2018-04-10T12:21:37.321Z
Thread[ForkJoinPool-1-worker-56,5,main] started managed 164
Thread[ForkJoinPool-1-worker-16,5,main] finished managed 29 :
2018-04-10T12:21:37.310Z
Thread[ForkJoinPool-1-worker-58,5,main] finished managed 60 :
2018-04-10T12:21:37.323Z
Thread[ForkJoinPool-1-worker-58,5,main] started managed 166
Thread[ForkJoinPool-1-worker-50,5,main] finished managed 57 :
2018-04-10T12:21:37.322Z
Thread[ForkJoinPool-1-worker-50,5,main] started managed 167
Thread[ForkJoinPool-1-worker-52,5,main] finished managed 42 :
2018-04-10T12:21:37.314Z
Thread[ForkJoinPool-1-worker-21,5,main] finished managed 28 :
2018-04-10T12:21:37.309Z
Thread[ForkJoinPool-1-worker-21,5,main] started managed 169
Thread[ForkJoinPool-1-worker-28,5,main] finished managed 27 :
2018-04-10T12:21:37.309Z
Thread[ForkJoinPool-1-worker-28,5,main] started managed 170
Thread[ForkJoinPool-1-worker-49,5,main] started managed 163
Thread[ForkJoinPool-1-worker-43,5,main] finished managed 62 :
2018-04-10T12:21:37.323Z
Thread[ForkJoinPool-1-worker-43,5,main] started managed 171
Thread[ForkJoinPool-1-worker-22,5,main] finished managed 26 :
2018-04-10T12:21:37.309Z
Thread[ForkJoinPool-1-worker-22,5,main] started managed 172
Thread[ForkJoinPool-1-worker-51,5,main] finished managed 33 :
2018-04-10T12:21:37.311Z
Thread[ForkJoinPool-1-worker-0,5,main] finished managed 2 :
2018-04-10T12:21:37.301Z
Thread[ForkJoinPool-1-worker-0,5,main] started managed 174
Thread[ForkJoinPool-1-worker-2,5,main] started managed 160
Thread[ForkJoinPool-1-worker-29,5,main] finished managed 25 :
2018-04-10T12:21:37.308Z
Thread[ForkJoinPool-1-worker-29,5,main] started managed 175
Thread[ForkJoinPool-1-worker-37,5,main] finished managed 35 :
2018-04-10T12:21:37.311Z
Thread[ForkJoinPool-1-worker-34,5,main] started managed 158
Thread[ForkJoinPool-1-worker-17,5,main] finished managed 24 :
2018-04-10T12:21:37.308Z
Thread[ForkJoinPool-1-worker-39,5,main] finished managed 53 :
2018-04-10T12:21:37.320Z
Thread[ForkJoinPool-1-worker-24,5,main] finished managed 23 :
2018-04-10T12:21:37.308Z
Thread[ForkJoinPool-1-worker-24,5,main] started managed 179
Thread[ForkJoinPool-1-worker-32,5,main] finished managed 59 :
2018-04-10T12:21:37.322Z
Thread[ForkJoinPool-1-worker-31,5,main] started managed 157
Thread[ForkJoinPool-1-worker-23,5,main] started managed 154
Thread[ForkJoinPool-1-worker-57,5,main] started managed 153
Thread[ForkJoinPool-1-worker-30,5,main] finished managed 20 :
2018-04-10T12:21:37.307Z
Thread[ForkJoinPool-1-worker-20,5,main] finished managed 19 :
2018-04-10T12:21:37.307Z
Thread[ForkJoinPool-1-worker-30,5,main] started managed 181
Thread[ForkJoinPool-1-worker-27,5,main] finished managed 18 :
2018-04-10T12:21:37.306Z
Thread[ForkJoinPool-1-worker-55,5,main] finished managed 46 :
2018-04-10T12:21:37.318Z
Thread[ForkJoinPool-1-worker-60,5,main] finished managed 50 :
2018-04-10T12:21:37.319Z
Thread[ForkJoinPool-1-worker-9,5,main] started managed 147
Thread[ForkJoinPool-1-worker-61,5,main] started managed 145
Thread[ForkJoinPool-1-worker-41,5,main] started managed 144
Thread[ForkJoinPool-1-worker-59,5,main] started managed 143
Thread[ForkJoinPool-1-worker-15,5,main] finished managed 14 :
2018-04-10T12:21:37.305Z
Thread[ForkJoinPool-1-worker-13,5,main] finished managed 13 :
2018-04-10T12:21:37.305Z
Thread[ForkJoinPool-1-worker-38,5,main] finished managed 44 :
2018-04-10T12:21:37.318Z
Thread[ForkJoinPool-1-worker-13,5,main] started managed 187
Thread[ForkJoinPool-1-worker-15,5,main] started managed 186
Thread[ForkJoinPool-1-worker-60,5,main] started managed 185
Thread[ForkJoinPool-1-worker-55,5,main] started managed 184
Thread[ForkJoinPool-1-worker-27,5,main] started managed 183
Thread[ForkJoinPool-1-worker-20,5,main] started managed 182
Thread[ForkJoinPool-1-worker-32,5,main] started managed 180
Thread[ForkJoinPool-1-worker-39,5,main] started managed 178
Thread[ForkJoinPool-1-worker-17,5,main] started managed 177
Thread[ForkJoinPool-1-worker-37,5,main] started managed 176
Thread[ForkJoinPool-1-worker-51,5,main] started managed 173
Thread[ForkJoinPool-1-worker-45,5,main] finished managed 43 :
2018-04-10T12:21:37.314Z
Thread[ForkJoinPool-1-worker-26,5,main] finished managed 30 :
2018-04-10T12:21:37.310Z
Thread[ForkJoinPool-1-worker-25,5,main] finished managed 31 :
2018-04-10T12:21:37.310Z
Thread[ForkJoinPool-1-worker-52,5,main] started managed 168
Thread[ForkJoinPool-1-worker-63,5,main] finished managed 54 :
2018-04-10T12:21:37.321Z
Thread[ForkJoinPool-1-worker-16,5,main] started managed 165
Thread[ForkJoinPool-1-worker-46,5,main] finished managed 52 :
2018-04-10T12:21:37.320Z
Thread[ForkJoinPool-1-worker-63,5,main] started managed 192
Thread[ForkJoinPool-1-worker-46,5,main] started managed 193
Thread[ForkJoinPool-1-worker-26,5,main] started managed 190
Thread[ForkJoinPool-1-worker-25,5,main] started managed 191
Thread[ForkJoinPool-1-worker-45,5,main] started managed 189
Thread[ForkJoinPool-1-worker-38,5,main] started managed 188
Thread[ForkJoinPool-1-worker-86,5,main] finished managed 125 :
2018-04-10T12:21:37.355Z
Thread[ForkJoinPool-1-worker-85,5,main] finished managed 124 :
2018-04-10T12:21:37.355Z
Thread[ForkJoinPool-1-worker-72,5,main] finished managed 123 :
2018-04-10T12:21:37.355Z
Thread[ForkJoinPool-1-worker-72,5,main] started managed 196
Thread[ForkJoinPool-1-worker-64,5,main] finished managed 121 :
2018-04-10T12:21:37.355Z
Thread[ForkJoinPool-1-worker-121,5,main] finished managed 119 :
2018-04-10T12:21:37.356Z
Thread[ForkJoinPool-1-worker-127,5,main] finished managed 118 :
2018-04-10T12:21:37.356Z
Thread[ForkJoinPool-1-worker-70,5,main] finished managed 117 :
2018-04-10T12:21:37.356Z
Thread[ForkJoinPool-1-worker-78,5,main] finished managed 116 :
2018-04-10T12:21:37.356Z
Thread[ForkJoinPool-1-worker-78,5,main] started managed 201
Thread[ForkJoinPool-1-worker-100,5,main] finished managed 114 :
2018-04-10T12:21:37.356Z
Thread[ForkJoinPool-1-worker-100,5,main] started managed 202
Thread[ForkJoinPool-1-worker-112,5,main] finished managed 111 :
2018-04-10T12:21:37.356Z
Thread[ForkJoinPool-1-worker-112,5,main] started managed 203
Thread[ForkJoinPool-1-worker-77,5,main] finished managed 107 :
2018-04-10T12:21:37.356Z
Thread[ForkJoinPool-1-worker-77,5,main] started managed 204
Thread[ForkJoinPool-1-worker-91,5,main] finished managed 105 :
2018-04-10T12:21:37.357Z
Thread[ForkJoinPool-1-worker-91,5,main] started managed 205
Thread[ForkJoinPool-1-worker-125,5,main] finished managed 100 :
2018-04-10T12:21:37.357Z
Thread[ForkJoinPool-1-worker-76,5,main] finished managed 98 :
2018-04-10T12:21:37.357Z
Thread[ForkJoinPool-1-worker-125,5,main] started managed 206
Thread[ForkJoinPool-1-worker-107,5,main] finished managed 66 :
2018-04-10T12:21:37.355Z
Thread[ForkJoinPool-1-worker-127,5,main] started managed 199
Thread[ForkJoinPool-1-worker-107,5,main] started managed 208
Thread[ForkJoinPool-1-worker-71,5,main] finished managed 126 :
2018-04-10T12:21:37.355Z
Thread[ForkJoinPool-1-worker-71,5,main] started managed 209
Thread[ForkJoinPool-1-worker-92,5,main] finished managed 96 :
2018-04-10T12:21:37.358Z
Thread[ForkJoinPool-1-worker-92,5,main] started managed 210
Thread[ForkJoinPool-1-worker-76,5,main] started managed 207
Thread[ForkJoinPool-1-worker-82,5,main] finished managed 97 :
2018-04-10T12:21:37.357Z
Thread[ForkJoinPool-1-worker-82,5,main] started managed 211
Thread[ForkJoinPool-1-worker-68,5,main] finished managed 99 :
2018-04-10T12:21:37.357Z
Thread[ForkJoinPool-1-worker-111,5,main] finished managed 102 :
2018-04-10T12:21:37.357Z
Thread[ForkJoinPool-1-worker-111,5,main] started managed 213
Thread[ForkJoinPool-1-worker-68,5,main] started managed 212
Thread[ForkJoinPool-1-worker-119,5,main] finished managed 101 :
2018-04-10T12:21:37.357Z
Thread[ForkJoinPool-1-worker-105,5,main] finished managed 103 :
2018-04-10T12:21:37.357Z
Thread[ForkJoinPool-1-worker-105,5,main] started managed 215
Thread[ForkJoinPool-1-worker-119,5,main] started managed 214
Thread[ForkJoinPool-1-worker-97,5,main] finished managed 104 :
2018-04-10T12:21:37.357Z
Thread[ForkJoinPool-1-worker-97,5,main] started managed 216
Thread[ForkJoinPool-1-worker-83,5,main] finished managed 106 :
2018-04-10T12:21:37.357Z
Thread[ForkJoinPool-1-worker-69,5,main] finished managed 108 :
2018-04-10T12:21:37.356Z
Thread[ForkJoinPool-1-worker-126,5,main] finished managed 109 :
2018-04-10T12:21:37.356Z
Thread[ForkJoinPool-1-worker-120,5,main] finished managed 110 :
2018-04-10T12:21:37.356Z
Thread[ForkJoinPool-1-worker-106,5,main] finished managed 112 :
2018-04-10T12:21:37.356Z
Thread[ForkJoinPool-1-worker-98,5,main] finished managed 113 :
2018-04-10T12:21:37.356Z
Thread[ForkJoinPool-1-worker-84,5,main] finished managed 115 :
2018-04-10T12:21:37.356Z
Thread[ForkJoinPool-1-worker-70,5,main] started managed 200
Thread[ForkJoinPool-1-worker-121,5,main] started managed 198
Thread[ForkJoinPool-1-worker-64,5,main] started managed 197
Thread[ForkJoinPool-1-worker-113,5,main] finished managed 120 :
2018-04-10T12:21:37.355Z
Thread[ForkJoinPool-1-worker-99,5,main] finished managed 122 :
2018-04-10T12:21:37.355Z
Thread[ForkJoinPool-1-worker-113,5,main] started managed 224
Thread[ForkJoinPool-1-worker-99,5,main] started managed 225
Thread[ForkJoinPool-1-worker-85,5,main] started managed 195
Thread[ForkJoinPool-1-worker-86,5,main] started managed 194
Thread[ForkJoinPool-1-worker-84,5,main] started managed 223
Thread[ForkJoinPool-1-worker-98,5,main] started managed 222
Thread[ForkJoinPool-1-worker-106,5,main] started managed 221
Thread[ForkJoinPool-1-worker-120,5,main] started managed 220
Thread[ForkJoinPool-1-worker-126,5,main] started managed 219
Thread[ForkJoinPool-1-worker-69,5,main] started managed 218
Thread[ForkJoinPool-1-worker-83,5,main] started managed 217
Thread[ForkJoinPool-1-worker-114,5,main] finished managed 127 :
2018-04-10T12:21:37.395Z
Thread[ForkJoinPool-1-worker-114,5,main] started managed 226
Thread[ForkJoinPool-1-worker-101,5,main] finished managed 67 :
2018-04-10T12:21:37.360Z
Thread[ForkJoinPool-1-worker-93,5,main] finished managed 68 :
2018-04-10T12:21:37.360Z
Thread[ForkJoinPool-1-worker-93,5,main] started managed 228
Thread[ForkJoinPool-1-worker-87,5,main] finished managed 69 :
2018-04-10T12:21:37.360Z
Thread[ForkJoinPool-1-worker-79,5,main] finished managed 70 :
2018-04-10T12:21:37.360Z
Thread[ForkJoinPool-1-worker-87,5,main] started managed 229
Thread[ForkJoinPool-1-worker-73,5,main] finished managed 71 :
2018-04-10T12:21:37.360Z
Thread[ForkJoinPool-1-worker-65,5,main] finished managed 72 :
2018-04-10T12:21:37.360Z
Thread[ForkJoinPool-1-worker-122,5,main] finished managed 73 :
2018-04-10T12:21:37.360Z
Thread[ForkJoinPool-1-worker-108,5,main] finished managed 75 :
2018-04-10T12:21:37.360Z
Thread[ForkJoinPool-1-worker-108,5,main] started managed 234
Thread[ForkJoinPool-1-worker-116,5,main] finished managed 74 :
2018-04-10T12:21:37.360Z
Thread[ForkJoinPool-1-worker-116,5,main] started managed 235
Thread[ForkJoinPool-1-worker-94,5,main] finished managed 77 :
2018-04-10T12:21:37.359Z
Thread[ForkJoinPool-1-worker-94,5,main] started managed 236
Thread[ForkJoinPool-1-worker-102,5,main] finished managed 76 :
2018-04-10T12:21:37.359Z
Thread[ForkJoinPool-1-worker-88,5,main] finished managed 78 :
2018-04-10T12:21:37.359Z
Thread[ForkJoinPool-1-worker-80,5,main] finished managed 79 :
2018-04-10T12:21:37.359Z
Thread[ForkJoinPool-1-worker-74,5,main] finished managed 80 :
2018-04-10T12:21:37.359Z
Thread[ForkJoinPool-1-worker-74,5,main] started managed 239
Thread[ForkJoinPool-1-worker-66,5,main] finished managed 81 :
2018-04-10T12:21:37.359Z
Thread[ForkJoinPool-1-worker-66,5,main] started managed 240
Thread[ForkJoinPool-1-worker-123,5,main] finished managed 82 :
2018-04-10T12:21:37.359Z
Thread[ForkJoinPool-1-worker-123,5,main] started managed 242
Thread[ForkJoinPool-1-worker-117,5,main] finished managed 83 :
2018-04-10T12:21:37.359Z
Thread[ForkJoinPool-1-worker-109,5,main] finished managed 84 :
2018-04-10T12:21:37.359Z
Thread[ForkJoinPool-1-worker-103,5,main] finished managed 85 :
2018-04-10T12:21:37.359Z
Thread[ForkJoinPool-1-worker-95,5,main] finished managed 86 :
2018-04-10T12:21:37.359Z
Thread[ForkJoinPool-1-worker-89,5,main] finished managed 87 :
2018-04-10T12:21:37.359Z
Thread[ForkJoinPool-1-worker-89,5,main] started managed 247
Thread[ForkJoinPool-1-worker-81,5,main] finished managed 88 :
2018-04-10T12:21:37.359Z
Thread[ForkJoinPool-1-worker-81,5,main] started managed 248
Thread[ForkJoinPool-1-worker-75,5,main] finished managed 89 :
2018-04-10T12:21:37.358Z
Thread[ForkJoinPool-1-worker-124,5,main] finished managed 91 :
2018-04-10T12:21:37.358Z
Thread[ForkJoinPool-1-worker-67,5,main] finished managed 90 :
2018-04-10T12:21:37.358Z
Thread[ForkJoinPool-1-worker-67,5,main] started managed 251
Thread[ForkJoinPool-1-worker-118,5,main] finished managed 92 :
2018-04-10T12:21:37.358Z
Thread[ForkJoinPool-1-worker-118,5,main] started managed 252
Thread[ForkJoinPool-1-worker-96,5,main] finished managed 95 :
2018-04-10T12:21:37.358Z
Thread[ForkJoinPool-1-worker-104,5,main] finished managed 94 :
2018-04-10T12:21:37.358Z
Thread[ForkJoinPool-1-worker-96,5,main] started managed 253
Thread[ForkJoinPool-1-worker-110,5,main] finished managed 93 :
2018-04-10T12:21:37.358Z
Thread[ForkJoinPool-1-worker-110,5,main] started managed 255
Thread[ForkJoinPool-1-worker-104,5,main] started managed 254
Thread[ForkJoinPool-1-worker-124,5,main] started managed 250
Thread[ForkJoinPool-1-worker-75,5,main] started managed 249
Thread[ForkJoinPool-1-worker-95,5,main] started managed 246
Thread[ForkJoinPool-1-worker-103,5,main] started managed 245
Thread[ForkJoinPool-1-worker-109,5,main] started managed 244
Thread[ForkJoinPool-1-worker-117,5,main] started managed 243
Thread[ForkJoinPool-1-worker-80,5,main] started managed 241
Thread[ForkJoinPool-1-worker-88,5,main] started managed 238
Thread[ForkJoinPool-1-worker-102,5,main] started managed 237
Thread[ForkJoinPool-1-worker-122,5,main] started managed 233
Thread[ForkJoinPool-1-worker-65,5,main] started managed 232
Thread[ForkJoinPool-1-worker-73,5,main] started managed 231
Thread[ForkJoinPool-1-worker-79,5,main] started managed 230
Thread[ForkJoinPool-1-worker-101,5,main] started managed 227
Thread[ForkJoinPool-1-worker-6,5,main] finished managed 133 :
2018-04-10T12:21:38.340Z
Thread[ForkJoinPool-1-worker-33,5,main] finished managed 140 :
2018-04-10T12:21:38.341Z
Thread[ForkJoinPool-1-worker-10,5,main] finished managed 139 :
2018-04-10T12:21:38.341Z
Thread[ForkJoinPool-1-worker-62,5,main] finished managed 149 :
2018-04-10T12:21:38.341Z
Thread[ForkJoinPool-1-worker-14,5,main] finished managed 150 :
2018-04-10T12:21:38.341Z
Thread[ForkJoinPool-1-worker-62,5,main] started managed 259
Thread[ForkJoinPool-1-worker-12,5,main] finished managed 138 :
2018-04-10T12:21:38.342Z
Thread[ForkJoinPool-1-worker-40,5,main] finished managed 137 :
2018-04-10T12:21:38.342Z
Thread[ForkJoinPool-1-worker-44,5,main] finished managed 152 :
2018-04-10T12:21:38.342Z
Thread[ForkJoinPool-1-worker-54,5,main] finished managed 156 :
2018-04-10T12:21:38.342Z
Thread[ForkJoinPool-1-worker-4,5,main] finished managed 134 :
2018-04-10T12:21:38.342Z
Thread[ForkJoinPool-1-worker-115,5,main] finished managed 128 :
2018-04-10T12:21:38.339Z
Thread[ForkJoinPool-1-worker-47,5,main] finished managed 162 :
2018-04-10T12:21:38.343Z
Thread[ForkJoinPool-1-worker-56,5,main] finished managed 164 :
2018-04-10T12:21:38.343Z
Thread[ForkJoinPool-1-worker-58,5,main] finished managed 166 :
2018-04-10T12:21:38.343Z
Thread[ForkJoinPool-1-worker-50,5,main] finished managed 167 :
2018-04-10T12:21:38.343Z
Thread[ForkJoinPool-1-worker-8,5,main] finished managed 146 :
2018-04-10T12:21:38.341Z
Thread[ForkJoinPool-1-worker-21,5,main] finished managed 169 :
2018-04-10T12:21:38.344Z
Thread[ForkJoinPool-1-worker-28,5,main] finished managed 170 :
2018-04-10T12:21:38.344Z
Thread[ForkJoinPool-1-worker-43,5,main] finished managed 171 :
2018-04-10T12:21:38.344Z
Thread[ForkJoinPool-1-worker-7,5,main] finished managed 142 :
2018-04-10T12:21:38.340Z
Thread[ForkJoinPool-1-worker-48,5,main] finished managed 129 :
2018-04-10T12:21:38.339Z
Thread[ForkJoinPool-1-worker-0,5,main] finished managed 174 :
2018-04-10T12:21:38.344Z
Thread[ForkJoinPool-1-worker-2,5,main] finished managed 160 :
2018-04-10T12:21:38.344Z
Thread[ForkJoinPool-1-worker-36,5,main] finished managed 141 :
2018-04-10T12:21:38.340Z
Thread[ForkJoinPool-1-worker-29,5,main] finished managed 175 :
2018-04-10T12:21:38.344Z
Thread[ForkJoinPool-1-worker-34,5,main] finished managed 158 :
2018-04-10T12:21:38.345Z
Thread[ForkJoinPool-1-worker-1,5,main] finished managed 132 :
2018-04-10T12:21:38.339Z
Thread[ForkJoinPool-1-worker-24,5,main] finished managed 179 :
2018-04-10T12:21:38.345Z
Thread[ForkJoinPool-1-worker-31,5,main] finished managed 157 :
2018-04-10T12:21:38.345Z
Thread[ForkJoinPool-1-worker-57,5,main] finished managed 153 :
2018-04-10T12:21:38.345Z
Thread[ForkJoinPool-1-worker-30,5,main] finished managed 181 :
2018-04-10T12:21:38.345Z
Thread[ForkJoinPool-1-worker-9,5,main] finished managed 147 :
2018-04-10T12:21:38.345Z
Thread[ForkJoinPool-1-worker-41,5,main] finished managed 144 :
2018-04-10T12:21:38.345Z
Thread[ForkJoinPool-1-worker-59,5,main] finished managed 143 :
2018-04-10T12:21:38.345Z
Thread[ForkJoinPool-1-worker-22,5,main] finished managed 172 :
2018-04-10T12:21:38.344Z
Thread[ForkJoinPool-1-worker-13,5,main] finished managed 187 :
2018-04-10T12:21:38.346Z
Thread[ForkJoinPool-1-worker-15,5,main] finished managed 186 :
2018-04-10T12:21:38.346Z
Thread[ForkJoinPool-1-worker-55,5,main] finished managed 184 :
2018-04-10T12:21:38.346Z
Thread[ForkJoinPool-1-worker-27,5,main] finished managed 183 :
2018-04-10T12:21:38.346Z
Thread[ForkJoinPool-1-worker-20,5,main] finished managed 182 :
2018-04-10T12:21:38.346Z
Thread[ForkJoinPool-1-worker-32,5,main] finished managed 180 :
2018-04-10T12:21:38.346Z
Thread[ForkJoinPool-1-worker-39,5,main] finished managed 178 :
2018-04-10T12:21:38.346Z
Thread[ForkJoinPool-1-worker-17,5,main] finished managed 177 :
2018-04-10T12:21:38.346Z
Thread[ForkJoinPool-1-worker-37,5,main] finished managed 176 :
2018-04-10T12:21:38.346Z
Thread[ForkJoinPool-1-worker-51,5,main] finished managed 173 :
2018-04-10T12:21:38.346Z
Thread[ForkJoinPool-1-worker-52,5,main] finished managed 168 :
2018-04-10T12:21:38.346Z
Thread[ForkJoinPool-1-worker-49,5,main] finished managed 163 :
2018-04-10T12:21:38.344Z
Thread[ForkJoinPool-1-worker-16,5,main] finished managed 165 :
2018-04-10T12:21:38.347Z
Thread[ForkJoinPool-1-worker-63,5,main] finished managed 192 :
2018-04-10T12:21:38.347Z
Thread[ForkJoinPool-1-worker-46,5,main] finished managed 193 :
2018-04-10T12:21:38.347Z
Thread[ForkJoinPool-1-worker-26,5,main] finished managed 190 :
2018-04-10T12:21:38.347Z
Thread[ForkJoinPool-1-worker-25,5,main] finished managed 191 :
2018-04-10T12:21:38.347Z
Thread[ForkJoinPool-1-worker-45,5,main] finished managed 189 :
2018-04-10T12:21:38.347Z
Thread[ForkJoinPool-1-worker-5,5,main] finished managed 131 :
2018-04-10T12:21:38.343Z
Thread[ForkJoinPool-1-worker-3,5,main] finished managed 130 :
2018-04-10T12:21:38.339Z
Thread[ForkJoinPool-1-worker-42,5,main] finished managed 161 :
2018-04-10T12:21:38.343Z
Thread[ForkJoinPool-1-worker-53,5,main] finished managed 159 :
2018-04-10T12:21:38.343Z
Thread[ForkJoinPool-1-worker-35,5,main] finished managed 135 :
2018-04-10T12:21:38.340Z
Thread[ForkJoinPool-1-worker-90,5,main] finished managed 155 :
2018-04-10T12:21:38.342Z
Thread[ForkJoinPool-1-worker-11,5,main] finished managed 136 :
2018-04-10T12:21:38.342Z
Thread[ForkJoinPool-1-worker-10,5,main] started managed 258
Thread[ForkJoinPool-1-worker-19,5,main] finished managed 151 :
2018-04-10T12:21:38.342Z
Thread[ForkJoinPool-1-worker-6,5,main] started managed 256
Thread[ForkJoinPool-1-worker-33,5,main] started managed 257
Thread[ForkJoinPool-1-worker-18,5,main] finished managed 148 :
2018-04-10T12:21:38.341Z
Thread[ForkJoinPool-1-worker-38,5,main] finished managed 188 :
2018-04-10T12:21:38.347Z
Thread[ForkJoinPool-1-worker-60,5,main] finished managed 185 :
2018-04-10T12:21:38.346Z
Thread[ForkJoinPool-1-worker-61,5,main] finished managed 145 :
2018-04-10T12:21:38.345Z
Thread[ForkJoinPool-1-worker-23,5,main] finished managed 154 :
2018-04-10T12:21:38.345Z
Thread[ForkJoinPool-1-worker-72,5,main] finished managed 196 :
2018-04-10T12:21:38.359Z
Thread[ForkJoinPool-1-worker-78,5,main] finished managed 201 :
2018-04-10T12:21:38.359Z
Thread[ForkJoinPool-1-worker-112,5,main] finished managed 203 :
2018-04-10T12:21:38.360Z
Thread[ForkJoinPool-1-worker-100,5,main] finished managed 202 :
2018-04-10T12:21:38.359Z
Thread[ForkJoinPool-1-worker-77,5,main] finished managed 204 :
2018-04-10T12:21:38.360Z
Thread[ForkJoinPool-1-worker-91,5,main] finished managed 205 :
2018-04-10T12:21:38.360Z
Thread[ForkJoinPool-1-worker-127,5,main] finished managed 199 :
2018-04-10T12:21:38.360Z
Thread[ForkJoinPool-1-worker-71,5,main] finished managed 209 :
2018-04-10T12:21:38.361Z
Thread[ForkJoinPool-1-worker-125,5,main] finished managed 206 :
2018-04-10T12:21:38.360Z
Thread[ForkJoinPool-1-worker-107,5,main] finished managed 208 :
2018-04-10T12:21:38.360Z
Thread[ForkJoinPool-1-worker-92,5,main] finished managed 210 :
2018-04-10T12:21:38.361Z
Thread[ForkJoinPool-1-worker-76,5,main] finished managed 207 :
2018-04-10T12:21:38.361Z
Thread[ForkJoinPool-1-worker-82,5,main] finished managed 211 :
2018-04-10T12:21:38.403Z
Thread[ForkJoinPool-1-worker-105,5,main] finished managed 215 :
2018-04-10T12:21:38.403Z
Thread[ForkJoinPool-1-worker-97,5,main] finished managed 216 :
2018-04-10T12:21:38.403Z
Thread[ForkJoinPool-1-worker-68,5,main] finished managed 212 :
2018-04-10T12:21:38.403Z
Thread[ForkJoinPool-1-worker-111,5,main] finished managed 213 :
2018-04-10T12:21:38.403Z
Thread[ForkJoinPool-1-worker-119,5,main] finished managed 214 :
2018-04-10T12:21:38.403Z
Thread[ForkJoinPool-1-worker-64,5,main] finished managed 197 :
2018-04-10T12:21:38.404Z
Thread[ForkJoinPool-1-worker-70,5,main] finished managed 200 :
2018-04-10T12:21:38.404Z
Thread[ForkJoinPool-1-worker-113,5,main] finished managed 224 :
2018-04-10T12:21:38.404Z
Thread[ForkJoinPool-1-worker-85,5,main] finished managed 195 :
2018-04-10T12:21:38.404Z
Thread[ForkJoinPool-1-worker-86,5,main] finished managed 194 :
2018-04-10T12:21:38.404Z
Thread[ForkJoinPool-1-worker-84,5,main] finished managed 223 :
2018-04-10T12:21:38.404Z
Thread[ForkJoinPool-1-worker-106,5,main] finished managed 221 :
2018-04-10T12:21:38.404Z
Thread[ForkJoinPool-1-worker-120,5,main] finished managed 220 :
2018-04-10T12:21:38.404Z
Thread[ForkJoinPool-1-worker-126,5,main] finished managed 219 :
2018-04-10T12:21:38.404Z
Thread[ForkJoinPool-1-worker-69,5,main] finished managed 218 :
2018-04-10T12:21:38.404Z
Thread[ForkJoinPool-1-worker-83,5,main] finished managed 217 :
2018-04-10T12:21:38.404Z
Thread[ForkJoinPool-1-worker-93,5,main] finished managed 228 :
2018-04-10T12:21:38.405Z
Thread[ForkJoinPool-1-worker-121,5,main] finished managed 198 :
2018-04-10T12:21:38.404Z
Thread[ForkJoinPool-1-worker-101,5,main] finished managed 227 :
2018-04-10T12:21:38.408Z
Thread[ForkJoinPool-1-worker-79,5,main] finished managed 230 :
2018-04-10T12:21:38.408Z
Thread[ForkJoinPool-1-worker-65,5,main] finished managed 232 :
2018-04-10T12:21:38.408Z
Thread[ForkJoinPool-1-worker-73,5,main] finished managed 231 :
2018-04-10T12:21:38.408Z
Thread[ForkJoinPool-1-worker-122,5,main] finished managed 233 :
2018-04-10T12:21:38.408Z
Thread[ForkJoinPool-1-worker-88,5,main] finished managed 238 :
2018-04-10T12:21:38.408Z
Thread[ForkJoinPool-1-worker-102,5,main] finished managed 237 :
2018-04-10T12:21:38.408Z
Thread[ForkJoinPool-1-worker-80,5,main] finished managed 241 :
2018-04-10T12:21:38.408Z
Thread[ForkJoinPool-1-worker-117,5,main] finished managed 243 :
2018-04-10T12:21:38.408Z
Thread[ForkJoinPool-1-worker-109,5,main] finished managed 244 :
2018-04-10T12:21:38.407Z
Thread[ForkJoinPool-1-worker-103,5,main] finished managed 245 :
2018-04-10T12:21:38.407Z
Thread[ForkJoinPool-1-worker-95,5,main] finished managed 246 :
2018-04-10T12:21:38.407Z
Thread[ForkJoinPool-1-worker-75,5,main] finished managed 249 :
2018-04-10T12:21:38.407Z
Thread[ForkJoinPool-1-worker-124,5,main] finished managed 250 :
2018-04-10T12:21:38.407Z
Thread[ForkJoinPool-1-worker-110,5,main] finished managed 255 :
2018-04-10T12:21:38.407Z
Thread[ForkJoinPool-1-worker-104,5,main] finished managed 254 :
2018-04-10T12:21:38.407Z
Thread[ForkJoinPool-1-worker-96,5,main] finished managed 253 :
2018-04-10T12:21:38.407Z
Thread[ForkJoinPool-1-worker-67,5,main] finished managed 251 :
2018-04-10T12:21:38.407Z
Thread[ForkJoinPool-1-worker-118,5,main] finished managed 252 :
2018-04-10T12:21:38.407Z
Thread[ForkJoinPool-1-worker-123,5,main] finished managed 242 :
2018-04-10T12:21:38.406Z
Thread[ForkJoinPool-1-worker-81,5,main] finished managed 248 :
2018-04-10T12:21:38.406Z
Thread[ForkJoinPool-1-worker-89,5,main] finished managed 247 :
2018-04-10T12:21:38.406Z
Thread[ForkJoinPool-1-worker-66,5,main] finished managed 240 :
2018-04-10T12:21:38.406Z
Thread[ForkJoinPool-1-worker-74,5,main] finished managed 239 :
2018-04-10T12:21:38.406Z
Thread[ForkJoinPool-1-worker-94,5,main] finished managed 236 :
2018-04-10T12:21:38.405Z
Thread[ForkJoinPool-1-worker-108,5,main] finished managed 234 :
2018-04-10T12:21:38.405Z
Thread[ForkJoinPool-1-worker-116,5,main] finished managed 235 :
2018-04-10T12:21:38.405Z
Thread[ForkJoinPool-1-worker-87,5,main] finished managed 229 :
2018-04-10T12:21:38.405Z
Thread[ForkJoinPool-1-worker-114,5,main] finished managed 226 :
2018-04-10T12:21:38.404Z
Thread[ForkJoinPool-1-worker-98,5,main] finished managed 222 :
2018-04-10T12:21:38.404Z
Thread[ForkJoinPool-1-worker-99,5,main] finished managed 225 :
2018-04-10T12:21:38.404Z
Thread[ForkJoinPool-1-worker-62,5,main] finished managed 259 :
2018-04-10T12:21:39.343Z
Thread[ForkJoinPool-1-worker-6,5,main] finished managed 256 :
2018-04-10T12:21:39.349Z
Thread[ForkJoinPool-1-worker-33,5,main] finished managed 257 :
2018-04-10T12:21:39.350Z
Thread[ForkJoinPool-1-worker-10,5,main] finished managed 258 :
2018-04-10T12:21:39.349Z
*Thread[ForkJoinPool-1-worker-10,5,main] started unmanaged 0*
*Thread[ForkJoinPool-1-worker-10,5,main] finished unmanaged 0 :
2018-04-10T12:21:40.357Z*
*Thread[ForkJoinPool-1-worker-10,5,main] started unmanaged 1*
*Thread[ForkJoinPool-1-worker-10,5,main] finished unmanaged 1 :
2018-04-10T12:21:41.362Z*
*Thread[ForkJoinPool-1-worker-10,5,main] started unmanaged 2*
*Thread[ForkJoinPool-1-worker-10,5,main] finished unmanaged 2 :
2018-04-10T12:21:42.365Z*
*Thread[ForkJoinPool-1-worker-10,5,main] started unmanaged 3*
*Thread[ForkJoinPool-1-worker-10,5,main] finished unmanaged 3 :
2018-04-10T12:21:43.371Z*
*Thread[ForkJoinPool-1-worker-10,5,main] started unmanaged 4*
*Thread[ForkJoinPool-1-worker-10,5,main] finished unmanaged 4 :
2018-04-10T12:21:44.375Z*
*Thread[ForkJoinPool-1-worker-10,5,main] started unmanaged 5*
*Thread[ForkJoinPool-1-worker-10,5,main] finished unmanaged 5 :
2018-04-10T12:21:45.376Z*
*Thread[ForkJoinPool-1-worker-10,5,main] started unmanaged 6*
*Thread[ForkJoinPool-1-worker-10,5,main] finished unmanaged 6 :
2018-04-10T12:21:46.381Z*
*Thread[ForkJoinPool-1-worker-10,5,main] started unmanaged 7*
*Thread[ForkJoinPool-1-worker-10,5,main] finished unmanaged 7 :
2018-04-10T12:21:47.385Z*
*Thread[ForkJoinPool-1-worker-10,5,main] started unmanaged 8*
*Thread[ForkJoinPool-1-worker-10,5,main] finished unmanaged 8 :
2018-04-10T12:21:48.390Z*
*Thread[ForkJoinPool-1-worker-10,5,main] started unmanaged 9*
*Thread[ForkJoinPool-1-worker-10,5,main] finished unmanaged 9 :
2018-04-10T12:21:49.392Z*
*Thread[ForkJoinPool-1-worker-10,5,main] started unmanaged 10*
*Thread[ForkJoinPool-1-worker-10,5,main] finished unmanaged 10 :
2018-04-10T12:21:50.397Z*
*Thread[ForkJoinPool-1-worker-10,5,main] started unmanaged 11*
*Thread[ForkJoinPool-1-worker-10,5,main] finished unmanaged 11 :
2018-04-10T12:21:51.398Z*
*Thread[ForkJoinPool-1-worker-10,5,main] started unmanaged 12*
*Thread[ForkJoinPool-1-worker-10,5,main] finished unmanaged 12 :
2018-04-10T12:21:52.403Z*
*Thread[ForkJoinPool-1-worker-10,5,main] started unmanaged 13*

-- 
Cheers,
√
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20180410/39c6a1d4/attachment-0001.html>

From dl at cs.oswego.edu  Tue Apr 10 18:44:10 2018
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 10 Apr 2018 18:44:10 -0400
Subject: [concurrency-interest] Strange FJP behavior (bug?)
In-Reply-To: <CANPzfU_wNi-9SJZK8ffgZqEgFyVN0FE6kJqfP_PhrDb7pLTFGg@mail.gmail.com>
References: <CANPzfU_wNi-9SJZK8ffgZqEgFyVN0FE6kJqfP_PhrDb7pLTFGg@mail.gmail.com>
Message-ID: <d2baede6-bcd2-f477-8aae-218185ea91de@cs.oswego.edu>

On 04/10/2018 08:25 AM, Viktor Klang via Concurrency-interest wrote:

> We've observed, and isolated, a strange behavior with ForkJoinPool in
> relation to capped ForkJoinThreadWorkerFactories and ManagedBlocker code
> followed by unmanaged blocking.
> 
> */And only a single thread continues to process the rest of the
> workload, one item at a time!/*
> 

Yes, this is unexpected, and possibly a policy bug.
I will further investigate.

-Doug

From viktor.klang at gmail.com  Fri Apr 13 06:56:38 2018
From: viktor.klang at gmail.com (Viktor Klang)
Date: Fri, 13 Apr 2018 12:56:38 +0200
Subject: [concurrency-interest] ForkJoinPool.managedBlock
Message-ID: <CANPzfU8wj+SLFA_r3ZPKxDsi2KPFE228ZVbrgtZszGkEEp=MFg@mail.gmail.com>

Experts,

The Javadoc for ForkJoinPool does not specify whether
ForkJoinPool.managedBlock() is reentrant or not. I would expect a task
which is already being managed to have nested managedblock()s be no-ops.

Thoughts?

-- 
Cheers,
√
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20180413/5c7f5197/attachment.html>

From dl at cs.oswego.edu  Fri Apr 13 07:09:23 2018
From: dl at cs.oswego.edu (Doug Lea)
Date: Fri, 13 Apr 2018 07:09:23 -0400
Subject: [concurrency-interest] ForkJoinPool.managedBlock
In-Reply-To: <CANPzfU8wj+SLFA_r3ZPKxDsi2KPFE228ZVbrgtZszGkEEp=MFg@mail.gmail.com>
References: <CANPzfU8wj+SLFA_r3ZPKxDsi2KPFE228ZVbrgtZszGkEEp=MFg@mail.gmail.com>
Message-ID: <e1068f88-2895-0c87-0875-1da9f420ae1d@cs.oswego.edu>

On 04/13/2018 06:56 AM, Viktor Klang via Concurrency-interest wrote:

> The Javadoc for ForkJoinPool does not specify whether
> ForkJoinPool.managedBlock() is reentrant or not. I would expect a task
> which is already being managed to have nested managedblock()s be no-ops.
> 
No, it is currently not reentrant, which is the cause of the
behavior in your last post. I've been sitting on this for a few
days trying to decide whether this is a bug or a feature.
I'm currently thinking feature, as an analog of standard advice
never to call arbitrary methods while holding locks.
If a block() method does anything except block, and in particular
adds or removes tasks, then it becomes very hard to reason about or
control. On the other hand, FJP does not currently even maintain
bookkeeping that would allow reliable detection of this case, which
probably should be added.

Other thoughts welcome.

-Doug

From viktor.klang at gmail.com  Fri Apr 13 07:53:06 2018
From: viktor.klang at gmail.com (Viktor Klang)
Date: Fri, 13 Apr 2018 13:53:06 +0200
Subject: [concurrency-interest] ForkJoinPool.managedBlock
In-Reply-To: <e1068f88-2895-0c87-0875-1da9f420ae1d@cs.oswego.edu>
References: <CANPzfU8wj+SLFA_r3ZPKxDsi2KPFE228ZVbrgtZszGkEEp=MFg@mail.gmail.com>
 <e1068f88-2895-0c87-0875-1da9f420ae1d@cs.oswego.edu>
Message-ID: <CANPzfU9Y10EDqhPp6i7bcMYqeidGX_smD573a42+EMrRJqaWCg@mail.gmail.com>

Thanks for the very rapid response, Doug!

On Fri, Apr 13, 2018 at 1:09 PM, Doug Lea via Concurrency-interest <
concurrency-interest at cs.oswego.edu> wrote:

> On 04/13/2018 06:56 AM, Viktor Klang via Concurrency-interest wrote:
>
> > The Javadoc for ForkJoinPool does not specify whether
> > ForkJoinPool.managedBlock() is reentrant or not. I would expect a task
> > which is already being managed to have nested managedblock()s be no-ops.
> >
> No, it is currently not reentrant, which is the cause of the
> behavior in your last post.


That sounds highly unlikely because my example is not reentrant?
Or could you explain further how it would create a situation of reentrancy?


> I've been sitting on this for a few
> days trying to decide whether this is a bug or a feature.
> I'm currently thinking feature, as an analog of standard advice
> never to call arbitrary methods while holding locks.
>

According to my understanding, managedBlock is a demarcation of a segment
of code which is likely to block—it's basically (conditional) IO—even if it
could be signalled purely across the memory bus.
Is that understanding flawed?

As a sidenote, I implemented a proof-of-concept of supporting the
equivalent of ManagedBlocking using ThreadPoolExecutor: https://gi
thub.com/akka/akka/commit/5e5e38747449ad1798f8aae4a1b5e5989c
ac5eda#diff-410abf8aac6a89a01b05db2cbcf1765cR71


> If a block() method does anything except block, and in particular
> adds or removes tasks, then it becomes very hard to reason about or
> control.


If it is not advised to execute arbitrary code under managedBlock() then in
my mind the feature is of very little value,
being able to signal to the pool that a worker will be unlikely to be able
to execute more work for a while is a *very* useful feature.



> On the other hand, FJP does not currently even maintain
> bookkeeping that would allow reliable detection of this case, which
> probably should be added.
>

I've added the detection within Scala (since a long time
<https://github.com/scala/scala/blob/2.13.x/src/library/scala/concurrent/impl/ExecutionContextImpl.scala#L75>),
to make sure that there is no reentrancy, but it seemed like something
which should either
be clear in the managedBlocker Javadoc, or should be made reentrant. I'd
prefer the second alternative, following the principle of least
surprise—the performance impact is only when actually block()-ing, which is
a performance hit anyway.


>
> Other thoughts welcome.
>
> -Doug
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



-- 
Cheers,
√
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20180413/ce1c28be/attachment.html>

From dl at cs.oswego.edu  Sun Apr 15 17:39:59 2018
From: dl at cs.oswego.edu (Doug Lea)
Date: Sun, 15 Apr 2018 17:39:59 -0400
Subject: [concurrency-interest] Strange FJP behavior (bug?)
In-Reply-To: <CANPzfU_wNi-9SJZK8ffgZqEgFyVN0FE6kJqfP_PhrDb7pLTFGg@mail.gmail.com>
References: <CANPzfU_wNi-9SJZK8ffgZqEgFyVN0FE6kJqfP_PhrDb7pLTFGg@mail.gmail.com>
Message-ID: <295934f6-8707-d1b4-8265-e359157ea0b4@cs.oswego.edu>

On 04/10/2018 08:25 AM, Viktor Klang via Concurrency-interest wrote:
> We've observed, and isolated, a strange behavior with ForkJoinPool in
> relation to capped ForkJoinThreadWorkerFactories and ManagedBlocker code
> followed by unmanaged blocking.
> 
> *Expected behavior:*
> Pool scales up to the limit of workers as given by the pool, as a result
> of the managed blocking, then either terminates those extra threads or
> uses them to help out processing the work.
> 
> *Observed behavior:*
> Threads are created during the managed blocking, but then get stuck in
> this stack frame:
>

To get the effect you want, use the jdk9+ FJP constructor that
was designed (with your input :-) to support it. Try it with:

  public static void main(final String[] args) throws Exception {
      final ForkJoinPool fjp =
          new ForkJoinPool(CORES,
                           ForkJoinPool.defaultForkJoinWorkerThreadFactory,
                           null,
                           true,
                           CORES,
                           MAX_THREADS,
                           1,
                           x -> true, // continue if saturated
                           60, TimeUnit.SECONDS);

Your attempt to emulate the "saturate" predicate by keeping
track of counts in allocate()/deallocate() maintains minimal
progress but as you saw, it will not usually reactivate
available threads.

I don't know of a way to avoid this on pre-jdk9 versions.

(Also, ignore the comment on this example in other thread.
The symptoms are almost the same as recursive blocking
because in both cases threads continue even though doing so
is not guaranteed to maintain liveness.)

-Doug


From dl at cs.oswego.edu  Sun Apr 15 18:21:42 2018
From: dl at cs.oswego.edu (Doug Lea)
Date: Sun, 15 Apr 2018 18:21:42 -0400
Subject: [concurrency-interest] ForkJoinPool.managedBlock
In-Reply-To: <CANPzfU9Y10EDqhPp6i7bcMYqeidGX_smD573a42+EMrRJqaWCg@mail.gmail.com>
References: <CANPzfU8wj+SLFA_r3ZPKxDsi2KPFE228ZVbrgtZszGkEEp=MFg@mail.gmail.com>
 <e1068f88-2895-0c87-0875-1da9f420ae1d@cs.oswego.edu>
 <CANPzfU9Y10EDqhPp6i7bcMYqeidGX_smD573a42+EMrRJqaWCg@mail.gmail.com>
Message-ID: <068f4293-65b6-b1aa-e25d-25ef9eb4d544@cs.oswego.edu>

On 04/13/2018 07:53 AM, Viktor Klang wrote:
> Thanks for the very rapid response, Doug!
> 
> On Fri, Apr 13, 2018 at 1:09 PM, Doug Lea via Concurrency-interest
> <concurrency-interest at cs.oswego.edu
> <mailto:concurrency-interest at cs.oswego.edu>> wrote:
> 
>     On 04/13/2018 06:56 AM, Viktor Klang via Concurrency-interest wrote:
> 
>     > The Javadoc for ForkJoinPool does not specify whether
>     > ForkJoinPool.managedBlock() is reentrant or not. I would expect a task
>     > which is already being managed to have nested managedblock()s be no-ops.
>     > 
>     No, it is currently not reentrant, which is the cause of the
>     behavior in your last post.

(Sorry; not quite the same issues, as explained in my other post.)

> 
> According to my understanding, managedBlock is a demarcation of a
> segment of code which is likely to block—it's basically (conditional)
> IO—even if it could be signalled purely across the memory bus.
> Is that understanding flawed?

The operational meaning of managedBlock to FJP is:

1. Ensure that there are enough threads to maintain minimal liveness
(default 1) even if this thread is not active; or return
early if isReleasable; or proceed anyway (at the risk of
liveness failure) if saturate predicate (jdk9+) says OK,
or throw exception if would exceed thread limit.

2. if (1) succeeded, record that there is one fewer thread available
before calling block(), and restore on exit from block().

It is possible to skip most mechanics if the calling thread is
already blocked and saturate() permits it, with increased risk of
liveness failures. Considering that we now allow users to
tell us about tolerance via saturate predicate, I'm not completely
against doing this. It will be hard to explain though.

But it might be more productive to explore the main behavioral
effects you are trying to achieve?

-Doug




From viktor.klang at gmail.com  Mon Apr 16 08:55:49 2018
From: viktor.klang at gmail.com (Viktor Klang)
Date: Mon, 16 Apr 2018 13:55:49 +0100
Subject: [concurrency-interest] Strange FJP behavior (bug?)
In-Reply-To: <295934f6-8707-d1b4-8265-e359157ea0b4@cs.oswego.edu>
References: <CANPzfU_wNi-9SJZK8ffgZqEgFyVN0FE6kJqfP_PhrDb7pLTFGg@mail.gmail.com>
 <295934f6-8707-d1b4-8265-e359157ea0b4@cs.oswego.edu>
Message-ID: <CANPzfU-jHbkgCp4rg5NSci7864O8EYRYyvKTN5_VFJG9f-_jvQ@mail.gmail.com>

On Sun, Apr 15, 2018 at 10:39 PM, Doug Lea via Concurrency-interest <
concurrency-interest at cs.oswego.edu> wrote:

> On 04/10/2018 08:25 AM, Viktor Klang via Concurrency-interest wrote:
> > We've observed, and isolated, a strange behavior with ForkJoinPool in
> > relation to capped ForkJoinThreadWorkerFactories and ManagedBlocker code
> > followed by unmanaged blocking.
> >
> > *Expected behavior:*
> > Pool scales up to the limit of workers as given by the pool, as a result
> > of the managed blocking, then either terminates those extra threads or
> > uses them to help out processing the work.
> >
> > *Observed behavior:*
> > Threads are created during the managed blocking, but then get stuck in
> > this stack frame:
> >
>
> To get the effect you want, use the jdk9+ FJP constructor that
> was designed (with your input :-) to support it. Try it with:
>
>   public static void main(final String[] args) throws Exception {
>       final ForkJoinPool fjp =
>           new ForkJoinPool(CORES,
>                            ForkJoinPool.defaultForkJoinW
> orkerThreadFactory,
>                            null,
>                            true,
>                            CORES,
>                            MAX_THREADS,
>                            1,
>                            x -> true, // continue if saturated
>                            60, TimeUnit.SECONDS);
>

I do need to try to work around this bug on non-jdk9 runtimes. :)


>
> Your attempt to emulate the "saturate" predicate by keeping
> track of counts in allocate()/deallocate() maintains minimal
> progress but as you saw, it will not usually reactivate
> available threads.
>

Alas, there's tons of work, and all but one of the workers are stuck in
awaitWork?


>
> I don't know of a way to avoid this on pre-jdk9 versions.
>
> (Also, ignore the comment on this example in other thread.
> The symptoms are almost the same as recursive blocking
> because in both cases threads continue even though doing so
> is not guaranteed to maintain liveness.)
>
> -Doug
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



-- 
Cheers,
√
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20180416/f9c34256/attachment.html>

From viktor.klang at gmail.com  Mon Apr 16 09:01:05 2018
From: viktor.klang at gmail.com (Viktor Klang)
Date: Mon, 16 Apr 2018 14:01:05 +0100
Subject: [concurrency-interest] ForkJoinPool.managedBlock
In-Reply-To: <068f4293-65b6-b1aa-e25d-25ef9eb4d544@cs.oswego.edu>
References: <CANPzfU8wj+SLFA_r3ZPKxDsi2KPFE228ZVbrgtZszGkEEp=MFg@mail.gmail.com>
 <e1068f88-2895-0c87-0875-1da9f420ae1d@cs.oswego.edu>
 <CANPzfU9Y10EDqhPp6i7bcMYqeidGX_smD573a42+EMrRJqaWCg@mail.gmail.com>
 <068f4293-65b6-b1aa-e25d-25ef9eb4d544@cs.oswego.edu>
Message-ID: <CANPzfU-=WYGajksbwwXohWT1TEoF32cKxJkNzhYcNm-nmUX_aQ@mail.gmail.com>

On Sun, Apr 15, 2018 at 11:21 PM, Doug Lea via Concurrency-interest <
concurrency-interest at cs.oswego.edu> wrote:

> On 04/13/2018 07:53 AM, Viktor Klang wrote:
> > Thanks for the very rapid response, Doug!
> >
> > On Fri, Apr 13, 2018 at 1:09 PM, Doug Lea via Concurrency-interest
> > <concurrency-interest at cs.oswego.edu
> > <mailto:concurrency-interest at cs.oswego.edu>> wrote:
> >
> >     On 04/13/2018 06:56 AM, Viktor Klang via Concurrency-interest wrote:
> >
> >     > The Javadoc for ForkJoinPool does not specify whether
> >     > ForkJoinPool.managedBlock() is reentrant or not. I would expect a
> task
> >     > which is already being managed to have nested managedblock()s be
> no-ops.
> >     >
> >     No, it is currently not reentrant, which is the cause of the
> >     behavior in your last post.
>
> (Sorry; not quite the same issues, as explained in my other post.)
>
> >
> > According to my understanding, managedBlock is a demarcation of a
> > segment of code which is likely to block—it's basically (conditional)
> > IO—even if it could be signalled purely across the memory bus.
> > Is that understanding flawed?
>
> The operational meaning of managedBlock to FJP is:
>
> 1. Ensure that there are enough threads to maintain minimal liveness
> (default 1) even if this thread is not active;


The problem I'm seeing is that the workers are getting stuck in awaitWork.


> or return
> early if isReleasable; or proceed anyway (at the risk of
> liveness failure) if saturate predicate (jdk9+) says OK,
> or throw exception if would exceed thread limit.


> 2. if (1) succeeded, record that there is one fewer thread available
> before calling block(), and restore on exit from block().
>
> It is possible to skip most mechanics if the calling thread is
> already blocked and saturate() permits it, with increased risk of
> liveness failures. Considering that we now allow users to
> tell us about tolerance via saturate predicate, I'm not completely
> against doing this. It will be hard to explain though.
>
> But it might be more productive to explore the main behavioral
> effects you are trying to achieve?
>

ForkJoinWorkerThreadFactory does not signal whether it is creating new
threads to maintain liveness due to ManagedBlocking, or otherwise, so the
only way to deal with the situation is to have a limit on max number of
concurrent workers.

I have an idea for a possible workaround. I'll see if I can get that up and
running ASAP.


> -Doug
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



-- 
Cheers,
√
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20180416/65c38712/attachment.html>

From viktor.klang at gmail.com  Mon Apr 16 10:20:38 2018
From: viktor.klang at gmail.com (Viktor Klang)
Date: Mon, 16 Apr 2018 15:20:38 +0100
Subject: [concurrency-interest] ForkJoinPool.managedBlock
In-Reply-To: <CANPzfU-=WYGajksbwwXohWT1TEoF32cKxJkNzhYcNm-nmUX_aQ@mail.gmail.com>
References: <CANPzfU8wj+SLFA_r3ZPKxDsi2KPFE228ZVbrgtZszGkEEp=MFg@mail.gmail.com>
 <e1068f88-2895-0c87-0875-1da9f420ae1d@cs.oswego.edu>
 <CANPzfU9Y10EDqhPp6i7bcMYqeidGX_smD573a42+EMrRJqaWCg@mail.gmail.com>
 <068f4293-65b6-b1aa-e25d-25ef9eb4d544@cs.oswego.edu>
 <CANPzfU-=WYGajksbwwXohWT1TEoF32cKxJkNzhYcNm-nmUX_aQ@mail.gmail.com>
Message-ID: <CANPzfU-N38JS+GdOzA3BgeoJFYWsFbbw_uGdohbABGgU1EvChA@mail.gmail.com>

On Mon, Apr 16, 2018 at 2:01 PM, Viktor Klang <viktor.klang at gmail.com>
wrote:

>
>
> On Sun, Apr 15, 2018 at 11:21 PM, Doug Lea via Concurrency-interest <
> concurrency-interest at cs.oswego.edu> wrote:
>
>> On 04/13/2018 07:53 AM, Viktor Klang wrote:
>> > Thanks for the very rapid response, Doug!
>> >
>> > On Fri, Apr 13, 2018 at 1:09 PM, Doug Lea via Concurrency-interest
>> > <concurrency-interest at cs.oswego.edu
>> > <mailto:concurrency-interest at cs.oswego.edu>> wrote:
>> >
>> >     On 04/13/2018 06:56 AM, Viktor Klang via Concurrency-interest wrote:
>> >
>> >     > The Javadoc for ForkJoinPool does not specify whether
>> >     > ForkJoinPool.managedBlock() is reentrant or not. I would expect a
>> task
>> >     > which is already being managed to have nested managedblock()s be
>> no-ops.
>> >     >
>> >     No, it is currently not reentrant, which is the cause of the
>> >     behavior in your last post.
>>
>> (Sorry; not quite the same issues, as explained in my other post.)
>>
>> >
>> > According to my understanding, managedBlock is a demarcation of a
>> > segment of code which is likely to block—it's basically (conditional)
>> > IO—even if it could be signalled purely across the memory bus.
>> > Is that understanding flawed?
>>
>> The operational meaning of managedBlock to FJP is:
>>
>> 1. Ensure that there are enough threads to maintain minimal liveness
>> (default 1) even if this thread is not active;
>
>
> The problem I'm seeing is that the workers are getting stuck in awaitWork.
>
>
>> or return
>> early if isReleasable; or proceed anyway (at the risk of
>> liveness failure) if saturate predicate (jdk9+) says OK,
>> or throw exception if would exceed thread limit.
>
>
>> 2. if (1) succeeded, record that there is one fewer thread available
>> before calling block(), and restore on exit from block().
>>
>> It is possible to skip most mechanics if the calling thread is
>> already blocked and saturate() permits it, with increased risk of
>> liveness failures. Considering that we now allow users to
>> tell us about tolerance via saturate predicate, I'm not completely
>> against doing this. It will be hard to explain though.
>>
>> But it might be more productive to explore the main behavioral
>> effects you are trying to achieve?
>>
>
> ForkJoinWorkerThreadFactory does not signal whether it is creating new
> threads to maintain liveness due to ManagedBlocking, or otherwise, so the
> only way to deal with the situation is to have a limit on max number of
> concurrent workers.
>
> I have an idea for a possible workaround. I'll see if I can get that up
> and running ASAP.
>

Looks possible to solve (in my case) like this:
https://github.com/scala/scala/pull/6529


>
>
>> -Doug
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
>
> --
> Cheers,
> √
>



-- 
Cheers,
√
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20180416/6398877b/attachment-0001.html>

From dl at cs.oswego.edu  Tue Apr 17 07:35:38 2018
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 17 Apr 2018 07:35:38 -0400
Subject: [concurrency-interest] ForkJoinPool.managedBlock
In-Reply-To: <CANPzfU-N38JS+GdOzA3BgeoJFYWsFbbw_uGdohbABGgU1EvChA@mail.gmail.com>
References: <CANPzfU8wj+SLFA_r3ZPKxDsi2KPFE228ZVbrgtZszGkEEp=MFg@mail.gmail.com>
 <e1068f88-2895-0c87-0875-1da9f420ae1d@cs.oswego.edu>
 <CANPzfU9Y10EDqhPp6i7bcMYqeidGX_smD573a42+EMrRJqaWCg@mail.gmail.com>
 <068f4293-65b6-b1aa-e25d-25ef9eb4d544@cs.oswego.edu>
 <CANPzfU-=WYGajksbwwXohWT1TEoF32cKxJkNzhYcNm-nmUX_aQ@mail.gmail.com>
 <CANPzfU-N38JS+GdOzA3BgeoJFYWsFbbw_uGdohbABGgU1EvChA@mail.gmail.com>
Message-ID: <8f7a03ca-f657-3a04-1fd0-28c8dc6d84b4@cs.oswego.edu>

On 04/16/2018 10:20 AM, Viktor Klang wrote:

>     I have an idea for a possible workaround. I'll see if I can get that
>     up and running ASAP.
> 
> 
> Looks possible to solve (in my case) like
> this: https://github.com/scala/scala/pull/6529

This seems OK as a better approximation of jdk9+ functionality,
but hopefully in the future you can replace with jdk9 version,
which avoids the raciness here of reserving and then unreserving
threads that don't actually materialize. Which can still have the
result of reducing parallelism (but much less so than your original).

-Doug


From viktor.klang at gmail.com  Tue Apr 17 13:50:10 2018
From: viktor.klang at gmail.com (Viktor Klang)
Date: Tue, 17 Apr 2018 18:50:10 +0100
Subject: [concurrency-interest] ForkJoinPool.managedBlock
In-Reply-To: <8f7a03ca-f657-3a04-1fd0-28c8dc6d84b4@cs.oswego.edu>
References: <CANPzfU8wj+SLFA_r3ZPKxDsi2KPFE228ZVbrgtZszGkEEp=MFg@mail.gmail.com>
 <e1068f88-2895-0c87-0875-1da9f420ae1d@cs.oswego.edu>
 <CANPzfU9Y10EDqhPp6i7bcMYqeidGX_smD573a42+EMrRJqaWCg@mail.gmail.com>
 <068f4293-65b6-b1aa-e25d-25ef9eb4d544@cs.oswego.edu>
 <CANPzfU-=WYGajksbwwXohWT1TEoF32cKxJkNzhYcNm-nmUX_aQ@mail.gmail.com>
 <CANPzfU-N38JS+GdOzA3BgeoJFYWsFbbw_uGdohbABGgU1EvChA@mail.gmail.com>
 <8f7a03ca-f657-3a04-1fd0-28c8dc6d84b4@cs.oswego.edu>
Message-ID: <CANPzfU9Z5Wq=wJUhWudQ-HTFT5AM7bnttp4mWAWzkuoVdAQ7UQ@mail.gmail.com>

On Tue, Apr 17, 2018 at 12:35 PM, Doug Lea <dl at cs.oswego.edu> wrote:

> On 04/16/2018 10:20 AM, Viktor Klang wrote:
>
> >     I have an idea for a possible workaround. I'll see if I can get that
> >     up and running ASAP.
> >
> >
> > Looks possible to solve (in my case) like
> > this: https://github.com/scala/scala/pull/6529
>
> This seems OK as a better approximation of jdk9+ functionality,
> but hopefully in the future you can replace with jdk9 version,
> which avoids the raciness here of reserving and then unreserving
> threads that don't actually materialize. Which can still have the
> result of reducing parallelism (but much less so than your original).
>

Thanks for the feedback, Doug—my workaround seems to pass all tests I'm
throwing at it.


>
> -Doug
>
>


-- 
Cheers,
√
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20180417/24eefe1c/attachment.html>

From viktor.klang at gmail.com  Wed Apr 18 10:16:20 2018
From: viktor.klang at gmail.com (Viktor Klang)
Date: Wed, 18 Apr 2018 15:16:20 +0100
Subject: [concurrency-interest] ForkJoinPool.managedBlock
In-Reply-To: <CANPzfU9Z5Wq=wJUhWudQ-HTFT5AM7bnttp4mWAWzkuoVdAQ7UQ@mail.gmail.com>
References: <CANPzfU8wj+SLFA_r3ZPKxDsi2KPFE228ZVbrgtZszGkEEp=MFg@mail.gmail.com>
 <e1068f88-2895-0c87-0875-1da9f420ae1d@cs.oswego.edu>
 <CANPzfU9Y10EDqhPp6i7bcMYqeidGX_smD573a42+EMrRJqaWCg@mail.gmail.com>
 <068f4293-65b6-b1aa-e25d-25ef9eb4d544@cs.oswego.edu>
 <CANPzfU-=WYGajksbwwXohWT1TEoF32cKxJkNzhYcNm-nmUX_aQ@mail.gmail.com>
 <CANPzfU-N38JS+GdOzA3BgeoJFYWsFbbw_uGdohbABGgU1EvChA@mail.gmail.com>
 <8f7a03ca-f657-3a04-1fd0-28c8dc6d84b4@cs.oswego.edu>
 <CANPzfU9Z5Wq=wJUhWudQ-HTFT5AM7bnttp4mWAWzkuoVdAQ7UQ@mail.gmail.com>
Message-ID: <CANPzfU_XkzgmPLmaUDpYbM4-=PGSm6tvoLi3ffmoBtzq+3Q40w@mail.gmail.com>

Quick followup question:

The javadoc of ManagedBlocker
<https://docs.oracle.com/javase/9/docs/api/java/util/concurrent/ForkJoinPool.ManagedBlocker.html>
doesn't
really mention any thread safety requirements, yet the examples seems to
indicate that they are needed (for instance, the volatile `item` in the
example code.

Can it be safely assumed that the only thread which will invoke any methods
on the instance of the ManagedBlocker is the thread calling
managedBlock(blocker)? (i.e. no safe publication needed, and no concurrency
control to avoid concurrent invocations of the ManagedBlocker methods—of
course assuming I do not submit the same ManagedBlocker instance more than
once)

All the code I've looked at when it comes the the managedBlocker
implementation looks safe, but I'm thinking about forward compatibility and
potentially other JDKs.


On Tue, Apr 17, 2018 at 6:50 PM, Viktor Klang <viktor.klang at gmail.com>
wrote:

>
>
> On Tue, Apr 17, 2018 at 12:35 PM, Doug Lea <dl at cs.oswego.edu> wrote:
>
>> On 04/16/2018 10:20 AM, Viktor Klang wrote:
>>
>> >     I have an idea for a possible workaround. I'll see if I can get that
>> >     up and running ASAP.
>> >
>> >
>> > Looks possible to solve (in my case) like
>> > this: https://github.com/scala/scala/pull/6529
>>
>> This seems OK as a better approximation of jdk9+ functionality,
>> but hopefully in the future you can replace with jdk9 version,
>> which avoids the raciness here of reserving and then unreserving
>> threads that don't actually materialize. Which can still have the
>> result of reducing parallelism (but much less so than your original).
>>
>
> Thanks for the feedback, Doug—my workaround seems to pass all tests I'm
> throwing at it.
>
>
>>
>> -Doug
>>
>>
>
>
> --
> Cheers,
> √
>



-- 
Cheers,
√
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20180418/cc9ffa08/attachment.html>

From jini at zeus.net.au  Sat Apr 21 04:42:05 2018
From: jini at zeus.net.au (Peter)
Date: Sat, 21 Apr 2018 18:42:05 +1000
Subject: [concurrency-interest] about JDK-8186628
In-Reply-To: <20e6db12-dfa4-781e-4d2b-435500484d3b@oracle.com>
References: <CALur2hVXAZ+jZ9Bo0mP-i=YFqGvB3186Bjzj7mNuQR0VEn9CZg@mail.gmail.com>
 <190e738f-4196-330b-5c81-6268a6f42202@oracle.com>
 <CALur2hWUqpt_rmiCUK-NE0C+c4N5mt87UG7uJbRaStd7VGUwMg@mail.gmail.com>
 <20e6db12-dfa4-781e-4d2b-435500484d3b@oracle.com>
Message-ID: <5ADAF95D.7080802@zeus.net.au>

I wrote caching software that decorated any Java collection with soft, 
weak, timed or strong references, it utilised a background thread to 
clean references from underlying collections.

It was non blocking if the underlying collection was, although it hasn't 
been updated to Java 8.

Regarding patch comments, I don't mean to sound rude, but I don't think 
there's such a thing as a harmless data race.

Regards,

Peter.

On 21/04/2018 3:45 AM, Ivan Gerasimov wrote:
>
> I'll go ahead with a review of the enhancement request JDK-8202086 
> shortly on this list.
>
> And we'll still need to decide what has to be done in the earlier 
> releases of JDK.
>
> With kind regards,
>
> Ivan
>
>
> <https://bugs.openjdk.java.net/browse/JDK-8202086>
>
>
> On 4/20/18 10:06 AM, nezih yigitbasi wrote:
>> Ivan, thanks for the information. Any ideas about when one of these 
>> solutions can be released?
>>
>> Nezih
>>
>> 2018-04-20 9:22 GMT-07:00 Ivan Gerasimov <ivan.gerasimov at oracle.com 
>> <mailto:ivan.gerasimov at oracle.com>>:
>>
>>     Hello Nezih!
>>
>>     This issue is still being discussed off-list.
>>     There have been two approaches proposed so far:  1) improve the
>>     session cache and 2) provide an option to turn the cache off
>>     completely.
>>
>>     The former one is good by itself, so I filed an enhancement
>>     request [1] with a link to proposal made by Peter Levart [2].
>>     However, as this is an enhancement, it seems unlikely it's going
>>     to be backported to earlier releases of JDK.
>>
>>     With kind regards,
>>     Ivan
>>
>>     [1] https://bugs.openjdk.java.net/browse/JDK-8202086
>>     <https://bugs.openjdk.java.net/browse/JDK-8202086>
>>     [2]
>>     http://mail.openjdk.java.net/pipermail/security-dev/2017-November/016512.html
>>     <http://mail.openjdk.java.net/pipermail/security-dev/2017-November/016512.html>
>>
>>
>>     On 4/18/18 9:32 PM, nezih yigitbasi wrote:
>>>     Hi,
>>>     We are hitting the scalability problem of the SSL session cache
>>>     in production that JDK-8186628 is addressing.
>>>     I see that JDK-8186628 has not been updated since Nov'17, so I
>>>     just wanted to get information about what the current plans are
>>>     regarding that issue.
>>>
>>>     Thanks,
>>>     Nezih
>>
>>     -- 
>>     With kind regards,
>>     Ivan Gerasimov
>>
>>
>
> -- 
> With kind regards,
> Ivan Gerasimov


From peter.levart at gmail.com  Sat Apr 21 05:51:50 2018
From: peter.levart at gmail.com (Peter Levart)
Date: Sat, 21 Apr 2018 11:51:50 +0200
Subject: [concurrency-interest] about JDK-8186628
In-Reply-To: <5ADAF95D.7080802@zeus.net.au>
References: <CALur2hVXAZ+jZ9Bo0mP-i=YFqGvB3186Bjzj7mNuQR0VEn9CZg@mail.gmail.com>
 <190e738f-4196-330b-5c81-6268a6f42202@oracle.com>
 <CALur2hWUqpt_rmiCUK-NE0C+c4N5mt87UG7uJbRaStd7VGUwMg@mail.gmail.com>
 <20e6db12-dfa4-781e-4d2b-435500484d3b@oracle.com>
 <5ADAF95D.7080802@zeus.net.au>
Message-ID: <ad64d0ff-890c-6645-1979-1273c0ebb565@gmail.com>

Hi Peter,

On 04/21/18 10:42, Peter wrote:
> I wrote caching software that decorated any Java collection with soft, 
> weak, timed or strong references, it utilised a background thread to 
> clean references from underlying collections.
>
> It was non blocking if the underlying collection was, although it 
> hasn't been updated to Java 8.
>
> Regarding patch comments, I don't mean to sound rude, but I don't 
> think there's such a thing as a harmless data race.
>
> Regards,
>
> Peter.

Perhaps harmless is not the right expression, but in this case, I think 
the data race(s) can do no "harm" to the correct / intended execution of 
program logic. That's my current belief, but you can prove me wrong if 
you want ;-) The data race is between CacheEntry.[getKey, getValue] and 
CacheIntry.invalidate that resets both key & value to null. When a 
CacheEntry is published, it is published without data race, with key & 
value being non-null. CacheEntry is private class, used only in 
MemoryCache, so it is never exposed to user code. User code only sees 
the result(s) of reading CacheEntry.[getKey, getValue] (returned from 
Cache.get(key) and from Cache.getCachedEntries()). The former allows 
null returns that logically mean an absence of value while the later 
constructs a map of live entries that are or were present in the cache 
at the time of invocation, filtering out null key(s)/value(s). The 
guarantees of those two methods are enough for correct functioning of 
user code (the SSL implementation), and this is what I called "harmless" 
data race because technically it is a data race.

Regards, Peter

>
> On 21/04/2018 3:45 AM, Ivan Gerasimov wrote:
>>
>> I'll go ahead with a review of the enhancement request JDK-8202086 
>> shortly on this list.
>>
>> And we'll still need to decide what has to be done in the earlier 
>> releases of JDK.
>>
>> With kind regards,
>>
>> Ivan
>>
>>
>> <https://bugs.openjdk.java.net/browse/JDK-8202086>
>>
>>
>> On 4/20/18 10:06 AM, nezih yigitbasi wrote:
>>> Ivan, thanks for the information. Any ideas about when one of these 
>>> solutions can be released?
>>>
>>> Nezih
>>>
>>> 2018-04-20 9:22 GMT-07:00 Ivan Gerasimov <ivan.gerasimov at oracle.com 
>>> <mailto:ivan.gerasimov at oracle.com>>:
>>>
>>>     Hello Nezih!
>>>
>>>     This issue is still being discussed off-list.
>>>     There have been two approaches proposed so far:  1) improve the
>>>     session cache and 2) provide an option to turn the cache off
>>>     completely.
>>>
>>>     The former one is good by itself, so I filed an enhancement
>>>     request [1] with a link to proposal made by Peter Levart [2].
>>>     However, as this is an enhancement, it seems unlikely it's going
>>>     to be backported to earlier releases of JDK.
>>>
>>>     With kind regards,
>>>     Ivan
>>>
>>>     [1] https://bugs.openjdk.java.net/browse/JDK-8202086
>>>     <https://bugs.openjdk.java.net/browse/JDK-8202086>
>>>     [2]
>>> http://mail.openjdk.java.net/pipermail/security-dev/2017-November/016512.html
>>> <http://mail.openjdk.java.net/pipermail/security-dev/2017-November/016512.html>
>>>
>>>
>>>     On 4/18/18 9:32 PM, nezih yigitbasi wrote:
>>>> Hi,
>>>>     We are hitting the scalability problem of the SSL session cache
>>>>     in production that JDK-8186628 is addressing.
>>>>     I see that JDK-8186628 has not been updated since Nov'17, so I
>>>>     just wanted to get information about what the current plans are
>>>>     regarding that issue.
>>>>
>>>>     Thanks,
>>>>     Nezih
>>>
>>>     --     With kind regards,
>>>     Ivan Gerasimov
>>>
>>>
>>
>> -- 
>> With kind regards,
>> Ivan Gerasimov
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20180421/b61a0c76/attachment.html>

From jini at zeus.net.au  Sat Apr 21 22:30:29 2018
From: jini at zeus.net.au (Peter)
Date: Sun, 22 Apr 2018 12:30:29 +1000
Subject: [concurrency-interest] about JDK-8186628
In-Reply-To: <ad64d0ff-890c-6645-1979-1273c0ebb565@gmail.com>
References: <CALur2hVXAZ+jZ9Bo0mP-i=YFqGvB3186Bjzj7mNuQR0VEn9CZg@mail.gmail.com>
 <190e738f-4196-330b-5c81-6268a6f42202@oracle.com>
 <CALur2hWUqpt_rmiCUK-NE0C+c4N5mt87UG7uJbRaStd7VGUwMg@mail.gmail.com>
 <20e6db12-dfa4-781e-4d2b-435500484d3b@oracle.com>
 <5ADAF95D.7080802@zeus.net.au>
 <ad64d0ff-890c-6645-1979-1273c0ebb565@gmail.com>
Message-ID: <5ADBF3C5.2070804@zeus.net.au>

Thanks for your explanation.

I worked on a lot of early java code circa Java 1.2 - 1.4, converting it 
to the post Java 5 memory model.   Latent race conditions caused 
hundreds of bugs, and problems would occur in other unrelated parts of 
the code, it was very brittle, any changes to code would result in 
inexplicable test failures elsewhere.  In the end I eliminated every 
race condition I found, harmless or otherwise, using both visual 
inspection and static analysis, the code lost it's brittle nature and 
became much easier to maintain since.

Regards,

Peter.


On 21/04/2018 7:51 PM, Peter Levart wrote:
> Hi Peter,
>
> On 04/21/18 10:42, Peter wrote:
>> I wrote caching software that decorated any Java collection with 
>> soft, weak, timed or strong references, it utilised a background 
>> thread to clean references from underlying collections.
>>
>> It was non blocking if the underlying collection was, although it 
>> hasn't been updated to Java 8.
>>
>> Regarding patch comments, I don't mean to sound rude, but I don't 
>> think there's such a thing as a harmless data race.
>>
>> Regards,
>>
>> Peter.
>
> Perhaps harmless is not the right expression, but in this case, I 
> think the data race(s) can do no "harm" to the correct / intended 
> execution of program logic. That's my current belief, but you can 
> prove me wrong if you want ;-) The data race is between 
> CacheEntry.[getKey, getValue] and CacheIntry.invalidate that resets 
> both key & value to null. When a CacheEntry is published, it is 
> published without data race, with key & value being non-null. 
> CacheEntry is private class, used only in MemoryCache, so it is never 
> exposed to user code. User code only sees the result(s) of reading 
> CacheEntry.[getKey, getValue] (returned from Cache.get(key) and from 
> Cache.getCachedEntries()). The former allows null returns that 
> logically mean an absence of value while the later constructs a map of 
> live entries that are or were present in the cache at the time of 
> invocation, filtering out null key(s)/value(s). The guarantees of 
> those two methods are enough for correct functioning of user code (the 
> SSL implementation), and this is what I called "harmless" data race 
> because technically it is a data race.
>
> Regards, Peter
>
>>
>> On 21/04/2018 3:45 AM, Ivan Gerasimov wrote:
>>>
>>> I'll go ahead with a review of the enhancement request JDK-8202086 
>>> shortly on this list.
>>>
>>> And we'll still need to decide what has to be done in the earlier 
>>> releases of JDK.
>>>
>>> With kind regards,
>>>
>>> Ivan
>>>
>>>
>>> <https://bugs.openjdk.java.net/browse/JDK-8202086>
>>>
>>>
>>> On 4/20/18 10:06 AM, nezih yigitbasi wrote:
>>>> Ivan, thanks for the information. Any ideas about when one of these 
>>>> solutions can be released?
>>>>
>>>> Nezih
>>>>
>>>> 2018-04-20 9:22 GMT-07:00 Ivan Gerasimov <ivan.gerasimov at oracle.com 
>>>> <mailto:ivan.gerasimov at oracle.com>>:
>>>>
>>>>     Hello Nezih!
>>>>
>>>>     This issue is still being discussed off-list.
>>>>     There have been two approaches proposed so far:  1) improve the
>>>>     session cache and 2) provide an option to turn the cache off
>>>>     completely.
>>>>
>>>>     The former one is good by itself, so I filed an enhancement
>>>>     request [1] with a link to proposal made by Peter Levart [2].
>>>>     However, as this is an enhancement, it seems unlikely it's going
>>>>     to be backported to earlier releases of JDK.
>>>>
>>>>     With kind regards,
>>>>     Ivan
>>>>
>>>>     [1] https://bugs.openjdk.java.net/browse/JDK-8202086
>>>> <https://bugs.openjdk.java.net/browse/JDK-8202086>
>>>>     [2]
>>>> http://mail.openjdk.java.net/pipermail/security-dev/2017-November/016512.html
>>>> <http://mail.openjdk.java.net/pipermail/security-dev/2017-November/016512.html>
>>>>
>>>>
>>>>     On 4/18/18 9:32 PM, nezih yigitbasi wrote:
>>>>>     Hi,
>>>>>     We are hitting the scalability problem of the SSL session cache
>>>>>     in production that JDK-8186628 is addressing.
>>>>>     I see that JDK-8186628 has not been updated since Nov'17, so I
>>>>>     just wanted to get information about what the current plans are
>>>>>     regarding that issue.
>>>>>
>>>>>     Thanks,
>>>>>     Nezih
>>>>
>>>>     --     With kind regards,
>>>>     Ivan Gerasimov
>>>>
>>>>
>>>
>>> -- 
>>> With kind regards,
>>> Ivan Gerasimov
>>
>


From dl at cs.oswego.edu  Sun Apr 22 08:59:12 2018
From: dl at cs.oswego.edu (Doug Lea)
Date: Sun, 22 Apr 2018 08:59:12 -0400
Subject: [concurrency-interest] ForkJoinPool.managedBlock
In-Reply-To: <CANPzfU_XkzgmPLmaUDpYbM4-=PGSm6tvoLi3ffmoBtzq+3Q40w@mail.gmail.com>
References: <CANPzfU8wj+SLFA_r3ZPKxDsi2KPFE228ZVbrgtZszGkEEp=MFg@mail.gmail.com>
 <e1068f88-2895-0c87-0875-1da9f420ae1d@cs.oswego.edu>
 <CANPzfU9Y10EDqhPp6i7bcMYqeidGX_smD573a42+EMrRJqaWCg@mail.gmail.com>
 <068f4293-65b6-b1aa-e25d-25ef9eb4d544@cs.oswego.edu>
 <CANPzfU-=WYGajksbwwXohWT1TEoF32cKxJkNzhYcNm-nmUX_aQ@mail.gmail.com>
 <CANPzfU-N38JS+GdOzA3BgeoJFYWsFbbw_uGdohbABGgU1EvChA@mail.gmail.com>
 <8f7a03ca-f657-3a04-1fd0-28c8dc6d84b4@cs.oswego.edu>
 <CANPzfU9Z5Wq=wJUhWudQ-HTFT5AM7bnttp4mWAWzkuoVdAQ7UQ@mail.gmail.com>
 <CANPzfU_XkzgmPLmaUDpYbM4-=PGSm6tvoLi3ffmoBtzq+3Q40w@mail.gmail.com>
Message-ID: <af3b0223-d3fc-664e-5132-1742b02ab515@cs.oswego.edu>

On 04/18/2018 10:16 AM, Viktor Klang wrote:
> Quick followup question:
> 
> The javadoc of ManagedBlocker
> <https://docs.oracle.com/javase/9/docs/api/java/util/concurrent/ForkJoinPool.ManagedBlocker.html> doesn't
> really mention any thread safety requirements, yet the examples seems to
> indicate that they are needed (for instance, the volatile `item` in the
> example code.

Not sure how to answer...

> 
> Can it be safely assumed that the only thread which will invoke any
> methods on the instance of the ManagedBlocker is the thread calling
> managedBlock(blocker)? 

FJP.managedBlock() calls ManagedBlocker.block() in the current thread.
The user implementation of block() must deal with other threads or IO.

I can't think of any javadoc that would further clarify?

-Doug



From viktor.klang at gmail.com  Mon Apr 23 03:14:14 2018
From: viktor.klang at gmail.com (Viktor Klang)
Date: Mon, 23 Apr 2018 09:14:14 +0200
Subject: [concurrency-interest] ForkJoinPool.managedBlock
In-Reply-To: <af3b0223-d3fc-664e-5132-1742b02ab515@cs.oswego.edu>
References: <CANPzfU8wj+SLFA_r3ZPKxDsi2KPFE228ZVbrgtZszGkEEp=MFg@mail.gmail.com>
 <e1068f88-2895-0c87-0875-1da9f420ae1d@cs.oswego.edu>
 <CANPzfU9Y10EDqhPp6i7bcMYqeidGX_smD573a42+EMrRJqaWCg@mail.gmail.com>
 <068f4293-65b6-b1aa-e25d-25ef9eb4d544@cs.oswego.edu>
 <CANPzfU-=WYGajksbwwXohWT1TEoF32cKxJkNzhYcNm-nmUX_aQ@mail.gmail.com>
 <CANPzfU-N38JS+GdOzA3BgeoJFYWsFbbw_uGdohbABGgU1EvChA@mail.gmail.com>
 <8f7a03ca-f657-3a04-1fd0-28c8dc6d84b4@cs.oswego.edu>
 <CANPzfU9Z5Wq=wJUhWudQ-HTFT5AM7bnttp4mWAWzkuoVdAQ7UQ@mail.gmail.com>
 <CANPzfU_XkzgmPLmaUDpYbM4-=PGSm6tvoLi3ffmoBtzq+3Q40w@mail.gmail.com>
 <af3b0223-d3fc-664e-5132-1742b02ab515@cs.oswego.edu>
Message-ID: <CANPzfU9=no+v4gww4w1Qvqbg+e40A_gVCY-G8QGwNt9jF3L_Bg@mail.gmail.com>

On Sun, Apr 22, 2018, 14:59 Doug Lea <dl at cs.oswego.edu> wrote:

> On 04/18/2018 10:16 AM, Viktor Klang wrote:
> > Quick followup question:
> >
> > The javadoc of ManagedBlocker
> > <https://docs.oracle.com/javase/9/docs/api/java/util/
> concurrent/ForkJoinPool.ManagedBlocker.html> doesn't
> > really mention any thread safety requirements, yet the examples seems to
> > indicate that they are needed (for instance, the volatile `item` in the
> > example code.
>
> Not sure how to answer...
>
>
Apologies, either I was unclear or the answer is so obvious that I missed
it :)


> >
> > Can it be safely assumed that the only thread which will invoke any
> > methods on the instance of the ManagedBlocker is the thread calling
> > managedBlock(blocker)?
>
> FJP.managedBlock() calls ManagedBlocker.block() in the current thread.
> The user implementation of block() must deal with other threads or IO.
>

Is this a quote from the documentation?


> I can't think of any javadoc that would further clarify?
>

The reason I ask is mainly because I didn't want to rely on the
implementation as the carrier of the spec, and perhaps I missed it, but
couldn't find a clear answer in the doc for managedBlock() or in
ManagedBlocker itself.

The situation was further a bit complicated by the two examples in
ManagedBlocker where in one "hasLock" is a plain field, and "item" is a
volatile field.

As it wouldn't be completely weird to have the ManagedBlocker handed over
in tryCompensate for the compensating thread to test periodically to see
whether what blocking it is compensating for is done yet (isReleasable()),
hence the question. :-)

However—given your answer—I'll assume that it is only ever the calling
thread of managedBlock() who will be invoking methods on it.

Also, it seems the contract is that managedBlocker is not "allowed" to call
block() if isReleasable() returned true, and it isn't "allowed" to call
block() is it has ever returned true. Will these invariants hold by spec
rather than implementation? (Just making sure I have the right guards in
place in my implementations.)


> -Doug
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20180423/be840515/attachment.html>

From alarmnummer at gmail.com  Fri Apr 27 00:43:20 2018
From: alarmnummer at gmail.com (Peter Veentjer)
Date: Fri, 27 Apr 2018 07:43:20 +0300
Subject: [concurrency-interest] CompletableFuture and cloning results
Message-ID: <CAGuAWdC7sqvq+zsWtPpsZeNtCBBTJbB0pwnxHQ=uVwuQYP6brQ@mail.gmail.com>

Hello,

what is the best way to clone/deserialize a result set on a
CompletableFuture?

So I want to be able to set a serialized value on a CompletableFuture, and
each get/join etc needs to deserialize the value so that each call to
get/join will lead to a new object.

We are currently using a custom Future implementation (Java 6
unfortunately) for remote invocations and I'm looking at how to switch to
the CompletableFuture.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20180427/e014e19c/attachment.html>

From martinrb at google.com  Fri Apr 27 00:56:16 2018
From: martinrb at google.com (Martin Buchholz)
Date: Thu, 26 Apr 2018 21:56:16 -0700
Subject: [concurrency-interest] CompletableFuture and cloning results
In-Reply-To: <CAGuAWdC7sqvq+zsWtPpsZeNtCBBTJbB0pwnxHQ=uVwuQYP6brQ@mail.gmail.com>
References: <CAGuAWdC7sqvq+zsWtPpsZeNtCBBTJbB0pwnxHQ=uVwuQYP6brQ@mail.gmail.com>
Message-ID: <CA+kOe0_GUTcwUnYtS3AP6DSe+j+eNquroxEavW9V-5B7yX9gBQ@mail.gmail.com>

There's a strong expectation that future.get() == future.get() for any
Future.

You probably want to build a new abstraction on top of an existing Future
implementation, like CompletableFuture.

On Thu, Apr 26, 2018 at 9:43 PM, Peter Veentjer via Concurrency-interest <
concurrency-interest at cs.oswego.edu> wrote:

> Hello,
>
> what is the best way to clone/deserialize a result set on a
> CompletableFuture?
>
> So I want to be able to set a serialized value on a CompletableFuture, and
> each get/join etc needs to deserialize the value so that each call to
> get/join will lead to a new object.
>
> We are currently using a custom Future implementation (Java 6
> unfortunately) for remote invocations and I'm looking at how to switch to
> the CompletableFuture.
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20180426/6298f897/attachment.html>

From alexei.kaigorodov at gmail.com  Fri Apr 27 17:41:00 2018
From: alexei.kaigorodov at gmail.com (Alexei Kaigorodov)
Date: Fri, 27 Apr 2018 14:41:00 -0700 (MST)
Subject: [concurrency-interest] CompletableFuture and cloning results
In-Reply-To: <CAGuAWdC7sqvq+zsWtPpsZeNtCBBTJbB0pwnxHQ=uVwuQYP6brQ@mail.gmail.com>
References: <CAGuAWdC7sqvq+zsWtPpsZeNtCBBTJbB0pwnxHQ=uVwuQYP6brQ@mail.gmail.com>
Message-ID: <1524865260370-0.post@n7.nabble.com>

You need not to deserialize your object each time - you can use
Object#clone() to get different instances.
That is, given you have 
CompletableFuture myFuture = ...;
just call:
myInstance = myFuture.get().clone();

Or override the method get():

class CloningCompletableFuture<T> extends CompletableFuture<T> {
    T get() {
       return super.get().clone();
    }
}
   
CompletableFuture myFuture = new CloningCompletableFuture();
...
myInstance = myFuture.get();




--
Sent from: http://jsr166-concurrency.10961.n7.nabble.com/

