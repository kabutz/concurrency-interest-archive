From nitsanw at yahoo.com  Fri Aug  2 05:43:08 2013
From: nitsanw at yahoo.com (Nitsan Wakart)
Date: Fri, 2 Aug 2013 02:43:08 -0700 (PDT)
Subject: [concurrency-interest] x86 NOOP memory barriers
Message-ID: <1375436588.50646.YahooMailNeo@web120701.mail.ne1.yahoo.com>

Hi,
For clarity's sake I'd like an official explanation for the often quoted "all barriers except STORE/LOAD are a no-op on x86" statement from the JMM cookbook.
Can someone (of authority, so I can later say: "But Mr. Authority here says...") please confirm/expand on/deny that while a volatile read or an AtomicLong.lazySet are a CPU noop (in the sense that they are a MOV like any other), they are also compiler instructions. One cannot simply replace a lazySet with a plain write to get the same effect. They might be cheap but they ain't free...?
I would appreciate some more careful wording on this topic.
Many thanks,
Nitsan


From vitalyd at gmail.com  Fri Aug  2 06:46:08 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Fri, 2 Aug 2013 06:46:08 -0400
Subject: [concurrency-interest] x86 NOOP memory barriers
In-Reply-To: <1375436588.50646.YahooMailNeo@web120701.mail.ne1.yahoo.com>
References: <1375436588.50646.YahooMailNeo@web120701.mail.ne1.yahoo.com>
Message-ID: <CAHjP37GGbpshebzxy8PF_+ZPKvJpc=1=RWsCmwCO-89XLdf1Lw@mail.gmail.com>

I'm not an authority so take this for what it's worth...

Yes, volatile loads and lazySet do not cause any cpu fence/barrier
instructions to be generated - in that sense, they're nop at the hardware
level.  However, they are also compiler barriers, which is where the "cheap
but aint free" phrase may apply.  The compiler cannot reorder these
instructions in ways that violate their documented/spec'd memory ordering
effects.  So for example, a plain store followed by lazySet cannot actually
be moved after the lazySet; whereas if you have two plain stores, the
compiler can technically reorder them as it sees fit (if we look at just
them two and disregard other surrounding code).

So, it may happen that compiler cannot do certain code motion/optimizations
due to these compiler fences and therefore you have some penalty vs using
plain load and stores.  For volatile loads, compiler cannot enregister the
value like it would with plain load, but even this may not have noticeable
perf diff if the data is in L1 dcache, for example.

HTH,
Vitaly

Sent from my phone
On Aug 2, 2013 5:55 AM, "Nitsan Wakart" <nitsanw at yahoo.com> wrote:

> Hi,
> For clarity's sake I'd like an official explanation for the often quoted
> "all barriers except STORE/LOAD are a no-op on x86" statement from the JMM
> cookbook.
> Can someone (of authority, so I can later say: "But Mr. Authority here
> says...") please confirm/expand on/deny that while a volatile read or an
> AtomicLong.lazySet are a CPU noop (in the sense that they are a MOV like
> any other), they are also compiler instructions. One cannot simply replace
> a lazySet with a plain write to get the same effect. They might be cheap
> but they ain't free...
> I would appreciate some more careful wording on this topic.
> Many thanks,
> Nitsan
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130802/b1ec637a/attachment.html>

From nitsanw at yahoo.com  Fri Aug  2 07:25:21 2013
From: nitsanw at yahoo.com (Nitsan Wakart)
Date: Fri, 2 Aug 2013 04:25:21 -0700 (PDT)
Subject: [concurrency-interest] x86 NOOP memory barriers
In-Reply-To: <CAHjP37GGbpshebzxy8PF_+ZPKvJpc=1=RWsCmwCO-89XLdf1Lw@mail.gmail.com>
References: <1375436588.50646.YahooMailNeo@web120701.mail.ne1.yahoo.com>
	<CAHjP37GGbpshebzxy8PF_+ZPKvJpc=1=RWsCmwCO-89XLdf1Lw@mail.gmail.com>
Message-ID: <1375442721.83454.YahooMailNeo@web120703.mail.ne1.yahoo.com>

Can the writes get re-ordered on the hardware level?

Don't volatile reads(LOAD/LOAD) also require reads to not get re-ordered to maintain happens-before/after relationships?


________________________________
 From: Vitaly Davidovich <vitalyd at gmail.com>
To: Nitsan Wakart <nitsanw at yahoo.com> 
Cc: Concurrency Interest <concurrency-interest at cs.oswego.edu> 
Sent: Friday, August 2, 2013 12:46 PM
Subject: Re: [concurrency-interest] x86 NOOP memory barriers
 


I'm not an authority so take this for what it's worth...
Yes, volatile loads and lazySet do not cause any cpu fence/barrier instructions to be generated - in that sense, they're nop at the hardware level.? However, they are also compiler barriers, which is where the "cheap but aint free" phrase may apply.? The compiler cannot reorder these instructions in ways that violate their documented/spec'd memory ordering effects.? So for example, a plain store followed by lazySet cannot actually be moved after the lazySet; whereas if you have two plain stores, the compiler can technically reorder them as it sees fit (if we look at just them two and disregard other surrounding code).
So, it may happen that compiler cannot do certain code motion/optimizations due to these compiler fences and therefore you have some penalty vs using plain load and stores.? For volatile loads, compiler cannot enregister the value like it would with plain load, but even this may not have noticeable perf diff if the data is in L1 dcache, for example.
HTH,
Vitaly
Sent from my phone
On Aug 2, 2013 5:55 AM, "Nitsan Wakart" <nitsanw at yahoo.com> wrote:

Hi,
>For clarity's sake I'd like an official explanation for the often quoted "all barriers except STORE/LOAD are a no-op on x86" statement from the JMM cookbook.
>Can someone (of authority, so I can later say: "But Mr. Authority here says...") please confirm/expand on/deny that while a volatile read or an AtomicLong.lazySet are a CPU noop (in the sense that they are a MOV like any other), they are also compiler instructions. One cannot simply replace a lazySet with a plain write to get the same effect. They might be cheap but they ain't free...?
>I would appreciate some more careful wording on this topic.
>Many thanks,
>Nitsan
>
>_______________________________________________
>Concurrency-interest mailing list
>Concurrency-interest at cs.oswego.edu
>http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130802/30500d32/attachment.html>

From mikeb01 at gmail.com  Fri Aug  2 08:15:36 2013
From: mikeb01 at gmail.com (Michael Barker)
Date: Sat, 3 Aug 2013 00:15:36 +1200
Subject: [concurrency-interest] x86 NOOP memory barriers
In-Reply-To: <1375442721.83454.YahooMailNeo@web120703.mail.ne1.yahoo.com>
References: <1375436588.50646.YahooMailNeo@web120701.mail.ne1.yahoo.com>
	<CAHjP37GGbpshebzxy8PF_+ZPKvJpc=1=RWsCmwCO-89XLdf1Lw@mail.gmail.com>
	<1375442721.83454.YahooMailNeo@web120703.mail.ne1.yahoo.com>
Message-ID: <CALwNKeSrxua6MpLktrKgFaDTKCuacWmv_ZpRZdA3R33FsO04HA@mail.gmail.com>

Hi Nitsan,

In short, no.  The place to look (as definitive as it gets for the
hardware) is section 8.2.2, volume 3A of the Intel programmer
manual[0].  It lists the rules that are applied regarding the
reordering of instructions under the X86 memory model.  I've
summarised the relevant ones here:

- Reads are not reordered with other reads.
- Writes are not reordered with older reads.
- Writes to memory are not reordered with other writes*.
- Reads may be reordered with older writes to different locations but
not with older writes to the same location.

The only reordering that will occur with x86 is allowing reads to be
executed before writes (to other locations), hence the need for a
LOCKed instruction to enforce the store/load barrier.  As you can see
with the above rules, store are not reordered with older stores and
loads are not reordered with older loads so a series of MOV
instructions is sufficient for a store/store or a load/load barrier.

I'm not really an authority either, but you'll be fairly safe
referencing in the Intel manuals with regards to the hardware
behaviour.

*There are exceptions with regards to writes that come into play when
using specific instructions, but won't be a factor here.  E.g. you can
use specific non-temporal stores to subvert the normal cache-coherency
rules.

Mike.

[0] http://www.intel.com/content/www/us/en/processors/architectures-software-developer-manuals.html


On 2 August 2013 23:25, Nitsan Wakart <nitsanw at yahoo.com> wrote:
> Can the writes get re-ordered on the hardware level?
> Don't volatile reads(LOAD/LOAD) also require reads to not get re-ordered to
> maintain happens-before/after relationships?
>
> ________________________________
> From: Vitaly Davidovich <vitalyd at gmail.com>
> To: Nitsan Wakart <nitsanw at yahoo.com>
> Cc: Concurrency Interest <concurrency-interest at cs.oswego.edu>
> Sent: Friday, August 2, 2013 12:46 PM
> Subject: Re: [concurrency-interest] x86 NOOP memory barriers
>
> I'm not an authority so take this for what it's worth...
> Yes, volatile loads and lazySet do not cause any cpu fence/barrier
> instructions to be generated - in that sense, they're nop at the hardware
> level.  However, they are also compiler barriers, which is where the "cheap
> but aint free" phrase may apply.  The compiler cannot reorder these
> instructions in ways that violate their documented/spec'd memory ordering
> effects.  So for example, a plain store followed by lazySet cannot actually
> be moved after the lazySet; whereas if you have two plain stores, the
> compiler can technically reorder them as it sees fit (if we look at just
> them two and disregard other surrounding code).
> So, it may happen that compiler cannot do certain code motion/optimizations
> due to these compiler fences and therefore you have some penalty vs using
> plain load and stores.  For volatile loads, compiler cannot enregister the
> value like it would with plain load, but even this may not have noticeable
> perf diff if the data is in L1 dcache, for example.
> HTH,
> Vitaly
> Sent from my phone
> On Aug 2, 2013 5:55 AM, "Nitsan Wakart" <nitsanw at yahoo.com> wrote:
>
> Hi,
> For clarity's sake I'd like an official explanation for the often quoted
> "all barriers except STORE/LOAD are a no-op on x86" statement from the JMM
> cookbook.
> Can someone (of authority, so I can later say: "But Mr. Authority here
> says...") please confirm/expand on/deny that while a volatile read or an
> AtomicLong.lazySet are a CPU noop (in the sense that they are a MOV like any
> other), they are also compiler instructions. One cannot simply replace a
> lazySet with a plain write to get the same effect. They might be cheap but
> they ain't free...
> I would appreciate some more careful wording on this topic.
> Many thanks,
> Nitsan
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

From aph at redhat.com  Fri Aug  2 08:21:31 2013
From: aph at redhat.com (Andrew Haley)
Date: Fri, 02 Aug 2013 13:21:31 +0100
Subject: [concurrency-interest] x86 NOOP memory barriers
In-Reply-To: <1375436588.50646.YahooMailNeo@web120701.mail.ne1.yahoo.com>
References: <1375436588.50646.YahooMailNeo@web120701.mail.ne1.yahoo.com>
Message-ID: <51FBA44B.2000409@redhat.com>

On 08/02/2013 10:43 AM, Nitsan Wakart wrote:
> Hi,
> For clarity's sake I'd like an official explanation for the often quoted "all barriers except STORE/LOAD are a no-op on x86" statement from the JMM cookbook.
> Can someone (of authority, so I can later say: "But Mr. Authority here says...") please confirm/expand on/deny that while a volatile read or an AtomicLong.lazySet are a CPU noop (in the sense that they are a MOV like any other), they are also compiler instructions. One cannot simply replace a lazySet with a plain write to get the same effect. They might be cheap but they ain't free... 
> I would appreciate some more careful wording on this topic.


Intel? 64 Architecture Memory Ordering White Paper
This document has been merged into Volume 3A of Intel 64 and IA-32 Architectures Software Developer?s Manual.

http://download.intel.com/products/processor/manual/253668.pdf
Section 8.2



From nitsanw at yahoo.com  Fri Aug  2 09:15:42 2013
From: nitsanw at yahoo.com (Nitsan Wakart)
Date: Fri, 2 Aug 2013 06:15:42 -0700 (PDT)
Subject: [concurrency-interest] x86 NOOP memory barriers
In-Reply-To: <CALwNKeSrxua6MpLktrKgFaDTKCuacWmv_ZpRZdA3R33FsO04HA@mail.gmail.com>
References: <1375436588.50646.YahooMailNeo@web120701.mail.ne1.yahoo.com>	<CAHjP37GGbpshebzxy8PF_+ZPKvJpc=1=RWsCmwCO-89XLdf1Lw@mail.gmail.com>	<1375442721.83454.YahooMailNeo@web120703.mail.ne1.yahoo.com>
	<CALwNKeSrxua6MpLktrKgFaDTKCuacWmv_ZpRZdA3R33FsO04HA@mail.gmail.com>
Message-ID: <1375449342.6944.YahooMailNeo@web120701.mail.ne1.yahoo.com>

So because a putOrdered is a write to memory it cannot be reordered with other writes, as per "8.2.3.2 Neither Loads Nor Stores Are Reordered with Like Operations".

________________________________
 From: Michael Barker <mikeb01 at gmail.com>
To: Nitsan Wakart <nitsanw at yahoo.com> 
Cc: Vitaly Davidovich <vitalyd at gmail.com>; "concurrency-interest at cs.oswego.edu" <concurrency-interest at cs.oswego.edu> 
Sent: Friday, August 2, 2013 2:15 PM
Subject: Re: [concurrency-interest] x86 NOOP memory barriers
 

Hi Nitsan,

In short, no.? The place to look (as definitive as it gets for the
hardware) is section 8.2.2, volume 3A of the Intel programmer
manual[0].? It lists the rules that are applied regarding the
reordering of instructions under the X86 memory model.? I've
summarised the relevant ones here:

- Reads are not reordered with other reads.
- Writes are not reordered with older reads.
- Writes to memory are not reordered with other writes*.
- Reads may be reordered with older writes to different locations but
not with older writes to the same location.

The only reordering that will occur with x86 is allowing reads to be
executed before writes (to other locations), hence the need for a
LOCKed instruction to enforce the store/load barrier.? As you can see
with the above rules, store are not reordered with older stores and
loads are not reordered with older loads so a series of MOV
instructions is sufficient for a store/store or a load/load barrier.

I'm not really an authority either, but you'll be fairly safe
referencing in the Intel manuals with regards to the hardware
behaviour.

*There are exceptions with regards to writes that come into play when
using specific instructions, but won't be a factor here.? E.g. you can
use specific non-temporal stores to subvert the normal cache-coherency
rules.

Mike.

[0] http://www.intel.com/content/www/us/en/processors/architectures-software-developer-manuals.html


On 2 August 2013 23:25, Nitsan Wakart <nitsanw at yahoo.com> wrote:
> Can the writes get re-ordered on the hardware level?
> Don't volatile reads(LOAD/LOAD) also require reads to not get re-ordered to
> maintain happens-before/after relationships?
>
> ________________________________
> From: Vitaly Davidovich <vitalyd at gmail.com>
> To: Nitsan Wakart <nitsanw at yahoo.com>
> Cc: Concurrency Interest <concurrency-interest at cs.oswego.edu>
> Sent: Friday, August 2, 2013 12:46 PM
> Subject: Re: [concurrency-interest] x86 NOOP memory barriers
>
> I'm not an authority so take this for what it's worth...
> Yes, volatile loads and lazySet do not cause any cpu fence/barrier
> instructions to be generated - in that sense, they're nop at the hardware
> level.? However, they are also compiler barriers, which is where the "cheap
> but aint free" phrase may apply.? The compiler cannot reorder these
> instructions in ways that violate their documented/spec'd memory ordering
> effects.? So for example, a plain store followed by lazySet cannot actually
> be moved after the lazySet; whereas if you have two plain stores, the
> compiler can technically reorder them as it sees fit (if we look at just
> them two and disregard other surrounding code).
> So, it may happen that compiler cannot do certain code motion/optimizations
> due to these compiler fences and therefore you have some penalty vs using
> plain load and stores.? For volatile loads, compiler cannot enregister the
> value like it would with plain load, but even this may not have noticeable
> perf diff if the data is in L1 dcache, for example.
> HTH,
> Vitaly
> Sent from my phone
> On Aug 2, 2013 5:55 AM, "Nitsan Wakart" <nitsanw at yahoo.com> wrote:
>
> Hi,
> For clarity's sake I'd like an official explanation for the often quoted
> "all barriers except STORE/LOAD are a no-op on x86" statement from the JMM
> cookbook.
> Can someone (of authority, so I can later say: "But Mr. Authority here
> says...") please confirm/expand on/deny that while a volatile read or an
> AtomicLong.lazySet are a CPU noop (in the sense that they are a MOV like any
> other), they are also compiler instructions. One cannot simply replace a
> lazySet with a plain write to get the same effect. They might be cheap but
> they ain't free...
> I would appreciate some more careful wording on this topic.
> Many thanks,
> Nitsan
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130802/3341238c/attachment.html>

From nathan.reynolds at oracle.com  Fri Aug  2 12:43:56 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Fri, 02 Aug 2013 09:43:56 -0700
Subject: [concurrency-interest] x86 NOOP memory barriers
In-Reply-To: <CALwNKeSrxua6MpLktrKgFaDTKCuacWmv_ZpRZdA3R33FsO04HA@mail.gmail.com>
References: <1375436588.50646.YahooMailNeo@web120701.mail.ne1.yahoo.com>
	<CAHjP37GGbpshebzxy8PF_+ZPKvJpc=1=RWsCmwCO-89XLdf1Lw@mail.gmail.com>
	<1375442721.83454.YahooMailNeo@web120703.mail.ne1.yahoo.com>
	<CALwNKeSrxua6MpLktrKgFaDTKCuacWmv_ZpRZdA3R33FsO04HA@mail.gmail.com>
Message-ID: <51FBE1CC.4040907@oracle.com>

Just to add, if the address of a load or store is not known yet (i.e. 
the core is still computing it), then no loads may reorder in front of 
it.  This is because if the store and load are for the same address then 
the load needs to pick up the value of the store.

Ignore any fences... Let's say there is a store sitting in the 
load/store buffer inside the core.  Let's say the address for the store 
has been computed.  Could a load from that same address simply use the 
value that is going to be stored?  Just checking if my understanding is 
correct.

Ignore any fences... Let's say there are 2 adjacent stores sitting in 
the load/store buffer inside the core.  Let's say the addresses for the 
stores have been computed and are identical.  Could the stores be 
combined into 1 store so that only the latter store actually pushes its 
data to L1D cache?  Again, just checking if my understanding is correct.

-Nathan

On 8/2/2013 5:15 AM, Michael Barker wrote:
> Hi Nitsan,
>
> In short, no.  The place to look (as definitive as it gets for the
> hardware) is section 8.2.2, volume 3A of the Intel programmer
> manual[0].  It lists the rules that are applied regarding the
> reordering of instructions under the X86 memory model.  I've
> summarised the relevant ones here:
>
> - Reads are not reordered with other reads.
> - Writes are not reordered with older reads.
> - Writes to memory are not reordered with other writes*.
> - Reads may be reordered with older writes to different locations but
> not with older writes to the same location.
>
> The only reordering that will occur with x86 is allowing reads to be
> executed before writes (to other locations), hence the need for a
> LOCKed instruction to enforce the store/load barrier.  As you can see
> with the above rules, store are not reordered with older stores and
> loads are not reordered with older loads so a series of MOV
> instructions is sufficient for a store/store or a load/load barrier.
>
> I'm not really an authority either, but you'll be fairly safe
> referencing in the Intel manuals with regards to the hardware
> behaviour.
>
> *There are exceptions with regards to writes that come into play when
> using specific instructions, but won't be a factor here.  E.g. you can
> use specific non-temporal stores to subvert the normal cache-coherency
> rules.
>
> Mike.
>
> [0] http://www.intel.com/content/www/us/en/processors/architectures-software-developer-manuals.html
>
>
> On 2 August 2013 23:25, Nitsan Wakart <nitsanw at yahoo.com> wrote:
>> Can the writes get re-ordered on the hardware level?
>> Don't volatile reads(LOAD/LOAD) also require reads to not get re-ordered to
>> maintain happens-before/after relationships?
>>
>> ________________________________
>> From: Vitaly Davidovich <vitalyd at gmail.com>
>> To: Nitsan Wakart <nitsanw at yahoo.com>
>> Cc: Concurrency Interest <concurrency-interest at cs.oswego.edu>
>> Sent: Friday, August 2, 2013 12:46 PM
>> Subject: Re: [concurrency-interest] x86 NOOP memory barriers
>>
>> I'm not an authority so take this for what it's worth...
>> Yes, volatile loads and lazySet do not cause any cpu fence/barrier
>> instructions to be generated - in that sense, they're nop at the hardware
>> level.  However, they are also compiler barriers, which is where the "cheap
>> but aint free" phrase may apply.  The compiler cannot reorder these
>> instructions in ways that violate their documented/spec'd memory ordering
>> effects.  So for example, a plain store followed by lazySet cannot actually
>> be moved after the lazySet; whereas if you have two plain stores, the
>> compiler can technically reorder them as it sees fit (if we look at just
>> them two and disregard other surrounding code).
>> So, it may happen that compiler cannot do certain code motion/optimizations
>> due to these compiler fences and therefore you have some penalty vs using
>> plain load and stores.  For volatile loads, compiler cannot enregister the
>> value like it would with plain load, but even this may not have noticeable
>> perf diff if the data is in L1 dcache, for example.
>> HTH,
>> Vitaly
>> Sent from my phone
>> On Aug 2, 2013 5:55 AM, "Nitsan Wakart" <nitsanw at yahoo.com> wrote:
>>
>> Hi,
>> For clarity's sake I'd like an official explanation for the often quoted
>> "all barriers except STORE/LOAD are a no-op on x86" statement from the JMM
>> cookbook.
>> Can someone (of authority, so I can later say: "But Mr. Authority here
>> says...") please confirm/expand on/deny that while a volatile read or an
>> AtomicLong.lazySet are a CPU noop (in the sense that they are a MOV like any
>> other), they are also compiler instructions. One cannot simply replace a
>> lazySet with a plain write to get the same effect. They might be cheap but
>> they ain't free...
>> I would appreciate some more careful wording on this topic.
>> Many thanks,
>> Nitsan
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130802/6411490e/attachment.html>

From vitalyd at gmail.com  Fri Aug  2 13:28:08 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Fri, 2 Aug 2013 13:28:08 -0400
Subject: [concurrency-interest] x86 NOOP memory barriers
In-Reply-To: <51FBE1CC.4040907@oracle.com>
References: <1375436588.50646.YahooMailNeo@web120701.mail.ne1.yahoo.com>
	<CAHjP37GGbpshebzxy8PF_+ZPKvJpc=1=RWsCmwCO-89XLdf1Lw@mail.gmail.com>
	<1375442721.83454.YahooMailNeo@web120703.mail.ne1.yahoo.com>
	<CALwNKeSrxua6MpLktrKgFaDTKCuacWmv_ZpRZdA3R33FsO04HA@mail.gmail.com>
	<51FBE1CC.4040907@oracle.com>
Message-ID: <CAHjP37Ff8xG2gu36qQDRMXb+SdAN4e7OAFgGh4K7Gg-PVF7ZLg@mail.gmail.com>

I think store-load forwarding can satisfy a load from store buffer.

Write combining store buffer will combine stores to same cacheline.  I
don't think writes go into the store buffer before their address is
computed, so don't think you'll have two adjacent stores to same address
sitting there at same time (I could be wrong on that though but seems like
core should know address by now since it would've detected a cache miss on
it and thus dropped the store into the buffer).

Sent from my phone
On Aug 2, 2013 12:58 PM, "Nathan Reynolds" <nathan.reynolds at oracle.com>
wrote:

>  Just to add, if the address of a load or store is not known yet (i.e.
> the core is still computing it), then no loads may reorder in front of it.
> This is because if the store and load are for the same address then the
> load needs to pick up the value of the store.
>
> Ignore any fences... Let's say there is a store sitting in the load/store
> buffer inside the core.  Let's say the address for the store has been
> computed.  Could a load from that same address simply use the value that is
> going to be stored?  Just checking if my understanding is correct.
>
> Ignore any fences... Let's say there are 2 adjacent stores sitting in the
> load/store buffer inside the core.  Let's say the addresses for the stores
> have been computed and are identical.  Could the stores be combined into 1
> store so that only the latter store actually pushes its data to L1D cache?
> Again, just checking if my understanding is correct.
>
> -Nathan
>
> On 8/2/2013 5:15 AM, Michael Barker wrote:
>
> Hi Nitsan,
>
> In short, no.  The place to look (as definitive as it gets for the
> hardware) is section 8.2.2, volume 3A of the Intel programmer
> manual[0].  It lists the rules that are applied regarding the
> reordering of instructions under the X86 memory model.  I've
> summarised the relevant ones here:
>
> - Reads are not reordered with other reads.
> - Writes are not reordered with older reads.
> - Writes to memory are not reordered with other writes*.
> - Reads may be reordered with older writes to different locations but
> not with older writes to the same location.
>
> The only reordering that will occur with x86 is allowing reads to be
> executed before writes (to other locations), hence the need for a
> LOCKed instruction to enforce the store/load barrier.  As you can see
> with the above rules, store are not reordered with older stores and
> loads are not reordered with older loads so a series of MOV
> instructions is sufficient for a store/store or a load/load barrier.
>
> I'm not really an authority either, but you'll be fairly safe
> referencing in the Intel manuals with regards to the hardware
> behaviour.
>
> *There are exceptions with regards to writes that come into play when
> using specific instructions, but won't be a factor here.  E.g. you can
> use specific non-temporal stores to subvert the normal cache-coherency
> rules.
>
> Mike.
>
> [0] http://www.intel.com/content/www/us/en/processors/architectures-software-developer-manuals.html
>
>
> On 2 August 2013 23:25, Nitsan Wakart <nitsanw at yahoo.com> <nitsanw at yahoo.com> wrote:
>
>  Can the writes get re-ordered on the hardware level?
> Don't volatile reads(LOAD/LOAD) also require reads to not get re-ordered to
> maintain happens-before/after relationships?
>
> ________________________________
> From: Vitaly Davidovich <vitalyd at gmail.com> <vitalyd at gmail.com>
> To: Nitsan Wakart <nitsanw at yahoo.com> <nitsanw at yahoo.com>
> Cc: Concurrency Interest <concurrency-interest at cs.oswego.edu> <concurrency-interest at cs.oswego.edu>
> Sent: Friday, August 2, 2013 12:46 PM
> Subject: Re: [concurrency-interest] x86 NOOP memory barriers
>
> I'm not an authority so take this for what it's worth...
> Yes, volatile loads and lazySet do not cause any cpu fence/barrier
> instructions to be generated - in that sense, they're nop at the hardware
> level.  However, they are also compiler barriers, which is where the "cheap
> but aint free" phrase may apply.  The compiler cannot reorder these
> instructions in ways that violate their documented/spec'd memory ordering
> effects.  So for example, a plain store followed by lazySet cannot actually
> be moved after the lazySet; whereas if you have two plain stores, the
> compiler can technically reorder them as it sees fit (if we look at just
> them two and disregard other surrounding code).
> So, it may happen that compiler cannot do certain code motion/optimizations
> due to these compiler fences and therefore you have some penalty vs using
> plain load and stores.  For volatile loads, compiler cannot enregister the
> value like it would with plain load, but even this may not have noticeable
> perf diff if the data is in L1 dcache, for example.
> HTH,
> Vitaly
> Sent from my phone
> On Aug 2, 2013 5:55 AM, "Nitsan Wakart" <nitsanw at yahoo.com> <nitsanw at yahoo.com> wrote:
>
> Hi,
> For clarity's sake I'd like an official explanation for the often quoted
> "all barriers except STORE/LOAD are a no-op on x86" statement from the JMM
> cookbook.
> Can someone (of authority, so I can later say: "But Mr. Authority here
> says...") please confirm/expand on/deny that while a volatile read or an
> AtomicLong.lazySet are a CPU noop (in the sense that they are a MOV like any
> other), they are also compiler instructions. One cannot simply replace a
> lazySet with a plain write to get the same effect. They might be cheap but
> they ain't free...
> I would appreciate some more careful wording on this topic.
> Many thanks,
> Nitsan
>
> _______________________________________________
> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>  _______________________________________________
> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130802/dc7f2b53/attachment-0001.html>

From nathan.reynolds at oracle.com  Fri Aug  2 13:39:44 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Fri, 02 Aug 2013 10:39:44 -0700
Subject: [concurrency-interest] x86 NOOP memory barriers
In-Reply-To: <CAHjP37Ff8xG2gu36qQDRMXb+SdAN4e7OAFgGh4K7Gg-PVF7ZLg@mail.gmail.com>
References: <1375436588.50646.YahooMailNeo@web120701.mail.ne1.yahoo.com>
	<CAHjP37GGbpshebzxy8PF_+ZPKvJpc=1=RWsCmwCO-89XLdf1Lw@mail.gmail.com>
	<1375442721.83454.YahooMailNeo@web120703.mail.ne1.yahoo.com>
	<CALwNKeSrxua6MpLktrKgFaDTKCuacWmv_ZpRZdA3R33FsO04HA@mail.gmail.com>
	<51FBE1CC.4040907@oracle.com>
	<CAHjP37Ff8xG2gu36qQDRMXb+SdAN4e7OAFgGh4K7Gg-PVF7ZLg@mail.gmail.com>
Message-ID: <51FBEEE0.10705@oracle.com>

I would guess that as soon as a store instruction is decoded, that the 
store is put into the load/store buffer.  It sits there waiting for the 
address and/or value to be computed and put into the buffer entry.  If 
this weren't the case, then how would following stores be aware of 
preceding stores that don't have computed addresses or values?  In order 
to enforce the memory model, the core would need a queue of all stores 
after the store with the unknown address.  Why not reuse the load/store 
buffer?

-Nathan

On 8/2/2013 10:28 AM, Vitaly Davidovich wrote:
>
> I think store-load forwarding can satisfy a load from store buffer.
>
> Write combining store buffer will combine stores to same cacheline.  I 
> don't think writes go into the store buffer before their address is 
> computed, so don't think you'll have two adjacent stores to same 
> address sitting there at same time (I could be wrong on that though 
> but seems like core should know address by now since it would've 
> detected a cache miss on it and thus dropped the store into the buffer).
>
> Sent from my phone
>
> On Aug 2, 2013 12:58 PM, "Nathan Reynolds" <nathan.reynolds at oracle.com 
> <mailto:nathan.reynolds at oracle.com>> wrote:
>
>     Just to add, if the address of a load or store is not known yet
>     (i.e. the core is still computing it), then no loads may reorder
>     in front of it.  This is because if the store and load are for the
>     same address then the load needs to pick up the value of the store.
>
>     Ignore any fences... Let's say there is a store sitting in the
>     load/store buffer inside the core.  Let's say the address for the
>     store has been computed.  Could a load from that same address
>     simply use the value that is going to be stored?  Just checking if
>     my understanding is correct.
>
>     Ignore any fences... Let's say there are 2 adjacent stores sitting
>     in the load/store buffer inside the core.  Let's say the addresses
>     for the stores have been computed and are identical.  Could the
>     stores be combined into 1 store so that only the latter store
>     actually pushes its data to L1D cache?  Again, just checking if my
>     understanding is correct.
>
>     -Nathan
>
>     On 8/2/2013 5:15 AM, Michael Barker wrote:
>>     Hi Nitsan,
>>
>>     In short, no.  The place to look (as definitive as it gets for the
>>     hardware) is section 8.2.2, volume 3A of the Intel programmer
>>     manual[0].  It lists the rules that are applied regarding the
>>     reordering of instructions under the X86 memory model.  I've
>>     summarised the relevant ones here:
>>
>>     - Reads are not reordered with other reads.
>>     - Writes are not reordered with older reads.
>>     - Writes to memory are not reordered with other writes*.
>>     - Reads may be reordered with older writes to different locations but
>>     not with older writes to the same location.
>>
>>     The only reordering that will occur with x86 is allowing reads to be
>>     executed before writes (to other locations), hence the need for a
>>     LOCKed instruction to enforce the store/load barrier.  As you can see
>>     with the above rules, store are not reordered with older stores and
>>     loads are not reordered with older loads so a series of MOV
>>     instructions is sufficient for a store/store or a load/load barrier.
>>
>>     I'm not really an authority either, but you'll be fairly safe
>>     referencing in the Intel manuals with regards to the hardware
>>     behaviour.
>>
>>     *There are exceptions with regards to writes that come into play when
>>     using specific instructions, but won't be a factor here.  E.g. you can
>>     use specific non-temporal stores to subvert the normal cache-coherency
>>     rules.
>>
>>     Mike.
>>
>>     [0]http://www.intel.com/content/www/us/en/processors/architectures-software-developer-manuals.html
>>
>>
>>     On 2 August 2013 23:25, Nitsan Wakart<nitsanw at yahoo.com>  <mailto:nitsanw at yahoo.com>  wrote:
>>>     Can the writes get re-ordered on the hardware level?
>>>     Don't volatile reads(LOAD/LOAD) also require reads to not get re-ordered to
>>>     maintain happens-before/after relationships?
>>>
>>>     ________________________________
>>>     From: Vitaly Davidovich<vitalyd at gmail.com>  <mailto:vitalyd at gmail.com>
>>>     To: Nitsan Wakart<nitsanw at yahoo.com>  <mailto:nitsanw at yahoo.com>
>>>     Cc: Concurrency Interest<concurrency-interest at cs.oswego.edu>  <mailto:concurrency-interest at cs.oswego.edu>
>>>     Sent: Friday, August 2, 2013 12:46 PM
>>>     Subject: Re: [concurrency-interest] x86 NOOP memory barriers
>>>
>>>     I'm not an authority so take this for what it's worth...
>>>     Yes, volatile loads and lazySet do not cause any cpu fence/barrier
>>>     instructions to be generated - in that sense, they're nop at the hardware
>>>     level.  However, they are also compiler barriers, which is where the "cheap
>>>     but aint free" phrase may apply.  The compiler cannot reorder these
>>>     instructions in ways that violate their documented/spec'd memory ordering
>>>     effects.  So for example, a plain store followed by lazySet cannot actually
>>>     be moved after the lazySet; whereas if you have two plain stores, the
>>>     compiler can technically reorder them as it sees fit (if we look at just
>>>     them two and disregard other surrounding code).
>>>     So, it may happen that compiler cannot do certain code motion/optimizations
>>>     due to these compiler fences and therefore you have some penalty vs using
>>>     plain load and stores.  For volatile loads, compiler cannot enregister the
>>>     value like it would with plain load, but even this may not have noticeable
>>>     perf diff if the data is in L1 dcache, for example.
>>>     HTH,
>>>     Vitaly
>>>     Sent from my phone
>>>     On Aug 2, 2013 5:55 AM, "Nitsan Wakart"<nitsanw at yahoo.com>  <mailto:nitsanw at yahoo.com>  wrote:
>>>
>>>     Hi,
>>>     For clarity's sake I'd like an official explanation for the often quoted
>>>     "all barriers except STORE/LOAD are a no-op on x86" statement from the JMM
>>>     cookbook.
>>>     Can someone (of authority, so I can later say: "But Mr. Authority here
>>>     says...") please confirm/expand on/deny that while a volatile read or an
>>>     AtomicLong.lazySet are a CPU noop (in the sense that they are a MOV like any
>>>     other), they are also compiler instructions. One cannot simply replace a
>>>     lazySet with a plain write to get the same effect. They might be cheap but
>>>     they ain't free...
>>>     I would appreciate some more careful wording on this topic.
>>>     Many thanks,
>>>     Nitsan
>>>
>>>     _______________________________________________
>>>     Concurrency-interest mailing list
>>>     Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>>
>>>
>>>     _______________________________________________
>>>     Concurrency-interest mailing list
>>>     Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>     _______________________________________________
>>     Concurrency-interest mailing list
>>     Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130802/9bc9f24b/attachment.html>

From mikeb01 at gmail.com  Fri Aug  2 18:33:33 2013
From: mikeb01 at gmail.com (Michael Barker)
Date: Sat, 3 Aug 2013 10:33:33 +1200
Subject: [concurrency-interest] x86 NOOP memory barriers
In-Reply-To: <1375449342.6944.YahooMailNeo@web120701.mail.ne1.yahoo.com>
References: <1375436588.50646.YahooMailNeo@web120701.mail.ne1.yahoo.com>
	<CAHjP37GGbpshebzxy8PF_+ZPKvJpc=1=RWsCmwCO-89XLdf1Lw@mail.gmail.com>
	<1375442721.83454.YahooMailNeo@web120703.mail.ne1.yahoo.com>
	<CALwNKeSrxua6MpLktrKgFaDTKCuacWmv_ZpRZdA3R33FsO04HA@mail.gmail.com>
	<1375449342.6944.YahooMailNeo@web120701.mail.ne1.yahoo.com>
Message-ID: <CALwNKeQcEWzGEr4BOd5M5U9i6CcqApg2+HM9JdnadpGeQ1B_uQ@mail.gmail.com>

> So because a putOrdered is a write to memory it cannot be reordered with
> other writes, as per "8.2.3.2 Neither Loads Nor Stores Are Reordered with
> Like Operations".

Yes in combination with the compiler reordering restrictions.  In
Hotspot this is implemented within the
LibraryCall::inline_unsafe_ordered_store (library_call.cpp) call.
Look for:

insert_mem_bar(Op_MemBarRelease);
insert_mem_bar(Op_MemBarCPUOrder);

Mike.

From sadhak001 at gmail.com  Sat Aug  3 10:12:11 2013
From: sadhak001 at gmail.com (Mani Sarkar)
Date: Sat, 3 Aug 2013 15:12:11 +0100
Subject: [concurrency-interest] subscribe
Message-ID: <CAGHtMWmrUTs4MO7fM=yDMXjyz=rNvZRjNtyWqMmFyAqGgodOZg@mail.gmail.com>

-- 
*Twitter:* @theNeomatrix369          *Blog:*
http://neomatrix369.wordpress.com
*JUG activity:* LJC Advocate (@adoptopenjdk & @adoptajsr programs)
*Meet-a-Project:* https://github.com/MutabilityDetector
*Bitbucket:* https://bitbucket.org/neomatrix369  * **Github:* https://github
.com/neomatrix369
*LinkedIn:* http://uk.linkedin.com/pub/mani-sarkar/71/a77/39b
*Devoxx UK 2013* was a grand success:
http://www.devoxx.com/display/UK13/Home

*Don't chase success, rather aim for "Excellence", and success will come
chasing after you!*
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130803/5a0ceacc/attachment.html>

From stephan.diestelhorst at gmail.com  Sun Aug  4 18:16:34 2013
From: stephan.diestelhorst at gmail.com (Stephan Diestelhorst)
Date: Sun, 04 Aug 2013 23:16:34 +0100
Subject: [concurrency-interest] Experimental Transactional Lock Elision with
	Hotspot
Message-ID: <1526979.z9BUu66VXM@d-allen>

Dear all,
  I have worked on a few changes to the JDK7 Hotspot JVM that are a
first start for eliding Java's monitors with an HTM (AMD's Advanced
Synchronization Facility [1]).  The code is pretty much a proof of
concept right now and does not contain any tuning etc.  Sadly, I also
have not found the time (yet) to do a full characterisation.

Please find this on top of the latest JDK7 code here:

https://bitbucket.org/stephand/hotspot-asfsle/commits/branch/ASF-SLE

I hope this is interesting to the wider parallel and transactional
communities, as it highlights the points where elision should come into
play.  I have modified both the fast- and slow-path logic, and
(hopefully) squeezed out all the code paths where the locks check their
own value (which under transactional elision is still FREE).

Another interesting take-away is the amazing (crazy?) locking on 32 bit
x86, where the fast-path code changes the lock from FREE -> TAKEN1, and
then slow-path logic changes that from TAKEN1 -> TAKEN2, whith TAKEN1 a
more easily available value in fast-path assembly and TAKEN2 a more
useful value for later inspection.

In any case, if you are brave enough, this *should* work with my newly
ported ASF simulator [2].

If this is useful, feel free to send me a postcard and / or please cite
our ASF-related papers.

Best,
  Stephan

Disclaimer: Not a product, and not supported / endorsed by any company.

[1] http://developer.amd.com/assets/45432-ASF_Spec_2.1.pdf
    http://developer.amd.com/community/blog/evaluation-of-the-advanced-synchronization-facility-asf/
    Dave Christie, Jae-Woong Chung, Stephan Diestelhorst, Michael
    Hohmuth, Martin Pohlack, Christof Fetzer, Martin Nowack, Torvald
    Riegel, Pascal Felber, Patrick Marlier, Etienne Riviere: Evaluation
    of AMD's advanced synchronization facility within a complete
    transactional memory stack. EuroSys 2010: 27-40

[2] https://bitbucket.org/stephand/marss86-asf/commits/branch/asf_ptlsim_merge


From nitsanw at yahoo.com  Tue Aug  6 04:38:04 2013
From: nitsanw at yahoo.com (Nitsan Wakart)
Date: Tue, 6 Aug 2013 01:38:04 -0700 (PDT)
Subject: [concurrency-interest] x86 NOOP memory barriers
In-Reply-To: <CALwNKeQcEWzGEr4BOd5M5U9i6CcqApg2+HM9JdnadpGeQ1B_uQ@mail.gmail.com>
References: <1375436588.50646.YahooMailNeo@web120701.mail.ne1.yahoo.com>	<CAHjP37GGbpshebzxy8PF_+ZPKvJpc=1=RWsCmwCO-89XLdf1Lw@mail.gmail.com>	<1375442721.83454.YahooMailNeo@web120703.mail.ne1.yahoo.com>	<CALwNKeSrxua6MpLktrKgFaDTKCuacWmv_ZpRZdA3R33FsO04HA@mail.gmail.com>	<1375449342.6944.YahooMailNeo@web120701.mail.ne1.yahoo.com>
	<CALwNKeQcEWzGEr4BOd5M5U9i6CcqApg2+HM9JdnadpGeQ1B_uQ@mail.gmail.com>
Message-ID: <1375778284.31864.YahooMailNeo@web120706.mail.ne1.yahoo.com>

Summarized in this blog post here:
http://psy-lob-saw.blogspot.com/2013/08/memory-barriers-are-not-free.html

Please point out any mistakes/omissions/oversight.
Thanks for the help guys.


________________________________
 From: Michael Barker <mikeb01 at gmail.com>
To: Nitsan Wakart <nitsanw at yahoo.com> 
Cc: Vitaly Davidovich <vitalyd at gmail.com>; "concurrency-interest at cs.oswego.edu" <concurrency-interest at cs.oswego.edu> 
Sent: Saturday, August 3, 2013 12:33 AM
Subject: Re: [concurrency-interest] x86 NOOP memory barriers
 

> So because a putOrdered is a write to memory it cannot be reordered with
> other writes, as per "8.2.3.2 Neither Loads Nor Stores Are Reordered with
> Like Operations".

Yes in combination with the compiler reordering restrictions.? In
Hotspot this is implemented within the
LibraryCall::inline_unsafe_ordered_store (library_call.cpp) call.
Look for:

insert_mem_bar(Op_MemBarRelease);
insert_mem_bar(Op_MemBarCPUOrder);

Mike.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130806/d8e8caf3/attachment.html>

From aph at redhat.com  Tue Aug  6 04:58:23 2013
From: aph at redhat.com (Andrew Haley)
Date: Tue, 06 Aug 2013 09:58:23 +0100
Subject: [concurrency-interest] x86 NOOP memory barriers
In-Reply-To: <1375778284.31864.YahooMailNeo@web120706.mail.ne1.yahoo.com>
References: <1375436588.50646.YahooMailNeo@web120701.mail.ne1.yahoo.com>	<CAHjP37GGbpshebzxy8PF_+ZPKvJpc=1=RWsCmwCO-89XLdf1Lw@mail.gmail.com>	<1375442721.83454.YahooMailNeo@web120703.mail.ne1.yahoo.com>	<CALwNKeSrxua6MpLktrKgFaDTKCuacWmv_ZpRZdA3R33FsO04HA@mail.gmail.com>	<1375449342.6944.YahooMailNeo@web120701.mail.ne1.yahoo.com>
	<CALwNKeQcEWzGEr4BOd5M5U9i6CcqApg2+HM9JdnadpGeQ1B_uQ@mail.gmail.com>
	<1375778284.31864.YahooMailNeo@web120706.mail.ne1.yahoo.com>
Message-ID: <5200BAAF.1090206@redhat.com>

On 08/06/2013 09:38 AM, Nitsan Wakart wrote:
> Summarized in this blog post here:
> http://psy-lob-saw.blogspot.com/2013/08/memory-barriers-are-not-free.html
> 
> Please point out any mistakes/omissions/oversight.
> Thanks for the help guys.

It's a bit of a garden path posting: you get a long way through
before you say that you're talking about compiler barriers.  You'll
have lost more than half of your audience by then.

Your post would be much more useful if you summarized that point in
your first paragraph.

Andrew.

From hans.boehm at hp.com  Tue Aug  6 12:15:44 2013
From: hans.boehm at hp.com (Boehm, Hans)
Date: Tue, 6 Aug 2013 16:15:44 +0000
Subject: [concurrency-interest] x86 NOOP memory barriers
In-Reply-To: <1375778284.31864.YahooMailNeo@web120706.mail.ne1.yahoo.com>
References: <1375436588.50646.YahooMailNeo@web120701.mail.ne1.yahoo.com>
	<CAHjP37GGbpshebzxy8PF_+ZPKvJpc=1=RWsCmwCO-89XLdf1Lw@mail.gmail.com>
	<1375442721.83454.YahooMailNeo@web120703.mail.ne1.yahoo.com>
	<CALwNKeSrxua6MpLktrKgFaDTKCuacWmv_ZpRZdA3R33FsO04HA@mail.gmail.com>
	<1375449342.6944.YahooMailNeo@web120701.mail.ne1.yahoo.com>
	<CALwNKeQcEWzGEr4BOd5M5U9i6CcqApg2+HM9JdnadpGeQ1B_uQ@mail.gmail.com>
	<1375778284.31864.YahooMailNeo@web120706.mail.ne1.yahoo.com>
Message-ID: <A3E67C2071F49C4CBC4F17E6D77CDDD23D46E140@G9W0725.americas.hpqcorp.net>

It would be nice to understand exactly what the difference in generated code is for the different versions whose performance you plotted in http://psy-lob-saw.blogspot.com/2013/05/using-jmh-to-benchmark-multi-threaded.html .  I'm surprised by the increasing differences in the unshared case at high processor counts.  That suggests you are generating different memory traffic for some reason, perhaps because of optimization artifacts for this particular implementation.  AFAICT, the unshared cases should be embarrassingly parallel; there should be no real thread interaction?  Are you spacing out the elements far enough to deal with prefetching artifacts?

I'm also surprised by the lazy vs. volatile differences in the shared case.   It seems to me the time should be completely dominated by coherence misses in either case.  There may be some unexpected odd optimization or lack thereof happening here.  In my limited experience, the impact of memory fences, etc. commonly decreases as scale increases, since those slowdowns are local to each core, and don't affect the amount of memory traffic.  See for example the microbenchmark measurements in http://www.hpl.hp.com/techreports/2012/HPL-2012-218.html .

This benchmark is such that I have a hard time guessing what optimizations would be applied in each case, and I would expect that to vary a lot across JVMs.  You're probably recalculating the addresses of the array indices more in some cases than others.  Can multiple increments even get combined in some cases?

Hans

From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Nitsan Wakart
Sent: Tuesday, August 06, 2013 1:38 AM
To: concurrency-interest at cs.oswego.edu
Subject: Re: [concurrency-interest] x86 NOOP memory barriers

Summarized in this blog post here:
http://psy-lob-saw.blogspot.com/2013/08/memory-barriers-are-not-free.html
Please point out any mistakes/omissions/oversight.
Thanks for the help guys.

________________________________
From: Michael Barker <mikeb01 at gmail.com<mailto:mikeb01 at gmail.com>>
To: Nitsan Wakart <nitsanw at yahoo.com<mailto:nitsanw at yahoo.com>>
Cc: Vitaly Davidovich <vitalyd at gmail.com<mailto:vitalyd at gmail.com>>; "concurrency-interest at cs.oswego.edu<mailto:concurrency-interest at cs.oswego.edu>" <concurrency-interest at cs.oswego.edu<mailto:concurrency-interest at cs.oswego.edu>>
Sent: Saturday, August 3, 2013 12:33 AM
Subject: Re: [concurrency-interest] x86 NOOP memory barriers

> So because a putOrdered is a write to memory it cannot be reordered with
> other writes, as per "8.2.3.2 Neither Loads Nor Stores Are Reordered with
> Like Operations".

Yes in combination with the compiler reordering restrictions.  In
Hotspot this is implemented within the
LibraryCall::inline_unsafe_ordered_store (library_call.cpp) call.
Look for:

insert_mem_bar(Op_MemBarRelease);
insert_mem_bar(Op_MemBarCPUOrder);

Mike.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130806/33be3936/attachment.html>

From dmitry.zaslavsky at gmail.com  Tue Aug  6 14:03:56 2013
From: dmitry.zaslavsky at gmail.com (Dmitry Zaslavsky)
Date: Tue, 6 Aug 2013 14:03:56 -0400
Subject: [concurrency-interest] x86 NOOP memory barriers
In-Reply-To: <A3E67C2071F49C4CBC4F17E6D77CDDD23D46E140@G9W0725.americas.hpqcorp.net>
References: <1375436588.50646.YahooMailNeo@web120701.mail.ne1.yahoo.com>
	<CAHjP37GGbpshebzxy8PF_+ZPKvJpc=1=RWsCmwCO-89XLdf1Lw@mail.gmail.com>
	<1375442721.83454.YahooMailNeo@web120703.mail.ne1.yahoo.com>
	<CALwNKeSrxua6MpLktrKgFaDTKCuacWmv_ZpRZdA3R33FsO04HA@mail.gmail.com>
	<1375449342.6944.YahooMailNeo@web120701.mail.ne1.yahoo.com>
	<CALwNKeQcEWzGEr4BOd5M5U9i6CcqApg2+HM9JdnadpGeQ1B_uQ@mail.gmail.com>
	<1375778284.31864.YahooMailNeo@web120706.mail.ne1.yahoo.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD23D46E140@G9W0725.americas.hpqcorp.net>
Message-ID: <655C8BA1-2141-4586-82BC-EED1E04DF12E@gmail.com>

I  didn't have a chance to run this benchmark but one very significant point here is that the system is hyper threaded its only 6 'real' cores
My guess would be that volatile access generated lock instruction cause real stall for the other hyper thread.

Another factor is that test has read and write. The description says that atomiclongarray was used. 
.get method is volatile get and I guess it was used even for lazy version?



Sent from mobile device

On Aug 6, 2013, at 12:15 PM, "Boehm, Hans" <hans.boehm at hp.com> wrote:

> It would be nice to understand exactly what the difference in generated code is for the different versions whose performance you plotted in http://psy-lob-saw.blogspot.com/2013/05/using-jmh-to-benchmark-multi-threaded.html .  I?m surprised by the increasing differences in the unshared case at high processor counts.  That suggests you are generating different memory traffic for some reason, perhaps because of optimization artifacts for this particular implementation.  AFAICT, the unshared cases should be embarrassingly parallel; there should be no real thread interaction?  Are you spacing out the elements far enough to deal with prefetching artifacts?
>  
> I?m also surprised by the lazy vs. volatile differences in the shared case.   It seems to me the time should be completely dominated by coherence misses in either case.  There may be some unexpected odd optimization or lack thereof happening here.  In my limited experience, the impact of memory fences, etc. commonly decreases as scale increases, since those slowdowns are local to each core, and don?t affect the amount of memory traffic.  See for example the microbenchmark measurements in http://www.hpl.hp.com/techreports/2012/HPL-2012-218.html .
>  
> This benchmark is such that I have a hard time guessing what optimizations would be applied in each case, and I would expect that to vary a lot across JVMs.  You?re probably recalculating the addresses of the array indices more in some cases than others.  Can multiple increments even get combined in some cases?
>  
> Hans
>  
> From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of Nitsan Wakart
> Sent: Tuesday, August 06, 2013 1:38 AM
> To: concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] x86 NOOP memory barriers
>  
> Summarized in this blog post here:
> http://psy-lob-saw.blogspot.com/2013/08/memory-barriers-are-not-free.html
> Please point out any mistakes/omissions/oversight.
> Thanks for the help guys.
>  
> From: Michael Barker <mikeb01 at gmail.com>
> To: Nitsan Wakart <nitsanw at yahoo.com> 
> Cc: Vitaly Davidovich <vitalyd at gmail.com>; "concurrency-interest at cs.oswego.edu" <concurrency-interest at cs.oswego.edu> 
> Sent: Saturday, August 3, 2013 12:33 AM
> Subject: Re: [concurrency-interest] x86 NOOP memory barriers
> 
> > So because a putOrdered is a write to memory it cannot be reordered with
> > other writes, as per "8.2.3.2 Neither Loads Nor Stores Are Reordered with
> > Like Operations".
> 
> Yes in combination with the compiler reordering restrictions.  In
> Hotspot this is implemented within the
> LibraryCall::inline_unsafe_ordered_store (library_call.cpp) call.
> Look for:
> 
> insert_mem_bar(Op_MemBarRelease);
> insert_mem_bar(Op_MemBarCPUOrder);
> 
> Mike.
> 
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130806/ce79bce1/attachment-0001.html>

From nitsanw at yahoo.com  Tue Aug  6 14:11:27 2013
From: nitsanw at yahoo.com (Nitsan Wakart)
Date: Tue, 6 Aug 2013 11:11:27 -0700 (PDT)
Subject: [concurrency-interest] x86 NOOP memory barriers
In-Reply-To: <A3E67C2071F49C4CBC4F17E6D77CDDD23D46E140@G9W0725.americas.hpqcorp.net>
References: <1375436588.50646.YahooMailNeo@web120701.mail.ne1.yahoo.com>	<CAHjP37GGbpshebzxy8PF_+ZPKvJpc=1=RWsCmwCO-89XLdf1Lw@mail.gmail.com>	<1375442721.83454.YahooMailNeo@web120703.mail.ne1.yahoo.com>	<CALwNKeSrxua6MpLktrKgFaDTKCuacWmv_ZpRZdA3R33FsO04HA@mail.gmail.com>	<1375449342.6944.YahooMailNeo@web120701.mail.ne1.yahoo.com>	<CALwNKeQcEWzGEr4BOd5M5U9i6CcqApg2+HM9JdnadpGeQ1B_uQ@mail.gmail.com>
	<1375778284.31864.YahooMailNeo@web120706.mail.ne1.yahoo.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD23D46E140@G9W0725.americas.hpqcorp.net>
Message-ID: <1375812687.3378.YahooMailNeo@web120704.mail.ne1.yahoo.com>

Hi,

> It would be nice to understand exactly what the difference in generated code is for the different versions whose performance you plotted in http://psy-lob-saw.blogspot.com/2013/05/using-jmh-to-benchmark-multi-threaded.html .?

I agree, at the time of writing I did printout the assembly, but considered the post long enough as it were (explaining the assembly would have added alot of work for me and reading for the audience). I don't recall the assembly to show anything suspect, but as all the code is included it is easy enough to generate. If I have the time I'll go through the exercise and add the printouts to the repository.

> ?I?m surprised by the increasing differences in the unshared case at high processor counts.? That suggests you are generating different memory traffic for some reason, perhaps because of optimization artifacts for this particular implementation.? AFAICT, the unshared cases should be embarrassingly parallel; there should be no real thread interaction?? Are you spacing out the elements far enough to deal with prefetching artifacts?


If you are referring to the break in scalability from 8 threads onwards for the volatile and lazy case I completely agree with you. I spaced the elements out by one cache line, but if prefetching was the issue it would have affected the lower thread counts equally. The JMH version I was using at the time was quite early in the life of the tool, it may have had issues that caused poor scaling or interacted badly with this benchmark which is a nano-benchmark to use Shipilev's qualification. Alternatively, my code might be at fault, it's short enough and I had others review it before publication, but it can happen.?

> ?I?m also surprised by the lazy vs. volatile differences in the shared case. ?It seems to me the time should be completely dominated by coherence misses in either case.? There may be some unexpected odd optimization or lack thereof happening here.? In my limited experience, the impact of memory fences, etc. commonly decreases as scale increases, since those slowdowns are local to each core, and don?t affect the amount of memory traffic.? See for example the microbenchmark measurements in http://www.hpl.hp.com/techreports/2012/HPL-2012-218.html .


It is my understanding that lazy set tends to dampen false sharing effects as the value is not immediately 'flushed' and can be modified while in the write queue. The less you 'force' the write, the less you contend on the cache line.

> This benchmark is such that I have a hard time guessing what optimizations would be applied in each case, and I would expect that to vary a lot across JVMs.??

I only tested on one JVM, I expect you are right and the results will differ somewhat.


> You?re probably recalculating the addresses of the array indices more in some cases than others. ?

The array offset is calculated once per thread context. So once in all cases.

> Can multiple increments even get combined in some cases?


I'll have to have another look at the assembly. I don't think it happens, I vaguely recall verifying it in the assembly, but it's been a few months.

Thanks for taking the time to read and make comments,
Nitsan


From nitsanw at yahoo.com  Tue Aug  6 15:16:47 2013
From: nitsanw at yahoo.com (Nitsan Wakart)
Date: Tue, 6 Aug 2013 12:16:47 -0700 (PDT)
Subject: [concurrency-interest] x86 NOOP memory barriers
In-Reply-To: <655C8BA1-2141-4586-82BC-EED1E04DF12E@gmail.com>
References: <1375436588.50646.YahooMailNeo@web120701.mail.ne1.yahoo.com>
	<CAHjP37GGbpshebzxy8PF_+ZPKvJpc=1=RWsCmwCO-89XLdf1Lw@mail.gmail.com>
	<1375442721.83454.YahooMailNeo@web120703.mail.ne1.yahoo.com>
	<CALwNKeSrxua6MpLktrKgFaDTKCuacWmv_ZpRZdA3R33FsO04HA@mail.gmail.com>
	<1375449342.6944.YahooMailNeo@web120701.mail.ne1.yahoo.com>
	<CALwNKeQcEWzGEr4BOd5M5U9i6CcqApg2+HM9JdnadpGeQ1B_uQ@mail.gmail.com>
	<1375778284.31864.YahooMailNeo@web120706.mail.ne1.yahoo.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD23D46E140@G9W0725.americas.hpqcorp.net>
	<655C8BA1-2141-4586-82BC-EED1E04DF12E@gmail.com>
Message-ID: <1375816607.82942.YahooMailNeo@web120703.mail.ne1.yahoo.com>

> I ?didn't have a chance to run this benchmark but one very significant point here is that the system is hyper threaded its only 6 'real' cores.?My guess would be that volatile access generated lock instruction cause real stall for the other hyper thread.


Could be, but I'm not sure if this makes the experiment more or less realistic... I'm not sure how you tie this fact to the data. What do you expect the result to be with/without hyper-threading?

> Another factor is that test has read and write. The description says that atomiclongarray was used. .get method is volatile get and I guess it was used even for lazy version?

Get is a volatile read, for both the volatile and lazySet cases. I consider that appropriate as most people use either volatile directly or Atomic* to utilize lazySet, either way they will be paring the write with a volatile read. For what it is worth I have experimented in the past with replacing the volatile read with a plain read and it made no difference.


Thanks for the feedback :-) very much appreciated.


From nathan.reynolds at oracle.com  Tue Aug  6 15:23:20 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Tue, 06 Aug 2013 12:23:20 -0700
Subject: [concurrency-interest] x86 NOOP memory barriers
In-Reply-To: <655C8BA1-2141-4586-82BC-EED1E04DF12E@gmail.com>
References: <1375436588.50646.YahooMailNeo@web120701.mail.ne1.yahoo.com>
	<CAHjP37GGbpshebzxy8PF_+ZPKvJpc=1=RWsCmwCO-89XLdf1Lw@mail.gmail.com>
	<1375442721.83454.YahooMailNeo@web120703.mail.ne1.yahoo.com>
	<CALwNKeSrxua6MpLktrKgFaDTKCuacWmv_ZpRZdA3R33FsO04HA@mail.gmail.com>
	<1375449342.6944.YahooMailNeo@web120701.mail.ne1.yahoo.com>
	<CALwNKeQcEWzGEr4BOd5M5U9i6CcqApg2+HM9JdnadpGeQ1B_uQ@mail.gmail.com>
	<1375778284.31864.YahooMailNeo@web120706.mail.ne1.yahoo.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD23D46E140@G9W0725.americas.hpqcorp.net>
	<655C8BA1-2141-4586-82BC-EED1E04DF12E@gmail.com>
Message-ID: <52014D28.7010708@oracle.com>

It would be interesting if the charts included data points for 6 
threads.  This will match the 6 "real" cores.

The beauty of hyper-threading is that if one thread stalls, then the 
other thread gets all of the computing resources.  So, if one thread 
executes a lock instruction or stalls waiting for a cache line to be 
fetched, then the other thread can execute its logic operations at full 
speed.

Also, the logical operation of a thread doesn't impact the operation of 
the other thread on the core.  So, if one thread executes a fence, it 
only logically impacts that thread.  The other thread is free to execute 
loads and stores as it wishes. However, the physical operation of a 
thread does impact the physical operation of the other thread since they 
are competing for the core's resources.

-Nathan

On 8/6/2013 11:03 AM, Dmitry Zaslavsky wrote:
> I  didn't have a chance to run this benchmark but one very significant 
> point here is that the system is hyper threaded its only 6 'real' cores
> My guess would be that volatile access generated lock instruction 
> cause real stall for the other hyper thread.
>
> Another factor is that test has read and write. The description says 
> that atomiclongarray was used.
> .get method is volatile get and I guess it was used even for lazy version?
>
>
>
> Sent from mobile device
>
> On Aug 6, 2013, at 12:15 PM, "Boehm, Hans" <hans.boehm at hp.com 
> <mailto:hans.boehm at hp.com>> wrote:
>
>> It would be nice to understand exactly what the difference in 
>> generated code is for the different versions whose performance you 
>> plotted in 
>> http://psy-lob-saw.blogspot.com/2013/05/using-jmh-to-benchmark-multi-threaded.html 
>> .  I'm surprised by the increasing differences in the unshared case 
>> at high processor counts.  That suggests you are generating different 
>> memory traffic for some reason, perhaps because of optimization 
>> artifacts for this particular implementation.  AFAICT, the unshared 
>> cases should be embarrassingly parallel; there should be no real 
>> thread interaction?  Are you spacing out the elements far enough to 
>> deal with prefetching artifacts?
>>
>> I'm also surprised by the lazy vs. volatile differences in the shared 
>> case.   It seems to me the time should be completely dominated by 
>> coherence misses in either case.  There may be some unexpected odd 
>> optimization or lack thereof happening here.  In my limited 
>> experience, the impact of memory fences, etc. commonly decreases as 
>> scale increases, since those slowdowns are local to each core, and 
>> don't affect the amount of memory traffic. See for example the 
>> microbenchmark measurements in 
>> http://www.hpl.hp.com/techreports/2012/HPL-2012-218.html .
>>
>> This benchmark is such that I have a hard time guessing what 
>> optimizations would be applied in each case, and I would expect that 
>> to vary a lot across JVMs.  You're probably recalculating the 
>> addresses of the array indices more in some cases than others.  Can 
>> multiple increments even get combined in some cases?
>>
>> Hans
>>
>> *From:*concurrency-interest-bounces at cs.oswego.edu 
>> <mailto:concurrency-interest-bounces at cs.oswego.edu> 
>> [mailto:concurrency-interest-bounces at cs.oswego.edu] *On Behalf Of 
>> *Nitsan Wakart
>> *Sent:* Tuesday, August 06, 2013 1:38 AM
>> *To:* concurrency-interest at cs.oswego.edu 
>> <mailto:concurrency-interest at cs.oswego.edu>
>> *Subject:* Re: [concurrency-interest] x86 NOOP memory barriers
>>
>> Summarized in this blog post here:
>>
>> http://psy-lob-saw.blogspot.com/2013/08/memory-barriers-are-not-free.html
>>
>> Please point out any mistakes/omissions/oversight.
>>
>> Thanks for the help guys.
>>
>> ------------------------------------------------------------------------
>>
>> *From:*Michael Barker <mikeb01 at gmail.com <mailto:mikeb01 at gmail.com>>
>> *To:* Nitsan Wakart <nitsanw at yahoo.com <mailto:nitsanw at yahoo.com>>
>> *Cc:* Vitaly Davidovich <vitalyd at gmail.com 
>> <mailto:vitalyd at gmail.com>>; "concurrency-interest at cs.oswego.edu 
>> <mailto:concurrency-interest at cs.oswego.edu>" 
>> <concurrency-interest at cs.oswego.edu 
>> <mailto:concurrency-interest at cs.oswego.edu>>
>> *Sent:* Saturday, August 3, 2013 12:33 AM
>> *Subject:* Re: [concurrency-interest] x86 NOOP memory barriers
>>
>>
>> > So because a putOrdered is a write to memory it cannot be reordered 
>> with
>> > other writes, as per "8.2.3.2 Neither Loads Nor Stores Are 
>> Reordered with
>> > Like Operations".
>>
>> Yes in combination with the compiler reordering restrictions.  In
>> Hotspot this is implemented within the
>> LibraryCall::inline_unsafe_ordered_store (library_call.cpp) call.
>> Look for:
>>
>> insert_mem_bar(Op_MemBarRelease);
>> insert_mem_bar(Op_MemBarCPUOrder);
>>
>> Mike.
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu 
>> <mailto:Concurrency-interest at cs.oswego.edu>
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130806/2020c5e1/attachment-0001.html>

From nathan.reynolds at oracle.com  Tue Aug  6 15:25:13 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Tue, 06 Aug 2013 12:25:13 -0700
Subject: [concurrency-interest] x86 NOOP memory barriers
In-Reply-To: <1375812687.3378.YahooMailNeo@web120704.mail.ne1.yahoo.com>
References: <1375436588.50646.YahooMailNeo@web120701.mail.ne1.yahoo.com>	<CAHjP37GGbpshebzxy8PF_+ZPKvJpc=1=RWsCmwCO-89XLdf1Lw@mail.gmail.com>	<1375442721.83454.YahooMailNeo@web120703.mail.ne1.yahoo.com>	<CALwNKeSrxua6MpLktrKgFaDTKCuacWmv_ZpRZdA3R33FsO04HA@mail.gmail.com>	<1375449342.6944.YahooMailNeo@web120701.mail.ne1.yahoo.com>	<CALwNKeQcEWzGEr4BOd5M5U9i6CcqApg2+HM9JdnadpGeQ1B_uQ@mail.gmail.com>
	<1375778284.31864.YahooMailNeo@web120706.mail.ne1.yahoo.com>
	<A3E67C2071F49C4CBC4F17E6D77CDDD23D46E140@G9W0725.americas.hpqcorp.net>
	<1375812687.3378.YahooMailNeo@web120704.mail.ne1.yahoo.com>
Message-ID: <52014D99.4050808@oracle.com>

Maybe you could provide links to the assembly so that the curious reader 
could dig deeper.

-Nathan

On 8/6/2013 11:11 AM, Nitsan Wakart wrote:
> Hi,
>
>> It would be nice to understand exactly what the difference in generated code is for the different versions whose performance you plotted in http://psy-lob-saw.blogspot.com/2013/05/using-jmh-to-benchmark-multi-threaded.html .
> I agree, at the time of writing I did printout the assembly, but considered the post long enough as it were (explaining the assembly would have added alot of work for me and reading for the audience). I don't recall the assembly to show anything suspect, but as all the code is included it is easy enough to generate. If I have the time I'll go through the exercise and add the printouts to the repository.
>
>>   I?m surprised by the increasing differences in the unshared case at high processor counts.  That suggests you are generating different memory traffic for some reason, perhaps because of optimization artifacts for this particular implementation.  AFAICT, the unshared cases should be embarrassingly parallel; there should be no real thread interaction?  Are you spacing out the elements far enough to deal with prefetching artifacts?
>
> If you are referring to the break in scalability from 8 threads onwards for the volatile and lazy case I completely agree with you. I spaced the elements out by one cache line, but if prefetching was the issue it would have affected the lower thread counts equally. The JMH version I was using at the time was quite early in the life of the tool, it may have had issues that caused poor scaling or interacted badly with this benchmark which is a nano-benchmark to use Shipilev's qualification. Alternatively, my code might be at fault, it's short enough and I had others review it before publication, but it can happen.
>
>>   I?m also surprised by the lazy vs. volatile differences in the shared case.  It seems to me the time should be completely dominated by coherence misses in either case.  There may be some unexpected odd optimization or lack thereof happening here.  In my limited experience, the impact of memory fences, etc. commonly decreases as scale increases, since those slowdowns are local to each core, and don?t affect the amount of memory traffic.  See for example the microbenchmark measurements in http://www.hpl.hp.com/techreports/2012/HPL-2012-218.html .
>
> It is my understanding that lazy set tends to dampen false sharing effects as the value is not immediately 'flushed' and can be modified while in the write queue. The less you 'force' the write, the less you contend on the cache line.
>
>> This benchmark is such that I have a hard time guessing what optimizations would be applied in each case, and I would expect that to vary a lot across JVMs.
> I only tested on one JVM, I expect you are right and the results will differ somewhat.
>
>
>> You?re probably recalculating the addresses of the array indices more in some cases than others.
> The array offset is calculated once per thread context. So once in all cases.
>
>> Can multiple increments even get combined in some cases?
>
> I'll have to have another look at the assembly. I don't think it happens, I vaguely recall verifying it in the assembly, but it's been a few months.
>
> Thanks for taking the time to read and make comments,
> Nitsan
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130806/1d20f20a/attachment.html>

From mjpt777 at gmail.com  Tue Aug  6 16:09:28 2013
From: mjpt777 at gmail.com (Martin Thompson)
Date: Tue, 6 Aug 2013 21:09:28 +0100
Subject: [concurrency-interest] x86 NOOP memory barriers
Message-ID: <CAChYfd-dnHGV93mfHdqDS9hXUK-sc_arV5QsU_J1vt5U+qBYGQ@mail.gmail.com>

> > ?I?m also surprised by the lazy vs. volatile differences in the shared
> case. ?It seems to me the time should be completely dominated by coherence
> misses in either case.? There may be some unexpected odd optimization or
> lack thereof happening here.? In my limited experience, the impact of
> memory fences, etc. commonly decreases as scale increases, since those
> slowdowns are local to each core, and don?t affect the amount of memory
> traffic.? See for example the microbenchmark measurements in
> http://www.hpl.hp.com/techreports/2012/HPL-2012-218.html .
>

It is my understanding that lazy set tends to dampen false sharing effects
> as the value is not immediately 'flushed' and can be modified while in the
> write queue. The less you 'force' the write, the less you contend on the
> cache line.
>

The lazySet resulting in a vanila MOV that can benefit from the write
combining buffers plus the core does not have to wait on the store buffer
draining as it does with the volatile write.  No flushing would be involved.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130806/8f371d99/attachment.html>

From niall at npgall.com  Wed Aug  7 18:22:18 2013
From: niall at npgall.com (Niall Gallagher)
Date: Wed, 7 Aug 2013 23:22:18 +0100
Subject: [concurrency-interest] Read-write-update lock
In-Reply-To: <4C5361F1-941A-42FB-878E-4B5B6619CAC2@npgall.com>
References: <E3D08A8F-5DDC-4910-839D-10FD594E54B9@npgall.com>
	<51E97712.2010306@oracle.com>
	<C14DFE22-0906-44E5-91DC-8FFF4A02D302@npgall.com>
	<51EDE2F1.40900@infinite-source.de>
	<27134512-6D21-4AD4-B071-7F62F2E22A0D@npgall.com>
	<CACuKZqGDB50RKqYfNZvY4MX=KAPa_S1c7QejaPyxVCxmG_e2fw@mail.gmail.com>
	<0F6BB315-447C-485F-AA31-B2F063557721@npgall.com>
	<CACuKZqFJHfOo4kLa=8z31TO8+5OfVeHZ+AL7QwFafHb6JZHQDA@mail.gmail.com>
	<4C5361F1-941A-42FB-878E-4B5B6619CAC2@npgall.com>
Message-ID: <4E62E544-16B6-40D9-AE78-246F0B35C2B8@npgall.com>

Hi All,

Just a heads up that I have released the Read-Write-Update lock implementation[1] to Maven central.

Thanks to all for the feedback on this mailing list. I've been following this list for a number of years and therefore I would not have felt comfortable releasing this lock had it not had some discussion with experts here first. The library has full test coverage, and Maven coordinates can be found on the site.

Kind regards,
Niall

[1] http://code.google.com/p/concurrent-locks/

On 23 Jul 2013, at 22:16, Niall Gallagher <niall at npgall.com> wrote:

> Yes indeed, that is the same concept.
> 
> I mentioned in my earlier post that it's not a new concept, as the earliest reference to it that I could find, was on the Linux kernel mailing list circa 2000 (thirteen years ago). It might be older still. I guess RDBMSes have implemented it, at least via cursors in that case. Your link shows that JavaDB/Derby implements it in Java, just not via a programmatic API (and perhaps not reentrant). I don't know if it was added to the Linux kernel in the end.
> 
> I also found ReaderWriterLockSlim[1] on the .Net platform. It is the default readers-writer lock implementation on that platform.
> 
> [1] http://msdn.microsoft.com/en-us/library/system.threading.readerwriterlockslim.aspx
> 
> 
> On 23 Jul 2013, at 15:06, Zhong Yu <zhong.j.yu at gmail.com> wrote:
> 
>> same concept in RDBMS -
>> http://docs.oracle.com/javadb/10.6.2.1/devguide/cdevconcepts36402.html
>> 
>> On Tue, Jul 23, 2013 at 12:28 AM, Niall Gallagher <niall at npgall.com> wrote:
>>> Hi Zhong,
>>> 
>>> Not exactly, but that is very close indeed. The only difference is in POST it does not acquire the read lock, so it's simply like this:
>>> 
>>>  updateLock.lock();  // exclusive
>>> ...
>>>  if(...)  // occasionally
>>>      writeLock.lock();  // upgrade
>>>      ...
>>>      writeLock.unlock();
>>>  ...
>>>  updateLock.unlock();
>>> 
>>> 
>>> I initially did think that I'd have to roll my own ReentrantReadWriteLock for the reason you mentioned, but then I found a way around it. My reasoning is, it's not necessary to acquire an actual read lock from ReentrantReadWriteLock after the update lock has been acquired, because here we basically invent a new type of lock anyway (the update lock) so we just say in the documentation that the update lock gives the application "permission" to read but not write. So the number of threads with read "permission" at any one time can be <number of threads holding regular read locks from ReentrantReadWriteLock> + <0 or 1 threads holding the update lock>.
>>> 
>>> The write lock is only granted when two conditions hold: the requesting thread must hold the update lock (enforced in the logic in RWULock), and other threads holding regular read locks must release those read locks (enforced by ReentrantReadWriteLock itself which RWULock delegates to internally). So this way it re-uses the ReentrantReadWriteLock of the JDK, no need for a custom implementation.
>>> 
>>> Best regards,
>>> Niall Gallagher
>>> 
>>> On 23 Jul 2013, at 05:29, Zhong Yu <zhong.j.yu at gmail.com> wrote:
>>> 
>>>> Hi Niall, my understanding is that you are trying to implement
>>>> something like this
>>>> 
>>>> Lock readLock = ...;
>>>> Lock writeLock = ...;
>>>> Lock updateLock = ...;
>>>> 
>>>> GET:
>>>> 
>>>>  readLock.lock();
>>>>  ...
>>>>  readLock.unlock();
>>>> 
>>>> 
>>>> POST:
>>>> 
>>>>  updateLock.lock();  // exclusive
>>>>  readLock.lock();
>>>> ...
>>>>  if(...)  // occasionally
>>>>      writeLock.lock();  // upgrade
>>>>      ...
>>>>      writeLock.unlock();
>>>>  ...
>>>>  readLock.unlock();
>>>>  updateLock.unlock();
>>>> 
>>>> 
>>>> except that upgrade r->w isn't allowed by ReentrantReadWriteLock so
>>>> you need to roll your own. Is that correct?
>>>> 
>>>> Zhong Yu
>>>> 
>>>> 
>>>> 
>>>> 
>>>> On Mon, Jul 22, 2013 at 10:48 PM, Niall Gallagher <niall at npgall.com> wrote:
>>>>> Actually the read-write-update lock solves that deadlock problem.
>>>>> 
>>>>> The update lock is not the same as the read lock[1]. Only one thread can acquire the update lock at a time. Only the thread holding the update lock can acquire the write lock. Threads which might need to write, should not acquire read locks but should acquire the update lock instead, and the implementation will enforce that correct usage with exceptions. So there is not a situation where two threads could mutually hold a lock that the other is waiting for, which prevents deadlock.
>>>>> 
>>>>> [1] http://code.google.com/p/concurrent-locks/
>>>>> 
>>>>> 
>>>>> On 23 Jul 2013, at 02:57, Aaron Grunthal <aaron.grunthal at infinite-source.de> wrote:
>>>>> 
>>>>>> The problem with a read-update is that the lock can never guarantee that the update is going to succeed. A write lock is generally considered an exclusive lock, think of transaction semantics. If two threads hold a read lock and both of them try to upgrade to a write lock neither can upgrade since an exclusive write lock would violate the shared semantics of a read lock. So at least one of them would have to stay in read mode, finished their logic in read-only mode and then relinquish the read lock before the other thread can even acquire the write lock.
>>>>>> 
>>>>>> So read-update will always have to support fail-and-retry or the logic would have to ensure that only one reader thread would try to upgrade to write at any given time and get priority over threads that try to acquire a write lock directly.
>>>>>> 
>>>>>> Of course there are special scenarios where you actually have "read up to X in read mode, acquire write mode for X+1" where the write lock would not violate violate the semantics of the earlier read chunks, but those things are different from a simple read-write lock, they're sets of range-locks, such as used by database engines.
>>>>>> 
>>>>>> So as far as I can see the "non-replayable request" issue wouldn't be solved by a generic read-tryUpgrade lock.
>>>>>> 
>>>>>> 
>>>>>> On 23.07.2013 01:43, Niall Gallagher wrote:
>>>>>>> The motivation for this read-write-update lock was actually a
>>>>>>> distributed systems problem.
>>>>>>> 
>>>>>>> The crux of the problem was that it was not feasible to "re-play" requests.
>>>>>>> 
>>>>>>> Broadly, requests entering a front-end system could be classified as
>>>>>>> being "read-only" (HTTP GET) or "possibly requiring write" (HTTP POST).
>>>>>>> On receiving a request, the front-end system would read from a local
>>>>>>> data store and then forward the request to back-end systems. The
>>>>>>> front-end system would gather responses from the back-end systems and
>>>>>>> from those determine if its local data store needed to be updated.
>>>>>>> 
>>>>>>> In a system like that, it's not possible for the front-end system to
>>>>>>> "try" the request using a read lock, and then if it later detects that
>>>>>>> it needs to write, drop the read lock and re-play the request using a
>>>>>>> write lock. First, because dropping the read lock would allow another
>>>>>>> thread to steal the write lock, causing lost updates, and second because
>>>>>>> re-playing the request over multiple back-end systems would be
>>>>>>> expensive. The only solution would be to acquire the write lock on every
>>>>>>> HTTP POST.
>>>>>>> 
>>>>>>> So I started to view the read-write lock as somewhat limiting.
>>>>>>> 
>>>>>>> There is also the performance aspect. For simplicity say 50% of requests
>>>>>>> are HTTP GET and 50% are HTTP POST. Any HTTP POST would acquire the
>>>>>>> write lock which would (1) block all reading threads even if only a
>>>>>>> small fraction of POSTs would require writes, and (2) would block all
>>>>>>> reading threads for the entire latency of interactions with back-end
>>>>>>> servers.
>>>>>>> 
>>>>>>> A read-write-update lock on the other hand, (1) does not block reading
>>>>>>> threads unless a write actually is performed, and (2) only blocks
>>>>>>> reading threads from the point at which the lock is upgraded, which in
>>>>>>> the case above is after responses from back-end systems have been
>>>>>>> gathered and so is for only a fraction of the overall processing time.
>>>>>>> 
>>>>>>> I think the example scales-down to simpler situations too. The key
>>>>>>> advantage of a read-write-update lock is in read-before-write access
>>>>>>> patterns.
>>>>>>> 
>>>>>>> Read-before-write is common. Many applications do it in three steps: (1)
>>>>>>> read data, (2) do some computations on the data, (3) write out the
>>>>>>> resulting data. A conventional read-write lock does not support this use
>>>>>>> case well.
>>>>>>> 
>>>>>>> At the very least, a read-write-update lock would increase concurrency
>>>>>>> by not blocking reading threads until a writing thread reaches step 3;
>>>>>>> concurrent reads would be allowed during steps 1 & 2. A conventional
>>>>>>> read-write lock would block reads for all three steps.
>>>>>>> 
>>>>>>> I think also in some systems it's basically tricky to implement re-play
>>>>>>> functionality, so a read-write-update lock[1] seems like a nice
>>>>>>> proposal. YMMV!
>>>>>>> 
>>>>>>> Niall Gallagher
>>>>>>> www.npgall.com <http://www.npgall.com>
>>>>>>> 
>>>>>>> [1] Concurrent-Locks: Read-write-update / upgradable read-write locks
>>>>>>> for Java
>>>>>>> http://code.google.com/p/concurrent-locks/
>>>>>>> 
>>>>>>> 
>>>>>>> On 19 Jul 2013, at 18:27, Nathan Reynolds <nathan.reynolds at oracle.com
>>>>>>> <mailto:nathan.reynolds at oracle.com>> wrote:
>>>>>>> 
>>>>>>>> I ran into upgrading a lock problem a while back.  The solution I took
>>>>>>>> is first have the thread read acquire the lock.  Then if the thread
>>>>>>>> detects that it needs the write lock, it releases the read lock and
>>>>>>>> starts over by acquiring the write lock.  Of course, all of the data
>>>>>>>> the thread collected in the read lock is considered invalid.  This
>>>>>>>> greatly improved scalability and the "upgrade" didn't hurt
>>>>>>>> performance.  This is because  writes mostly happen at start up and
>>>>>>>> are rare otherwise.
>>>>>>>> 
>>>>>>>> The above logic works great and there hasn't been a need for a
>>>>>>>> read-write-update lock.  The problem in my situation is that I can't
>>>>>>>> predict at the time of acquiring the lock if I will need to do any
>>>>>>>> writes.  The thread has to check the protected state and then decide
>>>>>>>> if any writes are necessary.  (I would guess this scenario is true of
>>>>>>>> most caches.)  If all threads do update acquires, then I am back to
>>>>>>>> the same contention I had with all threads doing write acquires.
>>>>>>>> 
>>>>>>>> I haven't needed this kind of lock.  Every time I run into lock
>>>>>>>> contention, I always solve the problem without it.  If I had such a
>>>>>>>> lock in my toolbox, maybe I would select it.
>>>>>>>> 
>>>>>>>> Can you give a good example of where update acquires can actually
>>>>>>>> help?  A toy example is one method which always needs to do reads and
>>>>>>>> another method which has to do a lot of reads with a very small
>>>>>>>> portion to do writes.  The first method is called very frequently
>>>>>>>> relative to the second method.  Is there such a situation in software?
>>>>>>>> -Nathan
>>>>>>>> On 7/19/2013 4:21 AM, Niall Gallagher wrote:
>>>>>>>>> Hi,
>>>>>>>>> 
>>>>>>>>> I guess it must have been discussed when ReentrantReadWriteLock was being written, the limitation that the read lock cannot be upgraded to a write lock.
>>>>>>>>> 
>>>>>>>>> Two threads hold a read lock, both then try to acquire the write lock -> deadlock. So if a read-mostly thread might ever need write access, it must either hold a write lock for the whole time (blocking other concurrent readers for its whole duration), or it must drop the read lock before acquiring the write lock, with the risk that another thread might steal the write lock from it rendering any data it read as stale.
>>>>>>>>> 
>>>>>>>>> I've written an extension to the basic readers-writer lock concept: a read-write-update lock for Java: ReentrantReadWriteUpdateLock in Concurrent-Locks on Google Code[1].
>>>>>>>>> 
>>>>>>>>> I was wondering if people on this mailing list would like to code review it, or get involved in the project, before I make a 1.0 release to Maven Central?
>>>>>>>>> 
>>>>>>>>> The idea for a read-write-update lock is not new. I see a reference to it on the Linux kernel mailing list from circa 2000 here[2]. Maybe the concept goes by other names also?
>>>>>>>>> 
>>>>>>>>> Some differences between my implementation and the Linux kernel mailing list discussion:
>>>>>>>>> - My implementation is reentrant, which means you actually can acquire the Mutex again if you already hold a Write lock
>>>>>>>>> - I didn't see any reason for Nothing -> Update to acquire both the Mutex and Read locks, so my implementation only acquires Mutex (which is enough to prevent other threads acquiring the Write lock anyway). In fact if it did acquire both, deadlock would occur in reentrant scenario Nothing -> Update (lock Mutex, lock Read), Update -> Update (lock Mutex again, lock Read again) followed by Update -> Write (drop Read, but hold count remains at 1, lock write = deadlock).
>>>>>>>>> 
>>>>>>>>> There is also an implementation of CompositeLock in the project for group-locking and group-unlocking collections of locks with rollback support, which I plan to use for locking nodes in hierarchical structures such as trees (I'm the author of Concurrent-Trees[3]).
>>>>>>>>> 
>>>>>>>>> For ReentrantReadWriteUpdateLock I re-used the JDK ReentrantReadWriteLock as much as possible, so you'll see the code is quite simple. So another question is do you think this kind of R-W-U-Lock might ever make it into the JDK, or is it still too esoteric?
>>>>>>>>> 
>>>>>>>>> Best regards,
>>>>>>>>> 
>>>>>>>>> Niall Gallagher
>>>>>>>>> www.npgall.com
>>>>>>>>> 
>>>>>>>>> [1] Concurrent-Locks: Read-write-update / upgradable read-write locks for Java
>>>>>>>>>  http://code.google.com/p/concurrent-locks/
>>>>>>>>> 
>>>>>>>>> [2] Linux kernel mailing list: Read/Write locks that can be changed into each other
>>>>>>>>>  http://lkml.indiana.edu/hypermail/linux/kernel/0004.3/0117.html
>>>>>>>>> 
>>>>>>>>> [3] Concurrent-Trees: Concurrent Radix and Concurrent Suffix Trees for Java
>>>>>>>>>  http://code.google.com/p/concurrent-trees/
>>>>>>>>> 
>>>>>>>>> 
>>>>>>>>> _______________________________________________
>>>>>>>>> Concurrency-interest mailing list
>>>>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>>> 
>>>>>>>> 
>>>>>>>> _______________________________________________
>>>>>>>> Concurrency-interest mailing list
>>>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>>>> <mailto:Concurrency-interest at cs.oswego.edu>
>>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>> 
>>>>>>> 
>>>>>>> 
>>>>>>> _______________________________________________
>>>>>>> Concurrency-interest mailing list
>>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>> 
>>>>>> 
>>>>>> _______________________________________________
>>>>>> Concurrency-interest mailing list
>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>> 
>>>>> 
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at cs.oswego.edu
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>> 
>>> 
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 



From oleksiy.stashok at oracle.com  Fri Aug  9 13:17:33 2013
From: oleksiy.stashok at oracle.com (Oleksiy Stashok)
Date: Fri, 09 Aug 2013 10:17:33 -0700
Subject: [concurrency-interest] ConcurrentHashMapV8 typo?
Message-ID: <5205242D.7060104@oracle.com>

Hi,

guys can you pls. doublecheck the method 
ConcurrentHashMapV8.contendedLock() : 2689
The comparison

else if ((s | WAITER) == 0)


looks suspicious, shouldn't it be

else if ((s & WAITER) == 0)

?

Thanks.

WBR,
Alexey.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130809/01182380/attachment.html>

From dl at cs.oswego.edu  Fri Aug  9 14:40:58 2013
From: dl at cs.oswego.edu (Doug Lea)
Date: Fri, 09 Aug 2013 14:40:58 -0400
Subject: [concurrency-interest] ConcurrentHashMapV8 typo?
In-Reply-To: <5205242D.7060104@oracle.com>
References: <5205242D.7060104@oracle.com>
Message-ID: <520537BA.3040601@cs.oswego.edu>

On 08/09/13 13:17, Oleksiy Stashok wrote:
> Hi,
>
> guys can you pls. doublecheck the method ConcurrentHashMapV8.contendedLock() : 2689
> The comparison
>
> else if ((s | WAITER) == 0)

Wow. Thanks/sorry. This crept in during a refactoring.
(And from there propagated elsewhere, including JDK8 CHM.)

-Doug


>
>
> looks suspicious, shouldn't it be
>
> else if ((s & WAITER) == 0)
>
> ?
>
> Thanks.
>
> WBR,
> Alexey.
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From gustav.r.akesson at gmail.com  Sat Aug 10 14:45:42 2013
From: gustav.r.akesson at gmail.com (=?ISO-8859-1?Q?Gustav_=C5kesson?=)
Date: Sat, 10 Aug 2013 20:45:42 +0200
Subject: [concurrency-interest]  Implicit parallelism
Message-ID: <CAKEw5+4rcZKiizq26fLaZbYb9NPtb6XWYfxZp5fEbNY9+iG9+A@mail.gmail.com>

Hi guys,

My discussion here concerns implicit parallelism. Let's say we have the
following setup:

@Immutable
public class Integer
{
  ...
}

@Immutable
public class ImmutableArrayList
{
   ...
}

I'm looking for a way so that the parallelism would be introduced without
hard-coding anything related to parallelism (i.e. not stating anything like
.parallel or .par on the collection). Only thing needed would be something
annotation-ish which tells the execution environment that this
datastructure with elements is inherently thread-safe. Then the execution
could determine if it would be beneficial to do so. For instance, for a
structure with e.g. five elements, then it would not, but for millions of
elements, it would most likely be. Perhaps it could even find some kind of
sweet-spot of number of elements in which the parallel overhead exceeds the
benefits.

Let's say we wish to sum all the integers in an ImmutableArrayList (setup
below), would it be possible for the compiler (javac, scalac or what have
you) and JVM to conspire and decide "hey, let's run this in parallel since
it doesn't violate application semantics and it can/will be faster"? Is
there any research in this area in regards to the JVM?


Best Regards,

Gustav ?kesson
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130810/b5ff3a49/attachment.html>

From aaron.grunthal at infinite-source.de  Sat Aug 10 17:18:48 2013
From: aaron.grunthal at infinite-source.de (Aaron Grunthal)
Date: Sat, 10 Aug 2013 23:18:48 +0200
Subject: [concurrency-interest] Implicit parallelism
In-Reply-To: <CAKEw5+4rcZKiizq26fLaZbYb9NPtb6XWYfxZp5fEbNY9+iG9+A@mail.gmail.com>
References: <CAKEw5+4rcZKiizq26fLaZbYb9NPtb6XWYfxZp5fEbNY9+iG9+A@mail.gmail.com>
Message-ID: <5206AE38.2030900@infinite-source.de>

Thread-safety in the context of data structure usually (but not always) 
refers to consistent behavior when it is concurrently *modified*.

In your example you mention summing over a list. That's a read-only 
action and doesn't require immutable lists or special concurrency 
support. Most data structures are well-behaved under concurrent 
read-only access, barring exceptions such as access-ordered LinkedHashMaps.

Java 8's parallel streams also are smart enough to not parallelize on 
small data sets where the overhead would kill any performance advantage. 
At least for data structures that allow size-estimates or actual sizes 
to be obtained.

I think there shouldn't be anything keeping you from adding a .parallel 
if you anticipate the possibility of your data transformations handling 
non-trivial amounts of data.

- Aaron


On 10.08.2013 20:45, Gustav ?kesson wrote:
> Hi guys,
>
> My discussion here concerns implicit parallelism. Let's say we have the
> following setup:
>
> @Immutable
> public class Integer
> {
>    ...
> }
>
> @Immutable
> public class ImmutableArrayList
> {
>     ...
> }
>
> I'm looking for a way so that the parallelism would be introduced
> without hard-coding anything related to parallelism (i.e. not stating
> anything like .parallel or .par on the collection). Only thing needed
> would be something annotation-ish which tells the execution environment
> that this datastructure with elements is inherently thread-safe. Then
> the execution could determine if it would be beneficial to do so. For
> instance, for a structure with e.g. five elements, then it would not,
> but for millions of elements, it would most likely be. Perhaps it could
> even find some kind of sweet-spot of number of elements in which the
> parallel overhead exceeds the benefits.
>
> Let's say we wish to sum all the integers in an ImmutableArrayList
> (setup below), would it be possible for the compiler (javac, scalac or
> what have you) and JVM to conspire and decide "hey, let's run this in
> parallel since it doesn't violate application semantics and it can/will
> be faster"? Is there any research in this area in regards to the JVM?
>
>
> Best Regards,
>
> Gustav ?kesson
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From gustav.r.akesson at gmail.com  Sun Aug 11 04:44:20 2013
From: gustav.r.akesson at gmail.com (=?ISO-8859-1?Q?Gustav_=C5kesson?=)
Date: Sun, 11 Aug 2013 10:44:20 +0200
Subject: [concurrency-interest] Implicit parallelism
In-Reply-To: <5206AE38.2030900@infinite-source.de>
References: <CAKEw5+4rcZKiizq26fLaZbYb9NPtb6XWYfxZp5fEbNY9+iG9+A@mail.gmail.com>
	<5206AE38.2030900@infinite-source.de>
Message-ID: <CAKEw5+4Qk-pMuzai+WH-KMGukkUMwkTj=XbreY5OJhPsJhvz7Q@mail.gmail.com>

Hi,

My point is that the programmer should (in some circumstances) not care if
it runs in parallel or not - the execution determines this for you as long
as the program semantics are not broken. That's why I don't want to see
.parallel or similar - that is somewhat explicit parallelism. As you say,
the example I gave was read-only so then it should (theoretically) be
possible to determine that this is safe to run in parallel. The list can't
change, nor the elements.

But let's take another example. Let's say we have a Scala (immutable) list
containing Int type. If we then add all integers from 0 to x...

for (i <- 0 to x) {
 immutableIntegerList = i :: immutableIntegerList
}

Or in Java, a locally declared array which we fill with values:

int[] arr = new int[x]

for (int i=0; i<x; ++i)
 arr[i] = i
}

In my mind it should be possible to split the loop constructs into y parts
and then concatenate the result in order. Similar to OpenMP but without the
explicit parallelism annotation. I know this question is of rather academic
type, but I'm wondering whether there has been any ideas or work to bring
this type of functionality to the JVM (and the compilers).


Best Regards,
Gustav ?kesson


On Sat, Aug 10, 2013 at 11:18 PM, Aaron Grunthal <
aaron.grunthal at infinite-source.de> wrote:

> Thread-safety in the context of data structure usually (but not always)
> refers to consistent behavior when it is concurrently *modified*.
>
> In your example you mention summing over a list. That's a read-only action
> and doesn't require immutable lists or special concurrency support. Most
> data structures are well-behaved under concurrent read-only access, barring
> exceptions such as access-ordered LinkedHashMaps.
>
> Java 8's parallel streams also are smart enough to not parallelize on
> small data sets where the overhead would kill any performance advantage. At
> least for data structures that allow size-estimates or actual sizes to be
> obtained.
>
> I think there shouldn't be anything keeping you from adding a .parallel if
> you anticipate the possibility of your data transformations handling
> non-trivial amounts of data.
>
> - Aaron
>
>
>
> On 10.08.2013 20:45, Gustav ?kesson wrote:
>
>> Hi guys,
>>
>> My discussion here concerns implicit parallelism. Let's say we have the
>> following setup:
>>
>> @Immutable
>> public class Integer
>> {
>>    ...
>> }
>>
>> @Immutable
>> public class ImmutableArrayList
>> {
>>     ...
>> }
>>
>> I'm looking for a way so that the parallelism would be introduced
>> without hard-coding anything related to parallelism (i.e. not stating
>> anything like .parallel or .par on the collection). Only thing needed
>> would be something annotation-ish which tells the execution environment
>> that this datastructure with elements is inherently thread-safe. Then
>> the execution could determine if it would be beneficial to do so. For
>> instance, for a structure with e.g. five elements, then it would not,
>> but for millions of elements, it would most likely be. Perhaps it could
>> even find some kind of sweet-spot of number of elements in which the
>> parallel overhead exceeds the benefits.
>>
>> Let's say we wish to sum all the integers in an ImmutableArrayList
>> (setup below), would it be possible for the compiler (javac, scalac or
>> what have you) and JVM to conspire and decide "hey, let's run this in
>> parallel since it doesn't violate application semantics and it can/will
>> be faster"? Is there any research in this area in regards to the JVM?
>>
>>
>> Best Regards,
>>
>> Gustav ?kesson
>>
>>
>> ______________________________**_________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
>> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>
>>
> ______________________________**_________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130811/8b2bb7eb/attachment.html>

From oleksandr.otenko at oracle.com  Mon Aug 12 07:01:46 2013
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Mon, 12 Aug 2013 12:01:46 +0100
Subject: [concurrency-interest] Implicit parallelism
In-Reply-To: <CAKEw5+4Qk-pMuzai+WH-KMGukkUMwkTj=XbreY5OJhPsJhvz7Q@mail.gmail.com>
References: <CAKEw5+4rcZKiizq26fLaZbYb9NPtb6XWYfxZp5fEbNY9+iG9+A@mail.gmail.com>
	<5206AE38.2030900@infinite-source.de>
	<CAKEw5+4Qk-pMuzai+WH-KMGukkUMwkTj=XbreY5OJhPsJhvz7Q@mail.gmail.com>
Message-ID: <5208C09A.6080806@oracle.com>

But the real problem is not determining a large enough array to sum in 
parallel, rather:

a. determining the operation you do over the array is monoidal
b. determining the actual cost of operation; summing has silly cost, you 
want to parallellize even arrays of tens of elements, if processing each 
element is costly

Alex

On 11/08/2013 09:44, Gustav ?kesson wrote:
> Hi,
>
> My point is that the programmer should (in some circumstances) not 
> care if it runs in parallel or not - the execution determines this for 
> you as long as the program semantics are not broken. That's why I 
> don't want to see .parallel or similar - that is somewhat explicit 
> parallelism. As you say, the example I gave was read-only so then it 
> should (theoretically) be possible to determine that this is safe to 
> run in parallel. The list can't change, nor the elements.
>
> But let's take another example. Let's say we have a Scala (immutable) 
> list containing Int type. If we then add all integers from 0 to x...
>
> for (i <- 0 to x) {
>  immutableIntegerList = i :: immutableIntegerList
> }
>
> Or in Java, a locally declared array which we fill with values:
>
> int[] arr = new int[x]
>
> for (int i=0; i<x; ++i)
>  arr[i] = i
> }
>
> In my mind it should be possible to split the loop constructs into y 
> parts and then concatenate the result in order. Similar to OpenMP but 
> without the explicit parallelism annotation. I know this question is 
> of rather academic type, but I'm wondering whether there has been any 
> ideas or work to bring this type of functionality to the JVM (and the 
> compilers).
>
>
> Best Regards,
> Gustav ?kesson
>
>
> On Sat, Aug 10, 2013 at 11:18 PM, Aaron Grunthal 
> <aaron.grunthal at infinite-source.de 
> <mailto:aaron.grunthal at infinite-source.de>> wrote:
>
>     Thread-safety in the context of data structure usually (but not
>     always) refers to consistent behavior when it is concurrently
>     *modified*.
>
>     In your example you mention summing over a list. That's a
>     read-only action and doesn't require immutable lists or special
>     concurrency support. Most data structures are well-behaved under
>     concurrent read-only access, barring exceptions such as
>     access-ordered LinkedHashMaps.
>
>     Java 8's parallel streams also are smart enough to not parallelize
>     on small data sets where the overhead would kill any performance
>     advantage. At least for data structures that allow size-estimates
>     or actual sizes to be obtained.
>
>     I think there shouldn't be anything keeping you from adding a
>     .parallel if you anticipate the possibility of your data
>     transformations handling non-trivial amounts of data.
>
>     - Aaron
>
>
>
>     On 10.08.2013 20:45, Gustav ?kesson wrote:
>
>         Hi guys,
>
>         My discussion here concerns implicit parallelism. Let's say we
>         have the
>         following setup:
>
>         @Immutable
>         public class Integer
>         {
>            ...
>         }
>
>         @Immutable
>         public class ImmutableArrayList
>         {
>             ...
>         }
>
>         I'm looking for a way so that the parallelism would be introduced
>         without hard-coding anything related to parallelism (i.e. not
>         stating
>         anything like .parallel or .par on the collection). Only thing
>         needed
>         would be something annotation-ish which tells the execution
>         environment
>         that this datastructure with elements is inherently
>         thread-safe. Then
>         the execution could determine if it would be beneficial to do
>         so. For
>         instance, for a structure with e.g. five elements, then it
>         would not,
>         but for millions of elements, it would most likely be. Perhaps
>         it could
>         even find some kind of sweet-spot of number of elements in
>         which the
>         parallel overhead exceeds the benefits.
>
>         Let's say we wish to sum all the integers in an ImmutableArrayList
>         (setup below), would it be possible for the compiler (javac,
>         scalac or
>         what have you) and JVM to conspire and decide "hey, let's run
>         this in
>         parallel since it doesn't violate application semantics and it
>         can/will
>         be faster"? Is there any research in this area in regards to
>         the JVM?
>
>
>         Best Regards,
>
>         Gustav ?kesson
>
>
>         _______________________________________________
>         Concurrency-interest mailing list
>         Concurrency-interest at cs.oswego.edu
>         <mailto:Concurrency-interest at cs.oswego.edu>
>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130812/8a5e295f/attachment.html>

From gustav.r.akesson at gmail.com  Mon Aug 12 07:59:51 2013
From: gustav.r.akesson at gmail.com (=?ISO-8859-1?Q?Gustav_=C5kesson?=)
Date: Mon, 12 Aug 2013 13:59:51 +0200
Subject: [concurrency-interest] Implicit parallelism
In-Reply-To: <5208C09A.6080806@oracle.com>
References: <CAKEw5+4rcZKiizq26fLaZbYb9NPtb6XWYfxZp5fEbNY9+iG9+A@mail.gmail.com>
	<5206AE38.2030900@infinite-source.de>
	<CAKEw5+4Qk-pMuzai+WH-KMGukkUMwkTj=XbreY5OJhPsJhvz7Q@mail.gmail.com>
	<5208C09A.6080806@oracle.com>
Message-ID: <CAKEw5+4m8At+tmZnL5vgN0VbKJ5B8UsgFvC7pvKw2Zi=vV99tQ@mail.gmail.com>

That is absolutely true, but if it a code snippet is deemed to be able to
run in parallel then profiling during runtime could determine how it should
behave depending on n and cost of operation. Some sort of dynamic
optimization which could even make better decisions than you as a
programmer.


Best Regards,
Gustav ?kesson


On Mon, Aug 12, 2013 at 1:01 PM, Oleksandr Otenko <
oleksandr.otenko at oracle.com> wrote:

>  But the real problem is not determining a large enough array to sum in
> parallel, rather:
>
> a. determining the operation you do over the array is monoidal
> b. determining the actual cost of operation; summing has silly cost, you
> want to parallellize even arrays of tens of elements, if processing each
> element is costly
>
> Alex
>
>
> On 11/08/2013 09:44, Gustav ?kesson wrote:
>
> Hi,
>
>  My point is that the programmer should (in some circumstances) not care
> if it runs in parallel or not - the execution determines this for you as
> long as the program semantics are not broken. That's why I don't want to
> see .parallel or similar - that is somewhat explicit parallelism. As you
> say, the example I gave was read-only so then it should (theoretically) be
> possible to determine that this is safe to run in parallel. The list can't
> change, nor the elements.
>
>  But let's take another example. Let's say we have a Scala (immutable)
> list containing Int type. If we then add all integers from 0 to x...
>
>  for (i <- 0 to x) {
>   immutableIntegerList = i :: immutableIntegerList
> }
>
>  Or in Java, a locally declared array which we fill with values:
>
>  int[] arr = new int[x]
>
>  for (int i=0; i<x; ++i)
>  arr[i] = i
> }
>
>  In my mind it should be possible to split the loop constructs into y
> parts and then concatenate the result in order. Similar to OpenMP but
> without the explicit parallelism annotation. I know this question is of
> rather academic type, but I'm wondering whether there has been any ideas or
> work to bring this type of functionality to the JVM (and the compilers).
>
>
>  Best Regards,
> Gustav ?kesson
>
>
> On Sat, Aug 10, 2013 at 11:18 PM, Aaron Grunthal <
> aaron.grunthal at infinite-source.de> wrote:
>
>> Thread-safety in the context of data structure usually (but not always)
>> refers to consistent behavior when it is concurrently *modified*.
>>
>> In your example you mention summing over a list. That's a read-only
>> action and doesn't require immutable lists or special concurrency support.
>> Most data structures are well-behaved under concurrent read-only access,
>> barring exceptions such as access-ordered LinkedHashMaps.
>>
>> Java 8's parallel streams also are smart enough to not parallelize on
>> small data sets where the overhead would kill any performance advantage. At
>> least for data structures that allow size-estimates or actual sizes to be
>> obtained.
>>
>> I think there shouldn't be anything keeping you from adding a .parallel
>> if you anticipate the possibility of your data transformations handling
>> non-trivial amounts of data.
>>
>> - Aaron
>>
>>
>>
>> On 10.08.2013 20:45, Gustav ?kesson wrote:
>>
>>>  Hi guys,
>>>
>>> My discussion here concerns implicit parallelism. Let's say we have the
>>> following setup:
>>>
>>> @Immutable
>>> public class Integer
>>> {
>>>    ...
>>> }
>>>
>>> @Immutable
>>> public class ImmutableArrayList
>>> {
>>>     ...
>>> }
>>>
>>> I'm looking for a way so that the parallelism would be introduced
>>> without hard-coding anything related to parallelism (i.e. not stating
>>> anything like .parallel or .par on the collection). Only thing needed
>>> would be something annotation-ish which tells the execution environment
>>> that this datastructure with elements is inherently thread-safe. Then
>>> the execution could determine if it would be beneficial to do so. For
>>> instance, for a structure with e.g. five elements, then it would not,
>>> but for millions of elements, it would most likely be. Perhaps it could
>>> even find some kind of sweet-spot of number of elements in which the
>>> parallel overhead exceeds the benefits.
>>>
>>> Let's say we wish to sum all the integers in an ImmutableArrayList
>>> (setup below), would it be possible for the compiler (javac, scalac or
>>> what have you) and JVM to conspire and decide "hey, let's run this in
>>> parallel since it doesn't violate application semantics and it can/will
>>> be faster"? Is there any research in this area in regards to the JVM?
>>>
>>>
>>> Best Regards,
>>>
>>> Gustav ?kesson
>>>
>>>
>>>  _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
>
> _______________________________________________
> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130812/6ddd14e5/attachment-0001.html>

From unmeshjoshi at gmail.com  Mon Aug 12 09:03:50 2013
From: unmeshjoshi at gmail.com (Unmesh Joshi)
Date: Mon, 12 Aug 2013 18:33:50 +0530
Subject: [concurrency-interest] Benchmark to demonstrate improvement in
	thread management over the years.
Message-ID: <CAOk+zfdkhXwf5AyXFb74_4diGDFagSoKTQrZwTA3qzM0ymXUCw@mail.gmail.com>

Hi,

Most of the books on node.js, Akka, Play or any other event IO based system
frequently talk about 'Threads' being heavy and there is cost we have to
pay for all the booking the OS or the JVM has to do with all the threads.
While I agree that there must be some cost and for doing CPU intensive
tasks like matrix multiplication, and fork-join kind of framework will be
more performant, I am not sure if for web server kind of IO intensive
application that's the case.

On the contrary, I am seeing web servers running on tomcat with 1000 +
threads without issues.  For web servers. I think that Linux level thread
management has improved a lot in last 10 years. Same is with the JVM.

Do we have any benchmark which shows how much Linux thread management and
JVM thread management have improved over the years?

Thanks,
Unmesh
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130812/df4fadea/attachment.html>

From vitalyd at gmail.com  Mon Aug 12 09:50:18 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Mon, 12 Aug 2013 09:50:18 -0400
Subject: [concurrency-interest] Benchmark to demonstrate improvement in
 thread management over the years.
In-Reply-To: <CAOk+zfdkhXwf5AyXFb74_4diGDFagSoKTQrZwTA3qzM0ymXUCw@mail.gmail.com>
References: <CAOk+zfdkhXwf5AyXFb74_4diGDFagSoKTQrZwTA3qzM0ymXUCw@mail.gmail.com>
Message-ID: <CAHjP37Gf6XaBtj_L1EtL-0rLAABrWQ=X_C7xHZV=poBHoGv65Q@mail.gmail.com>

I don't have any benchmarks to give, but I don't think the touted benefits
of an evented model includes CPU performance.  Rather, using an evented
model allows you to scale.  Specific to a web server, you want to be able
to handle lots of concurrent connections (most of them are probably idle at
any given time) while minimizing resource usage to accomplish that.

With a thread-per-request (threaded) model, you may end up using lots of
threads but most of them are blocked on i/o at any given time.  A slow
client/consumer can tie up a thread for a very long time.  This also makes
the server susceptible to a DDoS attack whereby new connections are
established, but the clients are purposely slow to tie up the server
threads.  Resource usage is also much higher in the threaded model when you
have tens of thousands of connections since you're going to pay for stack
space for each thread (granted it's VM space, but still).

With an evented model, you don't have the inefficiency of having thousands
of threads alive but that are blocked/waiting on i/o.  A single thread
dedicated to multiplexing i/o across all the connections will probably be
sufficient.  The rest is worker threads (most likely = # of CPUs for a
dedicated machine) that actually handle the request processing, but don't
do any (significant) i/o.  This design also means that you can handle slow
clients in a more robust manner.

So, the cost of threads can be "heavy" in the case of very busy web
servers.  The Linux kernel should handle a few thousand threads (most
blocked on io) quite well, but I don't think that will be the case for tens
or hundreds of thousands.  Even if there's sufficient RAM to handle that
many, there may be performance issues coming from the kernel itself, e.g.
scheduler.  At the very least, you'll be using resources of the machine
inefficiently under that setup.

Vitaly

Sent from my phone
On Aug 12, 2013 9:13 AM, "Unmesh Joshi" <unmeshjoshi at gmail.com> wrote:

> Hi,
>
> Most of the books on node.js, Akka, Play or any other event IO based
> system frequently talk about 'Threads' being heavy and there is cost we
> have to pay for all the booking the OS or the JVM has to do with all the
> threads.
> While I agree that there must be some cost and for doing CPU intensive
> tasks like matrix multiplication, and fork-join kind of framework will be
> more performant, I am not sure if for web server kind of IO intensive
> application that's the case.
>
> On the contrary, I am seeing web servers running on tomcat with 1000 +
> threads without issues.  For web servers. I think that Linux level thread
> management has improved a lot in last 10 years. Same is with the JVM.
>
> Do we have any benchmark which shows how much Linux thread management and
> JVM thread management have improved over the years?
>
> Thanks,
> Unmesh
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130812/8452c5aa/attachment.html>

From oleksandr.otenko at oracle.com  Mon Aug 12 10:06:28 2013
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Mon, 12 Aug 2013 15:06:28 +0100
Subject: [concurrency-interest] Benchmark to demonstrate improvement in
 thread management over the years.
In-Reply-To: <CAOk+zfdkhXwf5AyXFb74_4diGDFagSoKTQrZwTA3qzM0ymXUCw@mail.gmail.com>
References: <CAOk+zfdkhXwf5AyXFb74_4diGDFagSoKTQrZwTA3qzM0ymXUCw@mail.gmail.com>
Message-ID: <5208EBE4.5060406@oracle.com>

http://www.slideshare.net/e456/tyma-paulmultithreaded1

Alex

On 12/08/2013 14:03, Unmesh Joshi wrote:
> Hi,
>
> Most of the books on node.js, Akka, Play or any other event IO based 
> system frequently talk about 'Threads' being heavy and there is cost 
> we have to pay for all the booking the OS or the JVM has to do with 
> all the threads.
> While I agree that there must be some cost and for doing CPU intensive 
> tasks like matrix multiplication, and fork-join kind of framework will 
> be more performant, I am not sure if for web server kind of IO 
> intensive application that's the case.
>
> On the contrary, I am seeing web servers running on tomcat with 1000 + 
> threads without issues.  For web servers. I think that Linux level 
> thread management has improved a lot in last 10 years. Same is with 
> the JVM.
>
> Do we have any benchmark which shows how much Linux thread management 
> and JVM thread management have improved over the years?
>
> Thanks,
> Unmesh
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130812/504bb2ac/attachment.html>

From oleksandr.otenko at oracle.com  Mon Aug 12 10:18:04 2013
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Mon, 12 Aug 2013 15:18:04 +0100
Subject: [concurrency-interest] Benchmark to demonstrate improvement in
 thread management over the years.
In-Reply-To: <CAHjP37Gf6XaBtj_L1EtL-0rLAABrWQ=X_C7xHZV=poBHoGv65Q@mail.gmail.com>
References: <CAOk+zfdkhXwf5AyXFb74_4diGDFagSoKTQrZwTA3qzM0ymXUCw@mail.gmail.com>
	<CAHjP37Gf6XaBtj_L1EtL-0rLAABrWQ=X_C7xHZV=poBHoGv65Q@mail.gmail.com>
Message-ID: <5208EE9C.9070308@oracle.com>

In Soviet Russia, requests schedule you!

Jokes aside, you replace OS scheduler with your own scheduling of the 
tasks / continuations / connection states. The design permits management 
and cleanup of what otherwise would be threads in a application-specific 
manner:

  * client died? request is stale? component failed? just GC the
    connection state. In a threaded design you'd need to propagate some
    exception through the layers to unwind the stack to the necessary level.
  * some class of errors are not possible, because some designs are hard
    to do (eg locking is hard and because of that, deadlocking is hard, too)

But also some other designs are hard to do - try to associate security 
context, thread locals, classloader with the thread that is currently 
working on the task / continuation / connection state. The threaded API 
doesn't interoperate well with evented, and the other way around.

You can't just say "right, from now on I'll work in evented model". 
Replacing a couple of places where IO is done with evented, is not the 
panacea - the entire software stack needs CPT-ing.

Alex


On 12/08/2013 14:50, Vitaly Davidovich wrote:
>
> I don't have any benchmarks to give, but I don't think the touted 
> benefits of an evented model includes CPU performance.  Rather, using 
> an evented model allows you to scale.  Specific to a web server, you 
> want to be able to handle lots of concurrent connections (most of them 
> are probably idle at any given time) while minimizing resource usage 
> to accomplish that.
>
> With a thread-per-request (threaded) model, you may end up using lots 
> of threads but most of them are blocked on i/o at any given time.  A 
> slow client/consumer can tie up a thread for a very long time.  This 
> also makes the server susceptible to a DDoS attack whereby new 
> connections are established, but the clients are purposely slow to tie 
> up the server threads. Resource usage is also much higher in the 
> threaded model when you have tens of thousands of connections since 
> you're going to pay for stack space for each thread (granted it's VM 
> space, but still).
>
> With an evented model, you don't have the inefficiency of having 
> thousands of threads alive but that are blocked/waiting on i/o.  A 
> single thread dedicated to multiplexing i/o across all the connections 
> will probably be sufficient.  The rest is worker threads (most likely 
> = # of CPUs for a dedicated machine) that actually handle the request 
> processing, but don't do any (significant) i/o.  This design also 
> means that you can handle slow clients in a more robust manner.
>
> So, the cost of threads can be "heavy" in the case of very busy web 
> servers.  The Linux kernel should handle a few thousand threads (most 
> blocked on io) quite well, but I don't think that will be the case for 
> tens or hundreds of thousands. Even if there's sufficient RAM to 
> handle that many, there may be performance issues coming from the 
> kernel itself, e.g. scheduler.  At the very least, you'll be using 
> resources of the machine inefficiently under that setup.
>
> Vitaly
>
> Sent from my phone
>
> On Aug 12, 2013 9:13 AM, "Unmesh Joshi" <unmeshjoshi at gmail.com 
> <mailto:unmeshjoshi at gmail.com>> wrote:
>
>     Hi,
>
>     Most of the books on node.js, Akka, Play or any other event IO
>     based system frequently talk about 'Threads' being heavy and there
>     is cost we have to pay for all the booking the OS or the JVM has
>     to do with all the threads.
>     While I agree that there must be some cost and for doing CPU
>     intensive tasks like matrix multiplication, and fork-join kind of
>     framework will be more performant, I am not sure if for web server
>     kind of IO intensive application that's the case.
>
>     On the contrary, I am seeing web servers running on tomcat with
>     1000 + threads without issues.  For web servers. I think that
>     Linux level thread management has improved a lot in last 10 years.
>     Same is with the JVM.
>
>     Do we have any benchmark which shows how much Linux thread
>     management and JVM thread management have improved over the years?
>
>     Thanks,
>     Unmesh
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130812/c65fa121/attachment-0001.html>

From james.roper at typesafe.com  Mon Aug 12 10:27:50 2013
From: james.roper at typesafe.com (James Roper)
Date: Tue, 13 Aug 2013 00:27:50 +1000
Subject: [concurrency-interest] Benchmark to demonstrate improvement in
 thread management over the years.
In-Reply-To: <CAHjP37Gf6XaBtj_L1EtL-0rLAABrWQ=X_C7xHZV=poBHoGv65Q@mail.gmail.com>
References: <CAOk+zfdkhXwf5AyXFb74_4diGDFagSoKTQrZwTA3qzM0ymXUCw@mail.gmail.com>
	<CAHjP37Gf6XaBtj_L1EtL-0rLAABrWQ=X_C7xHZV=poBHoGv65Q@mail.gmail.com>
Message-ID: <CABY0rKNWMgPyxoS5TiLtQskJrwbtFk-AjzE_S2GZ0yQcKnMc9w@mail.gmail.com>

It's also worth pointing out that the thread per request model is becoming
less feasible even for simple web apps. Modern service oriented
architectures often require that a single web request may make many
requests to other backend services. At the extreme, we see users writing
Play apps that make hundreds of backend API calls per request. In order to
provide acceptable response times, these requests must be made in parallel.
With blocking IO, that would mean a single request might take 100 threads,
if you had just 100 concurrent requests, that's 10000 threads, if each
thread stack takes 100kb of real memory, that's 1GB memory just for thread
stacks. That's not cheap.

Regards,

James
On Aug 13, 2013 12:08 AM, "Vitaly Davidovich" <vitalyd at gmail.com> wrote:

> I don't have any benchmarks to give, but I don't think the touted benefits
> of an evented model includes CPU performance.  Rather, using an evented
> model allows you to scale.  Specific to a web server, you want to be able
> to handle lots of concurrent connections (most of them are probably idle at
> any given time) while minimizing resource usage to accomplish that.
>
> With a thread-per-request (threaded) model, you may end up using lots of
> threads but most of them are blocked on i/o at any given time.  A slow
> client/consumer can tie up a thread for a very long time.  This also makes
> the server susceptible to a DDoS attack whereby new connections are
> established, but the clients are purposely slow to tie up the server
> threads.  Resource usage is also much higher in the threaded model when you
> have tens of thousands of connections since you're going to pay for stack
> space for each thread (granted it's VM space, but still).
>
> With an evented model, you don't have the inefficiency of having thousands
> of threads alive but that are blocked/waiting on i/o.  A single thread
> dedicated to multiplexing i/o across all the connections will probably be
> sufficient.  The rest is worker threads (most likely = # of CPUs for a
> dedicated machine) that actually handle the request processing, but don't
> do any (significant) i/o.  This design also means that you can handle slow
> clients in a more robust manner.
>
> So, the cost of threads can be "heavy" in the case of very busy web
> servers.  The Linux kernel should handle a few thousand threads (most
> blocked on io) quite well, but I don't think that will be the case for tens
> or hundreds of thousands.  Even if there's sufficient RAM to handle that
> many, there may be performance issues coming from the kernel itself, e.g.
> scheduler.  At the very least, you'll be using resources of the machine
> inefficiently under that setup.
>
> Vitaly
>
> Sent from my phone
> On Aug 12, 2013 9:13 AM, "Unmesh Joshi" <unmeshjoshi at gmail.com> wrote:
>
>> Hi,
>>
>> Most of the books on node.js, Akka, Play or any other event IO based
>> system frequently talk about 'Threads' being heavy and there is cost we
>> have to pay for all the booking the OS or the JVM has to do with all the
>> threads.
>> While I agree that there must be some cost and for doing CPU intensive
>> tasks like matrix multiplication, and fork-join kind of framework will be
>> more performant, I am not sure if for web server kind of IO intensive
>> application that's the case.
>>
>> On the contrary, I am seeing web servers running on tomcat with 1000 +
>> threads without issues.  For web servers. I think that Linux level thread
>> management has improved a lot in last 10 years. Same is with the JVM.
>>
>> Do we have any benchmark which shows how much Linux thread management and
>> JVM thread management have improved over the years?
>>
>> Thanks,
>> Unmesh
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130813/c14073a7/attachment.html>

From vitalyd at gmail.com  Mon Aug 12 11:06:00 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Mon, 12 Aug 2013 11:06:00 -0400
Subject: [concurrency-interest] Benchmark to demonstrate improvement in
 thread management over the years.
In-Reply-To: <CABY0rKNWMgPyxoS5TiLtQskJrwbtFk-AjzE_S2GZ0yQcKnMc9w@mail.gmail.com>
References: <CAOk+zfdkhXwf5AyXFb74_4diGDFagSoKTQrZwTA3qzM0ymXUCw@mail.gmail.com>
	<CAHjP37Gf6XaBtj_L1EtL-0rLAABrWQ=X_C7xHZV=poBHoGv65Q@mail.gmail.com>
	<CABY0rKNWMgPyxoS5TiLtQskJrwbtFk-AjzE_S2GZ0yQcKnMc9w@mail.gmail.com>
Message-ID: <CAHjP37F8F_kVZqz4MJGr38CQcw-h1wJs2r+7Z7LT8dLyiFdxeg@mail.gmail.com>

Yes, that's a good point.  I think LinkedIn had a presentation on their use
of Play, and touched upon this exact scenario (web server having to
aggregate/join data from different backend systems).  The other problem
with calling out to backend servers using a threaded model (besides memory
charge) is that slowness in just one or two of them can ripple throughout
entire infrastructure, possibly leading to entire site being down.

Sent from my phone
On Aug 12, 2013 10:27 AM, "James Roper" <james.roper at typesafe.com> wrote:

> It's also worth pointing out that the thread per request model is becoming
> less feasible even for simple web apps. Modern service oriented
> architectures often require that a single web request may make many
> requests to other backend services. At the extreme, we see users writing
> Play apps that make hundreds of backend API calls per request. In order to
> provide acceptable response times, these requests must be made in parallel.
> With blocking IO, that would mean a single request might take 100 threads,
> if you had just 100 concurrent requests, that's 10000 threads, if each
> thread stack takes 100kb of real memory, that's 1GB memory just for thread
> stacks. That's not cheap.
>
> Regards,
>
> James
> On Aug 13, 2013 12:08 AM, "Vitaly Davidovich" <vitalyd at gmail.com> wrote:
>
>> I don't have any benchmarks to give, but I don't think the touted
>> benefits of an evented model includes CPU performance.  Rather, using an
>> evented model allows you to scale.  Specific to a web server, you want to
>> be able to handle lots of concurrent connections (most of them are probably
>> idle at any given time) while minimizing resource usage to accomplish that.
>>
>> With a thread-per-request (threaded) model, you may end up using lots of
>> threads but most of them are blocked on i/o at any given time.  A slow
>> client/consumer can tie up a thread for a very long time.  This also makes
>> the server susceptible to a DDoS attack whereby new connections are
>> established, but the clients are purposely slow to tie up the server
>> threads.  Resource usage is also much higher in the threaded model when you
>> have tens of thousands of connections since you're going to pay for stack
>> space for each thread (granted it's VM space, but still).
>>
>> With an evented model, you don't have the inefficiency of having
>> thousands of threads alive but that are blocked/waiting on i/o.  A single
>> thread dedicated to multiplexing i/o across all the connections will
>> probably be sufficient.  The rest is worker threads (most likely = # of
>> CPUs for a dedicated machine) that actually handle the request processing,
>> but don't do any (significant) i/o.  This design also means that you can
>> handle slow clients in a more robust manner.
>>
>> So, the cost of threads can be "heavy" in the case of very busy web
>> servers.  The Linux kernel should handle a few thousand threads (most
>> blocked on io) quite well, but I don't think that will be the case for tens
>> or hundreds of thousands.  Even if there's sufficient RAM to handle that
>> many, there may be performance issues coming from the kernel itself, e.g.
>> scheduler.  At the very least, you'll be using resources of the machine
>> inefficiently under that setup.
>>
>> Vitaly
>>
>> Sent from my phone
>> On Aug 12, 2013 9:13 AM, "Unmesh Joshi" <unmeshjoshi at gmail.com> wrote:
>>
>>> Hi,
>>>
>>> Most of the books on node.js, Akka, Play or any other event IO based
>>> system frequently talk about 'Threads' being heavy and there is cost we
>>> have to pay for all the booking the OS or the JVM has to do with all the
>>> threads.
>>> While I agree that there must be some cost and for doing CPU intensive
>>> tasks like matrix multiplication, and fork-join kind of framework will be
>>> more performant, I am not sure if for web server kind of IO intensive
>>> application that's the case.
>>>
>>> On the contrary, I am seeing web servers running on tomcat with 1000 +
>>> threads without issues.  For web servers. I think that Linux level thread
>>> management has improved a lot in last 10 years. Same is with the JVM.
>>>
>>> Do we have any benchmark which shows how much Linux thread management
>>> and JVM thread management have improved over the years?
>>>
>>> Thanks,
>>> Unmesh
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130812/c2042233/attachment.html>

From aaron.grunthal at infinite-source.de  Mon Aug 12 14:43:00 2013
From: aaron.grunthal at infinite-source.de (Aaron Grunthal)
Date: Mon, 12 Aug 2013 20:43:00 +0200
Subject: [concurrency-interest] Implicit parallelism
In-Reply-To: <CAKEw5+4Qk-pMuzai+WH-KMGukkUMwkTj=XbreY5OJhPsJhvz7Q@mail.gmail.com>
References: <CAKEw5+4rcZKiizq26fLaZbYb9NPtb6XWYfxZp5fEbNY9+iG9+A@mail.gmail.com>
	<5206AE38.2030900@infinite-source.de>
	<CAKEw5+4Qk-pMuzai+WH-KMGukkUMwkTj=XbreY5OJhPsJhvz7Q@mail.gmail.com>
Message-ID: <52092CB4.4040302@infinite-source.de>

Why? I see that it would be convenient in some situations. But that is - 
for my tastes - too much magic going on if the application would 
suddenly start spawning threads, potentially competing for resources 
with other tasks in the global thread pool just to execute something 
that appears to be a simple loop.

It would be pure madness if the compiler would suddenly turn a simple 
busy-wait loop with an accumulator (might be used in some locking 
abstractions to spin-wait before yielding) into multiple threads burning 
CPU time.

I could see this happening automatically in some high-throughput data 
processing library (you mentioned OpenMP) because you expect it to 
utilize available resources. But not something low-level as simple loops.

What java *should* do is perform loop-vectorization like GCC or LLVM do. 
I.e. unroll loops and parallelize - say - 4 iterations with SSE/AVX 
instructions where they're data-independent.

System.arraycopy, string cloning and array initialization have 
hand-coded AVX-intrinsics in the most recent hotspot builds I think. 
Instead of manually specializing a few methods this could be generalized.


thread-parallelism:
* consumes additional system resources
* can be done by the programmer or library
* there is decent support for it with parallel streams

vectorization:
* almost free
* can't be done by the programmer (no inline assembly)
* not supported at all unless you want to use JNI or GPGPU-bindings

On 11.08.2013 10:44, Gustav ?kesson wrote:
> Hi,
>
> My point is that the programmer should (in some circumstances) not care
> if it runs in parallel or not - the execution determines this for you as
> long as the program semantics are not broken. That's why I don't want to
> see .parallel or similar - that is somewhat explicit parallelism. As you
> say, the example I gave was read-only so then it should (theoretically)
> be possible to determine that this is safe to run in parallel. The list
> can't change, nor the elements.
>
> But let's take another example. Let's say we have a Scala (immutable)
> list containing Int type. If we then add all integers from 0 to x...
>
> for (i <- 0 to x) {
>   immutableIntegerList = i :: immutableIntegerList
> }
>
> Or in Java, a locally declared array which we fill with values:
>
> int[] arr = new int[x]
>
> for (int i=0; i<x; ++i)
>   arr[i] = i
> }
>
> In my mind it should be possible to split the loop constructs into y
> parts and then concatenate the result in order. Similar to OpenMP but
> without the explicit parallelism annotation. I know this question is of
> rather academic type, but I'm wondering whether there has been any ideas
> or work to bring this type of functionality to the JVM (and the compilers).
>
>
> Best Regards,
> Gustav ?kesson
>
>
> On Sat, Aug 10, 2013 at 11:18 PM, Aaron Grunthal
> <aaron.grunthal at infinite-source.de
> <mailto:aaron.grunthal at infinite-source.de>> wrote:
>
>     Thread-safety in the context of data structure usually (but not
>     always) refers to consistent behavior when it is concurrently
>     *modified*.
>
>     In your example you mention summing over a list. That's a read-only
>     action and doesn't require immutable lists or special concurrency
>     support. Most data structures are well-behaved under concurrent
>     read-only access, barring exceptions such as access-ordered
>     LinkedHashMaps.
>
>     Java 8's parallel streams also are smart enough to not parallelize
>     on small data sets where the overhead would kill any performance
>     advantage. At least for data structures that allow size-estimates or
>     actual sizes to be obtained.
>
>     I think there shouldn't be anything keeping you from adding a
>     .parallel if you anticipate the possibility of your data
>     transformations handling non-trivial amounts of data.
>
>     - Aaron
>
>
>
>     On 10.08.2013 20:45, Gustav ?kesson wrote:
>
>         Hi guys,
>
>         My discussion here concerns implicit parallelism. Let's say we
>         have the
>         following setup:
>
>         @Immutable
>         public class Integer
>         {
>             ...
>         }
>
>         @Immutable
>         public class ImmutableArrayList
>         {
>              ...
>         }
>
>         I'm looking for a way so that the parallelism would be introduced
>         without hard-coding anything related to parallelism (i.e. not
>         stating
>         anything like .parallel or .par on the collection). Only thing
>         needed
>         would be something annotation-ish which tells the execution
>         environment
>         that this datastructure with elements is inherently thread-safe.
>         Then
>         the execution could determine if it would be beneficial to do
>         so. For
>         instance, for a structure with e.g. five elements, then it would
>         not,
>         but for millions of elements, it would most likely be. Perhaps
>         it could
>         even find some kind of sweet-spot of number of elements in which the
>         parallel overhead exceeds the benefits.
>
>         Let's say we wish to sum all the integers in an ImmutableArrayList
>         (setup below), would it be possible for the compiler (javac,
>         scalac or
>         what have you) and JVM to conspire and decide "hey, let's run
>         this in
>         parallel since it doesn't violate application semantics and it
>         can/will
>         be faster"? Is there any research in this area in regards to the
>         JVM?
>
>
>         Best Regards,
>
>         Gustav ?kesson
>
>
>         _________________________________________________
>         Concurrency-interest mailing list
>         Concurrency-interest at cs.__oswego.edu
>         <mailto:Concurrency-interest at cs.oswego.edu>
>         http://cs.oswego.edu/mailman/__listinfo/concurrency-interest
>         <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>
>
>     _________________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.__oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/__listinfo/concurrency-interest
>     <http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>
>


From jasonk at bluedevel.com  Mon Aug 12 18:16:14 2013
From: jasonk at bluedevel.com (Jason Koch)
Date: Tue, 13 Aug 2013 08:16:14 +1000
Subject: [concurrency-interest] Benchmark to demonstrate improvement in
 thread management over the years.
In-Reply-To: <CAHjP37F8F_kVZqz4MJGr38CQcw-h1wJs2r+7Z7LT8dLyiFdxeg@mail.gmail.com>
References: <CAOk+zfdkhXwf5AyXFb74_4diGDFagSoKTQrZwTA3qzM0ymXUCw@mail.gmail.com>
	<CAHjP37Gf6XaBtj_L1EtL-0rLAABrWQ=X_C7xHZV=poBHoGv65Q@mail.gmail.com>
	<CABY0rKNWMgPyxoS5TiLtQskJrwbtFk-AjzE_S2GZ0yQcKnMc9w@mail.gmail.com>
	<CAHjP37F8F_kVZqz4MJGr38CQcw-h1wJs2r+7Z7LT8dLyiFdxeg@mail.gmail.com>
Message-ID: <CANM7VxwCZA+xhufGQm5OCmMJWKa=1HcZXAS7G-hDqKbQTbxpmw@mail.gmail.com>

Theoretically - they should both have similar behaviours in theory, and the
issue is the implementation. This is a very good background paper on the
topic - http://www.stanford.edu/class/cs240/readings/vonbehren.pdf - and is
well worth reading. In my interpretation, they present threads/events as
duals of each other.

There is no reason we can't have lighter threading models. Erlang/BEAM is
known to scale to extremely high concurrent process counts with ease, and
Kilim on the JVM looks like a promising way to get lightweight threading
(though I'm yet to try it unfortunately). Similarly, you can write a slow
evented implementation if you choose to.

In practice, OS threading tends to have a heavy footprint and challenges
context switching, where events can have a very controlled footprint. For
example, a full thread stack is usually a few hundred KB, where an event
state machine can often be modeled in a handful of bytes up to a few KB.

So, in practice on Linux, with C or Java or on the MS/.net platform, events
and async IO tend to be much more scalable. There are tradeoffs you need to
make in your design, but these tradeoffs for certain types of applications
are very worthwhile - eg: web servers.

Thanks
Jason






On Tue, Aug 13, 2013 at 1:06 AM, Vitaly Davidovich <vitalyd at gmail.com>wrote:

> Yes, that's a good point.  I think LinkedIn had a presentation on their
> use of Play, and touched upon this exact scenario (web server having to
> aggregate/join data from different backend systems).  The other problem
> with calling out to backend servers using a threaded model (besides memory
> charge) is that slowness in just one or two of them can ripple throughout
> entire infrastructure, possibly leading to entire site being down.
>
> Sent from my phone
> On Aug 12, 2013 10:27 AM, "James Roper" <james.roper at typesafe.com> wrote:
>
>> It's also worth pointing out that the thread per request model is
>> becoming less feasible even for simple web apps. Modern service oriented
>> architectures often require that a single web request may make many
>> requests to other backend services. At the extreme, we see users writing
>> Play apps that make hundreds of backend API calls per request. In order to
>> provide acceptable response times, these requests must be made in parallel.
>> With blocking IO, that would mean a single request might take 100 threads,
>> if you had just 100 concurrent requests, that's 10000 threads, if each
>> thread stack takes 100kb of real memory, that's 1GB memory just for thread
>> stacks. That's not cheap.
>>
>> Regards,
>>
>> James
>> On Aug 13, 2013 12:08 AM, "Vitaly Davidovich" <vitalyd at gmail.com> wrote:
>>
>>> I don't have any benchmarks to give, but I don't think the touted
>>> benefits of an evented model includes CPU performance.  Rather, using an
>>> evented model allows you to scale.  Specific to a web server, you want to
>>> be able to handle lots of concurrent connections (most of them are probably
>>> idle at any given time) while minimizing resource usage to accomplish that.
>>>
>>> With a thread-per-request (threaded) model, you may end up using lots of
>>> threads but most of them are blocked on i/o at any given time.  A slow
>>> client/consumer can tie up a thread for a very long time.  This also makes
>>> the server susceptible to a DDoS attack whereby new connections are
>>> established, but the clients are purposely slow to tie up the server
>>> threads.  Resource usage is also much higher in the threaded model when you
>>> have tens of thousands of connections since you're going to pay for stack
>>> space for each thread (granted it's VM space, but still).
>>>
>>> With an evented model, you don't have the inefficiency of having
>>> thousands of threads alive but that are blocked/waiting on i/o.  A single
>>> thread dedicated to multiplexing i/o across all the connections will
>>> probably be sufficient.  The rest is worker threads (most likely = # of
>>> CPUs for a dedicated machine) that actually handle the request processing,
>>> but don't do any (significant) i/o.  This design also means that you can
>>> handle slow clients in a more robust manner.
>>>
>>> So, the cost of threads can be "heavy" in the case of very busy web
>>> servers.  The Linux kernel should handle a few thousand threads (most
>>> blocked on io) quite well, but I don't think that will be the case for tens
>>> or hundreds of thousands.  Even if there's sufficient RAM to handle that
>>> many, there may be performance issues coming from the kernel itself, e.g.
>>> scheduler.  At the very least, you'll be using resources of the machine
>>> inefficiently under that setup.
>>>
>>> Vitaly
>>>
>>> Sent from my phone
>>> On Aug 12, 2013 9:13 AM, "Unmesh Joshi" <unmeshjoshi at gmail.com> wrote:
>>>
>>>> Hi,
>>>>
>>>> Most of the books on node.js, Akka, Play or any other event IO based
>>>> system frequently talk about 'Threads' being heavy and there is cost we
>>>> have to pay for all the booking the OS or the JVM has to do with all the
>>>> threads.
>>>> While I agree that there must be some cost and for doing CPU intensive
>>>> tasks like matrix multiplication, and fork-join kind of framework will be
>>>> more performant, I am not sure if for web server kind of IO intensive
>>>> application that's the case.
>>>>
>>>> On the contrary, I am seeing web servers running on tomcat with 1000 +
>>>> threads without issues.  For web servers. I think that Linux level thread
>>>> management has improved a lot in last 10 years. Same is with the JVM.
>>>>
>>>> Do we have any benchmark which shows how much Linux thread management
>>>> and JVM thread management have improved over the years?
>>>>
>>>> Thanks,
>>>> Unmesh
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130813/f08c52ba/attachment.html>

From unmeshjoshi at gmail.com  Tue Aug 13 00:59:17 2013
From: unmeshjoshi at gmail.com (Unmesh Joshi)
Date: Tue, 13 Aug 2013 10:29:17 +0530
Subject: [concurrency-interest] Benchmark to demonstrate improvement in
 thread management over the years.
In-Reply-To: <CABY0rKNWMgPyxoS5TiLtQskJrwbtFk-AjzE_S2GZ0yQcKnMc9w@mail.gmail.com>
References: <CAOk+zfdkhXwf5AyXFb74_4diGDFagSoKTQrZwTA3qzM0ymXUCw@mail.gmail.com>
	<CAHjP37Gf6XaBtj_L1EtL-0rLAABrWQ=X_C7xHZV=poBHoGv65Q@mail.gmail.com>
	<CABY0rKNWMgPyxoS5TiLtQskJrwbtFk-AjzE_S2GZ0yQcKnMc9w@mail.gmail.com>
Message-ID: <CAOk+zfeZLJyRSf9H9kjEKoDTmbv9e=vPoEO9My3goT0fxrCY8A@mail.gmail.com>

Hi James,

At what number of threads JVM or OS performance starts degrading? Or number
of threads start becoming the main bottleneck in the system?

Thanks,
Unmesh


On Mon, Aug 12, 2013 at 7:57 PM, James Roper <james.roper at typesafe.com>wrote:

> It's also worth pointing out that the thread per request model is becoming
> less feasible even for simple web apps. Modern service oriented
> architectures often require that a single web request may make many
> requests to other backend services. At the extreme, we see users writing
> Play apps that make hundreds of backend API calls per request. In order to
> provide acceptable response times, these requests must be made in parallel.
> With blocking IO, that would mean a single request might take 100 threads,
> if you had just 100 concurrent requests, that's 10000 threads, if each
> thread stack takes 100kb of real memory, that's 1GB memory just for thread
> stacks. That's not cheap.
>
> Regards,
>
> James
> On Aug 13, 2013 12:08 AM, "Vitaly Davidovich" <vitalyd at gmail.com> wrote:
>
>> I don't have any benchmarks to give, but I don't think the touted
>> benefits of an evented model includes CPU performance.  Rather, using an
>> evented model allows you to scale.  Specific to a web server, you want to
>> be able to handle lots of concurrent connections (most of them are probably
>> idle at any given time) while minimizing resource usage to accomplish that.
>>
>> With a thread-per-request (threaded) model, you may end up using lots of
>> threads but most of them are blocked on i/o at any given time.  A slow
>> client/consumer can tie up a thread for a very long time.  This also makes
>> the server susceptible to a DDoS attack whereby new connections are
>> established, but the clients are purposely slow to tie up the server
>> threads.  Resource usage is also much higher in the threaded model when you
>> have tens of thousands of connections since you're going to pay for stack
>> space for each thread (granted it's VM space, but still).
>>
>> With an evented model, you don't have the inefficiency of having
>> thousands of threads alive but that are blocked/waiting on i/o.  A single
>> thread dedicated to multiplexing i/o across all the connections will
>> probably be sufficient.  The rest is worker threads (most likely = # of
>> CPUs for a dedicated machine) that actually handle the request processing,
>> but don't do any (significant) i/o.  This design also means that you can
>> handle slow clients in a more robust manner.
>>
>> So, the cost of threads can be "heavy" in the case of very busy web
>> servers.  The Linux kernel should handle a few thousand threads (most
>> blocked on io) quite well, but I don't think that will be the case for tens
>> or hundreds of thousands.  Even if there's sufficient RAM to handle that
>> many, there may be performance issues coming from the kernel itself, e.g.
>> scheduler.  At the very least, you'll be using resources of the machine
>> inefficiently under that setup.
>>
>> Vitaly
>>
>> Sent from my phone
>> On Aug 12, 2013 9:13 AM, "Unmesh Joshi" <unmeshjoshi at gmail.com> wrote:
>>
>>> Hi,
>>>
>>> Most of the books on node.js, Akka, Play or any other event IO based
>>> system frequently talk about 'Threads' being heavy and there is cost we
>>> have to pay for all the booking the OS or the JVM has to do with all the
>>> threads.
>>> While I agree that there must be some cost and for doing CPU intensive
>>> tasks like matrix multiplication, and fork-join kind of framework will be
>>> more performant, I am not sure if for web server kind of IO intensive
>>> application that's the case.
>>>
>>> On the contrary, I am seeing web servers running on tomcat with 1000 +
>>> threads without issues.  For web servers. I think that Linux level thread
>>> management has improved a lot in last 10 years. Same is with the JVM.
>>>
>>> Do we have any benchmark which shows how much Linux thread management
>>> and JVM thread management have improved over the years?
>>>
>>> Thanks,
>>> Unmesh
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130813/31af3f79/attachment-0001.html>

From kirk at kodewerk.com  Tue Aug 13 02:20:54 2013
From: kirk at kodewerk.com (Kirk Pepperdine)
Date: Tue, 13 Aug 2013 08:20:54 +0200
Subject: [concurrency-interest] Benchmark to demonstrate improvement in
	thread management over the years.
In-Reply-To: <CAOk+zfeZLJyRSf9H9kjEKoDTmbv9e=vPoEO9My3goT0fxrCY8A@mail.gmail.com>
References: <CAOk+zfdkhXwf5AyXFb74_4diGDFagSoKTQrZwTA3qzM0ymXUCw@mail.gmail.com>
	<CAHjP37Gf6XaBtj_L1EtL-0rLAABrWQ=X_C7xHZV=poBHoGv65Q@mail.gmail.com>
	<CABY0rKNWMgPyxoS5TiLtQskJrwbtFk-AjzE_S2GZ0yQcKnMc9w@mail.gmail.com>
	<CAOk+zfeZLJyRSf9H9kjEKoDTmbv9e=vPoEO9My3goT0fxrCY8A@mail.gmail.com>
Message-ID: <2C737973-9ECE-419C-B8BF-D47194D64BF4@kodewerk.com>

The JVM is limited in the number of threads it can launch. The exact number depends on how the executable was packed at link time but it's generally greater than 1000. There are also a number of JVM and OS configurations that can alter that number. In one case I repacked a jvm.exe to give it more space for stacks as that implementation has tons of holes in the text area.

But, the one thread per request model is simple to code but has never been very scalable Unfortunately it is the most common way load injectors have been written.

Regards,
Kirk

On 2013-08-13, at 6:59 AM, Unmesh Joshi <unmeshjoshi at gmail.com> wrote:

> Hi James,
> 
> At what number of threads JVM or OS performance starts degrading? Or number of threads start becoming the main bottleneck in the system? 
> 
> Thanks,
> Unmesh
> 
> 
> On Mon, Aug 12, 2013 at 7:57 PM, James Roper <james.roper at typesafe.com> wrote:
> It's also worth pointing out that the thread per request model is becoming less feasible even for simple web apps. Modern service oriented architectures often require that a single web request may make many requests to other backend services. At the extreme, we see users writing Play apps that make hundreds of backend API calls per request. In order to provide acceptable response times, these requests must be made in parallel. With blocking IO, that would mean a single request might take 100 threads, if you had just 100 concurrent requests, that's 10000 threads, if each thread stack takes 100kb of real memory, that's 1GB memory just for thread stacks. That's not cheap.
> 
> Regards,
> 
> James
> 
> On Aug 13, 2013 12:08 AM, "Vitaly Davidovich" <vitalyd at gmail.com> wrote:
> I don't have any benchmarks to give, but I don't think the touted benefits of an evented model includes CPU performance.  Rather, using an evented model allows you to scale.  Specific to a web server, you want to be able to handle lots of concurrent connections (most of them are probably idle at any given time) while minimizing resource usage to accomplish that.
> 
> With a thread-per-request (threaded) model, you may end up using lots of threads but most of them are blocked on i/o at any given time.  A slow client/consumer can tie up a thread for a very long time.  This also makes the server susceptible to a DDoS attack whereby new connections are established, but the clients are purposely slow to tie up the server threads.  Resource usage is also much higher in the threaded model when you have tens of thousands of connections since you're going to pay for stack space for each thread (granted it's VM space, but still).
> 
> With an evented model, you don't have the inefficiency of having thousands of threads alive but that are blocked/waiting on i/o.  A single thread dedicated to multiplexing i/o across all the connections will probably be sufficient.  The rest is worker threads (most likely = # of CPUs for a dedicated machine) that actually handle the request processing, but don't do any (significant) i/o.  This design also means that you can handle slow clients in a more robust manner. 
> 
> So, the cost of threads can be "heavy" in the case of very busy web servers.  The Linux kernel should handle a few thousand threads (most blocked on io) quite well, but I don't think that will be the case for tens or hundreds of thousands.  Even if there's sufficient RAM to handle that many, there may be performance issues coming from the kernel itself, e.g. scheduler.  At the very least, you'll be using resources of the machine inefficiently under that setup.
> 
> Vitaly
> 
> Sent from my phone
> 
> On Aug 12, 2013 9:13 AM, "Unmesh Joshi" <unmeshjoshi at gmail.com> wrote:
> Hi,
> 
> Most of the books on node.js, Akka, Play or any other event IO based system frequently talk about 'Threads' being heavy and there is cost we have to pay for all the booking the OS or the JVM has to do with all the threads.
> While I agree that there must be some cost and for doing CPU intensive tasks like matrix multiplication, and fork-join kind of framework will be more performant, I am not sure if for web server kind of IO intensive application that's the case.
> 
> On the contrary, I am seeing web servers running on tomcat with 1000 + threads without issues.  For web servers. I think that Linux level thread management has improved a lot in last 10 years. Same is with the JVM. 
> 
> Do we have any benchmark which shows how much Linux thread management and JVM thread management have improved over the years?
> 
> Thanks,
> Unmesh
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130813/8c8f837e/attachment.html>

From james.roper at typesafe.com  Tue Aug 13 02:38:27 2013
From: james.roper at typesafe.com (James Roper)
Date: Tue, 13 Aug 2013 16:38:27 +1000
Subject: [concurrency-interest] Benchmark to demonstrate improvement in
 thread management over the years.
In-Reply-To: <CAOk+zfeZLJyRSf9H9kjEKoDTmbv9e=vPoEO9My3goT0fxrCY8A@mail.gmail.com>
References: <CAOk+zfdkhXwf5AyXFb74_4diGDFagSoKTQrZwTA3qzM0ymXUCw@mail.gmail.com>
	<CAHjP37Gf6XaBtj_L1EtL-0rLAABrWQ=X_C7xHZV=poBHoGv65Q@mail.gmail.com>
	<CABY0rKNWMgPyxoS5TiLtQskJrwbtFk-AjzE_S2GZ0yQcKnMc9w@mail.gmail.com>
	<CAOk+zfeZLJyRSf9H9kjEKoDTmbv9e=vPoEO9My3goT0fxrCY8A@mail.gmail.com>
Message-ID: <CABY0rKNxUz4Q=F_u3SANtXL8bX=-10UhX1Jv-oCZP9OqXnnwhA@mail.gmail.com>

On Tue, Aug 13, 2013 at 2:59 PM, Unmesh Joshi <unmeshjoshi at gmail.com> wrote:

> Hi James,
>
> At what number of threads JVM or OS performance starts degrading? Or
> number of threads start becoming the main bottleneck in the system?
>

Really, you just need to do your own load testing specific to your own
application, hardware and OS requirements.  The current Linux scheduler
runs in O(log N), so technically, that means performance starts degrading
at 2 threads, since every thread added increases the amount of time the
scheduler takes.  But of course, that degradation is negligible compared to
the amount of time your app spends waiting for IO.  So it all depends on
your app, what it's doing, and what its requirements are.

It's not just scheduling that gets impacted, another obvious one that I
already I pointed out was memory consumption, so once the thread stacks
have consumed all available RAM, then they won't just be the bottleneck,
your application will slow to a crawl or even crash.


> Thanks,
> Unmesh
>
>
> On Mon, Aug 12, 2013 at 7:57 PM, James Roper <james.roper at typesafe.com>wrote:
>
>> It's also worth pointing out that the thread per request model is
>> becoming less feasible even for simple web apps. Modern service oriented
>> architectures often require that a single web request may make many
>> requests to other backend services. At the extreme, we see users writing
>> Play apps that make hundreds of backend API calls per request. In order to
>> provide acceptable response times, these requests must be made in parallel.
>> With blocking IO, that would mean a single request might take 100 threads,
>> if you had just 100 concurrent requests, that's 10000 threads, if each
>> thread stack takes 100kb of real memory, that's 1GB memory just for thread
>> stacks. That's not cheap.
>>
>> Regards,
>>
>> James
>> On Aug 13, 2013 12:08 AM, "Vitaly Davidovich" <vitalyd at gmail.com> wrote:
>>
>>> I don't have any benchmarks to give, but I don't think the touted
>>> benefits of an evented model includes CPU performance.  Rather, using an
>>> evented model allows you to scale.  Specific to a web server, you want to
>>> be able to handle lots of concurrent connections (most of them are probably
>>> idle at any given time) while minimizing resource usage to accomplish that.
>>>
>>> With a thread-per-request (threaded) model, you may end up using lots of
>>> threads but most of them are blocked on i/o at any given time.  A slow
>>> client/consumer can tie up a thread for a very long time.  This also makes
>>> the server susceptible to a DDoS attack whereby new connections are
>>> established, but the clients are purposely slow to tie up the server
>>> threads.  Resource usage is also much higher in the threaded model when you
>>> have tens of thousands of connections since you're going to pay for stack
>>> space for each thread (granted it's VM space, but still).
>>>
>>> With an evented model, you don't have the inefficiency of having
>>> thousands of threads alive but that are blocked/waiting on i/o.  A single
>>> thread dedicated to multiplexing i/o across all the connections will
>>> probably be sufficient.  The rest is worker threads (most likely = # of
>>> CPUs for a dedicated machine) that actually handle the request processing,
>>> but don't do any (significant) i/o.  This design also means that you can
>>> handle slow clients in a more robust manner.
>>>
>>> So, the cost of threads can be "heavy" in the case of very busy web
>>> servers.  The Linux kernel should handle a few thousand threads (most
>>> blocked on io) quite well, but I don't think that will be the case for tens
>>> or hundreds of thousands.  Even if there's sufficient RAM to handle that
>>> many, there may be performance issues coming from the kernel itself, e.g.
>>> scheduler.  At the very least, you'll be using resources of the machine
>>> inefficiently under that setup.
>>>
>>> Vitaly
>>>
>>> Sent from my phone
>>> On Aug 12, 2013 9:13 AM, "Unmesh Joshi" <unmeshjoshi at gmail.com> wrote:
>>>
>>>> Hi,
>>>>
>>>> Most of the books on node.js, Akka, Play or any other event IO based
>>>> system frequently talk about 'Threads' being heavy and there is cost we
>>>> have to pay for all the booking the OS or the JVM has to do with all the
>>>> threads.
>>>> While I agree that there must be some cost and for doing CPU intensive
>>>> tasks like matrix multiplication, and fork-join kind of framework will be
>>>> more performant, I am not sure if for web server kind of IO intensive
>>>> application that's the case.
>>>>
>>>> On the contrary, I am seeing web servers running on tomcat with 1000 +
>>>> threads without issues.  For web servers. I think that Linux level thread
>>>> management has improved a lot in last 10 years. Same is with the JVM.
>>>>
>>>> Do we have any benchmark which shows how much Linux thread management
>>>> and JVM thread management have improved over the years?
>>>>
>>>> Thanks,
>>>> Unmesh
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>


-- 
*James Roper*
*Software Engineer*
*
*
Typesafe <http://typesafe.com/> ? Build reactive apps!
Twitter: @jroper <https://twitter.com/jroper>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130813/1ad7ed23/attachment-0001.html>

From kirk at kodewerk.com  Tue Aug 13 02:49:12 2013
From: kirk at kodewerk.com (Kirk Pepperdine)
Date: Tue, 13 Aug 2013 08:49:12 +0200
Subject: [concurrency-interest] Benchmark to demonstrate improvement in
	thread management over the years.
In-Reply-To: <CABY0rKNxUz4Q=F_u3SANtXL8bX=-10UhX1Jv-oCZP9OqXnnwhA@mail.gmail.com>
References: <CAOk+zfdkhXwf5AyXFb74_4diGDFagSoKTQrZwTA3qzM0ymXUCw@mail.gmail.com>
	<CAHjP37Gf6XaBtj_L1EtL-0rLAABrWQ=X_C7xHZV=poBHoGv65Q@mail.gmail.com>
	<CABY0rKNWMgPyxoS5TiLtQskJrwbtFk-AjzE_S2GZ0yQcKnMc9w@mail.gmail.com>
	<CAOk+zfeZLJyRSf9H9kjEKoDTmbv9e=vPoEO9My3goT0fxrCY8A@mail.gmail.com>
	<CABY0rKNxUz4Q=F_u3SANtXL8bX=-10UhX1Jv-oCZP9OqXnnwhA@mail.gmail.com>
Message-ID: <5C2C43A8-AE11-43A8-B343-8D94DC6CFB5B@kodewerk.com>


On 2013-08-13, at 8:38 AM, James Roper <james.roper at typesafe.com> wrote:

> On Tue, Aug 13, 2013 at 2:59 PM, Unmesh Joshi <unmeshjoshi at gmail.com> wrote:
> Hi James,
> 
> At what number of threads JVM or OS performance starts degrading? Or number of threads start becoming the main bottleneck in the system? 
> 
> Really, you just need to do your own load testing specific to your own application, hardware and OS requirements.  The current Linux scheduler runs in O(log N),

Really? I thought it was using an O(N) heap sort. Anyways, IME, the bigger problem is that getting the thread scheduler scheduled so that you can get threads scheduled... (there might be a Dr. Seuss thing going on here).

 
> so technically, that means performance starts degrading at 2 threads, since every thread added increases the amount of time the scheduler takes.  But of course, that degradation is negligible compared to the amount of time your app spends waiting for IO.  So it all depends on your app, what it's doing, and what its requirements are.
> 
> It's not just scheduling that gets impacted, another obvious one that I already I pointed out was memory consumption, so once the thread stacks have consumed all available RAM, then they won't just be the bottleneck, your application will slow to a crawl or even crash.

Slow is something due to something else. No more stack space leads to OOME Stack space exhausted. Is this what you mean?

Regards,
Kirk

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130813/780e584e/attachment.html>

From stanimir at riflexo.com  Tue Aug 13 04:29:36 2013
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Tue, 13 Aug 2013 11:29:36 +0300
Subject: [concurrency-interest] Benchmark to demonstrate improvement in
 thread management over the years.
In-Reply-To: <5208EBE4.5060406@oracle.com>
References: <CAOk+zfdkhXwf5AyXFb74_4diGDFagSoKTQrZwTA3qzM0ymXUCw@mail.gmail.com>
	<5208EBE4.5060406@oracle.com>
Message-ID: <CAEJX8ootKLJNxNKYbs6YNFZehRFSiTic7+ntUdbpaTa5VEhAqw@mail.gmail.com>

The main benefit of the NIO is the lower AND predictable latency per
client. You may apply your own policy, order and priority.
The paper mentions nothing about latency and you are at mercy of the OS
scheduler to serve your clients properly. Thread.setPriority virtually
doesn't work, so everything has equal priority  but the OS may boost some
threads if they feel so.
For some applications that might not be critical, of course...
Actually, 1000 I/O bound threads on modern hardware is probably low load.


Stanimir


On Mon, Aug 12, 2013 at 5:06 PM, Oleksandr Otenko <
oleksandr.otenko at oracle.com> wrote:

>  http://www.slideshare.net/e456/tyma-paulmultithreaded1
>
> Alex
>
>
> On 12/08/2013 14:03, Unmesh Joshi wrote:
>
> Hi,
>
>  Most of the books on node.js, Akka, Play or any other event IO based
> system frequently talk about 'Threads' being heavy and there is cost we
> have to pay for all the booking the OS or the JVM has to do with all the
> threads.
> While I agree that there must be some cost and for doing CPU intensive
> tasks like matrix multiplication, and fork-join kind of framework will be
> more performant, I am not sure if for web server kind of IO intensive
> application that's the case.
>
>  On the contrary, I am seeing web servers running on tomcat with 1000 +
> threads without issues.  For web servers. I think that Linux level thread
> management has improved a lot in last 10 years. Same is with the JVM.
>
>  Do we have any benchmark which shows how much Linux thread management
> and JVM thread management have improved over the years?
>
>  Thanks,
> Unmesh
>
>
> _______________________________________________
> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130813/ce5bea71/attachment.html>

From oleksandr.otenko at oracle.com  Tue Aug 13 05:44:22 2013
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Tue, 13 Aug 2013 10:44:22 +0100
Subject: [concurrency-interest] Benchmark to demonstrate improvement in
 thread management over the years.
In-Reply-To: <CAEJX8ootKLJNxNKYbs6YNFZehRFSiTic7+ntUdbpaTa5VEhAqw@mail.gmail.com>
References: <CAOk+zfdkhXwf5AyXFb74_4diGDFagSoKTQrZwTA3qzM0ymXUCw@mail.gmail.com>
	<5208EBE4.5060406@oracle.com>
	<CAEJX8ootKLJNxNKYbs6YNFZehRFSiTic7+ntUdbpaTa5VEhAqw@mail.gmail.com>
Message-ID: <5209FFF6.5010701@oracle.com>

Well, my takeaway from the slides is that the marketing of NIO is not 
necessarily the whole story, certainly not today.

Applying your own policy for ordering and priorities is good, but this 
no longer is a "simple fork-join" / ExecuteService thread pool. On the 
other hand, if you need to implement your own execute service just to 
get going, it seems too much buck for the bang.

Latency is usually a underestimated beast. To start with, diagnostic 
tools simply don't measure it. Then also load tests usually are not 
suited to test latency, and they hit the system with regularly spaced 
requests.


Alex

On 13/08/2013 09:29, Stanimir Simeonoff wrote:
> The main benefit of the NIO is the lower AND predictable latency per 
> client. You may apply your own policy, order and priority.
> The paper mentions nothing about latency and you are at mercy of the 
> OS scheduler to serve your clients properly. Thread.setPriority 
> virtually doesn't work, so everything has equal priority  but the OS 
> may boost some threads if they feel so.
> For some applications that might not be critical, of course...
> Actually, 1000 I/O bound threads on modern hardware is probably low load.
>
>
> Stanimir
>
>
> On Mon, Aug 12, 2013 at 5:06 PM, Oleksandr Otenko 
> <oleksandr.otenko at oracle.com <mailto:oleksandr.otenko at oracle.com>> wrote:
>
>     http://www.slideshare.net/e456/tyma-paulmultithreaded1
>
>     Alex
>
>
>     On 12/08/2013 14:03, Unmesh Joshi wrote:
>>     Hi,
>>
>>     Most of the books on node.js, Akka, Play or any other event IO
>>     based system frequently talk about 'Threads' being heavy and
>>     there is cost we have to pay for all the booking the OS or the
>>     JVM has to do with all the threads.
>>     While I agree that there must be some cost and for doing CPU
>>     intensive tasks like matrix multiplication, and fork-join kind of
>>     framework will be more performant, I am not sure if for web
>>     server kind of IO intensive application that's the case.
>>
>>     On the contrary, I am seeing web servers running on tomcat with
>>     1000 + threads without issues.  For web servers. I think that
>>     Linux level thread management has improved a lot in last 10
>>     years. Same is with the JVM.
>>
>>     Do we have any benchmark which shows how much Linux thread
>>     management and JVM thread management have improved over the years?
>>
>>     Thanks,
>>     Unmesh
>>
>>
>>     _______________________________________________
>>     Concurrency-interest mailing list
>>     Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130813/353b1af5/attachment.html>

From viktor.klang at gmail.com  Tue Aug 13 05:58:37 2013
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Tue, 13 Aug 2013 11:58:37 +0200
Subject: [concurrency-interest] Benchmark to demonstrate improvement in
 thread management over the years.
In-Reply-To: <5209FFF6.5010701@oracle.com>
References: <CAOk+zfdkhXwf5AyXFb74_4diGDFagSoKTQrZwTA3qzM0ymXUCw@mail.gmail.com>
	<5208EBE4.5060406@oracle.com>
	<CAEJX8ootKLJNxNKYbs6YNFZehRFSiTic7+ntUdbpaTa5VEhAqw@mail.gmail.com>
	<5209FFF6.5010701@oracle.com>
Message-ID: <CANPzfU8nzaejXgF0mYprhHNtAeUp9hCEQn0tWb78ABjHD5Y5QQ@mail.gmail.com>

Something I feel most people forget is fault tolerance.
If you are running code isolated in threads then how do you handle failures
in a thread? Who handles it? How do you recover from it?

Cheers,
?


On Tue, Aug 13, 2013 at 11:44 AM, Oleksandr Otenko <
oleksandr.otenko at oracle.com> wrote:

>  Well, my takeaway from the slides is that the marketing of NIO is not
> necessarily the whole story, certainly not today.
>
> Applying your own policy for ordering and priorities is good, but this no
> longer is a "simple fork-join" / ExecuteService thread pool. On the other
> hand, if you need to implement your own execute service just to get going,
> it seems too much buck for the bang.
>
> Latency is usually a underestimated beast. To start with, diagnostic tools
> simply don't measure it. Then also load tests usually are not suited to
> test latency, and they hit the system with regularly spaced requests.
>
>
> Alex
>
>
> On 13/08/2013 09:29, Stanimir Simeonoff wrote:
>
> The main benefit of the NIO is the lower AND predictable latency per
> client. You may apply your own policy, order and priority.
> The paper mentions nothing about latency and you are at mercy of the OS
> scheduler to serve your clients properly. Thread.setPriority virtually
> doesn't work, so everything has equal priority  but the OS may boost some
> threads if they feel so.
> For some applications that might not be critical, of course...
> Actually, 1000 I/O bound threads on modern hardware is probably low load.
>
>
> Stanimir
>
>
> On Mon, Aug 12, 2013 at 5:06 PM, Oleksandr Otenko <
> oleksandr.otenko at oracle.com> wrote:
>
>>  http://www.slideshare.net/e456/tyma-paulmultithreaded1
>>
>> Alex
>>
>>
>> On 12/08/2013 14:03, Unmesh Joshi wrote:
>>
>>  Hi,
>>
>>  Most of the books on node.js, Akka, Play or any other event IO based
>> system frequently talk about 'Threads' being heavy and there is cost we
>> have to pay for all the booking the OS or the JVM has to do with all the
>> threads.
>> While I agree that there must be some cost and for doing CPU intensive
>> tasks like matrix multiplication, and fork-join kind of framework will be
>> more performant, I am not sure if for web server kind of IO intensive
>> application that's the case.
>>
>>  On the contrary, I am seeing web servers running on tomcat with 1000 +
>> threads without issues.  For web servers. I think that Linux level thread
>> management has improved a lot in last 10 years. Same is with the JVM.
>>
>>  Do we have any benchmark which shows how much Linux thread management
>> and JVM thread management have improved over the years?
>>
>>  Thanks,
>> Unmesh
>>
>>
>>   _______________________________________________
>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


-- 
*Viktor Klang*
*Director of Engineering*
Typesafe <http://www.typesafe.com/>

Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130813/a1da3e47/attachment-0001.html>

From oleksandr.otenko at oracle.com  Tue Aug 13 06:07:24 2013
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Tue, 13 Aug 2013 11:07:24 +0100
Subject: [concurrency-interest] Benchmark to demonstrate improvement in
 thread management over the years.
In-Reply-To: <CANM7VxwCZA+xhufGQm5OCmMJWKa=1HcZXAS7G-hDqKbQTbxpmw@mail.gmail.com>
References: <CAOk+zfdkhXwf5AyXFb74_4diGDFagSoKTQrZwTA3qzM0ymXUCw@mail.gmail.com>
	<CAHjP37Gf6XaBtj_L1EtL-0rLAABrWQ=X_C7xHZV=poBHoGv65Q@mail.gmail.com>
	<CABY0rKNWMgPyxoS5TiLtQskJrwbtFk-AjzE_S2GZ0yQcKnMc9w@mail.gmail.com>
	<CAHjP37F8F_kVZqz4MJGr38CQcw-h1wJs2r+7Z7LT8dLyiFdxeg@mail.gmail.com>
	<CANM7VxwCZA+xhufGQm5OCmMJWKa=1HcZXAS7G-hDqKbQTbxpmw@mail.gmail.com>
Message-ID: <520A055C.10501@oracle.com>

Since they are duals, the event state machine cannot have "a few KB" to 
represent the same thing that a few hundred KB do in threaded case.

Consider the contents of the stack. Why is it so large? That's because 
it holds function-local variables that will be needed upon the return 
from a function call. The calls to the functions are "synchronous" in 
the sense the caller's termination depends on the termination of the 
callee. (disregard "holes" on the stack, where the optimizer doesn't 
care to reuse the space, because the stack is cheap; closures will also 
have "holes" - reserved space for a variable in the closure, which 
doesn't necessarily get filled)

A complete dual of this is a synchronous interaction between agents. The 
agents will keep the same variables that were in function-local 
variables as members of the closure capturing the state to be used by 
the agent receiving the response. It doesn't matter whether you 
represent this closure as the state of the agent, or pass around as part 
of the message: if you need those values after the response to the 
request is computed, you will have to preserve the reference to those 
values somehow.

What does make a difference, is that it is much harder to write 
synchronous interactions using the event state machine than using 
threaded design. So evented design encourages a different approach, a 
different solution, which is not, strictly speaking, */the/* dual. The 
converse is also true. If you translated this evented design into a 
threaded implementation, you'd have thin stacks and a bunch of message 
queues. Someone might even ask why it is called "threaded" in this case.

Also, take into account the allocation pattern difference. Allocating 
something on the stack is dirt-cheap: you just add a number to a 
register. This is very different from allocating a closure from heap.


Alex


On 12/08/2013 23:16, Jason Koch wrote:
> Theoretically - they should both have similar behaviours in theory, 
> and the issue is the implementation. This is a very good background 
> paper on the topic - 
> http://www.stanford.edu/class/cs240/readings/vonbehren.pdf - and is 
> well worth reading. In my interpretation, they present threads/events 
> as duals of each other.
>
> There is no reason we can't have lighter threading models. Erlang/BEAM 
> is known to scale to extremely high concurrent process counts with 
> ease, and Kilim on the JVM looks like a promising way to get 
> lightweight threading (though I'm yet to try it unfortunately). 
> Similarly, you can write a slow evented implementation if you choose to.
>
> In practice, OS threading tends to have a heavy footprint and 
> challenges context switching, where events can have a very controlled 
> footprint. For example, a full thread stack is usually a few hundred 
> KB, where an event state machine can often be modeled in a handful of 
> bytes up to a few KB.
>
> So, in practice on Linux, with C or Java or on the MS/.net platform, 
> events and async IO tend to be much more scalable. There are tradeoffs 
> you need to make in your design, but these tradeoffs for certain types 
> of applications are very worthwhile - eg: web servers.
>
> Thanks
> Jason
>
>
>
>
>
>
> On Tue, Aug 13, 2013 at 1:06 AM, Vitaly Davidovich <vitalyd at gmail.com 
> <mailto:vitalyd at gmail.com>> wrote:
>
>     Yes, that's a good point.  I think LinkedIn had a presentation on
>     their use of Play, and touched upon this exact scenario (web
>     server having to aggregate/join data from different backend
>     systems).  The other problem with calling out to backend servers
>     using a threaded model (besides memory charge) is that slowness in
>     just one or two of them can ripple throughout entire
>     infrastructure, possibly leading to entire site being down.
>
>     Sent from my phone
>
>     On Aug 12, 2013 10:27 AM, "James Roper" <james.roper at typesafe.com
>     <mailto:james.roper at typesafe.com>> wrote:
>
>         It's also worth pointing out that the thread per request model
>         is becoming less feasible even for simple web apps. Modern
>         service oriented architectures often require that a single web
>         request may make many requests to other backend services. At
>         the extreme, we see users writing Play apps that make hundreds
>         of backend API calls per request. In order to provide
>         acceptable response times, these requests must be made in
>         parallel. With blocking IO, that would mean a single request
>         might take 100 threads, if you had just 100 concurrent
>         requests, that's 10000 threads, if each thread stack takes
>         100kb of real memory, that's 1GB memory just for thread
>         stacks. That's not cheap.
>
>         Regards,
>
>         James
>
>         On Aug 13, 2013 12:08 AM, "Vitaly Davidovich"
>         <vitalyd at gmail.com <mailto:vitalyd at gmail.com>> wrote:
>
>             I don't have any benchmarks to give, but I don't think the
>             touted benefits of an evented model includes CPU
>             performance. Rather, using an evented model allows you to
>             scale.  Specific to a web server, you want to be able to
>             handle lots of concurrent connections (most of them are
>             probably idle at any given time) while minimizing resource
>             usage to accomplish that.
>
>             With a thread-per-request (threaded) model, you may end up
>             using lots of threads but most of them are blocked on i/o
>             at any given time.  A slow client/consumer can tie up a
>             thread for a very long time.  This also makes the server
>             susceptible to a DDoS attack whereby new connections are
>             established, but the clients are purposely slow to tie up
>             the server threads.  Resource usage is also much higher in
>             the threaded model when you have tens of thousands of
>             connections since you're going to pay for stack space for
>             each thread (granted it's VM space, but still).
>
>             With an evented model, you don't have the inefficiency of
>             having thousands of threads alive but that are
>             blocked/waiting on i/o.  A single thread dedicated to
>             multiplexing i/o across all the connections will probably
>             be sufficient.  The rest is worker threads (most likely =
>             # of CPUs for a dedicated machine) that actually handle
>             the request processing, but don't do any (significant)
>             i/o.  This design also means that you can handle slow
>             clients in a more robust manner.
>
>             So, the cost of threads can be "heavy" in the case of very
>             busy web servers. The Linux kernel should handle a few
>             thousand threads (most blocked on io) quite well, but I
>             don't think that will be the case for tens or hundreds of
>             thousands.  Even if there's sufficient RAM to handle that
>             many, there may be performance issues coming from the
>             kernel itself, e.g. scheduler.  At the very least, you'll
>             be using resources of the machine inefficiently under that
>             setup.
>
>             Vitaly
>
>             Sent from my phone
>
>             On Aug 12, 2013 9:13 AM, "Unmesh Joshi"
>             <unmeshjoshi at gmail.com <mailto:unmeshjoshi at gmail.com>> wrote:
>
>                 Hi,
>
>                 Most of the books on node.js, Akka, Play or any other
>                 event IO based system frequently talk about 'Threads'
>                 being heavy and there is cost we have to pay for all
>                 the booking the OS or the JVM has to do with all the
>                 threads.
>                 While I agree that there must be some cost and for
>                 doing CPU intensive tasks like matrix multiplication,
>                 and fork-join kind of framework will be more
>                 performant, I am not sure if for web server kind of IO
>                 intensive application that's the case.
>
>                 On the contrary, I am seeing web servers running on
>                 tomcat with 1000 + threads without issues.  For web
>                 servers. I think that Linux level thread management
>                 has improved a lot in last 10 years. Same is with the
>                 JVM.
>
>                 Do we have any benchmark which shows how much Linux
>                 thread management and JVM thread management have
>                 improved over the years?
>
>                 Thanks,
>                 Unmesh
>
>                 _______________________________________________
>                 Concurrency-interest mailing list
>                 Concurrency-interest at cs.oswego.edu
>                 <mailto:Concurrency-interest at cs.oswego.edu>
>                 http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>             _______________________________________________
>             Concurrency-interest mailing list
>             Concurrency-interest at cs.oswego.edu
>             <mailto:Concurrency-interest at cs.oswego.edu>
>             http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130813/c199cb2e/attachment.html>

From oleksandr.otenko at oracle.com  Tue Aug 13 06:12:14 2013
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Tue, 13 Aug 2013 11:12:14 +0100
Subject: [concurrency-interest] Benchmark to demonstrate improvement in
 thread management over the years.
In-Reply-To: <CANPzfU8nzaejXgF0mYprhHNtAeUp9hCEQn0tWb78ABjHD5Y5QQ@mail.gmail.com>
References: <CAOk+zfdkhXwf5AyXFb74_4diGDFagSoKTQrZwTA3qzM0ymXUCw@mail.gmail.com>
	<5208EBE4.5060406@oracle.com>
	<CAEJX8ootKLJNxNKYbs6YNFZehRFSiTic7+ntUdbpaTa5VEhAqw@mail.gmail.com>
	<5209FFF6.5010701@oracle.com>
	<CANPzfU8nzaejXgF0mYprhHNtAeUp9hCEQn0tWb78ABjHD5Y5QQ@mail.gmail.com>
Message-ID: <520A067E.8050601@oracle.com>

Yes, how to handle a failure that is not just "drop on the floor", you 
mean. A "drop on the floor" solution is to pass exceptions throughout 
the stack, which is the same as declaring "throws Exception" everywhere, 
or wrap all exceptions into RuntimeException, like some do. A 
"not-so-drop-on-the-floor" solution in evented case is not-so-simple either.

The difference is that one paradigm encourages a particular design, 
different from the design encouraged by the other.

Alex


On 13/08/2013 10:58, ?iktor ?lang wrote:
> Something I feel most people forget is fault tolerance.
> If you are running code isolated in threads then how do you handle 
> failures in a thread? Who handles it? How do you recover from it?
>
> Cheers,
> ?
>
>
> On Tue, Aug 13, 2013 at 11:44 AM, Oleksandr Otenko 
> <oleksandr.otenko at oracle.com <mailto:oleksandr.otenko at oracle.com>> wrote:
>
>     Well, my takeaway from the slides is that the marketing of NIO is
>     not necessarily the whole story, certainly not today.
>
>     Applying your own policy for ordering and priorities is good, but
>     this no longer is a "simple fork-join" / ExecuteService thread
>     pool. On the other hand, if you need to implement your own execute
>     service just to get going, it seems too much buck for the bang.
>
>     Latency is usually a underestimated beast. To start with,
>     diagnostic tools simply don't measure it. Then also load tests
>     usually are not suited to test latency, and they hit the system
>     with regularly spaced requests.
>
>
>     Alex
>
>
>     On 13/08/2013 09:29, Stanimir Simeonoff wrote:
>>     The main benefit of the NIO is the lower AND predictable latency
>>     per client. You may apply your own policy, order and priority.
>>     The paper mentions nothing about latency and you are at mercy of
>>     the OS scheduler to serve your clients properly.
>>     Thread.setPriority virtually doesn't work, so everything has
>>     equal priority  but the OS may boost some threads if they feel so.
>>     For some applications that might not be critical, of course...
>>     Actually, 1000 I/O bound threads on modern hardware is probably
>>     low load.
>>
>>
>>     Stanimir
>>
>>
>>     On Mon, Aug 12, 2013 at 5:06 PM, Oleksandr Otenko
>>     <oleksandr.otenko at oracle.com
>>     <mailto:oleksandr.otenko at oracle.com>> wrote:
>>
>>         http://www.slideshare.net/e456/tyma-paulmultithreaded1
>>
>>         Alex
>>
>>
>>         On 12/08/2013 14:03, Unmesh Joshi wrote:
>>>         Hi,
>>>
>>>         Most of the books on node.js, Akka, Play or any other event
>>>         IO based system frequently talk about 'Threads' being heavy
>>>         and there is cost we have to pay for all the booking the OS
>>>         or the JVM has to do with all the threads.
>>>         While I agree that there must be some cost and for doing CPU
>>>         intensive tasks like matrix multiplication, and fork-join
>>>         kind of framework will be more performant, I am not sure if
>>>         for web server kind of IO intensive application that's the case.
>>>
>>>         On the contrary, I am seeing web servers running on tomcat
>>>         with 1000 + threads without issues.  For web servers. I
>>>         think that Linux level thread management has improved a lot
>>>         in last 10 years. Same is with the JVM.
>>>
>>>         Do we have any benchmark which shows how much Linux thread
>>>         management and JVM thread management have improved over the
>>>         years?
>>>
>>>         Thanks,
>>>         Unmesh
>>>
>>>
>>>         _______________________________________________
>>>         Concurrency-interest mailing list
>>>         Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>         _______________________________________________
>>         Concurrency-interest mailing list
>>         Concurrency-interest at cs.oswego.edu
>>         <mailto:Concurrency-interest at cs.oswego.edu>
>>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
> -- 
> *Viktor Klang*
> /Director of Engineering/
> Typesafe <http://www.typesafe.com/>
>
> Twitter: @viktorklang

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130813/87a2b37c/attachment-0001.html>

From stanimir at riflexo.com  Tue Aug 13 06:20:59 2013
From: stanimir at riflexo.com (Stanimir Simeonoff)
Date: Tue, 13 Aug 2013 13:20:59 +0300
Subject: [concurrency-interest] Benchmark to demonstrate improvement in
 thread management over the years.
In-Reply-To: <5209FFF6.5010701@oracle.com>
References: <CAOk+zfdkhXwf5AyXFb74_4diGDFagSoKTQrZwTA3qzM0ymXUCw@mail.gmail.com>
	<5208EBE4.5060406@oracle.com>
	<CAEJX8ootKLJNxNKYbs6YNFZehRFSiTic7+ntUdbpaTa5VEhAqw@mail.gmail.com>
	<5209FFF6.5010701@oracle.com>
Message-ID: <CAEJX8opJzZaALtqS9q-MX7fYjZAW115UyF3M6SYPcOh+JWddLg@mail.gmail.com>

Latency is important when you have systems that most of the work is in the
form of subscribe and receive-a-lot (trading quotes and the like), in such
cases one would like all the clients to receive the information w/ similar
latencies, instead someone getting bursts.
Still writing the framework is like couple thousand lines of code (probably
w/ some optimistic lock free here and there). If you have mainly
request->response workload, the good old thread pool is just fine unless a
malicious attacker decides to DoS. You don't need DDoS w/ java
socket.getOutputStream().write() implementation. For instance, it's trivial
to starve the BIO thread pool of tomcat w/ a single machine.

Stanimir


On Tue, Aug 13, 2013 at 12:44 PM, Oleksandr Otenko <
oleksandr.otenko at oracle.com> wrote:

>  Well, my takeaway from the slides is that the marketing of NIO is not
> necessarily the whole story, certainly not today.
>
> Applying your own policy for ordering and priorities is good, but this no
> longer is a "simple fork-join" / ExecuteService thread pool. On the other
> hand, if you need to implement your own execute service just to get going,
> it seems too much buck for the bang.
>
> Latency is usually a underestimated beast. To start with, diagnostic tools
> simply don't measure it. Then also load tests usually are not suited to
> test latency, and they hit the system with regularly spaced requests.
>
>
> Alex
>
>
> On 13/08/2013 09:29, Stanimir Simeonoff wrote:
>
> The main benefit of the NIO is the lower AND predictable latency per
> client. You may apply your own policy, order and priority.
> The paper mentions nothing about latency and you are at mercy of the OS
> scheduler to serve your clients properly. Thread.setPriority virtually
> doesn't work, so everything has equal priority  but the OS may boost some
> threads if they feel so.
> For some applications that might not be critical, of course...
> Actually, 1000 I/O bound threads on modern hardware is probably low load.
>
>
> Stanimir
>
>
> On Mon, Aug 12, 2013 at 5:06 PM, Oleksandr Otenko <
> oleksandr.otenko at oracle.com> wrote:
>
>>  http://www.slideshare.net/e456/tyma-paulmultithreaded1
>>
>> Alex
>>
>>
>> On 12/08/2013 14:03, Unmesh Joshi wrote:
>>
>>  Hi,
>>
>>  Most of the books on node.js, Akka, Play or any other event IO based
>> system frequently talk about 'Threads' being heavy and there is cost we
>> have to pay for all the booking the OS or the JVM has to do with all the
>> threads.
>> While I agree that there must be some cost and for doing CPU intensive
>> tasks like matrix multiplication, and fork-join kind of framework will be
>> more performant, I am not sure if for web server kind of IO intensive
>> application that's the case.
>>
>>  On the contrary, I am seeing web servers running on tomcat with 1000 +
>> threads without issues.  For web servers. I think that Linux level thread
>> management has improved a lot in last 10 years. Same is with the JVM.
>>
>>  Do we have any benchmark which shows how much Linux thread management
>> and JVM thread management have improved over the years?
>>
>>  Thanks,
>> Unmesh
>>
>>
>>   _______________________________________________
>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130813/1fc9270a/attachment.html>

From kimo at webnetic.net  Tue Aug 13 06:33:29 2013
From: kimo at webnetic.net (Kimo Crossman)
Date: Tue, 13 Aug 2013 05:33:29 -0500
Subject: [concurrency-interest] Benchmark to demonstrate improvement in
 thread management over the years.
In-Reply-To: <5C2C43A8-AE11-43A8-B343-8D94DC6CFB5B@kodewerk.com>
References: <CAOk+zfdkhXwf5AyXFb74_4diGDFagSoKTQrZwTA3qzM0ymXUCw@mail.gmail.com>
	<CAHjP37Gf6XaBtj_L1EtL-0rLAABrWQ=X_C7xHZV=poBHoGv65Q@mail.gmail.com>
	<CABY0rKNWMgPyxoS5TiLtQskJrwbtFk-AjzE_S2GZ0yQcKnMc9w@mail.gmail.com>
	<CAOk+zfeZLJyRSf9H9kjEKoDTmbv9e=vPoEO9My3goT0fxrCY8A@mail.gmail.com>
	<CABY0rKNxUz4Q=F_u3SANtXL8bX=-10UhX1Jv-oCZP9OqXnnwhA@mail.gmail.com>
	<5C2C43A8-AE11-43A8-B343-8D94DC6CFB5B@kodewerk.com>
Message-ID: <CALV1V49mPEfRufaJVi2aPwFd1-9dR0MYaKPt2ydjzJArE6j9AQ@mail.gmail.com>

I thought current Linux scheduler CFS was O(1)

http://en.wikipedia.org/wiki/O(1)_scheduler


On Tue, Aug 13, 2013 at 1:49 AM, Kirk Pepperdine <kirk at kodewerk.com> wrote:

>
> On 2013-08-13, at 8:38 AM, James Roper <james.roper at typesafe.com> wrote:
>
> On Tue, Aug 13, 2013 at 2:59 PM, Unmesh Joshi <unmeshjoshi at gmail.com>wrote:
>
>> Hi James,
>>
>> At what number of threads JVM or OS performance starts degrading? Or
>> number of threads start becoming the main bottleneck in the system?
>>
>
> Really, you just need to do your own load testing specific to your own
> application, hardware and OS requirements.  The current Linux scheduler
> runs in O(log N),
>
>
> Really? I thought it was using an O(N) heap sort. Anyways, IME, the bigger
> problem is that getting the thread scheduler scheduled so that you can get
> threads scheduled... (there might be a Dr. Seuss thing going on here).
>
>
>
> so technically, that means performance starts degrading at 2 threads,
> since every thread added increases the amount of time the scheduler takes.
>  But of course, that degradation is negligible compared to the amount of
> time your app spends waiting for IO.  So it all depends on your app, what
> it's doing, and what its requirements are.
>
> It's not just scheduling that gets impacted, another obvious one that I
> already I pointed out was memory consumption, so once the thread stacks
> have consumed all available RAM, then they won't just be the bottleneck,
> your application will slow to a crawl or even crash.
>
>
> Slow is something due to something else. No more stack space leads to OOME
> Stack space exhausted. Is this what you mean?
>
> Regards,
> Kirk
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130813/67f6df4d/attachment.html>

From unmeshjoshi at gmail.com  Tue Aug 13 07:20:14 2013
From: unmeshjoshi at gmail.com (Unmesh Joshi)
Date: Tue, 13 Aug 2013 16:50:14 +0530
Subject: [concurrency-interest] Benchmark to demonstrate improvement in
 thread management over the years.
In-Reply-To: <CABY0rKNxUz4Q=F_u3SANtXL8bX=-10UhX1Jv-oCZP9OqXnnwhA@mail.gmail.com>
References: <CAOk+zfdkhXwf5AyXFb74_4diGDFagSoKTQrZwTA3qzM0ymXUCw@mail.gmail.com>
	<CAHjP37Gf6XaBtj_L1EtL-0rLAABrWQ=X_C7xHZV=poBHoGv65Q@mail.gmail.com>
	<CABY0rKNWMgPyxoS5TiLtQskJrwbtFk-AjzE_S2GZ0yQcKnMc9w@mail.gmail.com>
	<CAOk+zfeZLJyRSf9H9kjEKoDTmbv9e=vPoEO9My3goT0fxrCY8A@mail.gmail.com>
	<CABY0rKNxUz4Q=F_u3SANtXL8bX=-10UhX1Jv-oCZP9OqXnnwhA@mail.gmail.com>
Message-ID: <CAOk+zfedm3SxRm5Pn3SXwWFf0O2+2gvnA5NuXE7r2660yf52Mg@mail.gmail.com>

>>Really, you just need to do your own load testing specific to your own
application,
Agreed. But some decisions need to be taken upfront, before you can load
test the application.
Based on my experience, I have not seen number of threads causing issues
anytime, even with thousands of threads. When load goes beyond that, its
better to have more servers because memory and CPU requirements increase as
well (Just because of the CPU bound tasks, something as simple as parsing
response XMLs or preparing HTML response).
So I was trying to understand if there are any well known benchmarks to
show how many threads a JVM can manage well for typical tasks like simple
DB Access, preparing XML or HTML response etc, on a typical system (e.g.
say a quad core, 16GB system).



On Tue, Aug 13, 2013 at 12:08 PM, James Roper <james.roper at typesafe.com>wrote:

> On Tue, Aug 13, 2013 at 2:59 PM, Unmesh Joshi <unmeshjoshi at gmail.com>wrote:
>
>> Hi James,
>>
>> At what number of threads JVM or OS performance starts degrading? Or
>> number of threads start becoming the main bottleneck in the system?
>>
>
> Really, you just need to do your own load testing specific to your own
> application, hardware and OS requirements.  The current Linux scheduler
> runs in O(log N), so technically, that means performance starts degrading
> at 2 threads, since every thread added increases the amount of time the
> scheduler takes.  But of course, that degradation is negligible compared to
> the amount of time your app spends waiting for IO.  So it all depends on
> your app, what it's doing, and what its requirements are.
>
> It's not just scheduling that gets impacted, another obvious one that I
> already I pointed out was memory consumption, so once the thread stacks
> have consumed all available RAM, then they won't just be the bottleneck,
> your application will slow to a crawl or even crash.
>
>
>> Thanks,
>> Unmesh
>>
>>
>> On Mon, Aug 12, 2013 at 7:57 PM, James Roper <james.roper at typesafe.com>wrote:
>>
>>> It's also worth pointing out that the thread per request model is
>>> becoming less feasible even for simple web apps. Modern service oriented
>>> architectures often require that a single web request may make many
>>> requests to other backend services. At the extreme, we see users writing
>>> Play apps that make hundreds of backend API calls per request. In order to
>>> provide acceptable response times, these requests must be made in parallel.
>>> With blocking IO, that would mean a single request might take 100 threads,
>>> if you had just 100 concurrent requests, that's 10000 threads, if each
>>> thread stack takes 100kb of real memory, that's 1GB memory just for thread
>>> stacks. That's not cheap.
>>>
>>> Regards,
>>>
>>> James
>>> On Aug 13, 2013 12:08 AM, "Vitaly Davidovich" <vitalyd at gmail.com> wrote:
>>>
>>>> I don't have any benchmarks to give, but I don't think the touted
>>>> benefits of an evented model includes CPU performance.  Rather, using an
>>>> evented model allows you to scale.  Specific to a web server, you want to
>>>> be able to handle lots of concurrent connections (most of them are probably
>>>> idle at any given time) while minimizing resource usage to accomplish that.
>>>>
>>>> With a thread-per-request (threaded) model, you may end up using lots
>>>> of threads but most of them are blocked on i/o at any given time.  A slow
>>>> client/consumer can tie up a thread for a very long time.  This also makes
>>>> the server susceptible to a DDoS attack whereby new connections are
>>>> established, but the clients are purposely slow to tie up the server
>>>> threads.  Resource usage is also much higher in the threaded model when you
>>>> have tens of thousands of connections since you're going to pay for stack
>>>> space for each thread (granted it's VM space, but still).
>>>>
>>>> With an evented model, you don't have the inefficiency of having
>>>> thousands of threads alive but that are blocked/waiting on i/o.  A single
>>>> thread dedicated to multiplexing i/o across all the connections will
>>>> probably be sufficient.  The rest is worker threads (most likely = # of
>>>> CPUs for a dedicated machine) that actually handle the request processing,
>>>> but don't do any (significant) i/o.  This design also means that you can
>>>> handle slow clients in a more robust manner.
>>>>
>>>> So, the cost of threads can be "heavy" in the case of very busy web
>>>> servers.  The Linux kernel should handle a few thousand threads (most
>>>> blocked on io) quite well, but I don't think that will be the case for tens
>>>> or hundreds of thousands.  Even if there's sufficient RAM to handle that
>>>> many, there may be performance issues coming from the kernel itself, e.g.
>>>> scheduler.  At the very least, you'll be using resources of the machine
>>>> inefficiently under that setup.
>>>>
>>>> Vitaly
>>>>
>>>> Sent from my phone
>>>> On Aug 12, 2013 9:13 AM, "Unmesh Joshi" <unmeshjoshi at gmail.com> wrote:
>>>>
>>>>> Hi,
>>>>>
>>>>> Most of the books on node.js, Akka, Play or any other event IO based
>>>>> system frequently talk about 'Threads' being heavy and there is cost we
>>>>> have to pay for all the booking the OS or the JVM has to do with all the
>>>>> threads.
>>>>> While I agree that there must be some cost and for doing CPU intensive
>>>>> tasks like matrix multiplication, and fork-join kind of framework will be
>>>>> more performant, I am not sure if for web server kind of IO intensive
>>>>> application that's the case.
>>>>>
>>>>> On the contrary, I am seeing web servers running on tomcat with 1000 +
>>>>> threads without issues.  For web servers. I think that Linux level thread
>>>>> management has improved a lot in last 10 years. Same is with the JVM.
>>>>>
>>>>> Do we have any benchmark which shows how much Linux thread management
>>>>> and JVM thread management have improved over the years?
>>>>>
>>>>> Thanks,
>>>>> Unmesh
>>>>>
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at cs.oswego.edu
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>
>
>
> --
> *James Roper*
> *Software Engineer*
> *
> *
> Typesafe <http://typesafe.com/> ? Build reactive apps!
> Twitter: @jroper <https://twitter.com/jroper>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130813/f1f875ea/attachment-0001.html>

From kirk at kodewerk.com  Tue Aug 13 07:45:32 2013
From: kirk at kodewerk.com (Kirk Pepperdine)
Date: Tue, 13 Aug 2013 13:45:32 +0200
Subject: [concurrency-interest] Benchmark to demonstrate improvement in
	thread management over the years.
In-Reply-To: <CAOk+zfedm3SxRm5Pn3SXwWFf0O2+2gvnA5NuXE7r2660yf52Mg@mail.gmail.com>
References: <CAOk+zfdkhXwf5AyXFb74_4diGDFagSoKTQrZwTA3qzM0ymXUCw@mail.gmail.com>
	<CAHjP37Gf6XaBtj_L1EtL-0rLAABrWQ=X_C7xHZV=poBHoGv65Q@mail.gmail.com>
	<CABY0rKNWMgPyxoS5TiLtQskJrwbtFk-AjzE_S2GZ0yQcKnMc9w@mail.gmail.com>
	<CAOk+zfeZLJyRSf9H9kjEKoDTmbv9e=vPoEO9My3goT0fxrCY8A@mail.gmail.com>
	<CABY0rKNxUz4Q=F_u3SANtXL8bX=-10UhX1Jv-oCZP9OqXnnwhA@mail.gmail.com>
	<CAOk+zfedm3SxRm5Pn3SXwWFf0O2+2gvnA5NuXE7r2660yf52Mg@mail.gmail.com>
Message-ID: <846B751A-7409-4E0D-A7C0-5C5B26234D0B@kodewerk.com>

I have a bench that uses sleep as it's unit of work and that bench is very heavily affected by the number of threads running along with the OS it's running in. Version of JVM seems to make very little difference nor does choice of hardware (until you hit extreme conditions for the core count). Add virtualization and the numbers get much much worse *even for single threaded runs*. It's a fun bench to play with but you need a number of different hardware platforms to run it on to start making sense of it. Run it on once peace of hardware with one OS and it's a rather boring exercise.

Regards,
Kirk

On 2013-08-13, at 1:20 PM, Unmesh Joshi <unmeshjoshi at gmail.com> wrote:

> >>Really, you just need to do your own load testing specific to your own application, 
> Agreed. But some decisions need to be taken upfront, before you can load test the application.
> Based on my experience, I have not seen number of threads causing issues anytime, even with thousands of threads. When load goes beyond that, its better to have more servers because memory and CPU requirements increase as well (Just because of the CPU bound tasks, something as simple as parsing response XMLs or preparing HTML response). 
> So I was trying to understand if there are any well known benchmarks to show how many threads a JVM can manage well for typical tasks like simple DB Access, preparing XML or HTML response etc, on a typical system (e.g. say a quad core, 16GB system).
> 
> 
> 
> On Tue, Aug 13, 2013 at 12:08 PM, James Roper <james.roper at typesafe.com> wrote:
> On Tue, Aug 13, 2013 at 2:59 PM, Unmesh Joshi <unmeshjoshi at gmail.com> wrote:
> Hi James,
> 
> At what number of threads JVM or OS performance starts degrading? Or number of threads start becoming the main bottleneck in the system? 
> 
> Really, you just need to do your own load testing specific to your own application, hardware and OS requirements.  The current Linux scheduler runs in O(log N), so technically, that means performance starts degrading at 2 threads, since every thread added increases the amount of time the scheduler takes.  But of course, that degradation is negligible compared to the amount of time your app spends waiting for IO.  So it all depends on your app, what it's doing, and what its requirements are.
> 
> It's not just scheduling that gets impacted, another obvious one that I already I pointed out was memory consumption, so once the thread stacks have consumed all available RAM, then they won't just be the bottleneck, your application will slow to a crawl or even crash.
> 
> 
> Thanks,
> Unmesh
> 
> 
> On Mon, Aug 12, 2013 at 7:57 PM, James Roper <james.roper at typesafe.com> wrote:
> It's also worth pointing out that the thread per request model is becoming less feasible even for simple web apps. Modern service oriented architectures often require that a single web request may make many requests to other backend services. At the extreme, we see users writing Play apps that make hundreds of backend API calls per request. In order to provide acceptable response times, these requests must be made in parallel. With blocking IO, that would mean a single request might take 100 threads, if you had just 100 concurrent requests, that's 10000 threads, if each thread stack takes 100kb of real memory, that's 1GB memory just for thread stacks. That's not cheap.
> 
> Regards,
> 
> James
> 
> On Aug 13, 2013 12:08 AM, "Vitaly Davidovich" <vitalyd at gmail.com> wrote:
> I don't have any benchmarks to give, but I don't think the touted benefits of an evented model includes CPU performance.  Rather, using an evented model allows you to scale.  Specific to a web server, you want to be able to handle lots of concurrent connections (most of them are probably idle at any given time) while minimizing resource usage to accomplish that.
> 
> With a thread-per-request (threaded) model, you may end up using lots of threads but most of them are blocked on i/o at any given time.  A slow client/consumer can tie up a thread for a very long time.  This also makes the server susceptible to a DDoS attack whereby new connections are established, but the clients are purposely slow to tie up the server threads.  Resource usage is also much higher in the threaded model when you have tens of thousands of connections since you're going to pay for stack space for each thread (granted it's VM space, but still).
> 
> With an evented model, you don't have the inefficiency of having thousands of threads alive but that are blocked/waiting on i/o.  A single thread dedicated to multiplexing i/o across all the connections will probably be sufficient.  The rest is worker threads (most likely = # of CPUs for a dedicated machine) that actually handle the request processing, but don't do any (significant) i/o.  This design also means that you can handle slow clients in a more robust manner. 
> 
> So, the cost of threads can be "heavy" in the case of very busy web servers.  The Linux kernel should handle a few thousand threads (most blocked on io) quite well, but I don't think that will be the case for tens or hundreds of thousands.  Even if there's sufficient RAM to handle that many, there may be performance issues coming from the kernel itself, e.g. scheduler.  At the very least, you'll be using resources of the machine inefficiently under that setup.
> 
> Vitaly
> 
> Sent from my phone
> 
> On Aug 12, 2013 9:13 AM, "Unmesh Joshi" <unmeshjoshi at gmail.com> wrote:
> Hi,
> 
> Most of the books on node.js, Akka, Play or any other event IO based system frequently talk about 'Threads' being heavy and there is cost we have to pay for all the booking the OS or the JVM has to do with all the threads.
> While I agree that there must be some cost and for doing CPU intensive tasks like matrix multiplication, and fork-join kind of framework will be more performant, I am not sure if for web server kind of IO intensive application that's the case.
> 
> On the contrary, I am seeing web servers running on tomcat with 1000 + threads without issues.  For web servers. I think that Linux level thread management has improved a lot in last 10 years. Same is with the JVM. 
> 
> Do we have any benchmark which shows how much Linux thread management and JVM thread management have improved over the years?
> 
> Thanks,
> Unmesh
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> 
> 
> 
> 
> -- 
> James Roper
> Software Engineer
> 
> Typesafe ? Build reactive apps!
> Twitter: @jroper
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130813/8f9b9cad/attachment.html>

From viktor.klang at gmail.com  Tue Aug 13 08:43:50 2013
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Tue, 13 Aug 2013 14:43:50 +0200
Subject: [concurrency-interest] Benchmark to demonstrate improvement in
 thread management over the years.
In-Reply-To: <846B751A-7409-4E0D-A7C0-5C5B26234D0B@kodewerk.com>
References: <CAOk+zfdkhXwf5AyXFb74_4diGDFagSoKTQrZwTA3qzM0ymXUCw@mail.gmail.com>
	<CAHjP37Gf6XaBtj_L1EtL-0rLAABrWQ=X_C7xHZV=poBHoGv65Q@mail.gmail.com>
	<CABY0rKNWMgPyxoS5TiLtQskJrwbtFk-AjzE_S2GZ0yQcKnMc9w@mail.gmail.com>
	<CAOk+zfeZLJyRSf9H9kjEKoDTmbv9e=vPoEO9My3goT0fxrCY8A@mail.gmail.com>
	<CABY0rKNxUz4Q=F_u3SANtXL8bX=-10UhX1Jv-oCZP9OqXnnwhA@mail.gmail.com>
	<CAOk+zfedm3SxRm5Pn3SXwWFf0O2+2gvnA5NuXE7r2660yf52Mg@mail.gmail.com>
	<846B751A-7409-4E0D-A7C0-5C5B26234D0B@kodewerk.com>
Message-ID: <CANPzfU-sre3mMdZSnxfV-aoA=7+OBYLtMKC4adGOJe4XsPMwqw@mail.gmail.com>

And, of course, it is interesting to see how a loop over a short sleep
contrasts to longer periods of sleep (jitter etc).


On Tue, Aug 13, 2013 at 1:45 PM, Kirk Pepperdine <kirk at kodewerk.com> wrote:

> I have a bench that uses sleep as it's unit of work and that bench is very
> heavily affected by the number of threads running along with the OS it's
> running in. Version of JVM seems to make very little difference nor does
> choice of hardware (until you hit extreme conditions for the core count).
> Add virtualization and the numbers get much much worse *even for single
> threaded runs*. It's a fun bench to play with but you need a number of
> different hardware platforms to run it on to start making sense of it. Run
> it on once peace of hardware with one OS and it's a rather boring exercise.
>
> Regards,
> Kirk
>
> On 2013-08-13, at 1:20 PM, Unmesh Joshi <unmeshjoshi at gmail.com> wrote:
>
> >>Really, you just need to do your own load testing specific to your own
> application,
> Agreed. But some decisions need to be taken upfront, before you can load
> test the application.
> Based on my experience, I have not seen number of threads causing issues
> anytime, even with thousands of threads. When load goes beyond that, its
> better to have more servers because memory and CPU requirements increase as
> well (Just because of the CPU bound tasks, something as simple as parsing
> response XMLs or preparing HTML response).
> So I was trying to understand if there are any well known benchmarks to
> show how many threads a JVM can manage well for typical tasks like simple
> DB Access, preparing XML or HTML response etc, on a typical system (e.g.
> say a quad core, 16GB system).
>
>
>
> On Tue, Aug 13, 2013 at 12:08 PM, James Roper <james.roper at typesafe.com>wrote:
>
>> On Tue, Aug 13, 2013 at 2:59 PM, Unmesh Joshi <unmeshjoshi at gmail.com>wrote:
>>
>>> Hi James,
>>>
>>> At what number of threads JVM or OS performance starts degrading? Or
>>> number of threads start becoming the main bottleneck in the system?
>>>
>>
>> Really, you just need to do your own load testing specific to your own
>> application, hardware and OS requirements.  The current Linux scheduler
>> runs in O(log N), so technically, that means performance starts degrading
>> at 2 threads, since every thread added increases the amount of time the
>> scheduler takes.  But of course, that degradation is negligible compared to
>> the amount of time your app spends waiting for IO.  So it all depends on
>> your app, what it's doing, and what its requirements are.
>>
>> It's not just scheduling that gets impacted, another obvious one that I
>> already I pointed out was memory consumption, so once the thread stacks
>> have consumed all available RAM, then they won't just be the bottleneck,
>> your application will slow to a crawl or even crash.
>>
>>
>>> Thanks,
>>> Unmesh
>>>
>>>
>>> On Mon, Aug 12, 2013 at 7:57 PM, James Roper <james.roper at typesafe.com>wrote:
>>>
>>>> It's also worth pointing out that the thread per request model is
>>>> becoming less feasible even for simple web apps. Modern service oriented
>>>> architectures often require that a single web request may make many
>>>> requests to other backend services. At the extreme, we see users writing
>>>> Play apps that make hundreds of backend API calls per request. In order to
>>>> provide acceptable response times, these requests must be made in parallel.
>>>> With blocking IO, that would mean a single request might take 100 threads,
>>>> if you had just 100 concurrent requests, that's 10000 threads, if each
>>>> thread stack takes 100kb of real memory, that's 1GB memory just for thread
>>>> stacks. That's not cheap.
>>>>
>>>> Regards,
>>>>
>>>> James
>>>> On Aug 13, 2013 12:08 AM, "Vitaly Davidovich" <vitalyd at gmail.com>
>>>> wrote:
>>>>
>>>>> I don't have any benchmarks to give, but I don't think the touted
>>>>> benefits of an evented model includes CPU performance.  Rather, using an
>>>>> evented model allows you to scale.  Specific to a web server, you want to
>>>>> be able to handle lots of concurrent connections (most of them are probably
>>>>> idle at any given time) while minimizing resource usage to accomplish that.
>>>>>
>>>>> With a thread-per-request (threaded) model, you may end up using lots
>>>>> of threads but most of them are blocked on i/o at any given time.  A slow
>>>>> client/consumer can tie up a thread for a very long time.  This also makes
>>>>> the server susceptible to a DDoS attack whereby new connections are
>>>>> established, but the clients are purposely slow to tie up the server
>>>>> threads.  Resource usage is also much higher in the threaded model when you
>>>>> have tens of thousands of connections since you're going to pay for stack
>>>>> space for each thread (granted it's VM space, but still).
>>>>>
>>>>> With an evented model, you don't have the inefficiency of having
>>>>> thousands of threads alive but that are blocked/waiting on i/o.  A single
>>>>> thread dedicated to multiplexing i/o across all the connections will
>>>>> probably be sufficient.  The rest is worker threads (most likely = # of
>>>>> CPUs for a dedicated machine) that actually handle the request processing,
>>>>> but don't do any (significant) i/o.  This design also means that you can
>>>>> handle slow clients in a more robust manner.
>>>>>
>>>>> So, the cost of threads can be "heavy" in the case of very busy web
>>>>> servers.  The Linux kernel should handle a few thousand threads (most
>>>>> blocked on io) quite well, but I don't think that will be the case for tens
>>>>> or hundreds of thousands.  Even if there's sufficient RAM to handle that
>>>>> many, there may be performance issues coming from the kernel itself, e.g.
>>>>> scheduler.  At the very least, you'll be using resources of the machine
>>>>> inefficiently under that setup.
>>>>>
>>>>> Vitaly
>>>>>
>>>>> Sent from my phone
>>>>> On Aug 12, 2013 9:13 AM, "Unmesh Joshi" <unmeshjoshi at gmail.com> wrote:
>>>>>
>>>>>> Hi,
>>>>>>
>>>>>> Most of the books on node.js, Akka, Play or any other event IO based
>>>>>> system frequently talk about 'Threads' being heavy and there is cost we
>>>>>> have to pay for all the booking the OS or the JVM has to do with all the
>>>>>> threads.
>>>>>> While I agree that there must be some cost and for doing CPU
>>>>>> intensive tasks like matrix multiplication, and fork-join kind of framework
>>>>>> will be more performant, I am not sure if for web server kind of IO
>>>>>> intensive application that's the case.
>>>>>>
>>>>>> On the contrary, I am seeing web servers running on tomcat with 1000
>>>>>> + threads without issues.  For web servers. I think that Linux level thread
>>>>>> management has improved a lot in last 10 years. Same is with the JVM.
>>>>>>
>>>>>> Do we have any benchmark which shows how much Linux thread management
>>>>>> and JVM thread management have improved over the years?
>>>>>>
>>>>>> Thanks,
>>>>>> Unmesh
>>>>>>
>>>>>> _______________________________________________
>>>>>> Concurrency-interest mailing list
>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>
>>>>>>
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at cs.oswego.edu
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>>
>>>
>>
>>
>> --
>> *James Roper*
>> *Software Engineer*
>> *
>> *
>> Typesafe <http://typesafe.com/> ? Build reactive apps!
>> Twitter: @jroper <https://twitter.com/jroper>
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


-- 
*Viktor Klang*
*Director of Engineering*
Typesafe <http://www.typesafe.com/>

Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130813/178518ad/attachment-0001.html>

From gustav.r.akesson at gmail.com  Tue Aug 13 09:08:38 2013
From: gustav.r.akesson at gmail.com (=?ISO-8859-1?Q?Gustav_=C5kesson?=)
Date: Tue, 13 Aug 2013 15:08:38 +0200
Subject: [concurrency-interest] Implicit parallelism
In-Reply-To: <52092CB4.4040302@infinite-source.de>
References: <CAKEw5+4rcZKiizq26fLaZbYb9NPtb6XWYfxZp5fEbNY9+iG9+A@mail.gmail.com>
	<5206AE38.2030900@infinite-source.de>
	<CAKEw5+4Qk-pMuzai+WH-KMGukkUMwkTj=XbreY5OJhPsJhvz7Q@mail.gmail.com>
	<52092CB4.4040302@infinite-source.de>
Message-ID: <CAKEw5+4qFcJsaJdMa-qbm463cg1mZyxr_MXy6WBJVUo-7QMx6A@mail.gmail.com>

Because raising the abstraction level in these kinds of subjects is usually
a good thing (just as the JVM/JIT etc. do). I'm also not (necessarily)
referring to threads per-se, but parallelism which can be expressed in many
ways.

By the way, for those interested, I found a nice presentation on the
subject (by Guy Steele):
http://www.infoq.com/presentations/Thinking-Parallel-Programming


Best Regards,
Gustav ?kesson


On Mon, Aug 12, 2013 at 8:43 PM, Aaron Grunthal <
aaron.grunthal at infinite-source.de> wrote:

> Why? I see that it would be convenient in some situations. But that is -
> for my tastes - too much magic going on if the application would suddenly
> start spawning threads, potentially competing for resources with other
> tasks in the global thread pool just to execute something that appears to
> be a simple loop.
>
> It would be pure madness if the compiler would suddenly turn a simple
> busy-wait loop with an accumulator (might be used in some locking
> abstractions to spin-wait before yielding) into multiple threads burning
> CPU time.
>
> I could see this happening automatically in some high-throughput data
> processing library (you mentioned OpenMP) because you expect it to utilize
> available resources. But not something low-level as simple loops.
>
> What java *should* do is perform loop-vectorization like GCC or LLVM do.
> I.e. unroll loops and parallelize - say - 4 iterations with SSE/AVX
> instructions where they're data-independent.
>
> System.arraycopy, string cloning and array initialization have hand-coded
> AVX-intrinsics in the most recent hotspot builds I think. Instead of
> manually specializing a few methods this could be generalized.
>
>
> thread-parallelism:
> * consumes additional system resources
> * can be done by the programmer or library
> * there is decent support for it with parallel streams
>
> vectorization:
> * almost free
> * can't be done by the programmer (no inline assembly)
> * not supported at all unless you want to use JNI or GPGPU-bindings
>
>
> On 11.08.2013 10:44, Gustav ?kesson wrote:
>
>> Hi,
>>
>> My point is that the programmer should (in some circumstances) not care
>> if it runs in parallel or not - the execution determines this for you as
>> long as the program semantics are not broken. That's why I don't want to
>> see .parallel or similar - that is somewhat explicit parallelism. As you
>> say, the example I gave was read-only so then it should (theoretically)
>> be possible to determine that this is safe to run in parallel. The list
>> can't change, nor the elements.
>>
>> But let's take another example. Let's say we have a Scala (immutable)
>> list containing Int type. If we then add all integers from 0 to x...
>>
>> for (i <- 0 to x) {
>>   immutableIntegerList = i :: immutableIntegerList
>> }
>>
>> Or in Java, a locally declared array which we fill with values:
>>
>> int[] arr = new int[x]
>>
>> for (int i=0; i<x; ++i)
>>   arr[i] = i
>> }
>>
>> In my mind it should be possible to split the loop constructs into y
>> parts and then concatenate the result in order. Similar to OpenMP but
>> without the explicit parallelism annotation. I know this question is of
>> rather academic type, but I'm wondering whether there has been any ideas
>> or work to bring this type of functionality to the JVM (and the
>> compilers).
>>
>>
>> Best Regards,
>> Gustav ?kesson
>>
>>
>> On Sat, Aug 10, 2013 at 11:18 PM, Aaron Grunthal
>> <aaron.grunthal at infinite-**source.de <aaron.grunthal at infinite-source.de>
>> <mailto:aaron.grunthal@**infinite-source.de<aaron.grunthal at infinite-source.de>>>
>> wrote:
>>
>>     Thread-safety in the context of data structure usually (but not
>>     always) refers to consistent behavior when it is concurrently
>>     *modified*.
>>
>>     In your example you mention summing over a list. That's a read-only
>>     action and doesn't require immutable lists or special concurrency
>>     support. Most data structures are well-behaved under concurrent
>>     read-only access, barring exceptions such as access-ordered
>>     LinkedHashMaps.
>>
>>     Java 8's parallel streams also are smart enough to not parallelize
>>     on small data sets where the overhead would kill any performance
>>     advantage. At least for data structures that allow size-estimates or
>>     actual sizes to be obtained.
>>
>>     I think there shouldn't be anything keeping you from adding a
>>     .parallel if you anticipate the possibility of your data
>>     transformations handling non-trivial amounts of data.
>>
>>     - Aaron
>>
>>
>>
>>     On 10.08.2013 20:45, Gustav ?kesson wrote:
>>
>>         Hi guys,
>>
>>         My discussion here concerns implicit parallelism. Let's say we
>>         have the
>>         following setup:
>>
>>         @Immutable
>>         public class Integer
>>         {
>>             ...
>>         }
>>
>>         @Immutable
>>         public class ImmutableArrayList
>>         {
>>              ...
>>         }
>>
>>         I'm looking for a way so that the parallelism would be introduced
>>         without hard-coding anything related to parallelism (i.e. not
>>         stating
>>         anything like .parallel or .par on the collection). Only thing
>>         needed
>>         would be something annotation-ish which tells the execution
>>         environment
>>         that this datastructure with elements is inherently thread-safe.
>>         Then
>>         the execution could determine if it would be beneficial to do
>>         so. For
>>         instance, for a structure with e.g. five elements, then it would
>>         not,
>>         but for millions of elements, it would most likely be. Perhaps
>>         it could
>>         even find some kind of sweet-spot of number of elements in which
>> the
>>         parallel overhead exceeds the benefits.
>>
>>         Let's say we wish to sum all the integers in an ImmutableArrayList
>>         (setup below), would it be possible for the compiler (javac,
>>         scalac or
>>         what have you) and JVM to conspire and decide "hey, let's run
>>         this in
>>         parallel since it doesn't violate application semantics and it
>>         can/will
>>         be faster"? Is there any research in this area in regards to the
>>         JVM?
>>
>>
>>         Best Regards,
>>
>>         Gustav ?kesson
>>
>>
>>         ______________________________**___________________
>>         Concurrency-interest mailing list
>>         Concurrency-interest at cs.__oswe**go.edu <http://oswego.edu>
>>         <mailto:Concurrency-interest@**cs.oswego.edu<Concurrency-interest at cs.oswego.edu>
>> >
>>         http://cs.oswego.edu/mailman/_**_listinfo/concurrency-interest<http://cs.oswego.edu/mailman/__listinfo/concurrency-interest>
>>         <http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>> >
>>
>>
>>     ______________________________**___________________
>>     Concurrency-interest mailing list
>>     Concurrency-interest at cs.__oswe**go.edu <http://oswego.edu>
>>     <mailto:Concurrency-interest@**cs.oswego.edu<Concurrency-interest at cs.oswego.edu>
>> >
>>     http://cs.oswego.edu/mailman/_**_listinfo/concurrency-interest<http://cs.oswego.edu/mailman/__listinfo/concurrency-interest>
>>     <http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>> >
>>
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130813/87d5144c/attachment.html>

From vitalyd at gmail.com  Tue Aug 13 09:47:35 2013
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Tue, 13 Aug 2013 09:47:35 -0400
Subject: [concurrency-interest] Benchmark to demonstrate improvement in
 thread management over the years.
In-Reply-To: <CALV1V49mPEfRufaJVi2aPwFd1-9dR0MYaKPt2ydjzJArE6j9AQ@mail.gmail.com>
References: <CAOk+zfdkhXwf5AyXFb74_4diGDFagSoKTQrZwTA3qzM0ymXUCw@mail.gmail.com>
	<CAHjP37Gf6XaBtj_L1EtL-0rLAABrWQ=X_C7xHZV=poBHoGv65Q@mail.gmail.com>
	<CABY0rKNWMgPyxoS5TiLtQskJrwbtFk-AjzE_S2GZ0yQcKnMc9w@mail.gmail.com>
	<CAOk+zfeZLJyRSf9H9kjEKoDTmbv9e=vPoEO9My3goT0fxrCY8A@mail.gmail.com>
	<CABY0rKNxUz4Q=F_u3SANtXL8bX=-10UhX1Jv-oCZP9OqXnnwhA@mail.gmail.com>
	<5C2C43A8-AE11-43A8-B343-8D94DC6CFB5B@kodewerk.com>
	<CALV1V49mPEfRufaJVi2aPwFd1-9dR0MYaKPt2ydjzJArE6j9AQ@mail.gmail.com>
Message-ID: <CAHjP37HGa1BUhKwuEhwpwr_kOrHaHAJVO-4nkDt8W+3VZsMssQ@mail.gmail.com>

CFS is an O(logN) scheduler since it represents the runqueue as a red-black
tree (previous scheduler was O(1)).  It keeps a reference to the leftmost
leaf (next runnable task) so that removing from the tree is O(1) but after
running it, it reinserts into the tree which gives logN.

Sent from my phone
On Aug 13, 2013 6:37 AM, "Kimo Crossman" <kimo at webnetic.net> wrote:

> I thought current Linux scheduler CFS was O(1)
>
> http://en.wikipedia.org/wiki/O(1)_scheduler
>
>
> On Tue, Aug 13, 2013 at 1:49 AM, Kirk Pepperdine <kirk at kodewerk.com>wrote:
>
>>
>> On 2013-08-13, at 8:38 AM, James Roper <james.roper at typesafe.com> wrote:
>>
>> On Tue, Aug 13, 2013 at 2:59 PM, Unmesh Joshi <unmeshjoshi at gmail.com>wrote:
>>
>>> Hi James,
>>>
>>> At what number of threads JVM or OS performance starts degrading? Or
>>> number of threads start becoming the main bottleneck in the system?
>>>
>>
>> Really, you just need to do your own load testing specific to your own
>> application, hardware and OS requirements.  The current Linux scheduler
>> runs in O(log N),
>>
>>
>> Really? I thought it was using an O(N) heap sort. Anyways, IME, the
>> bigger problem is that getting the thread scheduler scheduled so that you
>> can get threads scheduled... (there might be a Dr. Seuss thing going on
>> here).
>>
>>
>>
>> so technically, that means performance starts degrading at 2 threads,
>> since every thread added increases the amount of time the scheduler takes.
>>  But of course, that degradation is negligible compared to the amount of
>> time your app spends waiting for IO.  So it all depends on your app, what
>> it's doing, and what its requirements are.
>>
>> It's not just scheduling that gets impacted, another obvious one that I
>> already I pointed out was memory consumption, so once the thread stacks
>> have consumed all available RAM, then they won't just be the bottleneck,
>> your application will slow to a crawl or even crash.
>>
>>
>> Slow is something due to something else. No more stack space leads to
>> OOME Stack space exhausted. Is this what you mean?
>>
>> Regards,
>> Kirk
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130813/75010771/attachment.html>

From ron.pressler at gmail.com  Tue Aug 13 10:58:49 2013
From: ron.pressler at gmail.com (Ron Pressler)
Date: Tue, 13 Aug 2013 17:58:49 +0300
Subject: [concurrency-interest] Benchmark to demonstrate improvement in
 thread management over the years.
In-Reply-To: <CAHjP37HGa1BUhKwuEhwpwr_kOrHaHAJVO-4nkDt8W+3VZsMssQ@mail.gmail.com>
References: <CAOk+zfdkhXwf5AyXFb74_4diGDFagSoKTQrZwTA3qzM0ymXUCw@mail.gmail.com>
	<CAHjP37Gf6XaBtj_L1EtL-0rLAABrWQ=X_C7xHZV=poBHoGv65Q@mail.gmail.com>
	<CABY0rKNWMgPyxoS5TiLtQskJrwbtFk-AjzE_S2GZ0yQcKnMc9w@mail.gmail.com>
	<CAOk+zfeZLJyRSf9H9kjEKoDTmbv9e=vPoEO9My3goT0fxrCY8A@mail.gmail.com>
	<CABY0rKNxUz4Q=F_u3SANtXL8bX=-10UhX1Jv-oCZP9OqXnnwhA@mail.gmail.com>
	<5C2C43A8-AE11-43A8-B343-8D94DC6CFB5B@kodewerk.com>
	<CALV1V49mPEfRufaJVi2aPwFd1-9dR0MYaKPt2ydjzJArE6j9AQ@mail.gmail.com>
	<CAHjP37HGa1BUhKwuEhwpwr_kOrHaHAJVO-4nkDt8W+3VZsMssQ@mail.gmail.com>
Message-ID: <CABg6-qiB_DvqQSW0xyTt3fL0OUQEyB3dsMLUD_mYdaDG5+ExrA@mail.gmail.com>

Events and threads are duals, but their implementations have usually
favored different goals. The key implementation difference, I think, is
that OS threads (or Java threads for that matter) were designed to support
long-running computations, while events support processes that are expected
to block more often than not. But we can design lightweight threads that
support the same expectation, and I've done exactly that in a relatively
new library called Quasar <https://github.com/puniverse/quasar>. Quasar
implements true lightweight threads (which we call fibers) that are
implemented as continuations (instrumented at class-load time to manage a
dynamically resizing stack), scheduled on top of a fork-join pool.

The Fiber class has a very similar API to that of Thread, and, most
importantly, supports the "park" and "unpark" operations. We general
transformation from an asynchronous callback to a park-unpark pair,
exploits the event/thread duality, and transforms any asynchronous (i.e.
callback based) operation to a "fiber-blocking" operation. Quasar already
transforms NIO operations to fiber-blocking operations (currently through a
slightly modified API, but as soon as tomorrow, we will employ NIO's
original interfaces), and in a couple of weeks we'll release fiber-based
servlet (one fiber per request) + fiber-blocking JDBC. The goal is to
provide the scaling of the evented model with the ease of use of the
one-thread-per-request model.

On Tue, Aug 13, 2013 at 4:47 PM, Vitaly Davidovich <vitalyd at gmail.com>wrote:

> CFS is an O(logN) scheduler since it represents the runqueue as a
> red-black tree (previous scheduler was O(1)).  It keeps a reference to the
> leftmost leaf (next runnable task) so that removing from the tree is O(1)
> but after running it, it reinserts into the tree which gives logN.
>
> Sent from my phone
> On Aug 13, 2013 6:37 AM, "Kimo Crossman" <kimo at webnetic.net> wrote:
>
>> I thought current Linux scheduler CFS was O(1)
>>
>> http://en.wikipedia.org/wiki/O(1)_scheduler
>>
>>
>> On Tue, Aug 13, 2013 at 1:49 AM, Kirk Pepperdine <kirk at kodewerk.com>wrote:
>>
>>>
>>> On 2013-08-13, at 8:38 AM, James Roper <james.roper at typesafe.com> wrote:
>>>
>>> On Tue, Aug 13, 2013 at 2:59 PM, Unmesh Joshi <unmeshjoshi at gmail.com>wrote:
>>>
>>>> Hi James,
>>>>
>>>> At what number of threads JVM or OS performance starts degrading? Or
>>>> number of threads start becoming the main bottleneck in the system?
>>>>
>>>
>>> Really, you just need to do your own load testing specific to your own
>>> application, hardware and OS requirements.  The current Linux scheduler
>>> runs in O(log N),
>>>
>>>
>>> Really? I thought it was using an O(N) heap sort. Anyways, IME, the
>>> bigger problem is that getting the thread scheduler scheduled so that you
>>> can get threads scheduled... (there might be a Dr. Seuss thing going on
>>> here).
>>>
>>>
>>>
>>> so technically, that means performance starts degrading at 2 threads,
>>> since every thread added increases the amount of time the scheduler takes.
>>>  But of course, that degradation is negligible compared to the amount of
>>> time your app spends waiting for IO.  So it all depends on your app, what
>>> it's doing, and what its requirements are.
>>>
>>> It's not just scheduling that gets impacted, another obvious one that I
>>> already I pointed out was memory consumption, so once the thread stacks
>>> have consumed all available RAM, then they won't just be the bottleneck,
>>> your application will slow to a crawl or even crash.
>>>
>>>
>>> Slow is something due to something else. No more stack space leads to
>>> OOME Stack space exhausted. Is this what you mean?
>>>
>>> Regards,
>>> Kirk
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130813/c05c1be9/attachment-0001.html>

From nathan.reynolds at oracle.com  Tue Aug 13 13:12:05 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Tue, 13 Aug 2013 10:12:05 -0700
Subject: [concurrency-interest] Benchmark to demonstrate improvement in
 thread management over the years.
In-Reply-To: <5208EBE4.5060406@oracle.com>
References: <CAOk+zfdkhXwf5AyXFb74_4diGDFagSoKTQrZwTA3qzM0ymXUCw@mail.gmail.com>
	<5208EBE4.5060406@oracle.com>
Message-ID: <520A68E5.4010207@oracle.com>

An event model requires copying the state into heap objects.  A thread 
model allows the state to stay on the stack.  So, an event model may be 
able to reuse the threads but puts more pressure on the heap and GC.  
HotSpot's escape analysis is trying to reduce heap pressure by putting 
more objects on the stack.  However, there is a cost for having the 
stack.  The hard question is which costs more.

-Nathan

On 8/12/2013 7:06 AM, Oleksandr Otenko wrote:
> http://www.slideshare.net/e456/tyma-paulmultithreaded1
>
> Alex
>
> On 12/08/2013 14:03, Unmesh Joshi wrote:
>> Hi,
>>
>> Most of the books on node.js, Akka, Play or any other event IO based 
>> system frequently talk about 'Threads' being heavy and there is cost 
>> we have to pay for all the booking the OS or the JVM has to do with 
>> all the threads.
>> While I agree that there must be some cost and for doing CPU 
>> intensive tasks like matrix multiplication, and fork-join kind of 
>> framework will be more performant, I am not sure if for web server 
>> kind of IO intensive application that's the case.
>>
>> On the contrary, I am seeing web servers running on tomcat with 1000 
>> + threads without issues.  For web servers. I think that Linux level 
>> thread management has improved a lot in last 10 years. Same is with 
>> the JVM.
>>
>> Do we have any benchmark which shows how much Linux thread management 
>> and JVM thread management have improved over the years?
>>
>> Thanks,
>> Unmesh
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130813/4de7bf8b/attachment.html>

From aaron.grunthal at infinite-source.de  Tue Aug 13 14:07:59 2013
From: aaron.grunthal at infinite-source.de (Aaron Grunthal)
Date: Tue, 13 Aug 2013 20:07:59 +0200
Subject: [concurrency-interest] Benchmark to demonstrate improvement in
 thread management over the years.
In-Reply-To: <520A68E5.4010207@oracle.com>
References: <CAOk+zfdkhXwf5AyXFb74_4diGDFagSoKTQrZwTA3qzM0ymXUCw@mail.gmail.com>
	<5208EBE4.5060406@oracle.com> <520A68E5.4010207@oracle.com>
Message-ID: <520A75FF.3030103@infinite-source.de>

Tail call optimization could reduce that cost (stack size). I think it's 
under development in the MLVM but afaik there are some constraints on 
how much it can optimize due to the stack-walking requirements of 
security contexts.

So continuation-passing style threaded code + TCO is quite close to 
evented code.

- Aaron

On 13.08.2013 19:12, Nathan Reynolds wrote:
> An event model requires copying the state into heap objects.  A thread
> model allows the state to stay on the stack.  So, an event model may be
> able to reuse the threads but puts more pressure on the heap and GC.
> HotSpot's escape analysis is trying to reduce heap pressure by putting
> more objects on the stack.  However, there is a cost for having the
> stack.  The hard question is which costs more.
>
> -Nathan
>
> On 8/12/2013 7:06 AM, Oleksandr Otenko wrote:
>> http://www.slideshare.net/e456/tyma-paulmultithreaded1
>>
>> Alex
>>
>> On 12/08/2013 14:03, Unmesh Joshi wrote:
>>> Hi,
>>>
>>> Most of the books on node.js, Akka, Play or any other event IO based
>>> system frequently talk about 'Threads' being heavy and there is cost
>>> we have to pay for all the booking the OS or the JVM has to do with
>>> all the threads.
>>> While I agree that there must be some cost and for doing CPU
>>> intensive tasks like matrix multiplication, and fork-join kind of
>>> framework will be more performant, I am not sure if for web server
>>> kind of IO intensive application that's the case.
>>>
>>> On the contrary, I am seeing web servers running on tomcat with 1000
>>> + threads without issues.  For web servers. I think that Linux level
>>> thread management has improved a lot in last 10 years. Same is with
>>> the JVM.
>>>
>>> Do we have any benchmark which shows how much Linux thread management
>>> and JVM thread management have improved over the years?
>>>
>>> Thanks,
>>> Unmesh
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From nathan.reynolds at oracle.com  Tue Aug 13 15:37:39 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Tue, 13 Aug 2013 12:37:39 -0700
Subject: [concurrency-interest] Benchmark to demonstrate improvement in
 thread management over the years.
In-Reply-To: <CANPzfU-sre3mMdZSnxfV-aoA=7+OBYLtMKC4adGOJe4XsPMwqw@mail.gmail.com>
References: <CAOk+zfdkhXwf5AyXFb74_4diGDFagSoKTQrZwTA3qzM0ymXUCw@mail.gmail.com>
	<CAHjP37Gf6XaBtj_L1EtL-0rLAABrWQ=X_C7xHZV=poBHoGv65Q@mail.gmail.com>
	<CABY0rKNWMgPyxoS5TiLtQskJrwbtFk-AjzE_S2GZ0yQcKnMc9w@mail.gmail.com>
	<CAOk+zfeZLJyRSf9H9kjEKoDTmbv9e=vPoEO9My3goT0fxrCY8A@mail.gmail.com>
	<CABY0rKNxUz4Q=F_u3SANtXL8bX=-10UhX1Jv-oCZP9OqXnnwhA@mail.gmail.com>
	<CAOk+zfedm3SxRm5Pn3SXwWFf0O2+2gvnA5NuXE7r2660yf52Mg@mail.gmail.com>
	<846B751A-7409-4E0D-A7C0-5C5B26234D0B@kodewerk.com>
	<CANPzfU-sre3mMdZSnxfV-aoA=7+OBYLtMKC4adGOJe4XsPMwqw@mail.gmail.com>
Message-ID: <520A8B03.5010509@oracle.com>

JIT may not optimize methods which sleep a lot.  So, the benchmark may 
be negatively impact by interpretation.  Consider adding 
-XX:-UseCounterDecay the JVM command line to have JIT still optimize the 
methods.

-Nathan

On 8/13/2013 5:43 AM, ?iktor ?lang wrote:
> And, of course, it is interesting to see how a loop over a short sleep 
> contrasts to longer periods of sleep (jitter etc).
>
>
> On Tue, Aug 13, 2013 at 1:45 PM, Kirk Pepperdine <kirk at kodewerk.com 
> <mailto:kirk at kodewerk.com>> wrote:
>
>     I have a bench that uses sleep as it's unit of work and that bench
>     is very heavily affected by the number of threads running along
>     with the OS it's running in. Version of JVM seems to make very
>     little difference nor does choice of hardware (until you hit
>     extreme conditions for the core count). Add virtualization and the
>     numbers get much much worse *even for single threaded runs*. It's
>     a fun bench to play with but you need a number of different
>     hardware platforms to run it on to start making sense of it. Run
>     it on once peace of hardware with one OS and it's a rather boring
>     exercise.
>
>     Regards,
>     Kirk
>
>     On 2013-08-13, at 1:20 PM, Unmesh Joshi <unmeshjoshi at gmail.com
>     <mailto:unmeshjoshi at gmail.com>> wrote:
>
>>     >>Really, you just need to do your own load testing specific to
>>     your own application,
>>     Agreed. But some decisions need to be taken upfront, before you
>>     can load test the application.
>>     Based on my experience, I have not seen number of threads causing
>>     issues anytime, even with thousands of threads. When load goes
>>     beyond that, its better to have more servers because memory and
>>     CPU requirements increase as well (Just because of the CPU bound
>>     tasks, something as simple as parsing response XMLs or preparing
>>     HTML response).
>>     So I was trying to understand if there are any well known
>>     benchmarks to show how many threads a JVM can manage well for
>>     typical tasks like simple DB Access, preparing XML or HTML
>>     response etc, on a typical system (e.g. say a quad core, 16GB
>>     system).
>>
>>
>>
>>     On Tue, Aug 13, 2013 at 12:08 PM, James Roper
>>     <james.roper at typesafe.com <mailto:james.roper at typesafe.com>> wrote:
>>
>>         On Tue, Aug 13, 2013 at 2:59 PM, Unmesh Joshi
>>         <unmeshjoshi at gmail.com <mailto:unmeshjoshi at gmail.com>> wrote:
>>
>>             Hi James,
>>
>>             At what number of threads JVM or OS performance starts
>>             degrading? Or number of threads start becoming the main
>>             bottleneck in the system?
>>
>>
>>         Really, you just need to do your own load testing specific to
>>         your own application, hardware and OS requirements.  The
>>         current Linux scheduler runs in O(log N), so technically,
>>         that means performance starts degrading at 2 threads, since
>>         every thread added increases the amount of time the scheduler
>>         takes.  But of course, that degradation is negligible
>>         compared to the amount of time your app spends waiting for
>>         IO.  So it all depends on your app, what it's doing, and what
>>         its requirements are.
>>
>>         It's not just scheduling that gets impacted, another obvious
>>         one that I already I pointed out was memory consumption, so
>>         once the thread stacks have consumed all available RAM, then
>>         they won't just be the bottleneck, your application will slow
>>         to a crawl or even crash.
>>
>>
>>             Thanks,
>>             Unmesh
>>
>>
>>             On Mon, Aug 12, 2013 at 7:57 PM, James Roper
>>             <james.roper at typesafe.com
>>             <mailto:james.roper at typesafe.com>> wrote:
>>
>>                 It's also worth pointing out that the thread per
>>                 request model is becoming less feasible even for
>>                 simple web apps. Modern service oriented
>>                 architectures often require that a single web request
>>                 may make many requests to other backend services. At
>>                 the extreme, we see users writing Play apps that make
>>                 hundreds of backend API calls per request. In order
>>                 to provide acceptable response times, these requests
>>                 must be made in parallel. With blocking IO, that
>>                 would mean a single request might take 100 threads,
>>                 if you had just 100 concurrent requests, that's 10000
>>                 threads, if each thread stack takes 100kb of real
>>                 memory, that's 1GB memory just for thread stacks.
>>                 That's not cheap.
>>
>>                 Regards,
>>
>>                 James
>>
>>                 On Aug 13, 2013 12:08 AM, "Vitaly Davidovich"
>>                 <vitalyd at gmail.com <mailto:vitalyd at gmail.com>> wrote:
>>
>>                     I don't have any benchmarks to give, but I don't
>>                     think the touted benefits of an evented model
>>                     includes CPU performance. Rather, using an
>>                     evented model allows you to scale.  Specific to a
>>                     web server, you want to be able to handle lots of
>>                     concurrent connections (most of them are probably
>>                     idle at any given time) while minimizing resource
>>                     usage to accomplish that.
>>
>>                     With a thread-per-request (threaded) model, you
>>                     may end up using lots of threads but most of them
>>                     are blocked on i/o at any given time.  A slow
>>                     client/consumer can tie up a thread for a very
>>                     long time. This also makes the server susceptible
>>                     to a DDoS attack whereby new connections are
>>                     established, but the clients are purposely slow
>>                     to tie up the server threads. Resource usage is
>>                     also much higher in the threaded model when you
>>                     have tens of thousands of connections since
>>                     you're going to pay for stack space for each
>>                     thread (granted it's VM space, but still).
>>
>>                     With an evented model, you don't have the
>>                     inefficiency of having thousands of threads alive
>>                     but that are blocked/waiting on i/o.  A single
>>                     thread dedicated to multiplexing i/o across all
>>                     the connections will probably be sufficient.  The
>>                     rest is worker threads (most likely = # of CPUs
>>                     for a dedicated machine) that actually handle the
>>                     request processing, but don't do any
>>                     (significant) i/o.  This design also means that
>>                     you can handle slow clients in a more robust manner.
>>
>>                     So, the cost of threads can be "heavy" in the
>>                     case of very busy web servers.  The Linux kernel
>>                     should handle a few thousand threads (most
>>                     blocked on io) quite well, but I don't think that
>>                     will be the case for tens or hundreds of
>>                     thousands.  Even if there's sufficient RAM to
>>                     handle that many, there may be performance issues
>>                     coming from the kernel itself, e.g. scheduler. 
>>                     At the very least, you'll be using resources of
>>                     the machine inefficiently under that setup.
>>
>>                     Vitaly
>>
>>                     Sent from my phone
>>
>>                     On Aug 12, 2013 9:13 AM, "Unmesh Joshi"
>>                     <unmeshjoshi at gmail.com
>>                     <mailto:unmeshjoshi at gmail.com>> wrote:
>>
>>                         Hi,
>>
>>                         Most of the books on node.js, Akka, Play or
>>                         any other event IO based system frequently
>>                         talk about 'Threads' being heavy and there is
>>                         cost we have to pay for all the booking the
>>                         OS or the JVM has to do with all the threads.
>>                         While I agree that there must be some cost
>>                         and for doing CPU intensive tasks like matrix
>>                         multiplication, and fork-join kind of
>>                         framework will be more performant, I am not
>>                         sure if for web server kind of IO intensive
>>                         application that's the case.
>>
>>                         On the contrary, I am seeing web servers
>>                         running on tomcat with 1000 + threads without
>>                         issues.  For web servers. I think that Linux
>>                         level thread management has improved a lot in
>>                         last 10 years. Same is with the JVM.
>>
>>                         Do we have any benchmark which shows how much
>>                         Linux thread management and JVM thread
>>                         management have improved over the years?
>>
>>                         Thanks,
>>                         Unmesh
>>
>>                         _______________________________________________
>>                         Concurrency-interest mailing list
>>                         Concurrency-interest at cs.oswego.edu
>>                         <mailto:Concurrency-interest at cs.oswego.edu>
>>                         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>                     _______________________________________________
>>                     Concurrency-interest mailing list
>>                     Concurrency-interest at cs.oswego.edu
>>                     <mailto:Concurrency-interest at cs.oswego.edu>
>>                     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>>
>>
>>         -- 
>>         *James Roper*
>>         /Software Engineer/
>>         /
>>         /
>>         Typesafe <http://typesafe.com/> -- Build reactive apps!
>>         Twitter: @jroper <https://twitter.com/jroper>
>>
>>
>>     _______________________________________________
>>     Concurrency-interest mailing list
>>     Concurrency-interest at cs.oswego.edu
>>     <mailto:Concurrency-interest at cs.oswego.edu>
>>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
> -- 
> *Viktor Klang*
> /Director of Engineering/
> Typesafe <http://www.typesafe.com/>
>
> Twitter: @viktorklang
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130813/5e63dd35/attachment-0001.html>

From mikeb01 at gmail.com  Tue Aug 13 16:55:13 2013
From: mikeb01 at gmail.com (Michael Barker)
Date: Wed, 14 Aug 2013 08:55:13 +1200
Subject: [concurrency-interest] Implicit parallelism
In-Reply-To: <52092CB4.4040302@infinite-source.de>
References: <CAKEw5+4rcZKiizq26fLaZbYb9NPtb6XWYfxZp5fEbNY9+iG9+A@mail.gmail.com>
	<5206AE38.2030900@infinite-source.de>
	<CAKEw5+4Qk-pMuzai+WH-KMGukkUMwkTj=XbreY5OJhPsJhvz7Q@mail.gmail.com>
	<52092CB4.4040302@infinite-source.de>
Message-ID: <CALwNKeQOuE6ZSdWGyXQ1PyEoRP9pvS2xUnMvoSew27O0zu3zYg@mail.gmail.com>

> What java *should* do is perform loop-vectorization like GCC or LLVM do.
> I.e. unroll loops and parallelize - say - 4 iterations with SSE/AVX
> instructions where they're data-independent.

AFAICT, the JVM already does this, for simple loops anyway.  E.g.

for (int i = 0; i < a.length && i < b.length && i < c.length; i++) {
   c[i] = a[i] + b[i];
}

If you print the assembly you will see the loop unrolled and
appropriate VMOVcc, VADDcc instructions.  I compared the above code to
some hand vectorised code using AVX and the difference was that Java
was slower by only around 5% when compiled with gcc -O3.  Without the
optimisation switch for gcc the Java code was twice as fast.

Mike.

From ashwin.jayaprakash at gmail.com  Thu Aug 15 22:36:11 2013
From: ashwin.jayaprakash at gmail.com (Ashwin Jayaprakash)
Date: Thu, 15 Aug 2013 19:36:11 -0700
Subject: [concurrency-interest] Reader-writer visibility in
	LinkedBlockingQueue
Message-ID: <CAF9YjSDs7ZR0UcXUEsD_97udbKZHbVgMc5xmkLS3yTL5wBv+6A@mail.gmail.com>

Hi, I have a (naive) question about the JDK 7+ LinkedBlockingQueue
implementation.

(I don't think this question has been asked before. I did search
here<http://markmail.org/search/?q=concurrency%20interest%20linkedblockingqueue#query:concurrency%20interest%20linkedblockingqueue+page:15+mid:ocig5f7ry2j5emhl+state:results>before
deciding to use your time. I think the answer is evident but just
need a confirmation)

In the source comment
here<http://www.grepcode.com/file/repository.grepcode.com/java/root/jdk/openjdk/7-b147/java/util/concurrent/LinkedBlockingQueue.java#97>it
says:

Visibility between writers and readers is provided as follows:
>
> Whenever an element is enqueued, the putLock is acquired and
> count updated.  A subsequent reader guarantees visibility to the
> enqueued Node by either acquiring the putLock (via fullyLock)
> or by acquiring the takeLock, and then reading n = count.get();
> this gives visibility to the first n items.
>

My understanding of the code:

   1. I noticed that the Node.next
   <http://www.grepcode.com/file/repository.grepcode.com/java/root/jdk/openjdk/7-b147/java/util/concurrent/LinkedBlockingQueue.java#120>is
   not marked volatile
   2. The dequeue thread always acquires the takeLock. The only time
it acquires,
   signals and quickly releases the
putLock<http://www.grepcode.com/file/repository.grepcode.com/java/root/jdk/openjdk/7-b147/java/util/concurrent/LinkedBlockingQueue.java#181>is
when the queue has room for just 1 more item before becoming full
again
   3. The enqueue thread does the opposite of the dequeue
   4. So, except for the edge cases (recently full/recently empty) where
   the reader and writer acquire and notify each other, how does the enqueue
   of a new node (Node.next) become visible to the dequeue?

Pardon my ignorance but does this mean that:

   1. 2 threads that acquire 2 independent locks are enough to make the
   "next" pointer visible?
   2. Or is it the AtomicInteger's count.inc/dec that is providing the
   desired memory effect? *I think this is it, but I'm seeking clarification
   *


Thanks!
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130815/368499ed/attachment.html>

From davidcholmes at aapt.net.au  Thu Aug 15 22:50:30 2013
From: davidcholmes at aapt.net.au (David Holmes)
Date: Fri, 16 Aug 2013 12:50:30 +1000
Subject: [concurrency-interest] Reader-writer visibility
	inLinkedBlockingQueue
In-Reply-To: <CAF9YjSDs7ZR0UcXUEsD_97udbKZHbVgMc5xmkLS3yTL5wBv+6A@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCIEMLJPAA.davidcholmes@aapt.net.au>

It is #2 - that is what the "and then reading n = count.get();" refers to.

David
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Ashwin
Jayaprakash
  Sent: Friday, 16 August 2013 12:36 PM
  To: concurrency-interest at cs.oswego.edu
  Subject: [concurrency-interest] Reader-writer visibility
inLinkedBlockingQueue


  Hi, I have a (naive) question about the JDK 7+ LinkedBlockingQueue
implementation.


  (I don't think this question has been asked before. I did search here
before deciding to use your time. I think the answer is evident but just
need a confirmation)


  In the source comment here it says:


    Visibility between writers and readers is provided as follows:

    Whenever an element is enqueued, the putLock is acquired and
    count updated.  A subsequent reader guarantees visibility to the
    enqueued Node by either acquiring the putLock (via fullyLock)
    or by acquiring the takeLock, and then reading n = count.get();
    this gives visibility to the first n items.



  My understanding of the code:

    1.. I noticed that the Node.next is not marked volatile
    2.. The dequeue thread always acquires the takeLock. The only time it
acquires, signals and quickly releases the putLock is when the queue has
room for just 1 more item before becoming full again

    3.. The enqueue thread does the opposite of the dequeue
    4.. So, except for the edge cases (recently full/recently empty) where
the reader and writer acquire and notify each other, how does the enqueue of
a new node (Node.next) become visible to the dequeue?

  Pardon my ignorance but does this mean that:

    1.. 2 threads that acquire 2 independent locks are enough to make the
"next" pointer visible?
    2.. Or is it the AtomicInteger's count.inc/dec that is providing the
desired memory effect? I think this is it, but I'm seeking clarification



  Thanks!
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130816/0a4b7f1d/attachment.html>

From ashwin.jayaprakash at gmail.com  Fri Aug 16 00:49:31 2013
From: ashwin.jayaprakash at gmail.com (Ashwin Jayaprakash)
Date: Thu, 15 Aug 2013 21:49:31 -0700
Subject: [concurrency-interest] Reader-writer visibility
	inLinkedBlockingQueue
In-Reply-To: <NFBBKALFDCPFIDBNKAPCIEMLJPAA.davidcholmes@aapt.net.au>
References: <CAF9YjSDs7ZR0UcXUEsD_97udbKZHbVgMc5xmkLS3yTL5wBv+6A@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCIEMLJPAA.davidcholmes@aapt.net.au>
Message-ID: <CAF9YjSBHF08vr+i==-LS4wAfYOwFBmoNfSr+YVDNStn3bJRGqQ@mail.gmail.com>

Thanks. Just another followup question.

Can you confirm if this is correct -  a volatile write or atomic
dec/inc *becomes
visible immediately* to other threads... even when it is done inside a
lock-unlock/synchronized block and the *writer thread has not yet
exited*the block called lock.unlock(). Like the code we are talking
about<http://www.grepcode.com/file/repository.grepcode.com/java/root/jdk/openjdk/7-b147/java/util/concurrent/LinkedBlockingQueue.java#456>.
Ex: If a writer were to write to volatile or atomic.xx  and then sleep
while still holding the lock.. the readers should be able to read the
volatile immediately.

Just making sure because these combination cases are not spelled out
anywhere... at least I couldn't find it.

Thanks again.



On Thu, Aug 15, 2013 at 7:50 PM, David Holmes <davidcholmes at aapt.net.au>wrote:

> **
> It is #2 - that is what the "and then reading n = count.get();" refers to.
>
> David
>
> -----Original Message-----
> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
> concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of *Ashwin
> Jayaprakash
> *Sent:* Friday, 16 August 2013 12:36 PM
> *To:* concurrency-interest at cs.oswego.edu
> *Subject:* [concurrency-interest] Reader-writer visibility
> inLinkedBlockingQueue
>
>    Hi, I have a (naive) question about the JDK 7+ LinkedBlockingQueue
> implementation.
>
> (I don't think this question has been asked before. I did search here<http://markmail.org/search/?q=concurrency%20interest%20linkedblockingqueue#query:concurrency%20interest%20linkedblockingqueue+page:15+mid:ocig5f7ry2j5emhl+state:results>before deciding to use your time. I think the answer is evident but just
> need a confirmation)
>
> In the source comment here<http://www.grepcode.com/file/repository.grepcode.com/java/root/jdk/openjdk/7-b147/java/util/concurrent/LinkedBlockingQueue.java#97>it says:
>
> Visibility between writers and readers is provided as follows:
>>
>> Whenever an element is enqueued, the putLock is acquired and
>> count updated.  A subsequent reader guarantees visibility to the
>> enqueued Node by either acquiring the putLock (via fullyLock)
>> or by acquiring the takeLock, and then reading n = count.get();
>> this gives visibility to the first n items.
>>
>
> My understanding of the code:
>
>    1. I noticed that the Node.next
>    <http://www.grepcode.com/file/repository.grepcode.com/java/root/jdk/openjdk/7-b147/java/util/concurrent/LinkedBlockingQueue.java#120>is
>    not marked volatile
>    2. The dequeue thread always acquires the takeLock. The only time it acquires,
>    signals and quickly releases the putLock<http://www.grepcode.com/file/repository.grepcode.com/java/root/jdk/openjdk/7-b147/java/util/concurrent/LinkedBlockingQueue.java#181>is when the queue has room for just 1 more item before becoming full again
>    3. The enqueue thread does the opposite of the dequeue
>    4. So, except for the edge cases (recently full/recently empty) where
>    the reader and writer acquire and notify each other, how does the enqueue
>    of a new node (Node.next) become visible to the dequeue?
>
> Pardon my ignorance but does this mean that:
>
>    1. 2 threads that acquire 2 independent locks are enough to make the
>    "next" pointer visible?
>    2. Or is it the AtomicInteger's count.inc/dec that is providing the
>    desired memory effect? *I think this is it, but I'm seeking
>    clarification*
>
>
> Thanks!
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130815/4b7673e0/attachment.html>

From davidcholmes at aapt.net.au  Fri Aug 16 01:02:15 2013
From: davidcholmes at aapt.net.au (David Holmes)
Date: Fri, 16 Aug 2013 15:02:15 +1000
Subject: [concurrency-interest] Reader-writer
	visibilityinLinkedBlockingQueue
In-Reply-To: <CAF9YjSBHF08vr+i==-LS4wAfYOwFBmoNfSr+YVDNStn3bJRGqQ@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCIEMNJPAA.davidcholmes@aapt.net.au>

No there is no requirement for writes to become visible "immediately". Only
the happens-before ordering guarantees visibility - if you see a given write
then you must also see writes that happen-before that first write. In
practice writes don't take long to become visible on most architectures -
there is typically a delay until a write buffer drains.

David
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Ashwin
Jayaprakash
  Sent: Friday, 16 August 2013 2:50 PM
  To: concurrency-interest at cs.oswego.edu
  Subject: Re: [concurrency-interest] Reader-writer
visibilityinLinkedBlockingQueue


  Thanks. Just another followup question.

  Can you confirm if this is correct -  a volatile write or atomic dec/inc
becomes visible immediately to other threads... even when it is done inside
a lock-unlock/synchronized block and the writer thread has not yet exited
the block called lock.unlock(). Like the code we are talking about. Ex: If a
writer were to write to volatile or atomic.xx  and then sleep while still
holding the lock.. the readers should be able to read the volatile
immediately.


  Just making sure because these combination cases are not spelled out
anywhere... at least I couldn't find it.



  Thanks again.






  On Thu, Aug 15, 2013 at 7:50 PM, David Holmes <davidcholmes at aapt.net.au>
wrote:

    It is #2 - that is what the "and then reading n = count.get();" refers
to.

    David
      -----Original Message-----
      From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Ashwin
Jayaprakash
      Sent: Friday, 16 August 2013 12:36 PM
      To: concurrency-interest at cs.oswego.edu
      Subject: [concurrency-interest] Reader-writer visibility
inLinkedBlockingQueue


      Hi, I have a (naive) question about the JDK 7+ LinkedBlockingQueue
implementation.


      (I don't think this question has been asked before. I did search here
before deciding to use your time. I think the answer is evident but just
need a confirmation)


      In the source comment here it says:


        Visibility between writers and readers is provided as follows:

        Whenever an element is enqueued, the putLock is acquired and
        count updated.  A subsequent reader guarantees visibility to the
        enqueued Node by either acquiring the putLock (via fullyLock)
        or by acquiring the takeLock, and then reading n = count.get();
        this gives visibility to the first n items.



      My understanding of the code:

        1.. I noticed that the Node.next is not marked volatile
        2.. The dequeue thread always acquires the takeLock. The only time
it acquires, signals and quickly releases the putLock is when the queue has
room for just 1 more item before becoming full again

        3.. The enqueue thread does the opposite of the dequeue
        4.. So, except for the edge cases (recently full/recently empty)
where the reader and writer acquire and notify each other, how does the
enqueue of a new node (Node.next) become visible to the dequeue?

      Pardon my ignorance but does this mean that:

        1.. 2 threads that acquire 2 independent locks are enough to make
the "next" pointer visible?
        2.. Or is it the AtomicInteger's count.inc/dec that is providing the
desired memory effect? I think this is it, but I'm seeking clarification



      Thanks!


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130816/659015ce/attachment-0001.html>

From oleksandr.otenko at oracle.com  Tue Aug 20 10:35:25 2013
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Tue, 20 Aug 2013 15:35:25 +0100
Subject: [concurrency-interest] Reader-writer visibility
	inLinkedBlockingQueue
In-Reply-To: <CAF9YjSBHF08vr+i==-LS4wAfYOwFBmoNfSr+YVDNStn3bJRGqQ@mail.gmail.com>
References: <CAF9YjSDs7ZR0UcXUEsD_97udbKZHbVgMc5xmkLS3yTL5wBv+6A@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCIEMLJPAA.davidcholmes@aapt.net.au>
	<CAF9YjSBHF08vr+i==-LS4wAfYOwFBmoNfSr+YVDNStn3bJRGqQ@mail.gmail.com>
Message-ID: <52137EAD.1050105@oracle.com>

When someone talks about "visible immediately", we need to ask them what 
they mean by "immediately". In particular, how would a external observer 
know that something became visible "not immediately".

You will quickly see that the external observer can only make judgements 
about the order of some events with respect to other events. If 
observing arrival of event A implies arrival of event B can be observed, 
ie A implies B, then you have B happens before A. This is pretty much 
all the external observer can figure out. But this is enough to reason 
about the state of the system.

Alex

On 16/08/2013 05:49, Ashwin Jayaprakash wrote:
> Thanks. Just another followup question.
>
> Can you confirm if this is correct -  a volatile write or atomic 
> dec/inc *becomes visible immediately* to other threads... even when it 
> is done inside a lock-unlock/synchronized block and the *writer thread 
> has not yet exited* the block called lock.unlock(). Like the code we 
> are talking about 
> <http://www.grepcode.com/file/repository.grepcode.com/java/root/jdk/openjdk/7-b147/java/util/concurrent/LinkedBlockingQueue.java#456>. 
> Ex: If a writer were to write to volatile or atomic.xx  and then sleep 
> while still holding the lock.. the readers should be able to read the 
> volatile immediately.
>
> Just making sure because these combination cases are not spelled out 
> anywhere... at least I couldn't find it.
>
> Thanks again.
>
>
>
> On Thu, Aug 15, 2013 at 7:50 PM, David Holmes 
> <davidcholmes at aapt.net.au <mailto:davidcholmes at aapt.net.au>> wrote:
>
>     It is #2 - that is what the "and then reading n = count.get();"
>     refers to.
>     David
>
>         -----Original Message-----
>         *From:* concurrency-interest-bounces at cs.oswego.edu
>         <mailto:concurrency-interest-bounces at cs.oswego.edu>
>         [mailto:concurrency-interest-bounces at cs.oswego.edu
>         <mailto:concurrency-interest-bounces at cs.oswego.edu>]*On Behalf
>         Of *Ashwin Jayaprakash
>         *Sent:* Friday, 16 August 2013 12:36 PM
>         *To:* concurrency-interest at cs.oswego.edu
>         <mailto:concurrency-interest at cs.oswego.edu>
>         *Subject:* [concurrency-interest] Reader-writer visibility
>         inLinkedBlockingQueue
>
>         Hi, I have a (naive) question about the JDK 7+
>         LinkedBlockingQueue implementation.
>
>         (I don't think this question has been asked before. I did
>         search here
>         <http://markmail.org/search/?q=concurrency%20interest%20linkedblockingqueue#query:concurrency%20interest%20linkedblockingqueue+page:15+mid:ocig5f7ry2j5emhl+state:results>
>         before deciding to use your time. I think the answer is
>         evident but just need a confirmation)
>
>         In the source comment here
>         <http://www.grepcode.com/file/repository.grepcode.com/java/root/jdk/openjdk/7-b147/java/util/concurrent/LinkedBlockingQueue.java#97>
>         it says:
>
>             Visibility between writers and readers is provided as follows:
>
>             Whenever an element is enqueued, the putLock is acquired and
>             count updated.  A subsequent reader guarantees visibility
>             to the
>             enqueued Node by either acquiring the putLock (via fullyLock)
>             or by acquiring the takeLock, and then reading n =
>             count.get();
>             this gives visibility to the first n items.
>
>
>         My understanding of the code:
>
>          1. I noticed that the Node.next
>             <http://www.grepcode.com/file/repository.grepcode.com/java/root/jdk/openjdk/7-b147/java/util/concurrent/LinkedBlockingQueue.java#120>is
>             not marked volatile
>          2. The dequeue thread always acquires the takeLock. The only
>             time it acquires, signals and quickly releases the putLock
>             <http://www.grepcode.com/file/repository.grepcode.com/java/root/jdk/openjdk/7-b147/java/util/concurrent/LinkedBlockingQueue.java#181>
>             is when the queue has room for just 1 more item before
>             becoming full again
>          3. The enqueue thread does the opposite of the dequeue
>          4. So, except for the edge cases (recently full/recently
>             empty) where the reader and writer acquire and notify each
>             other, how does the enqueue of a new node (Node.next)
>             become visible to the dequeue?
>
>         Pardon my ignorance but does this mean that:
>
>          1. 2 threads that acquire 2 independent locks are enough to
>             make the "next" pointer visible?
>          2. Or is it the AtomicInteger's count.inc/dec that is
>             providing the desired memory effect? *I think this is it,
>             but I'm seeking clarification*
>
>
>         Thanks!
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130820/ca726fd9/attachment.html>

From aph at redhat.com  Wed Aug 21 13:58:01 2013
From: aph at redhat.com (Andrew Haley)
Date: Wed, 21 Aug 2013 18:58:01 +0100
Subject: [concurrency-interest] jcstress: weirdness in class
	AtomicLongArrayPairwiseTests
Message-ID: <5214FFA9.10909@redhat.com>

    public static class DecAndGet_GetAndAdd extends AbstractTest {
        @Override public void actor1(State s, LongResult2 r) { r.r1 = s.a.decrementAndGet(s.idx); }
        @Override public void actor2(State s, LongResult2 r) { r.r2 = s.a.decrementAndGet(s.idx); }
    }

This has got to be wrong, surely?

Andrew.


From aleksey.shipilev at oracle.com  Wed Aug 21 14:54:50 2013
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Wed, 21 Aug 2013 22:54:50 +0400
Subject: [concurrency-interest] jcstress: weirdness in class
	AtomicLongArrayPairwiseTests
In-Reply-To: <5214FFA9.10909@redhat.com>
References: <5214FFA9.10909@redhat.com>
Message-ID: <52150CFA.7030708@oracle.com>

On 08/21/2013 09:58 PM, Andrew Haley wrote:
>     public static class DecAndGet_GetAndAdd extends AbstractTest {
>         @Override public void actor1(State s, LongResult2 r) { r.r1 = s.a.decrementAndGet(s.idx); }
>         @Override public void actor2(State s, LongResult2 r) { r.r2 = s.a.decrementAndGet(s.idx); }
>     }
> 
> This has got to be wrong, surely?

Ah, copy-paste error, thanks. It had proliferated to multiple places in
AtomicX tests. I should switch to generated test cases for this area
some time soon.

Fixed:
 http://mail.openjdk.java.net/pipermail/jcstress-dev/2013-August/000018.html

-Aleksey.


From aph at redhat.com  Thu Aug 22 05:43:55 2013
From: aph at redhat.com (Andrew Haley)
Date: Thu, 22 Aug 2013 10:43:55 +0100
Subject: [concurrency-interest] Difficulty interpreting jcstress results
Message-ID: <5215DD5B.1040208@redhat.com>

I'm getting an odd result:

FloatBufferAtomicityTests$FloatTest           	ERROR 	Error running the test

But when I open the test

Observed state 	Occurrence 	Expectation 	Interpretation 	Refs
[-1] 	0 	ACCEPTABLE 	Seeing the complete update. 	
[0] 	484 	ACCEPTABLE 	Seeing the default value, this is valid under race. 	
[-1082130432] 	456 	KNOWN_ACCEPTABLE 	All other cases are unexpected, but legal

So, if it's legal, why is there an error?

Andrew.

From aleksey.shipilev at oracle.com  Thu Aug 22 08:02:26 2013
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Thu, 22 Aug 2013 16:02:26 +0400
Subject: [concurrency-interest] Difficulty interpreting jcstress results
In-Reply-To: <5215DD5B.1040208@redhat.com>
References: <5215DD5B.1040208@redhat.com>
Message-ID: <4A629F3D-5696-43B8-A005-A0BA884F64D7@oracle.com>

This seems to indicate the fatal error, not the test failure. Maybe the VM crashed after a while? The forked VM can still have communicated a few results before the crash. Let me see if I can get more reliable reporting in these cases.

In the mean time, can you try to disable forking with "-f 0" and re-run the test selectively with "-t .*FloatBuffer.*"?

-Aleksey.

On 22.08.2013, at 13:43, Andrew Haley <aph at redhat.com> wrote:

> I'm getting an odd result:
> 
> FloatBufferAtomicityTests$FloatTest               ERROR    Error running the test
> 
> But when I open the test
> 
> Observed state    Occurrence    Expectation    Interpretation    Refs
> [-1]    0    ACCEPTABLE    Seeing the complete update.    
> [0]    484    ACCEPTABLE    Seeing the default value, this is valid under race.    
> [-1082130432]    456    KNOWN_ACCEPTABLE    All other cases are unexpected, but legal
> 
> So, if it's legal, why is there an error?
> 
> Andrew.


From aph at redhat.com  Thu Aug 22 09:41:47 2013
From: aph at redhat.com (Andrew Haley)
Date: Thu, 22 Aug 2013 14:41:47 +0100
Subject: [concurrency-interest] Difficulty interpreting jcstress results
In-Reply-To: <4A629F3D-5696-43B8-A005-A0BA884F64D7@oracle.com>
References: <5215DD5B.1040208@redhat.com>
	<4A629F3D-5696-43B8-A005-A0BA884F64D7@oracle.com>
Message-ID: <5216151B.9000301@redhat.com>

On 08/22/2013 01:02 PM, Aleksey Shipilev wrote:

> This seems to indicate the fatal error, not the test failure. Maybe
> the VM crashed after a while? The forked VM can still have
> communicated a few results before the crash.

I wouldn't be surprised.  It's still rather flakey.

> Let me see if I can get more reliable reporting in these cases.
> 
> In the mean time, can you try to disable forking with "-f 0" and
> re-run the test selectively with "-t .*FloatBuffer.*"?

Everything runs just fine.

Andrew.

From aleksey.shipilev at oracle.com  Fri Aug 23 08:30:21 2013
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Fri, 23 Aug 2013 16:30:21 +0400
Subject: [concurrency-interest] Difficulty interpreting jcstress results
In-Reply-To: <5216151B.9000301@redhat.com>
References: <5215DD5B.1040208@redhat.com>
	<4A629F3D-5696-43B8-A005-A0BA884F64D7@oracle.com>
	<5216151B.9000301@redhat.com>
Message-ID: <521755DD.7050400@oracle.com>

On 08/22/2013 05:41 PM, Andrew Haley wrote:
>> Let me see if I can get more reliable reporting in these cases.
>>
>> In the mean time, can you try to disable forking with "-f 0" and
>> re-run the test selectively with "-t .*FloatBuffer.*"?
> 
> Everything runs just fine.

Maybe run longer? E.g. "-iters 1000"?

Otherwise, try to run the test with "-t .*FloatBuffer.*", but capture
*all* the logs: stdout, stderr, hs_err, etc, and send them back.

Thanks!
-Aleksey.


From brian at briangoetz.com  Fri Aug 23 12:43:21 2013
From: brian at briangoetz.com (Brian Goetz)
Date: Fri, 23 Aug 2013 12:43:21 -0400
Subject: [concurrency-interest] Implicit parallelism
In-Reply-To: <CAKEw5+4rcZKiizq26fLaZbYb9NPtb6XWYfxZp5fEbNY9+iG9+A@mail.gmail.com>
References: <CAKEw5+4rcZKiizq26fLaZbYb9NPtb6XWYfxZp5fEbNY9+iG9+A@mail.gmail.com>
Message-ID: <52179129.6040201@briangoetz.com>

For what its worth, we've made the follwing choice in the JDK libraries: 
No Implicit Parallelism.  We can make parallelism unobtrusive, but we 
won't be making it implicit.

Of course, other library writers can make their own choices.

On 8/10/2013 2:45 PM, Gustav ?kesson wrote:
> Hi guys,
>
> My discussion here concerns implicit parallelism. Let's say we have the
> following setup:
>
> @Immutable
> public class Integer
> {
>    ...
> }
>
> @Immutable
> public class ImmutableArrayList
> {
>     ...
> }
>
> I'm looking for a way so that the parallelism would be introduced
> without hard-coding anything related to parallelism (i.e. not stating
> anything like .parallel or .par on the collection). Only thing needed
> would be something annotation-ish which tells the execution environment
> that this datastructure with elements is inherently thread-safe. Then
> the execution could determine if it would be beneficial to do so. For
> instance, for a structure with e.g. five elements, then it would not,
> but for millions of elements, it would most likely be. Perhaps it could
> even find some kind of sweet-spot of number of elements in which the
> parallel overhead exceeds the benefits.
>
> Let's say we wish to sum all the integers in an ImmutableArrayList
> (setup below), would it be possible for the compiler (javac, scalac or
> what have you) and JVM to conspire and decide "hey, let's run this in
> parallel since it doesn't violate application semantics and it can/will
> be faster"? Is there any research in this area in regards to the JVM?
>
>
> Best Regards,
>
> Gustav ?kesson
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

From studdugie at gmail.com  Fri Aug 23 13:34:21 2013
From: studdugie at gmail.com (Dane Foster)
Date: Fri, 23 Aug 2013 13:34:21 -0400
Subject: [concurrency-interest] Implicit parallelism
In-Reply-To: <52179129.6040201@briangoetz.com>
References: <CAKEw5+4rcZKiizq26fLaZbYb9NPtb6XWYfxZp5fEbNY9+iG9+A@mail.gmail.com>
	<52179129.6040201@briangoetz.com>
Message-ID: <CA+WxinJ2SFM+HH8z6ZWZV0h6QA6OFE2fv+jW9dWxiyP13uMoCQ@mail.gmail.com>

There are [research] languages like Fortress where parallelism is implicit
and where sequential behavior is the thing you must be explicit about.

Dane


On Fri, Aug 23, 2013 at 12:43 PM, Brian Goetz <brian at briangoetz.com> wrote:

> For what its worth, we've made the follwing choice in the JDK libraries:
> No Implicit Parallelism.  We can make parallelism unobtrusive, but we won't
> be making it implicit.
>
> Of course, other library writers can make their own choices.
>
> On 8/10/2013 2:45 PM, Gustav ?kesson wrote:
>
>> Hi guys,
>>
>> My discussion here concerns implicit parallelism. Let's say we have the
>> following setup:
>>
>> @Immutable
>> public class Integer
>> {
>>    ...
>> }
>>
>> @Immutable
>> public class ImmutableArrayList
>> {
>>     ...
>> }
>>
>> I'm looking for a way so that the parallelism would be introduced
>> without hard-coding anything related to parallelism (i.e. not stating
>> anything like .parallel or .par on the collection). Only thing needed
>> would be something annotation-ish which tells the execution environment
>> that this datastructure with elements is inherently thread-safe. Then
>> the execution could determine if it would be beneficial to do so. For
>> instance, for a structure with e.g. five elements, then it would not,
>> but for millions of elements, it would most likely be. Perhaps it could
>> even find some kind of sweet-spot of number of elements in which the
>> parallel overhead exceeds the benefits.
>>
>> Let's say we wish to sum all the integers in an ImmutableArrayList
>> (setup below), would it be possible for the compiler (javac, scalac or
>> what have you) and JVM to conspire and decide "hey, let's run this in
>> parallel since it doesn't violate application semantics and it can/will
>> be faster"? Is there any research in this area in regards to the JVM?
>>
>>
>> Best Regards,
>>
>> Gustav ?kesson
>>
>>
>> ______________________________**_________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
>> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>
>>  ______________________________**_________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.**oswego.edu <Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/**listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130823/0c7f351b/attachment.html>

From kasperni at gmail.com  Sat Aug 24 13:16:04 2013
From: kasperni at gmail.com (Kasper Nielsen)
Date: Sat, 24 Aug 2013 19:16:04 +0200
Subject: [concurrency-interest] CHMv8 and Bulk update operations.
Message-ID: <CAPs6152w_HkaANTHJy-t9JYaV00Te7+o6KRhSec2Tf6kwvTLjA@mail.gmail.com>

Hi,

I've been wondering why there aren't any bulk parallel update operations
such as:

void removeAll(BiPredicate<K,V>)
void computeAll(BiFunction<K,V,V>)

in CHMv8. (replaceAll is serial and does not allow removes)
I have been needing them a couple of times.
Technical issues?

Cheers
  Kasper
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130824/50292768/attachment.html>

From dl at cs.oswego.edu  Mon Aug 26 06:24:05 2013
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon, 26 Aug 2013 06:24:05 -0400
Subject: [concurrency-interest] CHMv8 and Bulk update operations.
In-Reply-To: <CAPs6152w_HkaANTHJy-t9JYaV00Te7+o6KRhSec2Tf6kwvTLjA@mail.gmail.com>
References: <CAPs6152w_HkaANTHJy-t9JYaV00Te7+o6KRhSec2Tf6kwvTLjA@mail.gmail.com>
Message-ID: <521B2CC5.30401@cs.oswego.edu>

On 08/24/2013 01:16 PM, Kasper Nielsen wrote:
> Hi,
>
> I've been wondering why there aren't any bulk parallel update operations such as:
>
> void removeAll(BiPredicate<K,V>)
> void computeAll(BiFunction<K,V,V>)
>
> in CHMv8. (replaceAll is serial and does not allow removes)
> I have been needing them a couple of times.
> Technical issues?
>

There is no java.util.Stream API for Pairs, Map.Entries, or any
other compound type. But CHM has a secondary parallel API
to handle these cases. For example, you could do
  chm.forEachKey(1, (k) -> pred(k) ? k : null, (k) -> remove(k));
This secondary API is sometimes a little awkward to use, but
gives you complete control.

-Doug







From kasperni at gmail.com  Mon Aug 26 07:22:16 2013
From: kasperni at gmail.com (Kasper Nielsen)
Date: Mon, 26 Aug 2013 13:22:16 +0200
Subject: [concurrency-interest] CHMv8 and Bulk update operations.
In-Reply-To: <521B2CC5.30401@cs.oswego.edu>
References: <CAPs6152w_HkaANTHJy-t9JYaV00Te7+o6KRhSec2Tf6kwvTLjA@mail.gmail.com>
	<521B2CC5.30401@cs.oswego.edu>
Message-ID: <CAPs6152kU+oU77C-0tTNpbOZ-=LoT0USvr0dp4mLVfdXB6fGMQ@mail.gmail.com>

I actually use the forEachKey (forEachEntry for removing) approach now.

But, for some reason, I assumed I would get some speedup with custom
operations for bulk updating.
Because you only need to take the lock of a bin one time instead of every
time you process a node in that bin.

But I can see by looking at the code that we only have more than more node
in a bin around 10 % of the time
(given random hash codes). So I guess the speedup would be negligible.

- Kasper

On Mon, Aug 26, 2013 at 12:24 PM, Doug Lea <dl at cs.oswego.edu> wrote:

> On 08/24/2013 01:16 PM, Kasper Nielsen wrote:
>
>> Hi,
>>
>> I've been wondering why there aren't any bulk parallel update operations
>> such as:
>>
>> void removeAll(BiPredicate<K,V>)
>> void computeAll(BiFunction<K,V,V>)
>>
>> in CHMv8. (replaceAll is serial and does not allow removes)
>> I have been needing them a couple of times.
>> Technical issues?
>>
>>
> There is no java.util.Stream API for Pairs, Map.Entries, or any
> other compound type. But CHM has a secondary parallel API
> to handle these cases. For example, you could do
>  chm.forEachKey(1, (k) -> pred(k) ? k : null, (k) -> remove(k));
> This secondary API is sometimes a little awkward to use, but
> gives you complete control.
>
> -Doug
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130826/d334c7a3/attachment.html>

From headius at headius.com  Wed Aug 28 17:36:18 2013
From: headius at headius.com (Charles Oliver Nutter)
Date: Wed, 28 Aug 2013 16:36:18 -0500
Subject: [concurrency-interest] Coroutine-like control transfer?
Message-ID: <CAE-f1xR-VDOfs9cGAaZY3xvZXudLtY1SHcXWhVx+8PXSivTyeQ@mail.gmail.com>

I've been struggling for years to find the right (i.e. fastest) way to
do a coroutine-like control transfer between two threads, and I
figured I'd try asking here again...

What I need to emulate is a true coroutine, where a thread transfers
execution (ip and stack) to a different piece of code, that code runs
for a while, and then execution transfers back. There's always exactly
one piece of code executing at a given time, since there's literally
only one thread bouncing between stacks.

On the JVM, where we don't have real coroutines, the best we can do is
to emulate this with threads (ignoring bytecode-weaving stack tricks
like Kilim for the moment). I do not believe any of the structures in
j.u.concurrent currently have this exact pattern in mind.

The patterns I've used to emulate this:

1. Explicit park/unpark.

The parent thread starts up the coroutine, which immediately parks
itself. Parent thread wakes up coroutine by unparking it and giving it
an initial value, at which point parent parks itself. Child runs for a
bit, then unparks parent, gives it a value, and parks itself.

This is logically closest to what I want, but the park/unpark
operations are too expensive for fast transfers. This mechanism ended
up being the slowest way when measuring raw transfer rate (i.e. very
little work being done between transfers)

2. SynchronousQueue

Instead of using explicit parking and unparking, the parent pushes a
value on child's sync queue, and then waits on its own sync queue.
Child signals parent by pushing a value on parent's sync queue and
then waits on its own.

This was about 3x faster than explicit park/unpark.

3. Exchanger

Substitute Exchanger for SynchronousQueue, where the "take" operation
just exchanges null and the "put" operation ignores the result. This
was the fastest...around 15x faster than explicit park/unpark and 5x
faster than SynchronousQueue

4. Just spin

Do nothing but spin on a null volatile field waiting for it to become
non-null. Another 3x faster than Exchanger.

...

It seems like I'm building a conceptually simple pattern on top of
structures not designed for it (or rather, designed for more complex
patterns). In SynchronousQueue's case, I have to do two put/take
operations for every transfer. With Exchanger, two exchanges. What I
really want is something like LockSupport.park/unpark that's more like
LockSupport.parkMeAndUnparkHimPassingValue(value), explicitly passing
control and a value to the other thread.

What am I missing? Do I need to hand-roll this?

- Charlie

From davidcholmes at aapt.net.au  Wed Aug 28 21:23:23 2013
From: davidcholmes at aapt.net.au (David Holmes)
Date: Thu, 29 Aug 2013 11:23:23 +1000
Subject: [concurrency-interest] Coroutine-like control transfer?
In-Reply-To: <CAE-f1xR-VDOfs9cGAaZY3xvZXudLtY1SHcXWhVx+8PXSivTyeQ@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCEECFKAAA.davidcholmes@aapt.net.au>

Hi Charles,

Can you clarify exactly what semantics you want for your "co-routines". All
your suggested implementations involve switching threads and then waiting
for the co-routine to complete - so why not just execute it directly in the
first place?

David

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Charles
> Oliver Nutter
> Sent: Thursday, 29 August 2013 7:36 AM
> To: concurrency-interest
> Subject: [concurrency-interest] Coroutine-like control transfer?
>
>
> I've been struggling for years to find the right (i.e. fastest) way to
> do a coroutine-like control transfer between two threads, and I
> figured I'd try asking here again...
>
> What I need to emulate is a true coroutine, where a thread transfers
> execution (ip and stack) to a different piece of code, that code runs
> for a while, and then execution transfers back. There's always exactly
> one piece of code executing at a given time, since there's literally
> only one thread bouncing between stacks.
>
> On the JVM, where we don't have real coroutines, the best we can do is
> to emulate this with threads (ignoring bytecode-weaving stack tricks
> like Kilim for the moment). I do not believe any of the structures in
> j.u.concurrent currently have this exact pattern in mind.
>
> The patterns I've used to emulate this:
>
> 1. Explicit park/unpark.
>
> The parent thread starts up the coroutine, which immediately parks
> itself. Parent thread wakes up coroutine by unparking it and giving it
> an initial value, at which point parent parks itself. Child runs for a
> bit, then unparks parent, gives it a value, and parks itself.
>
> This is logically closest to what I want, but the park/unpark
> operations are too expensive for fast transfers. This mechanism ended
> up being the slowest way when measuring raw transfer rate (i.e. very
> little work being done between transfers)
>
> 2. SynchronousQueue
>
> Instead of using explicit parking and unparking, the parent pushes a
> value on child's sync queue, and then waits on its own sync queue.
> Child signals parent by pushing a value on parent's sync queue and
> then waits on its own.
>
> This was about 3x faster than explicit park/unpark.
>
> 3. Exchanger
>
> Substitute Exchanger for SynchronousQueue, where the "take" operation
> just exchanges null and the "put" operation ignores the result. This
> was the fastest...around 15x faster than explicit park/unpark and 5x
> faster than SynchronousQueue
>
> 4. Just spin
>
> Do nothing but spin on a null volatile field waiting for it to become
> non-null. Another 3x faster than Exchanger.
>
> ...
>
> It seems like I'm building a conceptually simple pattern on top of
> structures not designed for it (or rather, designed for more complex
> patterns). In SynchronousQueue's case, I have to do two put/take
> operations for every transfer. With Exchanger, two exchanges. What I
> really want is something like LockSupport.park/unpark that's more like
> LockSupport.parkMeAndUnparkHimPassingValue(value), explicitly passing
> control and a value to the other thread.
>
> What am I missing? Do I need to hand-roll this?
>
> - Charlie
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> -----
> No virus found in this message.
> Checked by AVG - www.avg.com
> Version: 2013.0.3392 / Virus Database: 3211/6614 - Release Date: 08/27/13
>


From howard.lovatt at gmail.com  Wed Aug 28 21:57:56 2013
From: howard.lovatt at gmail.com (Howard Lovatt)
Date: Thu, 29 Aug 2013 11:57:56 +1000
Subject: [concurrency-interest] Coroutine-like control transfer?
In-Reply-To: <CAE-f1xR-VDOfs9cGAaZY3xvZXudLtY1SHcXWhVx+8PXSivTyeQ@mail.gmail.com>
References: <CAE-f1xR-VDOfs9cGAaZY3xvZXudLtY1SHcXWhVx+8PXSivTyeQ@mail.gmail.com>
Message-ID: <CACR_FB4XQhk7vwvB4-gjZAKAmrh=-BAx507s4HYyCsaXVDp1vA@mail.gmail.com>

Hi,

I was interested in a similar problem and wrote the microbenchmark below to
investigate. This problem suspends a thread until both its arguments are
ready and then the thread runs and produces a result. I found that double
check locking was the quickest, class DoubleCheck__Sum below. Code may be
helpful.

  -- Howard.

====================================================================================

package cyclicbarriertest;

import java.util.concurrent.ArrayBlockingQueue;
import java.util.concurrent.BlockingQueue;
import java.util.concurrent.Callable;
import java.util.concurrent.CyclicBarrier;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import java.util.concurrent.LinkedBlockingDeque;
import java.util.concurrent.LinkedBlockingQueue;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicReference;
import java.util.concurrent.locks.Lock;
import java.util.concurrent.locks.ReentrantLock;

import static java.lang.System.*;

/**
 * Timing test for cyclic barrier implementations.
 *
 * @author  Howard Lovatt
 */
public class CyclicBarrierTest {
  private static final int seconds = 20; // Might need to tweak this value
to allow enough time for slow versions to complete
  private static final int loops = 1000 * 1000;

  public static void main( final String... args ) throws Exception {
    test( new CyclicBarrierSum() );
    test( new Volatile_____Sum() );
    test( new _____________Sum() );
    test( new Synchronized_Sum() );
    test( new SynchYield___Sum() );
    test( new DoubleCheck__Sum() );
    test( new DbleChYield__Sum() );
    test( new Lock_________Sum() );
    test( new LockDoubleCh_Sum() );
    test( new AtomicRefLongSum() );
    test( new LockAtomicRefSum() );
    test( new BlockingQueueSum() );
    test( new BlockingDequeSum() );
    test( new ArrayBlockingSum() );
  }

  private static void test( final Sum method ) throws Exception {
    gc();
    gc();
    final Callable<Void> setA1 = new Callable<Void>() {
      @Override public Void call() throws Exception {
        for ( int l = 0; l < loops; l++ ) { method.setA1( l ); }
        return null;
      }
    };
    final Callable<Void> setA2 = new Callable<Void>() {
      @Override public Void call() throws Exception {
        for ( int l = 0; l < loops; l++ ) { method.setA2( l ); }
        return null;
      }
    };
    final ExecutorService pool = Executors.newCachedThreadPool();
    final long start = nanoTime();
    pool.submit( setA1 );
    pool.submit( setA2 );
    pool.shutdown();
    final boolean ok = pool.awaitTermination( seconds, TimeUnit.SECONDS );
    final long time = ( nanoTime() - start ) / loops;
    pool.shutdownNow();
    out.println( method + ": result = " + method.getSum() + ", av. time = "
+ ( time / 1000.0 ) +
                     " ms, " + ( ok ? "and terminated OK" : "but failed to
terminate" ) );
  }
}


abstract class Sum {
  protected long sum = 0;

  public static void checkForInterrupt() throws InterruptedException {
    if ( Thread.interrupted() ) { throw new InterruptedException(); }
  }

  public abstract void setA1( long a1 ) throws Exception;

  public abstract void setA2( long a2 ) throws Exception;

  public long getSum() { return sum; }

  @Override public String toString() { return getClass().getSimpleName(); }
}


final class CyclicBarrierSum extends Sum {
  private long a1;
  private long a2;
  private final Runnable summer =
    new Runnable() {
      @Override public void run() { sum += a1 + a2; }
    };
  private final CyclicBarrier sumBarrier = new CyclicBarrier( 2, summer );

  @Override public void setA1( final long a1Arg ) throws Exception {
    a1 = a1Arg;
    sumBarrier.await();
  }

  @Override public void setA2( final long a2Arg ) throws Exception {
    a2 = a2Arg;
    sumBarrier.await();
  }
}


final class Volatile_____Sum extends Sum {
  private volatile Long a1 = null;
  private volatile Long a2 = null;

  @Override public void setA1( final long a1Arg ) throws Exception {
    for ( ;; ) {
      if ( a1 == null ) {
        a1 = a1Arg;
        checkFinished();
        return;
      }
      checkForInterrupt();
    }
  }

  @Override public void setA2( final long a2Arg ) throws Exception {
    for ( ;; ) {
      if ( a2 == null ) {
        a2 = a2Arg;
        checkFinished();
        return;
      }
      checkForInterrupt();
    }
  }

  private synchronized void checkFinished() {
    if ( ( a1 == null ) || ( a2 == null ) ) { return; }
    sum += a1 + a2;
    a1 = a2 = null;
  }
}


final class _____________Sum extends Sum { // Could fail because a set
could be missed, but works on OSX
  private Long a1 = null;
  private Long a2 = null;

  @Override public void setA1( final long a1Arg ) throws Exception {
    for ( ;; ) {
      if ( a1 == null ) {
        a1 = a1Arg;
        checkFinished();
        return;
      }
      checkForInterrupt();
    }
  }

  @Override public void setA2( final long a2Arg ) throws Exception {
    for ( ;; ) {
      if ( a2 == null ) {
        a2 = a2Arg;
        checkFinished();
        return;
      }
      checkForInterrupt();
    }
  }

  private synchronized void checkFinished() {
    if ( ( a1 == null ) || ( a2 == null ) ) { return; }
    sum += a1 + a2;
    a1 = a2 = null;
  }
}


abstract class AbstractSynchronized_Sum extends Sum {
  protected Long a1 = null;
  protected Long a2 = null;

  public synchronized boolean maybeSetA1( final long a1Arg ) {
    if ( a1 == null ) {
      a1 = a1Arg;
      checkFinished();
      return true;
    }
    return false;
  }

  public synchronized boolean maybeSetA2( final long a2Arg ) {
    if ( a2 == null ) {
      a2 = a2Arg;
      checkFinished();
      return true;
    }
    return false;
  }

  private void checkFinished() {
    if ( ( a1 == null ) || ( a2 == null ) ) { return; }
    sum += a1 + a2;
    a1 = a2 = null;
  }
}


final class Synchronized_Sum extends AbstractSynchronized_Sum {
  @Override public void setA1( final long a1Arg ) throws Exception {
    for ( ;; ) {
      if ( maybeSetA1( a1Arg ) ) { return; }
      checkForInterrupt();
    }
  }

  @Override public void setA2( final long a2Arg ) throws Exception {
    for ( ;; ) {
      if ( maybeSetA2( a2Arg ) ) { return; }
      checkForInterrupt();
    }
  }
}


final class SynchYield___Sum extends AbstractSynchronized_Sum {
  @Override public void setA1( final long a1Arg ) throws Exception {
    for ( ;; ) {
      if ( maybeSetA1( a1Arg ) ) { return; }
      checkForInterrupt();
      Thread.yield();
    }
  }

  @Override public void setA2( final long a2Arg ) throws Exception {
    for ( ;; ) {
      if ( maybeSetA2( a2Arg ) ) { return; }
      checkForInterrupt();
      Thread.yield();
    }
  }
}


final class DoubleCheck__Sum extends AbstractSynchronized_Sum {
  @Override public void setA1( final long a1Arg ) throws Exception {
    for ( ;; ) {
      if ( a1 == null ) {
        if ( maybeSetA1( a1Arg ) ) { return; }
      }
      checkForInterrupt();
    }
  }

  @Override public void setA2( final long a2Arg ) throws Exception {
    for ( ;; ) {
      if ( a2 == null ) {
        if ( maybeSetA2( a2Arg ) ) { return; }
      }
      checkForInterrupt();
    }
  }
}

final class DbleChYield__Sum extends AbstractSynchronized_Sum {
  @Override public void setA1( final long a1Arg ) throws Exception {
    for ( ;; ) {
      if ( a1 == null ) {
        if ( maybeSetA1( a1Arg ) ) { return; }
      }
      checkForInterrupt();
      Thread.yield();
    }
  }

  @Override public void setA2( final long a2Arg ) throws Exception {
    for ( ;; ) {
      if ( a2 == null ) {
        if ( maybeSetA2( a2Arg ) ) { return; }
      }
      checkForInterrupt();
      Thread.yield();
    }
  }
}


abstract class AbstractLock_________Sum extends Sum {
  protected Long a1 = null;
  protected Long a2 = null;
  private final Lock lock = new ReentrantLock();

  public boolean maybeSetA1( final long a1Arg ) {
    lock.lock();
    try {
      if ( a1 == null ) {
        a1 = a1Arg;
        checkFinished();
        return true;
      }
      return false;
    } finally {
      lock.unlock();
    }
  }

  public boolean maybeSetA2( final long a2Arg ) {
    lock.lock();
    try {
      if ( a2 == null ) {
        a2 = a2Arg;
        checkFinished();
        return true;
      }
      return false;
    } finally {
      lock.unlock();
    }
  }

  private void checkFinished() {
    if ( ( a1 == null ) || ( a2 == null ) ) { return; }
    sum += a1 + a2;
    a1 = a2 = null;
  }
}


final class Lock_________Sum extends AbstractLock_________Sum {
  @Override public void setA1( final long a1Arg ) throws Exception {
    for ( ;; ) {
      if ( maybeSetA1( a1Arg ) ) { return; }
      checkForInterrupt();
    }
  }

  @Override public void setA2( final long a2Arg ) throws Exception {
    for ( ;; ) {
      if ( maybeSetA2( a2Arg ) ) { return; }
      checkForInterrupt();
    }
  }
}


final class LockDoubleCh_Sum extends AbstractLock_________Sum {
  @Override public void setA1( final long a1Arg ) throws Exception {
    for ( ;; ) {
      if ( a1 == null ) {
        if ( maybeSetA1( a1Arg ) ) { return; }
      }
      checkForInterrupt();
    }
  }

  @Override public void setA2( final long a2Arg ) throws Exception {
    for ( ;; ) {
      if ( a2 == null ) {
        if ( maybeSetA2( a2Arg ) ) { return; }
      }
      checkForInterrupt();
    }
  }
}


final class AtomicRefLongSum extends Sum {
  private final AtomicReference<Long> a1 = new AtomicReference<>( null );
  private final AtomicReference<Long> a2 = new AtomicReference<>( null );

  @Override public void setA1( final long a1Arg ) throws Exception {
    while ( !a1.compareAndSet( null, a1Arg ) ) {
      checkForInterrupt();
    }
    checkFinished();
  }

  @Override public void setA2( final long a2Arg ) throws Exception {
    while ( !a2.compareAndSet( null, a2Arg ) ) {
      checkForInterrupt();
    }
    checkFinished();
  }

  private synchronized void checkFinished() {
    if ( ( a1.get() == null ) || ( a2.get() == null ) ) { return; }
    final long a1T = a1.getAndSet( null );
    final long a2T = a2.getAndSet( null );
    sum += a1T + a2T;
  }
}


final class LockAtomicRefSum extends Sum {
  private final AtomicReference<Long> a1 = new AtomicReference<>( null );
  private final AtomicReference<Long> a2 = new AtomicReference<>( null );
  private final Lock lock = new ReentrantLock();

  @Override public void setA1( final long a1Arg ) throws Exception {
    while ( !a1.compareAndSet( null, a1Arg ) ) {
      checkForInterrupt();
    }
    checkFinished();
  }

  @Override public void setA2( final long a2Arg ) throws Exception {
    while ( !a2.compareAndSet( null, a2Arg ) ) {
      checkForInterrupt();
    }
    checkFinished();
  }

  private void checkFinished() {
    lock.lock();
    try {
      if ( ( a1.get() == null ) || ( a2.get() == null ) ) { return; }
      final long a1T = a1.getAndSet( null );
      final long a2T = a2.getAndSet( null );
      sum += a1T + a2T;
    } finally {
      lock.unlock();
    }
  }
}


abstract class AbstractBlockingQueueSum extends Sum {
  protected final BlockingQueue<Long> a1;
  protected final BlockingQueue<Long> a2;

  protected AbstractBlockingQueueSum( final BlockingQueue<Long> a1Arg,
                                      final BlockingQueue<Long> a2Arg ) {
    a1 = a1Arg;
    a2 = a2Arg;
  }

  @Override public void setA1( final long a1Arg ) throws Exception {
    a1.put( a1Arg );
    checkFinished();
  }

  @Override public void setA2( final long a2Arg ) throws Exception {
    a2.put( a2Arg );
    checkFinished();
  }

  private synchronized void checkFinished() throws Exception {
    if ( ( a1.peek() == null ) || ( a2.peek() == null ) ) { return; }
    final long a1T = a1.take();
    final long a2T = a2.take();
    sum += a1T + a2T;
  }
}


final class BlockingQueueSum extends AbstractBlockingQueueSum {
  public BlockingQueueSum() {
    super( new LinkedBlockingQueue<Long>(), new LinkedBlockingQueue<Long>()
);
  }
}


final class BlockingDequeSum extends AbstractBlockingQueueSum {
  public BlockingDequeSum() {
    super( new LinkedBlockingDeque<Long>(), new LinkedBlockingDeque<Long>()
);
  }
}


final class ArrayBlockingSum extends AbstractBlockingQueueSum {
  public ArrayBlockingSum() {
    super( new ArrayBlockingQueue<Long>( 10 ), new
ArrayBlockingQueue<Long>( 10 ) );
  }
}


On 29 August 2013 07:36, Charles Oliver Nutter <headius at headius.com> wrote:

> I've been struggling for years to find the right (i.e. fastest) way to
> do a coroutine-like control transfer between two threads, and I
> figured I'd try asking here again...
>
> What I need to emulate is a true coroutine, where a thread transfers
> execution (ip and stack) to a different piece of code, that code runs
> for a while, and then execution transfers back. There's always exactly
> one piece of code executing at a given time, since there's literally
> only one thread bouncing between stacks.
>
> On the JVM, where we don't have real coroutines, the best we can do is
> to emulate this with threads (ignoring bytecode-weaving stack tricks
> like Kilim for the moment). I do not believe any of the structures in
> j.u.concurrent currently have this exact pattern in mind.
>
> The patterns I've used to emulate this:
>
> 1. Explicit park/unpark.
>
> The parent thread starts up the coroutine, which immediately parks
> itself. Parent thread wakes up coroutine by unparking it and giving it
> an initial value, at which point parent parks itself. Child runs for a
> bit, then unparks parent, gives it a value, and parks itself.
>
> This is logically closest to what I want, but the park/unpark
> operations are too expensive for fast transfers. This mechanism ended
> up being the slowest way when measuring raw transfer rate (i.e. very
> little work being done between transfers)
>
> 2. SynchronousQueue
>
> Instead of using explicit parking and unparking, the parent pushes a
> value on child's sync queue, and then waits on its own sync queue.
> Child signals parent by pushing a value on parent's sync queue and
> then waits on its own.
>
> This was about 3x faster than explicit park/unpark.
>
> 3. Exchanger
>
> Substitute Exchanger for SynchronousQueue, where the "take" operation
> just exchanges null and the "put" operation ignores the result. This
> was the fastest...around 15x faster than explicit park/unpark and 5x
> faster than SynchronousQueue
>
> 4. Just spin
>
> Do nothing but spin on a null volatile field waiting for it to become
> non-null. Another 3x faster than Exchanger.
>
> ...
>
> It seems like I'm building a conceptually simple pattern on top of
> structures not designed for it (or rather, designed for more complex
> patterns). In SynchronousQueue's case, I have to do two put/take
> operations for every transfer. With Exchanger, two exchanges. What I
> really want is something like LockSupport.park/unpark that's more like
> LockSupport.parkMeAndUnparkHimPassingValue(value), explicitly passing
> control and a value to the other thread.
>
> What am I missing? Do I need to hand-roll this?
>
> - Charlie
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



-- 
  -- Howard.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130829/c7267c85/attachment-0001.html>

From alexei.kaigorodov at gmail.com  Thu Aug 29 08:09:33 2013
From: alexei.kaigorodov at gmail.com (Alexei Kaigorodov)
Date: Thu, 29 Aug 2013 12:09:33 +0000 (UTC)
Subject: [concurrency-interest] Coordinated Runnables
Message-ID: <loom.20130829T134152-308@post.gmane.org>

Package java util.concurrent provides many facilities to synchronize
threads, but none to coordinate tasks running on a thread pool. Generally,
using thread synchronization in tasks is undesirable, as synchronization
blocks pooled threads and may lead to thread starvation or, if unlimited
pool is used, leads to uncontrolled growth of the number of threads and so
eliminating the advantages of thread pool. However, non-blocking
synchronization facilities are essential for developing complex asynchronous
programs.

An illustrative example of how coordinated runnables can work and what
support is required can be found at
https://docs.google.com/document/d/1a-ycXfvVtDExDRBLKCukSjDQHlyidrVE7yV27jlEmrs
.

I also have and implementation of the proposed execution model at
https://github.com/rfqu/df4j.

I believe, for the sake of completeness, standard java libraries must
include non-blocking sychronization facilities. Dot Net platform has Task
Parallel Library (TPL), Groovy has GPars, and java lags behind.



From zhong.j.yu at gmail.com  Thu Aug 29 10:26:08 2013
From: zhong.j.yu at gmail.com (Zhong Yu)
Date: Thu, 29 Aug 2013 09:26:08 -0500
Subject: [concurrency-interest] Coordinated Runnables
In-Reply-To: <loom.20130829T134152-308@post.gmane.org>
References: <loom.20130829T134152-308@post.gmane.org>
Message-ID: <CACuKZqESAPOuhU9Dw8uSDG=Dw8wR=HqtRGp1gQQdhZQsTzrmxA@mail.gmail.com>

Hi Alexei, if in your model one place can be connected to only one
transition, can't we model one transition with its input places as one
actor?

Zhong Yu

On Thu, Aug 29, 2013 at 7:09 AM, Alexei Kaigorodov
<alexei.kaigorodov at gmail.com> wrote:
> Package java util.concurrent provides many facilities to synchronize
> threads, but none to coordinate tasks running on a thread pool. Generally,
> using thread synchronization in tasks is undesirable, as synchronization
> blocks pooled threads and may lead to thread starvation or, if unlimited
> pool is used, leads to uncontrolled growth of the number of threads and so
> eliminating the advantages of thread pool. However, non-blocking
> synchronization facilities are essential for developing complex asynchronous
> programs.
>
> An illustrative example of how coordinated runnables can work and what
> support is required can be found at
> https://docs.google.com/document/d/1a-ycXfvVtDExDRBLKCukSjDQHlyidrVE7yV27jlEmrs
> .
>
> I also have and implementation of the proposed execution model at
> https://github.com/rfqu/df4j.
>
> I believe, for the sake of completeness, standard java libraries must
> include non-blocking sychronization facilities. Dot Net platform has Task
> Parallel Library (TPL), Groovy has GPars, and java lags behind.
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

From zhong.j.yu at gmail.com  Thu Aug 29 10:39:36 2013
From: zhong.j.yu at gmail.com (Zhong Yu)
Date: Thu, 29 Aug 2013 09:39:36 -0500
Subject: [concurrency-interest] Coroutine-like control transfer?
In-Reply-To: <CAE-f1xR-VDOfs9cGAaZY3xvZXudLtY1SHcXWhVx+8PXSivTyeQ@mail.gmail.com>
References: <CAE-f1xR-VDOfs9cGAaZY3xvZXudLtY1SHcXWhVx+8PXSivTyeQ@mail.gmail.com>
Message-ID: <CACuKZqE8QGudAzJZ8V71J-oLKgYpcag2znEiYx+n0jDvUK4OdQ@mail.gmail.com>

On Wed, Aug 28, 2013 at 4:36 PM, Charles Oliver Nutter
<headius at headius.com> wrote:
> I've been struggling for years to find the right (i.e. fastest) way to
> do a coroutine-like control transfer between two threads, and I
> figured I'd try asking here again...
>
> What I need to emulate is a true coroutine, where a thread transfers
> execution (ip and stack) to a different piece of code, that code runs
> for a while, and then execution transfers back. There's always exactly
> one piece of code executing at a given time, since there's literally
> only one thread bouncing between stacks.
>
> On the JVM, where we don't have real coroutines, the best we can do is
> to emulate this with threads (ignoring bytecode-weaving stack tricks
> like Kilim for the moment). I do not believe any of the structures in
> j.u.concurrent currently have this exact pattern in mind.
>
> The patterns I've used to emulate this:
>
> 1. Explicit park/unpark.
>
> The parent thread starts up the coroutine, which immediately parks
> itself. Parent thread wakes up coroutine by unparking it and giving it
> an initial value, at which point parent parks itself. Child runs for a
> bit, then unparks parent, gives it a value, and parks itself.
>
> This is logically closest to what I want, but the park/unpark
> operations are too expensive for fast transfers. This mechanism ended
> up being the slowest way when measuring raw transfer rate (i.e. very
> little work being done between transfers)
>
> 2. SynchronousQueue
>
> Instead of using explicit parking and unparking, the parent pushes a
> value on child's sync queue, and then waits on its own sync queue.
> Child signals parent by pushing a value on parent's sync queue and
> then waits on its own.
>
> This was about 3x faster than explicit park/unpark.
>
> 3. Exchanger
>
> Substitute Exchanger for SynchronousQueue, where the "take" operation
> just exchanges null and the "put" operation ignores the result. This
> was the fastest...around 15x faster than explicit park/unpark and 5x
> faster than SynchronousQueue
>
> 4. Just spin
>
> Do nothing but spin on a null volatile field waiting for it to become
> non-null. Another 3x faster than Exchanger.
>
> ...
>
> It seems like I'm building a conceptually simple pattern on top of
> structures not designed for it (or rather, designed for more complex
> patterns). In SynchronousQueue's case, I have to do two put/take
> operations for every transfer. With Exchanger, two exchanges. What I
> really want is something like LockSupport.park/unpark that's more like
> LockSupport.parkMeAndUnparkHimPassingValue(value), explicitly passing
> control and a value to the other thread.

It seems that the more spin there is, the faster it is; or, the sooner
you do park(), the slower it is. If you do full spin, 50% CPU is
wasted, the throughput would be half of the true coroutine solution -
which is probably not too bad.

>
> What am I missing? Do I need to hand-roll this?
>
> - Charlie
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

From alexei.kaigorodov at gmail.com  Thu Aug 29 10:42:37 2013
From: alexei.kaigorodov at gmail.com (Alexei Kaigorodov)
Date: Thu, 29 Aug 2013 21:42:37 +0700
Subject: [concurrency-interest] Coordinated Runnables
In-Reply-To: <CACuKZqESAPOuhU9Dw8uSDG=Dw8wR=HqtRGp1gQQdhZQsTzrmxA@mail.gmail.com>
References: <loom.20130829T134152-308@post.gmane.org>
	<CACuKZqESAPOuhU9Dw8uSDG=Dw8wR=HqtRGp1gQQdhZQsTzrmxA@mail.gmail.com>
Message-ID: <CALCS1ZVfVMaqDO-EVF0b0jqiesmC+6B5_oFgDL6gK9qR2MJUQA@mail.gmail.com>

Yes it is an actor, in its initial meaning. But unfortunately, the term
"actor" today mainly means restricted actor of Scala or Akka style (with
single place of type queue), and I am in doubt if "actor" would not cause
misunderstanding.


On Thu, Aug 29, 2013 at 9:26 PM, Zhong Yu <zhong.j.yu at gmail.com> wrote:

> Hi Alexei, if in your model one place can be connected to only one
> transition, can't we model one transition with its input places as one
> actor?
>
> Zhong Yu
>
> On Thu, Aug 29, 2013 at 7:09 AM, Alexei Kaigorodov
> <alexei.kaigorodov at gmail.com> wrote:
> > Package java util.concurrent provides many facilities to synchronize
> > threads, but none to coordinate tasks running on a thread pool.
> Generally,
> > using thread synchronization in tasks is undesirable, as synchronization
> > blocks pooled threads and may lead to thread starvation or, if unlimited
> > pool is used, leads to uncontrolled growth of the number of threads and
> so
> > eliminating the advantages of thread pool. However, non-blocking
> > synchronization facilities are essential for developing complex
> asynchronous
> > programs.
> >
> > An illustrative example of how coordinated runnables can work and what
> > support is required can be found at
> >
> https://docs.google.com/document/d/1a-ycXfvVtDExDRBLKCukSjDQHlyidrVE7yV27jlEmrs
> > .
> >
> > I also have and implementation of the proposed execution model at
> > https://github.com/rfqu/df4j.
> >
> > I believe, for the sake of completeness, standard java libraries must
> > include non-blocking sychronization facilities. Dot Net platform has Task
> > Parallel Library (TPL), Groovy has GPars, and java lags behind.
> >
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130829/3a9c5202/attachment.html>

From viktor.klang at gmail.com  Thu Aug 29 12:04:38 2013
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Thu, 29 Aug 2013 09:04:38 -0700
Subject: [concurrency-interest] Coordinated Runnables
In-Reply-To: <CALCS1ZVfVMaqDO-EVF0b0jqiesmC+6B5_oFgDL6gK9qR2MJUQA@mail.gmail.com>
References: <loom.20130829T134152-308@post.gmane.org>
	<CACuKZqESAPOuhU9Dw8uSDG=Dw8wR=HqtRGp1gQQdhZQsTzrmxA@mail.gmail.com>
	<CALCS1ZVfVMaqDO-EVF0b0jqiesmC+6B5_oFgDL6gK9qR2MJUQA@mail.gmail.com>
Message-ID: <CANPzfU86zVgaE8hL5j+Jz1VWOJrzALv5Suy_vygvjw=Z7VoVQg@mail.gmail.com>

Single place of type queue <-- wdym?


On Thu, Aug 29, 2013 at 7:42 AM, Alexei Kaigorodov <
alexei.kaigorodov at gmail.com> wrote:

> Yes it is an actor, in its initial meaning. But unfortunately, the term
> "actor" today mainly means restricted actor of Scala or Akka style (with
> single place of type queue), and I am in doubt if "actor" would not cause
> misunderstanding.
>
>
> On Thu, Aug 29, 2013 at 9:26 PM, Zhong Yu <zhong.j.yu at gmail.com> wrote:
>
>> Hi Alexei, if in your model one place can be connected to only one
>> transition, can't we model one transition with its input places as one
>> actor?
>>
>> Zhong Yu
>>
>> On Thu, Aug 29, 2013 at 7:09 AM, Alexei Kaigorodov
>> <alexei.kaigorodov at gmail.com> wrote:
>> > Package java util.concurrent provides many facilities to synchronize
>> > threads, but none to coordinate tasks running on a thread pool.
>> Generally,
>> > using thread synchronization in tasks is undesirable, as synchronization
>> > blocks pooled threads and may lead to thread starvation or, if unlimited
>> > pool is used, leads to uncontrolled growth of the number of threads and
>> so
>> > eliminating the advantages of thread pool. However, non-blocking
>> > synchronization facilities are essential for developing complex
>> asynchronous
>> > programs.
>> >
>> > An illustrative example of how coordinated runnables can work and what
>> > support is required can be found at
>> >
>> https://docs.google.com/document/d/1a-ycXfvVtDExDRBLKCukSjDQHlyidrVE7yV27jlEmrs
>> > .
>> >
>> > I also have and implementation of the proposed execution model at
>> > https://github.com/rfqu/df4j.
>> >
>> > I believe, for the sake of completeness, standard java libraries must
>> > include non-blocking sychronization facilities. Dot Net platform has
>> Task
>> > Parallel Library (TPL), Groovy has GPars, and java lags behind.
>> >
>> >
>> > _______________________________________________
>> > Concurrency-interest mailing list
>> > Concurrency-interest at cs.oswego.edu
>> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


-- 
*Viktor Klang*
*Director of Engineering*
Typesafe <http://www.typesafe.com/>

Twitter: @viktorklang
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130829/85d709ba/attachment.html>

From nathan.reynolds at oracle.com  Thu Aug 29 12:09:02 2013
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Thu, 29 Aug 2013 09:09:02 -0700
Subject: [concurrency-interest] Coroutine-like control transfer?
In-Reply-To: <CACuKZqE8QGudAzJZ8V71J-oLKgYpcag2znEiYx+n0jDvUK4OdQ@mail.gmail.com>
References: <CAE-f1xR-VDOfs9cGAaZY3xvZXudLtY1SHcXWhVx+8PXSivTyeQ@mail.gmail.com>
	<CACuKZqE8QGudAzJZ8V71J-oLKgYpcag2znEiYx+n0jDvUK4OdQ@mail.gmail.com>
Message-ID: <521F721E.80500@oracle.com>

 > If you do full spin, 50% CPU is wasted, the throughput would be half 
of the true coroutine solution

The OS thread scheduler will let a thread run for its entire time slice 
unless a higher priority thread needs to get on the logical core.  Most 
of the time though, threads will block before their time slice runs out.

However, when spin waiting, calling yield will ensure that threads can 
run that want to do useful work.  This is because when a thread calls 
yield, it will be put at the end of the run queue. Other threads that 
want processor time will be allowed to run for a full time slice.  So, 
throughput won't suffer.

But what about the cost of context switches?

The cost is nothing as long as the # of runnable threads is ? the # of 
logical cores.  This is because when the thread calls yield, it is put 
at the end of an empty run queue and hence it doesn't context switch.  
In fact, even if a context switch cost a lot of time, it wouldn't matter 
because all of the runnable threads are running.  One caveat is that a 
spinning thread is consuming the physical core's compute resources and 
so it will slow down the other thread that is running on the same 
physical core.  On x86, a pause instruction is a good idea since the 
thread doesn't consume compute resources... unless you are running in a 
virtual machine.

If the # of runnable threads is > the # of logical cores, then threads 
will context switch.  However, the cost of switching the logical core's 
state from one thread to another is too small to be of any concern.  The 
real cost of context switches comes from having to reload the cache.  In 
the case of spin-yield loops, very little of the cache has to be reload 
since the spinning thread probably isn't accessing very many cache lines.

-Nathan

On 8/29/2013 7:39 AM, Zhong Yu wrote:
> On Wed, Aug 28, 2013 at 4:36 PM, Charles Oliver Nutter
> <headius at headius.com> wrote:
>> I've been struggling for years to find the right (i.e. fastest) way to
>> do a coroutine-like control transfer between two threads, and I
>> figured I'd try asking here again...
>>
>> What I need to emulate is a true coroutine, where a thread transfers
>> execution (ip and stack) to a different piece of code, that code runs
>> for a while, and then execution transfers back. There's always exactly
>> one piece of code executing at a given time, since there's literally
>> only one thread bouncing between stacks.
>>
>> On the JVM, where we don't have real coroutines, the best we can do is
>> to emulate this with threads (ignoring bytecode-weaving stack tricks
>> like Kilim for the moment). I do not believe any of the structures in
>> j.u.concurrent currently have this exact pattern in mind.
>>
>> The patterns I've used to emulate this:
>>
>> 1. Explicit park/unpark.
>>
>> The parent thread starts up the coroutine, which immediately parks
>> itself. Parent thread wakes up coroutine by unparking it and giving it
>> an initial value, at which point parent parks itself. Child runs for a
>> bit, then unparks parent, gives it a value, and parks itself.
>>
>> This is logically closest to what I want, but the park/unpark
>> operations are too expensive for fast transfers. This mechanism ended
>> up being the slowest way when measuring raw transfer rate (i.e. very
>> little work being done between transfers)
>>
>> 2. SynchronousQueue
>>
>> Instead of using explicit parking and unparking, the parent pushes a
>> value on child's sync queue, and then waits on its own sync queue.
>> Child signals parent by pushing a value on parent's sync queue and
>> then waits on its own.
>>
>> This was about 3x faster than explicit park/unpark.
>>
>> 3. Exchanger
>>
>> Substitute Exchanger for SynchronousQueue, where the "take" operation
>> just exchanges null and the "put" operation ignores the result. This
>> was the fastest...around 15x faster than explicit park/unpark and 5x
>> faster than SynchronousQueue
>>
>> 4. Just spin
>>
>> Do nothing but spin on a null volatile field waiting for it to become
>> non-null. Another 3x faster than Exchanger.
>>
>> ...
>>
>> It seems like I'm building a conceptually simple pattern on top of
>> structures not designed for it (or rather, designed for more complex
>> patterns). In SynchronousQueue's case, I have to do two put/take
>> operations for every transfer. With Exchanger, two exchanges. What I
>> really want is something like LockSupport.park/unpark that's more like
>> LockSupport.parkMeAndUnparkHimPassingValue(value), explicitly passing
>> control and a value to the other thread.
> It seems that the more spin there is, the faster it is; or, the sooner
> you do park(), the slower it is. If you do full spin, 50% CPU is
> wasted, the throughput would be half of the true coroutine solution -
> which is probably not too bad.
>
>> What am I missing? Do I need to hand-roll this?
>>
>> - Charlie
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130829/0e3e631e/attachment.html>

From alexei.kaigorodov at gmail.com  Thu Aug 29 12:24:33 2013
From: alexei.kaigorodov at gmail.com (Alexei Kaigorodov)
Date: Thu, 29 Aug 2013 23:24:33 +0700
Subject: [concurrency-interest] Coordinated Runnables
In-Reply-To: <CANPzfU86zVgaE8hL5j+Jz1VWOJrzALv5Suy_vygvjw=Z7VoVQg@mail.gmail.com>
References: <loom.20130829T134152-308@post.gmane.org>
	<CACuKZqESAPOuhU9Dw8uSDG=Dw8wR=HqtRGp1gQQdhZQsTzrmxA@mail.gmail.com>
	<CALCS1ZVfVMaqDO-EVF0b0jqiesmC+6B5_oFgDL6gK9qR2MJUQA@mail.gmail.com>
	<CANPzfU86zVgaE8hL5j+Jz1VWOJrzALv5Suy_vygvjw=Z7VoVQg@mail.gmail.com>
Message-ID: <CALCS1ZV2sneM8Bd=Wa_DHPq8A+=MVJD=Ojh8JHxUgnTOvqUTQQ@mail.gmail.com>

Place is a term in Petri Net model - place where tokens wait to be
processed. "Of type queue" means that tokens are stored in a queue. Another
possible type, for "colorless" tokens, is just a counter for events, like
semaphore. Single means actors in Scala and Akka can have only one input
place (queue) for messages, while Petri Net Nodes (transitions) can have
several, and this is very  natural for asynchronous programming, as
illustrated at the picture in the referenced googledoc paper.

On Thu, Aug 29, 2013 at 11:04 PM, ?iktor ?lang <viktor.klang at gmail.com>wrote:

> Single place of type queue <-- wdym?
>
>
> On Thu, Aug 29, 2013 at 7:42 AM, Alexei Kaigorodov <
> alexei.kaigorodov at gmail.com> wrote:
>
>> Yes it is an actor, in its initial meaning. But unfortunately, the term
>> "actor" today mainly means restricted actor of Scala or Akka style (with
>> single place of type queue), and I am in doubt if "actor" would not cause
>> misunderstanding.
>>
>>
>> On Thu, Aug 29, 2013 at 9:26 PM, Zhong Yu <zhong.j.yu at gmail.com> wrote:
>>
>>> Hi Alexei, if in your model one place can be connected to only one
>>> transition, can't we model one transition with its input places as one
>>> actor?
>>>
>>> Zhong Yu  ://cs.oswego.edu/mailman/listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>>
>>
>>
>
>
> --
> *Viktor Klang*
> *Director of Engineering*
> Typesafe <http://www.typesafe.com/>
>
> Twitter: @viktorklang
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130829/f7d83723/attachment-0001.html>

From rk at rkuhn.info  Thu Aug 29 12:40:34 2013
From: rk at rkuhn.info (Roland Kuhn)
Date: Thu, 29 Aug 2013 18:40:34 +0200
Subject: [concurrency-interest] Coordinated Runnables
In-Reply-To: <CALCS1ZV2sneM8Bd=Wa_DHPq8A+=MVJD=Ojh8JHxUgnTOvqUTQQ@mail.gmail.com>
References: <loom.20130829T134152-308@post.gmane.org>
	<CACuKZqESAPOuhU9Dw8uSDG=Dw8wR=HqtRGp1gQQdhZQsTzrmxA@mail.gmail.com>
	<CALCS1ZVfVMaqDO-EVF0b0jqiesmC+6B5_oFgDL6gK9qR2MJUQA@mail.gmail.com>
	<CANPzfU86zVgaE8hL5j+Jz1VWOJrzALv5Suy_vygvjw=Z7VoVQg@mail.gmail.com>
	<CALCS1ZV2sneM8Bd=Wa_DHPq8A+=MVJD=Ojh8JHxUgnTOvqUTQQ@mail.gmail.com>
Message-ID: <17354802-F051-4A26-90B4-6B48B84707D4@rkuhn.info>

Hi Alexei,

Akka implements the Actor Model as described by Carl Hewitt in 1973 quite closely (the deviations are not relevant for this discussion); what you describe is closer to CSP or the (synchronous) ? calculus. Asynchronous message passing results in a better decoupling of sender and receiver of a communication, which is the important distinction here.

Petri nets can be used to model anything you want, including Actors or CSP, but directly implementing them will probably not achieve good scalability due to their synchronous (and therefore synchronized) transition semantics.

Regards,

Dr. Roland Kuhn
Akka Tech Lead
Typesafe ? Reactive apps on the JVM
twitter: @rolandkuhn

29 aug 2013 kl. 18:24 skrev Alexei Kaigorodov:

> 
> Place is a term in Petri Net model - place where tokens wait to be processed. "Of type queue" means that tokens are stored in a queue. Another possible type, for "colorless" tokens, is just a counter for events, like semaphore. Single means actors in Scala and Akka can have only one input place (queue) for messages, while Petri Net Nodes (transitions) can have several, and this is very  natural for asynchronous programming, as illustrated at the picture in the referenced googledoc paper.
> 
> On Thu, Aug 29, 2013 at 11:04 PM, ?iktor ?lang <viktor.klang at gmail.com> wrote:
> Single place of type queue <-- wdym?
> 
> 
> On Thu, Aug 29, 2013 at 7:42 AM, Alexei Kaigorodov <alexei.kaigorodov at gmail.com> wrote:
> Yes it is an actor, in its initial meaning. But unfortunately, the term "actor" today mainly means restricted actor of Scala or Akka style (with single place of type queue), and I am in doubt if "actor" would not cause misunderstanding.
> 
> 
> On Thu, Aug 29, 2013 at 9:26 PM, Zhong Yu <zhong.j.yu at gmail.com> wrote:
> Hi Alexei, if in your model one place can be connected to only one
> transition, can't we model one transition with its input places as one
> actor?
> 
> Zhong Yu  ://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> 
> 
> 
> -- 
> Viktor Klang
> Director of Engineering
> Typesafe
> 
> Twitter: @viktorklang
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

--
Simplicity and elegance are unpopular because they require hard work and discipline to achieve and education to be appreciated.
  -- Dijkstra

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130829/4f71483f/attachment.html>

From alexei.kaigorodov at gmail.com  Thu Aug 29 13:03:38 2013
From: alexei.kaigorodov at gmail.com (Alexei Kaigorodov)
Date: Fri, 30 Aug 2013 00:03:38 +0700
Subject: [concurrency-interest] Coordinated Runnables
In-Reply-To: <17354802-F051-4A26-90B4-6B48B84707D4@rkuhn.info>
References: <loom.20130829T134152-308@post.gmane.org>
	<CACuKZqESAPOuhU9Dw8uSDG=Dw8wR=HqtRGp1gQQdhZQsTzrmxA@mail.gmail.com>
	<CALCS1ZVfVMaqDO-EVF0b0jqiesmC+6B5_oFgDL6gK9qR2MJUQA@mail.gmail.com>
	<CANPzfU86zVgaE8hL5j+Jz1VWOJrzALv5Suy_vygvjw=Z7VoVQg@mail.gmail.com>
	<CALCS1ZV2sneM8Bd=Wa_DHPq8A+=MVJD=Ojh8JHxUgnTOvqUTQQ@mail.gmail.com>
	<17354802-F051-4A26-90B4-6B48B84707D4@rkuhn.info>
Message-ID: <CALCS1ZUsdUd=5SKdSS+0n0r8yWN=o4QeevB_eP9nGjjMO6J45A@mail.gmail.com>

Okay, I was wrong and Akka is closer to the Hewit's original concept. On
the other hand, I do not propose to implement pure Petri nets - indeed,
message passing should be asynchronous. The closest concepts are dataflow
programming and http://en.wikipedia.org/wiki/Dataflow_architecture .


On Thu, Aug 29, 2013 at 11:40 PM, Roland Kuhn <rk at rkuhn.info> wrote:

> Hi Alexei,
>
> Akka implements the Actor Model as described by Carl Hewitt in 1973 quite
> closely (the deviations are not relevant for this discussion); what you
> describe is closer to CSP or the (synchronous) ? calculus. Asynchronous
> message passing results in a better decoupling of sender and receiver of a
> communication, which is the important distinction here.
>
> Petri nets can be used to model anything you want, including Actors or
> CSP, but directly implementing them will probably not achieve good
> scalability due to their synchronous (and therefore synchronized)
> transition semantics.
>
> Regards,
>
> *Dr. Roland Kuhn*
> *Akka Tech Lead*
> Typesafe ? Reactive apps on the JVM
> twitter: @rolandkuhn
>
> 29 aug 2013 kl. 18:24 skrev Alexei Kaigorodov:
>
>
> Place is a term in Petri Net model - place where tokens wait to be
> processed. "Of type queue" means that tokens are stored in a queue. Another
> possible type, for "colorless" tokens, is just a counter for events, like
> semaphore. Single means actors in Scala and Akka can have only one input
> place (queue) for messages, while Petri Net Nodes (transitions) can have
> several, and this is very  natural for asynchronous programming, as
> illustrated at the picture in the referenced googledoc paper.
>
> On Thu, Aug 29, 2013 at 11:04 PM, ?iktor ?lang <viktor.klang at gmail.com>wrote:
>
>> Single place of type queue <-- wdym?
>>
>>
>> On Thu, Aug 29, 2013 at 7:42 AM, Alexei Kaigorodov <
>> alexei.kaigorodov at gmail.com> wrote:
>>
>>> Yes it is an actor, in its initial meaning. But unfortunately, the term
>>> "actor" today mainly means restricted actor of Scala or Akka style (with
>>> single place of type queue), and I am in doubt if "actor" would not cause
>>> misunderstanding.
>>>
>>>
>>> On Thu, Aug 29, 2013 at 9:26 PM, Zhong Yu <zhong.j.yu at gmail.com> wrote:
>>>
>>>> Hi Alexei, if in your model one place can be connected to only one
>>>> transition, can't we model one transition with its input places as one
>>>> actor?
>>>>
>>>> Zhong Yu  ://cs.oswego.edu/mailman/listinfo/concurrency-interest<http://cs.oswego.edu/mailman/listinfo/concurrency-interest>
>>>>
>>>
>>>
>>
>>
>> --
>> *Viktor Klang*
>> *Director of Engineering*
>> Typesafe <http://www.typesafe.com/>
>>
>> Twitter: @viktorklang
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> --
> Simplicity and elegance are unpopular because they require hard work and
> discipline to achieve and education to be appreciated.
>   -- Dijkstra
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20130830/11e5bbb3/attachment.html>

