From alarmnummer at gmail.com  Fri Sep  1 01:43:51 2006
From: alarmnummer at gmail.com (Peter Veentjer)
Date: Fri, 1 Sep 2006 07:43:51 +0200
Subject: [concurrency-interest] difference latch and lock
In-Reply-To: <44F61395.8070305@quiotix.com>
References: <1466c1d60608301314r13ed0416k5c44cffd69455175@mail.gmail.com>
	<44F61395.8070305@quiotix.com>
Message-ID: <1466c1d60608312243ye8d9b9fgf23fee3e8e56d0af@mail.gmail.com>

Thanks for your answer Brian. Could you give an example how a Latch
would be used?

On 8/31/06, Brian Goetz <brian at quiotix.com> wrote:
> Latches and locks are different.  It is easiest to think of them in
> terms of their state machines.  A lock goes from unlocked to locked and
> back to unlocked with the right sequence of lock and unlock operations.
>  A latch goes from closed to open after some sequence of state
> transitions, and NEVER GOES BACK.  Once it transitions to the terminal
> state, it never changes.
>
> Both offer blocking behavior.  For Lock, if you try to acquire an
> already-held lock, you block until the lock is available.  For Latch, if
> you try to wait for the latch, you block until someone else puts the
> latch in the terminal (open) state.
>
>

From dcholmes at optusnet.com.au  Fri Sep  1 02:11:46 2006
From: dcholmes at optusnet.com.au (David Holmes)
Date: Fri, 1 Sep 2006 16:11:46 +1000
Subject: [concurrency-interest] difference latch and lock
In-Reply-To: <1466c1d60608312243ye8d9b9fgf23fee3e8e56d0af@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCCEADHCAA.dcholmes@optusnet.com.au>

I think we're encountering a number of different uses of the word "latch".

As Brian describes one notion of state "latching" is that it progresses to a
terminal value from whence it no longer changes. It is permanently
"latched". A synchronization object that behaves in this way is the
CountDownLatch - once "open" it never closes and can't be reset.

More generally though latches can be reset - consider a digital flip-flop
such as a "gated D-latch", the flip-flop latches the value of the data when
the gate is pulsed/strobed; if you change the data without changing the gate
then the latched value is unchanged, but change the gate and the latched
value is updated.

In synchronization object terms a latch is sometimes called a "gate" - the
connotation being that if the gate is open anyone/everyone can pass through;
while if it is shut no one can pass through. The CountDownLatch operates
this way, but a more general "gate" is the CyclicBarrier which can also be
reset (and automatically does so). Of course the semantics of CountDownLatch
and CyclicBarrier are somewhat different. CyclicBarrier is what can be
called a "weighted gate" or "weighted bucket" - it is set up to expect N
threads to arrive, when they arrive they have sufficient "weight" to open
the gate, or tip the bucket - in this case the gate/bucket is spring-loaded
and closes/rights-itself as soon as the threads leave, so it is ready for
the next set of threads to use. CountDownLatch on the other hand is like a
gate with multiple padlocks - when the last padlock is removed, the gate
opens and it stays open. Aren't these analogies quaint :-) We could have
defined CountDownLatch to allow reset but reset semantics are messy and
usually not needed for CDL usage, in contrast barrier designs typically
always use the barrier multiple times.

It seems the database folk are using the term "latch" for lightweight lock,
which is an uncommon usage from a synchronization perspective and a poor
choice in my view, though arguably there is an analogy between "locking a
door/window" and just "latching it shut".  In that sense "latching" is a
weaker form of "locking". But I don't like the usage.

Cheers,
David Holmes

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Peter
> Veentjer
> Sent: Friday, 1 September 2006 3:44 PM
> To: Brian Goetz; concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] difference latch and lock
>
>
> Thanks for your answer Brian. Could you give an example how a Latch
> would be used?
>
> On 8/31/06, Brian Goetz <brian at quiotix.com> wrote:
> > Latches and locks are different.  It is easiest to think of them in
> > terms of their state machines.  A lock goes from unlocked to locked and
> > back to unlocked with the right sequence of lock and unlock operations.
> >  A latch goes from closed to open after some sequence of state
> > transitions, and NEVER GOES BACK.  Once it transitions to the terminal
> > state, it never changes.
> >
> > Both offer blocking behavior.  For Lock, if you try to acquire an
> > already-held lock, you block until the lock is available.  For Latch, if
> > you try to wait for the latch, you block until someone else puts the
> > latch in the terminal (open) state.
> >
> >
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest


From dawidk at mathcs.emory.edu  Mon Sep  4 11:37:47 2006
From: dawidk at mathcs.emory.edu (Dawid Kurzyniec)
Date: Mon, 04 Sep 2006 11:37:47 -0400
Subject: [concurrency-interest] Backport: serialization bug in
	Collections
In-Reply-To: <44EE11FB.6030807@brittanysoftware.com>
References: <44EE11FB.6030807@brittanysoftware.com>
Message-ID: <44FC484B.3020509@mathcs.emory.edu>

Xavier Le Vourch wrote:
> I've just used FindBugs on my code and it shows an error in Collections:
>
> =============
> Se: Class is Serializable but its superclass doesn't define a void
> constructor
>
> (...)
> When I made the CheckedMap class implement Serializable, the code worked
> and all tests in the test suite still passed.
>   

Thanks for the fix Xavier! I have committed it to CVS:

http://dcl.mathcs.emory.edu/cgi-bin/viewcvs.cgi/software/util/backport-util-concurrent/src/edu/emory/mathcs/backport/java/util/Collections.java

Sorry for not responding earlier; my processing queue grew quite large 
these past couple of weeks :)

Regards to all,
Dawid


From brian at quiotix.com  Mon Sep  4 13:09:41 2006
From: brian at quiotix.com (Brian Goetz)
Date: Mon, 04 Sep 2006 13:09:41 -0400
Subject: [concurrency-interest] difference latch and lock
In-Reply-To: <1466c1d60608312243ye8d9b9fgf23fee3e8e56d0af@mail.gmail.com>
References: <1466c1d60608301314r13ed0416k5c44cffd69455175@mail.gmail.com>	<44F61395.8070305@quiotix.com>
	<1466c1d60608312243ye8d9b9fgf23fee3e8e56d0af@mail.gmail.com>
Message-ID: <44FC5DD5.3030800@quiotix.com>

See JCiP 5.5.1 and 5.5.2.  Also 8.5.1 and 14.2.5 for more examples.

Peter Veentjer wrote:
> Thanks for your answer Brian. Could you give an example how a Latch
> would be used?
> 
> On 8/31/06, Brian Goetz <brian at quiotix.com> wrote:
>> Latches and locks are different.  It is easiest to think of them in
>> terms of their state machines.  A lock goes from unlocked to locked and
>> back to unlocked with the right sequence of lock and unlock operations.
>>  A latch goes from closed to open after some sequence of state
>> transitions, and NEVER GOES BACK.  Once it transitions to the terminal
>> state, it never changes.
>>
>> Both offer blocking behavior.  For Lock, if you try to acquire an
>> already-held lock, you block until the lock is available.  For Latch, if
>> you try to wait for the latch, you block until someone else puts the
>> latch in the terminal (open) state.
>>
>>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest

From brian at quiotix.com  Mon Sep  4 15:49:01 2006
From: brian at quiotix.com (Brian Goetz)
Date: Mon, 04 Sep 2006 15:49:01 -0400
Subject: [concurrency-interest] Simple ScheduledFuture problem
In-Reply-To: <aa067ea10608301734h5721b10ev76b7cb156d1972a0@mail.gmail.com>
References: <NFBBKALFDCPFIDBNKAPCIEKIHBAA.dcholmes@optusnet.com.au>	<44F5F538.5000505@quiotix.com>
	<44F6292C.3050500@mathcs.emory.edu>
	<aa067ea10608301734h5721b10ev76b7cb156d1972a0@mail.gmail.com>
Message-ID: <44FC832D.40307@quiotix.com>

> yes, but presumably a more elegant solution is for the other "setting
> methods" to invoke the single setter, and treat the volatile field as
> a javabeans property.

No, that wouldn't work.  Otherwise, two threads racing to increment 
could result in a lost update.  Volatile is tricky!

From jed at atlassian.com  Tue Sep  5 03:13:57 2006
From: jed at atlassian.com (Jed Wesley-Smith)
Date: Tue, 05 Sep 2006 17:13:57 +1000
Subject: [concurrency-interest] ReentrantReadWriteLock should throw ex on
 writeLock().lock() if read lock held?
Message-ID: <44FD23B5.8070802@atlassian.com>

Just a quick question regarding the acquisition policy of the 
ReentrantReadWriteLock if a reader tries to grab the write lock. What 
currently happens is that the call to writeLock().lock() blocks forever. 
Wouldn't it be better to throw some sort of exception like 
IllegalMonitorStateException (not that one specifically, but something 
like it)?. That way you would fail-fast and find the problem fairly 
quickly...

Or in other words, is there a specific reason you would want to call 
something in such a way that the only return possible is an 
InterruptedException if you are lucky enough that another thread notices 
your plight and interrupts you :-)

-- 
cheers,
- jed.


From dhanji at gmail.com  Tue Sep  5 06:42:23 2006
From: dhanji at gmail.com (Dhanji R. Prasanna)
Date: Tue, 5 Sep 2006 20:42:23 +1000
Subject: [concurrency-interest] Simple ScheduledFuture problem
In-Reply-To: <44FC832D.40307@quiotix.com>
References: <NFBBKALFDCPFIDBNKAPCIEKIHBAA.dcholmes@optusnet.com.au>
	<44F5F538.5000505@quiotix.com> <44F6292C.3050500@mathcs.emory.edu>
	<aa067ea10608301734h5721b10ev76b7cb156d1972a0@mail.gmail.com>
	<44FC832D.40307@quiotix.com>
Message-ID: <aa067ea10609050342i45fcc138k220c98e3a6a90c39@mail.gmail.com>

Thanks Brian and guys  for the wise words. =)

On 9/5/06, Brian Goetz <brian at quiotix.com> wrote:
> > yes, but presumably a more elegant solution is for the other "setting
> > methods" to invoke the single setter, and treat the volatile field as
> > a javabeans property.
>
> No, that wouldn't work.  Otherwise, two threads racing to increment
> could result in a lost update.  Volatile is tricky!
>

From giuliano.mega at gmail.com  Tue Sep  5 09:57:25 2006
From: giuliano.mega at gmail.com (Giuliano Mega)
Date: Tue, 5 Sep 2006 10:57:25 -0300
Subject: [concurrency-interest] ReentrantReadWriteLock should throw ex
	on writeLock().lock() if read lock held?
In-Reply-To: <44FD23B5.8070802@atlassian.com>
References: <44FD23B5.8070802@atlassian.com>
Message-ID: <7547a9ff0609050657i3a3cede7xae70f92a5c23bf42@mail.gmail.com>

Hi Jed,

I always thought that Lock.tryLock was supposed to play this role
(except that it returns false instead of throwing an exception). Am I
missing something from your problem that prevents you from using
tryLock?

Best,

On 9/5/06, Jed Wesley-Smith <jed at atlassian.com> wrote:
> Just a quick question regarding the acquisition policy of the
> ReentrantReadWriteLock if a reader tries to grab the write lock. What
> currently happens is that the call to writeLock().lock() blocks forever.
> Wouldn't it be better to throw some sort of exception like
> IllegalMonitorStateException (not that one specifically, but something
> like it)?. That way you would fail-fast and find the problem fairly
> quickly...
>
> Or in other words, is there a specific reason you would want to call
> something in such a way that the only return possible is an
> InterruptedException if you are lucky enough that another thread notices
> your plight and interrupts you :-)
>
> --
> cheers,
> - jed.
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>


-- 
Giuliano Mega <giuliano at ime.usp.br>

From the.mindstorm.mailinglist at gmail.com  Tue Sep  5 17:22:18 2006
From: the.mindstorm.mailinglist at gmail.com (Alexandru Popescu)
Date: Wed, 6 Sep 2006 00:22:18 +0300
Subject: [concurrency-interest] unknown version of concurrent backport
Message-ID: <c6f400460609051422pab45d5ucf4f112d860fdbe2@mail.gmail.com>

Hi!

I know this may sound as a very stupid question, but I couldn't find
any references through Google or in the mailing list. I have in one
project a dependency on a jar file: util-concurrent (with the package:
edu.emory.matchs.util). My problem is that I cannot figure out the
original of this file:
it is not the initial Doug's work on concurrent
(http://g.oswego.edu/dl/classes/EDU/oswego/cs/dl/util/concurrent/intro.html)
nor the backport available here
(http://dcl.mathcs.emory.edu/util/backport-util-concurrent/dist/). Can
anybody remember anything related to such a jar?

many thanks to anybody with better memories than mine

./alex
--
.w( the_mindstorm )p.

PS: I am very sure that it is not something built/modified in house.

From dcholmes at optusnet.com.au  Tue Sep  5 19:33:02 2006
From: dcholmes at optusnet.com.au (David Holmes)
Date: Wed, 6 Sep 2006 09:33:02 +1000
Subject: [concurrency-interest] ReentrantReadWriteLock should throw ex
	on writeLock().lock() if read lock held?
In-Reply-To: <44FD23B5.8070802@atlassian.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCIECIHCAA.dcholmes@optusnet.com.au>

Hi Jed,

> Just a quick question regarding the acquisition policy of the
> ReentrantReadWriteLock if a reader tries to grab the write lock. What
> currently happens is that the call to writeLock().lock() blocks forever.
> Wouldn't it be better to throw some sort of exception like
> IllegalMonitorStateException (not that one specifically, but something
> like it)?. That way you would fail-fast and find the problem fairly
> quickly...

"better" is subjective. Correctly written code simply won't do this. If a
check were put in for this then correctly written code would pay the price
on the every writeLock acquisition.

You can easily write your own wrapper that would add such a check via an
assert; or place the assert directly in your code.

Cheers,
David Holmes


From jed at atlassian.com  Tue Sep  5 20:02:23 2006
From: jed at atlassian.com (Jed Wesley-Smith)
Date: Wed, 06 Sep 2006 10:02:23 +1000
Subject: [concurrency-interest] ReentrantReadWriteLock should throw ex
 on writeLock().lock() if read lock held?
In-Reply-To: <NFBBKALFDCPFIDBNKAPCIECIHCAA.dcholmes@optusnet.com.au>
References: <NFBBKALFDCPFIDBNKAPCIECIHCAA.dcholmes@optusnet.com.au>
Message-ID: <44FE100F.9090000@atlassian.com>

Hi David,

Thanks for the reply.

David Holmes wrote:
> Hi Jed,
>   
>> Just a quick question regarding the acquisition policy of the
>> ReentrantReadWriteLock if a reader tries to grab the write lock. What
>> currently happens is that the call to writeLock().lock() blocks forever.
>> Wouldn't it be better to throw some sort of exception like
>> IllegalMonitorStateException (not that one specifically, but something
>> like it)?. That way you would fail-fast and find the problem fairly
>> quickly...
>>     
>
> "better" is subjective. Correctly written code simply won't do this. If a
> check were put in for this then correctly written code would pay the price
> on the every writeLock acquisition.
>
> You can easily write your own wrapper that would add such a check via an
> assert; or place the assert directly in your code.
>   
These are definitely solutions. My only point is that the client of the 
class needs to be aware of this behaviour of the class in order to be 
correct. The documentation of the ReentrantReadWriteLock class says 
clearly that upgrading of a read lock to a write lock is _not_ possible, 
but the behaviour in that use-case is not specified and quite frankly 
surprising.

Personally as a client of the class I would prefer it to either (a) 
throw an exception (preferred) or (b) document that the attempt will 
block forever.

-- 
cheers,
- jed.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060906/497748c3/attachment.html 

From dcholmes at optusnet.com.au  Tue Sep  5 20:11:14 2006
From: dcholmes at optusnet.com.au (David Holmes)
Date: Wed, 6 Sep 2006 10:11:14 +1000
Subject: [concurrency-interest] ReentrantReadWriteLock should throw ex
	on writeLock().lock() if read lock held?
In-Reply-To: <44FE100F.9090000@atlassian.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCMECJHCAA.dcholmes@optusnet.com.au>

I guess the documentation could clarify what "not possible" means.

Cheers,
David
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Jed
Wesley-Smith
  Sent: Wednesday, 6 September 2006 10:02 AM
  To: concurrency-interest at cs.oswego.edu
  Subject: Re: [concurrency-interest] ReentrantReadWriteLock should throw ex
on writeLock().lock() if read lock held?


  Hi David,

  Thanks for the reply.

  David Holmes wrote:
Hi Jed,
  Just a quick question regarding the acquisition policy of the
ReentrantReadWriteLock if a reader tries to grab the write lock. What
currently happens is that the call to writeLock().lock() blocks forever.
Wouldn't it be better to throw some sort of exception like
IllegalMonitorStateException (not that one specifically, but something
like it)?. That way you would fail-fast and find the problem fairly
quickly...

"better" is subjective. Correctly written code simply won't do this. If a
check were put in for this then correctly written code would pay the price
on the every writeLock acquisition.

You can easily write your own wrapper that would add such a check via an
assert; or place the assert directly in your code.
  These are definitely solutions. My only point is that the client of the
class needs to be aware of this behaviour of the class in order to be
correct. The documentation of the ReentrantReadWriteLock class says clearly
that upgrading of a read lock to a write lock is _not_ possible, but the
behaviour in that use-case is not specified and quite frankly surprising.

  Personally as a client of the class I would prefer it to either (a) throw
an exception (preferred) or (b) document that the attempt will block
forever.

--
cheers,
- jed.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060906/f6cad4b8/attachment.html 

From jed at atlassian.com  Tue Sep  5 20:30:05 2006
From: jed at atlassian.com (Jed Wesley-Smith)
Date: Wed, 06 Sep 2006 10:30:05 +1000
Subject: [concurrency-interest] ReentrantReadWriteLock should throw ex
 on writeLock().lock() if read lock held?
In-Reply-To: <7547a9ff0609050657i3a3cede7xae70f92a5c23bf42@mail.gmail.com>
References: <44FD23B5.8070802@atlassian.com>
	<7547a9ff0609050657i3a3cede7xae70f92a5c23bf42@mail.gmail.com>
Message-ID: <44FE168D.5080208@atlassian.com>

Hi Giuliano,

tryLock() does the job, and I don't have a problem using it. I don't 
bring this up as a particular problem I have (apart from the first time 
I na?vely used it :-) , but as a general discussion point. If some code 
does call writeLock().lock() while holding a read lock it will block 
forever - which seems to me undesirable.

Giuliano Mega wrote:
> Hi Jed,
>
> I always thought that Lock.tryLock was supposed to play this role
> (except that it returns false instead of throwing an exception). Am I
> missing something from your problem that prevents you from using
> tryLock?
>
> Best,
>
> On 9/5/06, Jed Wesley-Smith <jed at atlassian.com> wrote:
>   
>> Just a quick question regarding the acquisition policy of the
>> ReentrantReadWriteLock if a reader tries to grab the write lock. What
>> currently happens is that the call to writeLock().lock() blocks forever.
>> Wouldn't it be better to throw some sort of exception like
>> IllegalMonitorStateException (not that one specifically, but something
>> like it)?. That way you would fail-fast and find the problem fairly
>> quickly...
>>
>> Or in other words, is there a specific reason you would want to call
>> something in such a way that the only return possible is an
>> InterruptedException if you are lucky enough that another thread notices
>> your plight and interrupts you :-)
>>     
-- 
cheers,
- jed.



From dawidk at mathcs.emory.edu  Tue Sep  5 21:14:21 2006
From: dawidk at mathcs.emory.edu (Dawid Kurzyniec)
Date: Tue, 05 Sep 2006 21:14:21 -0400
Subject: [concurrency-interest] unknown version of concurrent backport
In-Reply-To: <c6f400460609051422pab45d5ucf4f112d860fdbe2@mail.gmail.com>
References: <c6f400460609051422pab45d5ucf4f112d860fdbe2@mail.gmail.com>
Message-ID: <44FE20ED.80608@mathcs.emory.edu>

Alexandru Popescu wrote:
> Hi!
>
> I know this may sound as a very stupid question, but I couldn't find
> any references through Google or in the mailing list. I have in one
> project a dependency on a jar file: util-concurrent (with the package:
> edu.emory.matchs.util). My problem is that I cannot figure out the
> original of this file:
> it is not the initial Doug's work on concurrent
> (http://g.oswego.edu/dl/classes/EDU/oswego/cs/dl/util/concurrent/intro.html)
> nor the backport available here
> (http://dcl.mathcs.emory.edu/util/backport-util-concurrent/dist/). Can
> anybody remember anything related to such a jar?
>
> many thanks to anybody with better memories than mine
>   

Is it emory-util-concurrent.jar? If yes, then it is a part of Emory 
Utilities package, that has a few extra concurrency utils built on top 
of the standard ones: http://dcl.mathcs.emory.edu/util/ . Otherwise, it 
might be a very old version of the backport, in which case you should 
upgrade ASAP.

Regards,
Dawid


From the.mindstorm.mailinglist at gmail.com  Wed Sep  6 05:45:45 2006
From: the.mindstorm.mailinglist at gmail.com (Alexandru Popescu)
Date: Wed, 6 Sep 2006 12:45:45 +0300
Subject: [concurrency-interest] unknown version of concurrent backport
In-Reply-To: <44FE20ED.80608@mathcs.emory.edu>
References: <c6f400460609051422pab45d5ucf4f112d860fdbe2@mail.gmail.com>
	<44FE20ED.80608@mathcs.emory.edu>
Message-ID: <c6f400460609060245g7f77cbo5e811939d25b5f9f@mail.gmail.com>

On 9/6/06, Dawid Kurzyniec <dawidk at mathcs.emory.edu> wrote:
> Alexandru Popescu wrote:
> > Hi!
> >
> > I know this may sound as a very stupid question, but I couldn't find
> > any references through Google or in the mailing list. I have in one
> > project a dependency on a jar file: util-concurrent (with the package:
> > edu.emory.matchs.util). My problem is that I cannot figure out the
> > original of this file:
> > it is not the initial Doug's work on concurrent
> > (http://g.oswego.edu/dl/classes/EDU/oswego/cs/dl/util/concurrent/intro.html)
> > nor the backport available here
> > (http://dcl.mathcs.emory.edu/util/backport-util-concurrent/dist/). Can
> > anybody remember anything related to such a jar?
> >
> > many thanks to anybody with better memories than mine
> >
>
> Is it emory-util-concurrent.jar? If yes, then it is a part of Emory
> Utilities package, that has a few extra concurrency utils built on top
> of the standard ones: http://dcl.mathcs.emory.edu/util/ . Otherwise, it
> might be a very old version of the backport, in which case you should
> upgrade ASAP.
>

Thanks for the answer Dawid. As I pointed out it is not something from
emory (because of the package names). And yes, I think it is very very
old (somewhere around 2 years, maybe 3).

We are using a very limited part of it and I have checked the backlogs
looking for bug fixes in the classes we are using but without finding
anything critical. I can confess that the size of the jar is one of
the biggest concerns in our case (TestNG: http://testng.org) and the
latest backport-util distributions have doubled their sizes (probably
I may try to trim it down to just the classes we need, but I usually
don't like modifying the distributions because IANAL).

Any advise?

./alex
--
.w( the_mindstorm )p.
  TestNG co-founder


> Regards,
> Dawid
>
>

From tim at peierls.net  Wed Sep  6 10:30:23 2006
From: tim at peierls.net (Tim Peierls)
Date: Wed, 6 Sep 2006 10:30:23 -0400
Subject: [concurrency-interest] ReentrantReadWriteLock should throw ex
	on writeLock().lock() if read lock held?
In-Reply-To: <44FE168D.5080208@atlassian.com>
References: <44FD23B5.8070802@atlassian.com>
	<7547a9ff0609050657i3a3cede7xae70f92a5c23bf42@mail.gmail.com>
	<44FE168D.5080208@atlassian.com>
Message-ID: <63b4e4050609060730n46c4e968qcda7d8052cbd35ad@mail.gmail.com>

On 9/5/06, Jed Wesley-Smith <jed at atlassian.com> wrote:
>
> tryLock() does the job, and I don't have a problem using it. I don't
> bring this up as a particular problem I have (apart from the first time
> I na?vely used it :-) , but as a general discussion point. If some code
> does call writeLock().lock() while holding a read lock it will block
> forever - which seems to me undesirable.


Yes, it is undesirable. It is essential to encapsulate access to the RWL to
ensure that client code cannot get the program into such undesirable states.

There are sharp edges in java.util.concurrent.locks. These are low-level
tools; JCiP treats them as an advanced topic. It wouldn't be right to add
expensive runtime checks to protect developers who fail to encapsulate their
use of these tools.

--tim
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060906/43c65f34/attachment-0001.html 

From brian at quiotix.com  Wed Sep  6 10:51:56 2006
From: brian at quiotix.com (Brian Goetz)
Date: Wed, 06 Sep 2006 10:51:56 -0400
Subject: [concurrency-interest] unknown version of concurrent backport
In-Reply-To: <c6f400460609060245g7f77cbo5e811939d25b5f9f@mail.gmail.com>
References: <c6f400460609051422pab45d5ucf4f112d860fdbe2@mail.gmail.com>	<44FE20ED.80608@mathcs.emory.edu>
	<c6f400460609060245g7f77cbo5e811939d25b5f9f@mail.gmail.com>
Message-ID: <44FEE08C.7000700@quiotix.com>

> And yes, I think it is very very
> old (somewhere around 2 years, maybe 3).

Time keeps speeding up.  Now, "2 or 3 years" translates to "very, very 
old".  How ancient does that make me?  :)

From Jan.Nielsen at sungardhe.com  Wed Sep  6 10:56:04 2006
From: Jan.Nielsen at sungardhe.com (Jan Nielsen)
Date: Wed, 6 Sep 2006 08:56:04 -0600
Subject: [concurrency-interest] unknown version of concurrent backport
In-Reply-To: <c6f400460609060245g7f77cbo5e811939d25b5f9f@mail.gmail.com>
Message-ID: <OFBAB21F08.5D09E420-ON872571E1.0051AFD7-872571E1.00520A0C@sungardhe.com>

Just curious - while I agree small is nice, what is the issue with JAR 
size in TestNG? Memory? Download? Diskspace??

Thanks,

-Jan

Jan Nielsen * System Architect * SunGard Higher Education * Tel +1 801 257 
4155 * Fax +1 801 485 6606 * jan.nielsen at sungardhe.com * www.sungardhe.com 
* 90 South 400 West, Suite 500, Salt Lake City, UT USA

CONFIDENTIALITY: This email (including any attachments) may contain 
confidential, proprietary and privileged information, and unauthorized 
disclosure or use is prohibited.  If you received this email in error, 
please notify the sender and delete this email from your system. Thank 
you.



"Alexandru Popescu" <the.mindstorm.mailinglist at gmail.com> 
Sent by: concurrency-interest-bounces at cs.oswego.edu
09/06/2006 03:45 AM

To
userConcurrency <concurrency-interest at cs.oswego.edu>
cc

Subject
Re: [concurrency-interest] unknown version of concurrent backport






On 9/6/06, Dawid Kurzyniec <dawidk at mathcs.emory.edu> wrote:
> Alexandru Popescu wrote:
> > Hi!
> >
> > I know this may sound as a very stupid question, but I couldn't find
> > any references through Google or in the mailing list. I have in one
> > project a dependency on a jar file: util-concurrent (with the package:
> > edu.emory.matchs.util). My problem is that I cannot figure out the
> > original of this file:
> > it is not the initial Doug's work on concurrent
> > (
http://g.oswego.edu/dl/classes/EDU/oswego/cs/dl/util/concurrent/intro.html
)
> > nor the backport available here
> > (http://dcl.mathcs.emory.edu/util/backport-util-concurrent/dist/). Can
> > anybody remember anything related to such a jar?
> >
> > many thanks to anybody with better memories than mine
> >
>
> Is it emory-util-concurrent.jar? If yes, then it is a part of Emory
> Utilities package, that has a few extra concurrency utils built on top
> of the standard ones: http://dcl.mathcs.emory.edu/util/ . Otherwise, it
> might be a very old version of the backport, in which case you should
> upgrade ASAP.
>

Thanks for the answer Dawid. As I pointed out it is not something from
emory (because of the package names). And yes, I think it is very very
old (somewhere around 2 years, maybe 3).

We are using a very limited part of it and I have checked the backlogs
looking for bug fixes in the classes we are using but without finding
anything critical. I can confess that the size of the jar is one of
the biggest concerns in our case (TestNG: http://testng.org) and the
latest backport-util distributions have doubled their sizes (probably
I may try to trim it down to just the classes we need, but I usually
don't like modifying the distributions because IANAL).

Any advise?

./alex
--
.w( the_mindstorm )p.
  TestNG co-founder


> Regards,
> Dawid
>
>
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at altair.cs.oswego.edu
http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest



From rbalamohan at sonoasystems.com  Wed Sep  6 11:06:54 2006
From: rbalamohan at sonoasystems.com (Rajesh Balamohan)
Date: Wed, 6 Sep 2006 08:06:54 -0700 (PDT)
Subject: [concurrency-interest] hprof reporting LockSupport.park() taking
	highest CPU usage
Message-ID: <2372.59.92.136.41.1157555214.squirrel@webmail.sonoasystems.com>

Hi Folks,

When profiling my application with hprof ("cpu=time" option),I noticed
that LockSupport.park() being reported as the most CPU consuming method.

We have some Linked blocking queues in our application. So I think, its
waiting on take() or so rather than really using the CPU.

What I am wondering is, whether the JVM is internally spinning on the CPU
rather than really waiting. Probably that is making this method to crop up
to the top of the list. If so, I guess I need to look for a parameter
which can disable that option so that the CPU cycle can be used for some
other work rather than spinning.

Have you folks faced similar situation?. I am using sun JVM 1.5_04.

~Rajesh.B

From MOKEEFFE at amfam.com  Wed Sep  6 11:10:11 2006
From: MOKEEFFE at amfam.com (OKeeffe, Michael K)
Date: Wed, 6 Sep 2006 10:10:11 -0500
Subject: [concurrency-interest] unknown version of concurrent backport
Message-ID: <2E8B2DCB7DE50B42808F8063C4E2858F0958384C@NHQ1ACCOEX05VS1.corporate.amfam.com>



We are using a very limited part of it and I have checked the backlogs
looking for bug fixes in the classes we are using but without finding
anything critical. I can confess that the size of the jar is one of
the biggest concerns in our case (TestNG: http://testng.org) and the
latest backport-util distributions have doubled their sizes (probably
I may try to trim it down to just the classes we need, but I usually
don't like modifying the distributions because IANAL).

Any advise?

___________________________________________

>From a user standpoint I would highly recommend trimming down the
distribution if you only use a fraction of it.  I recently installed
some software that, to build, required cygwin (50MB?) and Python
(10MB?).   Fortunately they also supplied just the few utilities from
cygwin required (make for sure, directory utilities, and a few others),
and Python (header files, .dll, and Python code), which made it a snap
to build.  

This is also what makes installing software on Linux a mess (various,
conflicting c libraries), although there are also utilities that seem to
make this easier.  I think in your specific case, where it is just one
jar or so, it might not be as big an issue, but if you have many other
dependencies, i.e. Apache, Spring, etc, then it adds up.

My 2c from a user perspective.

-Mike


From dl at cs.oswego.edu  Wed Sep  6 11:23:44 2006
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed, 06 Sep 2006 11:23:44 -0400
Subject: [concurrency-interest] hprof reporting LockSupport.park()
 taking highest CPU usage
In-Reply-To: <2372.59.92.136.41.1157555214.squirrel@webmail.sonoasystems.com>
References: <2372.59.92.136.41.1157555214.squirrel@webmail.sonoasystems.com>
Message-ID: <44FEE800.3040802@cs.oswego.edu>

Rajesh Balamohan wrote:
> 
> What I am wondering is, whether the JVM is internally spinning on the CPU
> rather than really waiting. Probably that is making this method to crop up
> to the top of the list. If so, I guess I need to look for a parameter
> which can disable that option so that the CPU cycle can be used for some
> other work rather than spinning.
> 

There is not much spinning in most j.u.c classes, including
LinkedBlockingQueue.

My offhand guess is that profiling tools have a hard time accurately
reporting CPU times as threads become blocked and unblocked.

Usually, the best tools for evaluating such things are visual.
If you are running on unix, get a copy of perfbar
   http://gee.cs.oswego.edu/dl/code/
and watch how much time is spent in system-calls (red) which in
this case will reflect block/unblock context-switch times.

-Doug



From the.mindstorm.mailinglist at gmail.com  Wed Sep  6 11:37:15 2006
From: the.mindstorm.mailinglist at gmail.com (Alexandru Popescu)
Date: Wed, 6 Sep 2006 18:37:15 +0300
Subject: [concurrency-interest] unknown version of concurrent backport
In-Reply-To: <44FEE08C.7000700@quiotix.com>
References: <c6f400460609051422pab45d5ucf4f112d860fdbe2@mail.gmail.com>
	<44FE20ED.80608@mathcs.emory.edu>
	<c6f400460609060245g7f77cbo5e811939d25b5f9f@mail.gmail.com>
	<44FEE08C.7000700@quiotix.com>
Message-ID: <c6f400460609060837i46b15ae0i4c564663cfa54936@mail.gmail.com>

On 9/6/06, Brian Goetz <brian at quiotix.com> wrote:
> > And yes, I think it is very very
> > old (somewhere around 2 years, maybe 3).
>
> Time keeps speeding up.  Now, "2 or 3 years" translates to "very, very
> old".  How ancient does that make me?  :)
>

I got this vertigo feeling when I noticed I cannot find any references
to this jar. Don't worry about the how ancient we are... at some point
we stop counting because we are too old :-].

./alex
--
.w( the_mindstorm )p.

From dawidk at mathcs.emory.edu  Wed Sep  6 20:16:11 2006
From: dawidk at mathcs.emory.edu (Dawid Kurzyniec)
Date: Wed, 06 Sep 2006 20:16:11 -0400
Subject: [concurrency-interest] unknown version of concurrent backport
In-Reply-To: <c6f400460609060245g7f77cbo5e811939d25b5f9f@mail.gmail.com>
References: <c6f400460609051422pab45d5ucf4f112d860fdbe2@mail.gmail.com><44FE	20ED.80608@mathcs.emory.edu>
	<c6f400460609060245g7f77cbo5e811939d25b5f9f@mail.gmail.com>
Message-ID: <44FF64CB.8050805@mathcs.emory.edu>

Alexandru Popescu wrote:
> We are using a very limited part of it and I have checked the backlogs
> looking for bug fixes in the classes we are using but without finding
> anything critical. I can confess that the size of the jar is one of
> the biggest concerns in our case (TestNG: http://testng.org) and the
> latest backport-util distributions have doubled their sizes (probably
> I may try to trim it down to just the classes we need, but I usually
> don't like modifying the distributions because IANAL).
>
> Any advise?
>
>   


That JAR was an early prototype, even before the first public release. 
If you're really using only a few files, you might be able to pick them 
up from the backport source distribution, what may let you save even 
more space. The backport is public-domain so there is absolutely no 
licensing concerns - you can do whatever the heck you want with it. 
(That's why I love JSR 166 :)

If all your stuff works then maybe this is not critical indeed, but the 
early versions of backport had a few nasty little bugs that would come 
up only on some platforms. The version you're using wasn't even 
unit-tested. It's your call...

Regards,
Dawid


From the.mindstorm.mailinglist at gmail.com  Wed Sep  6 20:47:52 2006
From: the.mindstorm.mailinglist at gmail.com (Alexandru Popescu)
Date: Thu, 7 Sep 2006 03:47:52 +0300
Subject: [concurrency-interest] unknown version of concurrent backport
In-Reply-To: <44FF64CB.8050805@mathcs.emory.edu>
References: <c6f400460609051422pab45d5ucf4f112d860fdbe2@mail.gmail.com>
	<c6f400460609060245g7f77cbo5e811939d25b5f9f@mail.gmail.com>
	<44FF64CB.8050805@mathcs.emory.edu>
Message-ID: <c6f400460609061747x37a8b7b3wc27375198adb516a@mail.gmail.com>

On 9/7/06, Dawid Kurzyniec <dawidk at mathcs.emory.edu> wrote:
> Alexandru Popescu wrote:
> > We are using a very limited part of it and I have checked the backlogs
> > looking for bug fixes in the classes we are using but without finding
> > anything critical. I can confess that the size of the jar is one of
> > the biggest concerns in our case (TestNG: http://testng.org) and the
> > latest backport-util distributions have doubled their sizes (probably
> > I may try to trim it down to just the classes we need, but I usually
> > don't like modifying the distributions because IANAL).
> >
> > Any advise?
> >
> >
>
>
> That JAR was an early prototype, even before the first public release.
> If you're really using only a few files, you might be able to pick them
> up from the backport source distribution, what may let you save even
> more space. The backport is public-domain so there is absolutely no
> licensing concerns - you can do whatever the heck you want with it.
> (That's why I love JSR 166 :)
>
> If all your stuff works then maybe this is not critical indeed, but the
> early versions of backport had a few nasty little bugs that would come
> up only on some platforms. The version you're using wasn't even
> unit-tested. It's your call...
>

Not that I got scared, but a couple of hours ago I have already
decided to upgrade ;-]. Many thanks Dawid for all your advises.

./alex
--
.w( the_mindstorm )p.

> Regards,
> Dawid
>
>

From jed at atlassian.com  Wed Sep  6 21:32:02 2006
From: jed at atlassian.com (Jed Wesley-Smith)
Date: Thu, 07 Sep 2006 11:32:02 +1000
Subject: [concurrency-interest] ReentrantReadWriteLock should throw ex
 on writeLock().lock() if read lock held?
In-Reply-To: <63b4e4050609060730n46c4e968qcda7d8052cbd35ad@mail.gmail.com>
References: <44FD23B5.8070802@atlassian.com>	
	<7547a9ff0609050657i3a3cede7xae70f92a5c23bf42@mail.gmail.com>	
	<44FE168D.5080208@atlassian.com>
	<63b4e4050609060730n46c4e968qcda7d8052cbd35ad@mail.gmail.com>
Message-ID: <44FF7692.1020905@atlassian.com>

Thanks Tim,

If lock detection is expensive in the uncontended path then this is an 
entirely sensible explanation.

I would still suggest the docs be clarified though.

Tim Peierls wrote:
> On 9/5/06, *Jed Wesley-Smith* <jed at atlassian.com 
> <mailto:jed at atlassian.com>> wrote:
>
>     tryLock() does the job, and I don't have a problem using it. I don't
>     bring this up as a particular problem I have (apart from the first
>     time
>     I na?vely used it :-) , but as a general discussion point. If some
>     code
>     does call writeLock().lock() while holding a read lock it will block
>     forever - which seems to me undesirable.
>
>
> Yes, it is undesirable. It is essential to encapsulate access to the 
> RWL to ensure that client code cannot get the program into such 
> undesirable states.
>
> There are sharp edges in java.util.concurrent.locks. These are 
> low-level tools; JCiP treats them as an advanced topic. It wouldn't be 
> right to add expensive runtime checks to protect developers who fail 
> to encapsulate their use of these tools.
-- 
cheers,
- jed.



From hanson.char at gmail.com  Thu Sep  7 02:13:46 2006
From: hanson.char at gmail.com (Hanson Char)
Date: Wed, 6 Sep 2006 23:13:46 -0700
Subject: [concurrency-interest] ConcurrentLinkedBlockingQueue ?
Message-ID: <ca53c8f80609062313h502eb3f9n9217820f4d8b46a0@mail.gmail.com>

Hi,

I've been wondering why there is ConcurrentLinkedQueue in Java 5+, but not
something like a ConcurrentLinkedBlockingQueue, which would allow the client
to block on an empty queue via a "take" method, or block on an empty for a
limited time via a "poll" method.

Please find below an attempt to build such queue on other existing
structures.

Any concurrency problem with these classes ?  Is there a better way ?

Hanson

-------------------------------------------
public class ConcurrentLinkedBlockingQueue<E> extends AbstractQueue<E>
    implements java.io.Serializable
{
    private final ConcurrentLinkedQueue<E> q;
    // See below for details of RelaxedSemaphore
    private final RelaxedSemaphore sem = new RelaxedSemaphore(1);

    public ConcurrentLinkedBlockingQueue() {
        q = new ConcurrentLinkedQueue<E>();
    }

    public ConcurrentLinkedBlockingQueue(Collection<? extends E> c) {
        q = new ConcurrentLinkedQueue<E>(c);
    }

    @Override
    public Iterator<E> iterator() {
        return q.iterator();
    }

    @Override
    public int size() {
        return q.size();
    }

    public boolean offer(E e) {
        boolean b = q.offer(e);
        sem.release();
        return b;
    }

    public E peek() {
        return q.peek();
    }

    public E poll() {
        return q.poll();
    }

    /**
     * Retrieves and removes the head of this queue, waiting if necessary
     * until an element becomes available.
     *
     * @return the head of this queue
     * @throws InterruptedException if interrupted while waiting
     */
    public E take() throws InterruptedException
    {
        for (;;)
        {
            E e = q.poll();

            if (e != null)
                return e;
            final int permits = sem.availablePermits();
            if (permits > 1)
                sem.reducePermits(permits-1);
            sem.acquire();
        }
    }

    /**
     * Retrieves and removes the head of this queue, waiting up to the
     * specified wait time if necessary for an element to become available.
     *
     * @param timeout how long to wait before giving up, in units of
     *        <tt>unit</tt>
     * @param unit a <tt>TimeUnit</tt> determining how to interpret the
     *        <tt>timeout</tt> parameter
     * @return the head of this queue, or <tt>null</tt> if the
     *         specified waiting time elapses before an element is available
     * @throws InterruptedException if interrupted while waiting
     */
    public E poll(long timeout, TimeUnit unit) throws InterruptedException
    {
        for (;;)
        {
            E e = q.poll();

            if (e != null)
                return e;
            final int permits = sem.availablePermits();
            if (permits > 1)
                sem.reducePermits(permits-1);

            if (!sem.tryAcquire(timeout, unit))
                return null;    // timeout
        }
    }
}

------------------------
// make reducePermits public rather than protected
public class RelaxedSemaphore extends Semaphore {
    private static final long serialVersionUID = -3533893015748638023L;

    public RelaxedSemaphore(int permits) {
        super(permits);
    }

    public RelaxedSemaphore(int permits, boolean fair) {
        super(permits, fair);
    }

    @Override
    public void reducePermits(int reduction) {
        super.reducePermits(reduction);
    }
}
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060906/8a07a3a2/attachment.html 

From dl at cs.oswego.edu  Thu Sep  7 07:24:46 2006
From: dl at cs.oswego.edu (Doug Lea)
Date: Thu, 07 Sep 2006 07:24:46 -0400
Subject: [concurrency-interest] ConcurrentLinkedBlockingQueue ?
In-Reply-To: <ca53c8f80609062313h502eb3f9n9217820f4d8b46a0@mail.gmail.com>
References: <ca53c8f80609062313h502eb3f9n9217820f4d8b46a0@mail.gmail.com>
Message-ID: <4500017E.4040202@cs.oswego.edu>

Hanson Char wrote:
> Hi,
> 
> I've been wondering why there is ConcurrentLinkedQueue in Java 5+, but 
> not something like a ConcurrentLinkedBlockingQueue, which would allow 
> the client to block on an empty queue via a "take" method, or block on 
> an empty for a limited time via a "poll" method.

This IS a good thought.

> 
> Please find below an attempt to build such queue on other existing 
> structures.
> 
> Any concurrency problem with these classes ?  Is there a better way ?
> 

This way works, but reduces concurrency by using a single semaphore,
so is a bit less scalable than current LinkedBlockingQueue. However,
there is a path to much better scalability by using the "dual-queue"
approach similar to what we did for Java 6 SynchronousQueue.
My initial intent was to find a way to internally use such
techniques to replace the unbounded case of LinkedBlockingQueue.
But this turns out not to work out too well because of all the
little compatibility problems encountered -- for example, maintaining
the same Serialization form. So it is more likely that we'll
put out a separate ConcurrentLinkedBlockingQueue that will be
preferable to LinkedBlockingQueue unless you need capacity constraints.
Stay tuned for it...

-Doug


From gergg at cox.net  Wed Sep  6 19:07:33 2006
From: gergg at cox.net (Gregg Wonderly)
Date: Wed, 06 Sep 2006 18:07:33 -0500
Subject: [concurrency-interest] hprof reporting LockSupport.park()
 taking highest CPU usage
In-Reply-To: <2372.59.92.136.41.1157555214.squirrel@webmail.sonoasystems.com>
References: <2372.59.92.136.41.1157555214.squirrel@webmail.sonoasystems.com>
Message-ID: <44FF54B5.5010506@cox.net>

Rajesh Balamohan wrote:
> What I am wondering is, whether the JVM is internally spinning on the CPU
> rather than really waiting. Probably that is making this method to crop up
> to the top of the list. If so, I guess I need to look for a parameter
> which can disable that option so that the CPU cycle can be used for some
> other work rather than spinning.

The profiler only knows the amount of wall clock time spent underneath a 
particular part of the call graph/tree of methods.  It is telling you that most 
of the wall clock time is spent at park.  This just indicates the places where 
your application is 'waiting' for something to do, and is thus idle time, not 
busy time.

Gregg Wonderly


From hanson.char at gmail.com  Thu Sep  7 12:20:31 2006
From: hanson.char at gmail.com (Hanson Char)
Date: Thu, 7 Sep 2006 09:20:31 -0700
Subject: [concurrency-interest] Changing delays in DelayQueue ?
Message-ID: <ca53c8f80609070920i4479add1o59fa715a2f0a59f4@mail.gmail.com>

Javadoc of j.u.c.DelayQueue:

    "The *head* of the queue is that Delayed element whose delay expired
furthest in the past."

I've been wondering, after the elements have been enqueued, if the above
"invariant" could still be maintained even if the delays of the elements are
modified (such that the order of the delays become different from those as
at the time when the elements were inserted.)

Example,

1) enqueue elements [a, b], whereas a.delay < b.delay
2) modify a and b such that b.delay < a.delay
3) dequeue elements

It would be nice if (3) could result in the order of [b,a], instead of
[a,b], which is the existing behavior of DelayQueue.

Is there an easy way to achieve the desired behavior without compromising
concurrency ?

Hanson
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060907/5d657daf/attachment.html 

From tim at peierls.net  Thu Sep  7 17:13:20 2006
From: tim at peierls.net (Tim Peierls)
Date: Thu, 7 Sep 2006 17:13:20 -0400
Subject: [concurrency-interest] Changing delays in DelayQueue ?
In-Reply-To: <ca53c8f80609070920i4479add1o59fa715a2f0a59f4@mail.gmail.com>
References: <ca53c8f80609070920i4479add1o59fa715a2f0a59f4@mail.gmail.com>
Message-ID: <63b4e4050609071413y61559511v890305c740ebd15c@mail.gmail.com>

You can remove a and b, change their delays, and then put them back into the
delay queue.

Too bad we couldn't find a nice way for PriorityQueue (and PBQ) to support a
decreaseKey operation.

--tim

On 9/7/06, Hanson Char <hanson.char at gmail.com> wrote:
>
> Javadoc of j.u.c.DelayQueue:
>
>     "The *head* of the queue is that Delayed element whose delay expired
> furthest in the past."
>
> I've been wondering, after the elements have been enqueued, if the above
> "invariant" could still be maintained even if the delays of the elements are
> modified (such that the order of the delays become different from those as
> at the time when the elements were inserted.)
>
> Example,
>
> 1) enqueue elements [a, b], whereas a.delay < b.delay
> 2) modify a and b such that b.delay < a.delay
> 3) dequeue elements
>
> It would be nice if (3) could result in the order of [b,a], instead of
> [a,b], which is the existing behavior of DelayQueue.
>
> Is there an easy way to achieve the desired behavior without compromising
> concurrency ?
>
> Hanson
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060907/d7370fb8/attachment.html 

From hanson.char at gmail.com  Thu Sep  7 18:21:13 2006
From: hanson.char at gmail.com (Hanson Char)
Date: Thu, 7 Sep 2006 15:21:13 -0700
Subject: [concurrency-interest] Changing delays in DelayQueue ?
In-Reply-To: <63b4e4050609071413y61559511v890305c740ebd15c@mail.gmail.com>
References: <ca53c8f80609070920i4479add1o59fa715a2f0a59f4@mail.gmail.com>
	<63b4e4050609071413y61559511v890305c740ebd15c@mail.gmail.com>
Message-ID: <ca53c8f80609071521v3e860fack40f3ade8f733b854@mail.gmail.com>

Wouldn't the DelayQueue.remove(Object) take O(n) time ?  And wouldn't that
lock the entire queue in the meantime ?  If so, the concurrency is
significantly compromised!

Hanson

On 9/7/06, Tim Peierls <tim at peierls.net> wrote:
>
> You can remove a and b, change their delays, and then put them back into
> the delay queue.
>
> Too bad we couldn't find a nice way for PriorityQueue (and PBQ) to support
> a decreaseKey operation.
>
> --tim
>
>  On 9/7/06, Hanson Char <hanson.char at gmail.com> wrote:
>
> > Javadoc of j.u.c.DelayQueue:
>
>     "The *head* of the queue is that Delayed element whose delay expired
> furthest in the past."
>
> I've been wondering, after the elements have been enqueued, if the above
> "invariant" could still be maintained even if the delays of the elements are
> modified (such that the order of the delays become different from those as
> at the time when the elements were inserted.)
>
> Example,
>
> 1) enqueue elements [a, b], whereas a.delay < b.delay
> 2) modify a and b such that b.delay < a.delay
> 3) dequeue elements
>
> It would be nice if (3) could result in the order of [b,a], instead of
> [a,b], which is the existing behavior of DelayQueue.
>
> Is there an easy way to achieve the desired behavior without compromising
> concurrency ?
>
> Hanson
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060907/44525f2c/attachment.html 

From tim at peierls.net  Thu Sep  7 20:03:00 2006
From: tim at peierls.net (Tim Peierls)
Date: Thu, 7 Sep 2006 20:03:00 -0400
Subject: [concurrency-interest] Changing delays in DelayQueue ?
In-Reply-To: <ca53c8f80609071521v3e860fack40f3ade8f733b854@mail.gmail.com>
References: <ca53c8f80609070920i4479add1o59fa715a2f0a59f4@mail.gmail.com>
	<63b4e4050609071413y61559511v890305c740ebd15c@mail.gmail.com>
	<ca53c8f80609071521v3e860fack40f3ade8f733b854@mail.gmail.com>
Message-ID: <63b4e4050609071703x5831fcbas652da72b02a0ffbd@mail.gmail.com>

I was assuming the change of delay was a rare occurrence.

As I said, too bad we don't have decreaseKey.

If you are free to use Mustang, consider rolling your own with
ConcurrentSkipListMap.

--tim

On 9/7/06, Hanson Char <hanson.char at gmail.com> wrote:
>
> Wouldn't the DelayQueue.remove(Object) take O(n) time ?  And wouldn't that
> lock the entire queue in the meantime ?  If so, the concurrency is
> significantly compromised!
>
> Hanson
>
>
> On 9/7/06, Tim Peierls <tim at peierls.net> wrote:
> >
> > You can remove a and b, change their delays, and then put them back into
> > the delay queue.
> >
> > Too bad we couldn't find a nice way for PriorityQueue (and PBQ) to
> > support a decreaseKey operation.
> >
> > --tim
> >
> >  On 9/7/06, Hanson Char <hanson.char at gmail.com> wrote:
> >
> > > Javadoc of j.u.c.DelayQueue:
> >
> >     "The *head* of the queue is that Delayed element whose delay expired
> > furthest in the past."
> >
> > I've been wondering, after the elements have been enqueued, if the above
> > "invariant" could still be maintained even if the delays of the elements are
> > modified (such that the order of the delays become different from those as
> > at the time when the elements were inserted.)
> >
> > Example,
> >
> > 1) enqueue elements [a, b], whereas a.delay < b.delay
> > 2) modify a and b such that b.delay < a.delay
> > 3) dequeue elements
> >
> > It would be nice if (3) could result in the order of [b,a], instead of
> > [a,b], which is the existing behavior of DelayQueue.
> >
> > Is there an easy way to achieve the desired behavior without
> > compromising concurrency ?
> >
> >
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060907/17e9d51d/attachment.html 

From hanson.char at gmail.com  Sat Sep  9 21:18:54 2006
From: hanson.char at gmail.com (Hanson Char)
Date: Sat, 9 Sep 2006 18:18:54 -0700
Subject: [concurrency-interest] ConcurrentLinkedBlockingQueue ?
In-Reply-To: <4500017E.4040202@cs.oswego.edu>
References: <ca53c8f80609062313h502eb3f9n9217820f4d8b46a0@mail.gmail.com>
	<4500017E.4040202@cs.oswego.edu>
Message-ID: <ca53c8f80609091818x71495fbbtf598233fe69c3271@mail.gmail.com>

Hi Doug,

>This way works, but reduces concurrency by using a single semaphore,
>so is a bit less scalable than current LinkedBlockingQueue.

I think I found a better way that seems promising enough to provide better
scalability than LinkedBlockingQueue.   Details below.

This time the constructs I use include
* an extra ConcurrentLinkedQueue for the parking threads
* volatile for marking if a thread is potentially parked or not
* LockSupport.[un]park[Nano])

Does this work ?

Hanson

public class ConcurrentLinkedBlockingQueue<E> extends AbstractQueue<E>
        implements java.io.Serializable
{
    private static class ThreadMarker {
        final Thread thread;
        // assumed parked until found otherwise.
        volatile boolean parked = true;

        ThreadMarker(Thread thread)
        {
            this.thread = thread;
        }
    }

    private final ConcurrentLinkedQueue<ThreadMarker> parkq = new
ConcurrentLinkedQueue<ThreadMarker>();

    private final ConcurrentLinkedQueue<E> q;

    public ConcurrentLinkedBlockingQueue() {
        q = new ConcurrentLinkedQueue<E>();
    }

    public ConcurrentLinkedBlockingQueue(Collection<? extends E> c) {
        q = new ConcurrentLinkedQueue<E>(c);
    }

    @Override
    public Iterator<E> iterator() {
        return q.iterator();
    }

    @Override
    public int size() {
        return q.size();
    }

    public boolean offer(E e) {
        boolean b = q.offer(e);

        for (;;)
        {
            ThreadMarker marker = parkq.poll();

            if (marker == null)
                break;
            if (marker.parked)
            {
                LockSupport.unpark(marker.thread);
                break;
            }
        }
        return b;
    }

    public E peek() {
        return q.peek();
    }

    public E poll() {
        return q.poll();
    }

    /**
     * Retrieves and removes the head of this queue, waiting if necessary
until
     * an element becomes available.
     *
     * @return the head of this queue
     * @throws InterruptedException
     *             if interrupted while waiting
     */
    public E take() throws InterruptedException {
        for (;;) {
            E e = q.poll();

            if (e != null)
                return e;
            ThreadMarker m = new ThreadMarker(Thread.currentThread());
            parkq.offer(m);
            // check again in case there is data race
            e = q.poll();

            if (e != null)
            {
                // data race indeed
                m.parked = false;
                return e;
            }
            LockSupport.park();
        }
    }

    /**
     * Retrieves and removes the head of this queue, waiting up to the
specified
     * wait time if necessary for an element to become available.
     *
     * @param timeout
     *            how long to wait before giving up, in units of
<tt>unit</tt>
     * @param unit
     *            a <tt>TimeUnit</tt> determining how to interpret the
     *            <tt>timeout</tt> parameter
     * @return the head of this queue, or <tt>null</tt> if the specified
     *         waiting time elapses before an element is available
     * @throws InterruptedException
     *             if interrupted while waiting
     */
    public E poll(long timeout, TimeUnit unit) {
        for (;;) {
            E e = q.poll();

            if (e != null)
                return e;
            ThreadMarker m = new ThreadMarker(Thread.currentThread());
            parkq.offer(m);
            e = q.poll();

            if (e != null) {
                m.parked = false;
                return e;
            }
            LockSupport.parkNanos(unit.toNanos(timeout));
        }
    }
}
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060909/32a35ba3/attachment.html 

From hanson.char at gmail.com  Sat Sep  9 23:51:35 2006
From: hanson.char at gmail.com (Hanson Char)
Date: Sat, 9 Sep 2006 20:51:35 -0700
Subject: [concurrency-interest] ConcurrentLinkedBlockingQueue ?
In-Reply-To: <ca53c8f80609091818x71495fbbtf598233fe69c3271@mail.gmail.com>
References: <ca53c8f80609062313h502eb3f9n9217820f4d8b46a0@mail.gmail.com>
	<4500017E.4040202@cs.oswego.edu>
	<ca53c8f80609091818x71495fbbtf598233fe69c3271@mail.gmail.com>
Message-ID: <ca53c8f80609092051u7f6f8b24n4882c96acbc45fb6@mail.gmail.com>

One benefit of such ConcurrentLinkedBlockingQueue is that it is unbounded,
where as even LinkedBlockingQueue has a max capacity of Integer.MAX_VALUE.

Hanson

On 9/9/06, Hanson Char <hanson.char at gmail.com> wrote:
>
> Hi Doug,
>
>
> >This way works, but reduces concurrency by using a single semaphore,
> >so is a bit less scalable than current LinkedBlockingQueue.
>
> I think I found a better way that seems promising enough to provide better
> scalability than LinkedBlockingQueue.   Details below.
>
> This time the constructs I use include
> * an extra ConcurrentLinkedQueue for the parking threads
> * volatile for marking if a thread is potentially parked or not
> * LockSupport.[un]park[Nano])
>
> Does this work ?
>
>
> Hanson
>
> public class ConcurrentLinkedBlockingQueue<E> extends AbstractQueue<E>
>         implements java.io.Serializable
> {
>     private static class ThreadMarker {
>         final Thread thread;
>         // assumed parked until found otherwise.
>         volatile boolean parked = true;
>
>         ThreadMarker(Thread thread)
>         {
>             this.thread = thread;
>         }
>     }
>
>     private final ConcurrentLinkedQueue<ThreadMarker> parkq = new
> ConcurrentLinkedQueue<ThreadMarker>();
>
>
>     private final ConcurrentLinkedQueue<E> q;
>
>     public ConcurrentLinkedBlockingQueue() {
>         q = new ConcurrentLinkedQueue<E>();
>     }
>
>     public ConcurrentLinkedBlockingQueue(Collection<? extends E> c) {
>         q = new ConcurrentLinkedQueue<E>(c);
>     }
>
>     @Override
>     public Iterator<E> iterator() {
>         return q.iterator();
>     }
>
>     @Override
>     public int size() {
>         return q.size();
>     }
>
>     public boolean offer(E e) {
>         boolean b = q.offer(e);
>
>         for (;;)
>         {
>             ThreadMarker marker = parkq.poll();
>
>             if (marker == null)
>                 break;
>             if ( marker.parked)
>             {
>                 LockSupport.unpark(marker.thread);
>                 break;
>
>             }
>         }
>         return b;
>     }
>
>     public E peek() {
>         return q.peek ();
>     }
>
>     public E poll() {
>         return q.poll();
>     }
>
>     /**
>      * Retrieves and removes the head of this queue, waiting if necessary
> until
>      * an element becomes available.
>      *
>      * @return the head of this queue
>      * @throws InterruptedException
>      *             if interrupted while waiting
>      */
>     public E take() throws InterruptedException {
>         for (;;) {
>             E e = q.poll();
>
>             if (e != null)
>                 return e;
>             ThreadMarker m = new ThreadMarker(Thread.currentThread());
>             parkq.offer(m);
>             // check again in case there is data race
>
>             e = q.poll();
>
>             if (e != null)
>             {
>                 // data race indeed
>                 m.parked = false;
>                 return e;
>             }
>             LockSupport.park ();
>
>         }
>     }
>
>     /**
>      * Retrieves and removes the head of this queue, waiting up to the
> specified
>      * wait time if necessary for an element to become available.
>      *
>      * @param timeout
>      *            how long to wait before giving up, in units of
> <tt>unit</tt>
>      * @param unit
>      *            a <tt>TimeUnit</tt> determining how to interpret the
>      *            <tt>timeout</tt> parameter
>      * @return the head of this queue, or <tt>null</tt> if the specified
>      *         waiting time elapses before an element is available
>      * @throws InterruptedException
>      *             if interrupted while waiting
>      */
>     public E poll(long timeout, TimeUnit unit) {
>
>         for (;;) {
>             E e = q.poll();
>
>             if (e != null)
>                 return e;
>             ThreadMarker m = new ThreadMarker( Thread.currentThread());
>             parkq.offer(m);
>             e = q.poll();
>
>             if (e != null) {
>                 m.parked = false;
>                 return e;
>             }
>             LockSupport.parkNanos (unit.toNanos(timeout));
>         }
>     }
> }
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060909/74e43ee0/attachment.html 

From hanson.char at gmail.com  Sun Sep 10 02:21:55 2006
From: hanson.char at gmail.com (Hanson Char)
Date: Sat, 9 Sep 2006 23:21:55 -0700
Subject: [concurrency-interest] ConcurrentLinkedBlockingQueue ?
In-Reply-To: <ca53c8f80609092051u7f6f8b24n4882c96acbc45fb6@mail.gmail.com>
References: <ca53c8f80609062313h502eb3f9n9217820f4d8b46a0@mail.gmail.com>
	<4500017E.4040202@cs.oswego.edu>
	<ca53c8f80609091818x71495fbbtf598233fe69c3271@mail.gmail.com>
	<ca53c8f80609092051u7f6f8b24n4882c96acbc45fb6@mail.gmail.com>
Message-ID: <ca53c8f80609092321y2f585d28k5073d7e73f02416a@mail.gmail.com>

There is a bug in poll(long, TimeUnit).  See below for the fix.

H

    public E poll(long timeout, TimeUnit unit) throws InterruptedException {
        if (timeout < 0)
            return take();    // treat -ve timeout same as to wait forever
        long t0=0;

        for (;;) {
            E e = q.poll();

            if (e != null)
                return e;
            if (t0 > 0 && System.nanoTime() >= (t0 + unit.toNanos(timeout)))
                return null;    // time out
            ThreadMarker m = new ThreadMarker(Thread.currentThread());
            parkq.offer(m);
            e = q.poll();

            if (e != null) {
                m.parked = false;
                return e;
            }
            t0 = System.nanoTime();
            LockSupport.parkNanos(unit.toNanos(timeout));
        }
    }


On 9/9/06, Hanson Char <hanson.char at gmail.com> wrote:
>
> One benefit of such ConcurrentLinkedBlockingQueue is that it is unbounded,
> where as even LinkedBlockingQueue has a max capacity of Integer.MAX_VALUE.
>
> Hanson
>
>
> On 9/9/06, Hanson Char <hanson.char at gmail.com> wrote:
> >
> > Hi Doug,
> >
> >
> > >This way works, but reduces concurrency by using a single semaphore,
> > >so is a bit less scalable than current LinkedBlockingQueue.
> >
> > I think I found a better way that seems promising enough to provide
> > better scalability than LinkedBlockingQueue.   Details below.
> >
> > This time the constructs I use include
> > * an extra ConcurrentLinkedQueue for the parking threads
> > * volatile for marking if a thread is potentially parked or not
> > * LockSupport.[un]park[Nano])
> >
> > Does this work ?
> >
> >
> > Hanson
> >
> > public class ConcurrentLinkedBlockingQueue<E> extends AbstractQueue<E>
> >         implements java.io.Serializable
> > {
> >     private static class ThreadMarker {
> >         final Thread thread;
> >         // assumed parked until found otherwise.
> >         volatile boolean parked = true;
> >
> >         ThreadMarker(Thread thread)
> >         {
> >             this.thread = thread;
> >         }
> >     }
> >
> >     private final ConcurrentLinkedQueue<ThreadMarker> parkq = new
> > ConcurrentLinkedQueue<ThreadMarker>();
> >
> >
> >     private final ConcurrentLinkedQueue<E> q;
> >
> >     public ConcurrentLinkedBlockingQueue() {
> >         q = new ConcurrentLinkedQueue<E>();
> >     }
> >
> >     public ConcurrentLinkedBlockingQueue(Collection<? extends E> c) {
> >         q = new ConcurrentLinkedQueue<E>(c);
> >     }
> >
> >     @Override
> >     public Iterator<E> iterator() {
> >         return q.iterator();
> >     }
> >
> >     @Override
> >     public int size() {
> >         return q.size();
> >     }
> >
> >     public boolean offer(E e) {
> >         boolean b = q.offer(e);
> >
> >         for (;;)
> >         {
> >             ThreadMarker marker = parkq.poll();
> >
> >             if (marker == null)
> >                 break;
> >             if ( marker.parked)
> >             {
> >                 LockSupport.unpark(marker.thread);
> >                 break;
> >
> >             }
> >         }
> >         return b;
> >     }
> >
> >     public E peek() {
> >         return q.peek ();
> >     }
> >
> >     public E poll() {
> >         return q.poll();
> >     }
> >
> >     /**
> >      * Retrieves and removes the head of this queue, waiting if
> > necessary until
> >      * an element becomes available.
> >      *
> >      * @return the head of this queue
> >      * @throws InterruptedException
> >      *             if interrupted while waiting
> >      */
> >     public E take() throws InterruptedException {
> >         for (;;) {
> >             E e = q.poll();
> >
> >             if (e != null)
> >                 return e;
> >             ThreadMarker m = new ThreadMarker(Thread.currentThread());
> >             parkq.offer(m);
> >             // check again in case there is data race
> >
> >             e = q.poll();
> >
> >             if (e != null)
> >             {
> >                 // data race indeed
> >                 m.parked = false;
> >                 return e;
> >             }
> >             LockSupport.park ();
> >
> >         }
> >     }
> >
> >     /**
> >      * Retrieves and removes the head of this queue, waiting up to the
> > specified
> >      * wait time if necessary for an element to become available.
> >      *
> >      * @param timeout
> >      *            how long to wait before giving up, in units of
> > <tt>unit</tt>
> >      * @param unit
> >      *            a <tt>TimeUnit</tt> determining how to interpret the
> >      *            <tt>timeout</tt> parameter
> >      * @return the head of this queue, or <tt>null</tt> if the specified
> >      *         waiting time elapses before an element is available
> >      * @throws InterruptedException
> >      *             if interrupted while waiting
> >      */
> >     public E poll(long timeout, TimeUnit unit) {
> >
> >         for (;;) {
> >             E e = q.poll();
> >
> >             if (e != null)
> >                 return e;
> >             ThreadMarker m = new ThreadMarker( Thread.currentThread());
> >             parkq.offer(m);
> >             e = q.poll();
> >
> >             if (e != null) {
> >                 m.parked = false;
> >                 return e;
> >             }
> >             LockSupport.parkNanos (unit.toNanos(timeout));
> >         }
> >     }
> > }
> >
> >
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060909/fede6011/attachment-0001.html 

From hanson.char at gmail.com  Sun Sep 10 02:58:56 2006
From: hanson.char at gmail.com (Hanson Char)
Date: Sat, 9 Sep 2006 23:58:56 -0700
Subject: [concurrency-interest] ConcurrentLinkedBlockingQueue ?
In-Reply-To: <ca53c8f80609092321y2f585d28k5073d7e73f02416a@mail.gmail.com>
References: <ca53c8f80609062313h502eb3f9n9217820f4d8b46a0@mail.gmail.com>
	<4500017E.4040202@cs.oswego.edu>
	<ca53c8f80609091818x71495fbbtf598233fe69c3271@mail.gmail.com>
	<ca53c8f80609092051u7f6f8b24n4882c96acbc45fb6@mail.gmail.com>
	<ca53c8f80609092321y2f585d28k5073d7e73f02416a@mail.gmail.com>
Message-ID: <ca53c8f80609092358o7f9c08dcrdea39e87e71ee580@mail.gmail.com>

Some initial load testings (with 100,000 inserts by 10 concurrent threads
into the same queue) shows the ConcurrentLinkedBlockingQueue is consistently
faster than the LinkedBlockingQueue.  If interested, I can also post the
junit tests code.

Hanson

On 9/9/06, Hanson Char <hanson.char at gmail.com> wrote:
>
> There is a bug in poll(long, TimeUnit).  See below for the fix.
>
> H
>
>     public E poll(long timeout, TimeUnit unit) throws InterruptedException
> {
>         if (timeout < 0)
>             return take();    // treat -ve timeout same as to wait forever
>
>         long t0=0;
>
>
>         for (;;) {
>             E e = q.poll();
>
>             if (e != null)
>                 return e;
>             if (t0 > 0 && System.nanoTime() >= (t0 + unit.toNanos
> (timeout)))
>                 return null;    // time out
>
>             ThreadMarker m = new ThreadMarker(Thread.currentThread());
>             parkq.offer(m);
>             e = q.poll();
>
>             if (e != null) {
>                 m.parked = false;
>                 return e;
>             }
>             t0 = System.nanoTime();
>
>             LockSupport.parkNanos(unit.toNanos(timeout));
>         }
>     }
>
>
> On 9/9/06, Hanson Char <hanson.char at gmail.com> wrote:
> >
> > One benefit of such ConcurrentLinkedBlockingQueue is that it is
> > unbounded, where as even LinkedBlockingQueue has a max capacity of
> > Integer.MAX_VALUE.
> >
> > Hanson
> >
> >
> > On 9/9/06, Hanson Char <hanson.char at gmail.com> wrote:
> > >
> > > Hi Doug,
> > >
> > >
> > > >This way works, but reduces concurrency by using a single semaphore,
> > > >so is a bit less scalable than current LinkedBlockingQueue.
> > >
> > > I think I found a better way that seems promising enough to provide
> > > better scalability than LinkedBlockingQueue.   Details below.
> > >
> > > This time the constructs I use include
> > > * an extra ConcurrentLinkedQueue for the parking threads
> > > * volatile for marking if a thread is potentially parked or not
> > > * LockSupport.[un]park[Nano])
> > >
> > > Does this work ?
> > >
> > >
> > > Hanson
> > >
> > > public class ConcurrentLinkedBlockingQueue<E> extends AbstractQueue<E>
> > >         implements java.io.Serializable
> > > {
> > >     private static class ThreadMarker {
> > >         final Thread thread;
> > >         // assumed parked until found otherwise.
> > >         volatile boolean parked = true;
> > >
> > >         ThreadMarker(Thread thread)
> > >         {
> > >             this.thread = thread;
> > >         }
> > >     }
> > >
> > >     private final ConcurrentLinkedQueue<ThreadMarker> parkq = new
> > > ConcurrentLinkedQueue<ThreadMarker>();
> > >
> > >
> > >     private final ConcurrentLinkedQueue<E> q;
> > >
> > >     public ConcurrentLinkedBlockingQueue() {
> > >         q = new ConcurrentLinkedQueue<E>();
> > >     }
> > >
> > >     public ConcurrentLinkedBlockingQueue(Collection<? extends E> c) {
> > >         q = new ConcurrentLinkedQueue<E>(c);
> > >     }
> > >
> > >     @Override
> > >     public Iterator<E> iterator() {
> > >         return q.iterator();
> > >     }
> > >
> > >     @Override
> > >     public int size() {
> > >         return q.size();
> > >     }
> > >
> > >     public boolean offer(E e) {
> > >         boolean b = q.offer(e);
> > >
> > >         for (;;)
> > >         {
> > >             ThreadMarker marker = parkq.poll();
> > >
> > >             if (marker == null)
> > >                 break;
> > >             if ( marker.parked)
> > >             {
> > >                 LockSupport.unpark(marker.thread);
> > >                 break;
> > >
> > >             }
> > >         }
> > >         return b;
> > >     }
> > >
> > >     public E peek() {
> > >         return q.peek ();
> > >     }
> > >
> > >     public E poll() {
> > >         return q.poll();
> > >     }
> > >
> > >     /**
> > >      * Retrieves and removes the head of this queue, waiting if
> > > necessary until
> > >      * an element becomes available.
> > >      *
> > >      * @return the head of this queue
> > >      * @throws InterruptedException
> > >      *             if interrupted while waiting
> > >      */
> > >     public E take() throws InterruptedException {
> > >         for (;;) {
> > >             E e = q.poll();
> > >
> > >             if (e != null)
> > >                 return e;
> > >             ThreadMarker m = new ThreadMarker(Thread.currentThread());
> > >
> > >             parkq.offer(m);
> > >             // check again in case there is data race
> > >
> > >             e = q.poll();
> > >
> > >             if (e != null)
> > >             {
> > >                 // data race indeed
> > >                 m.parked = false;
> > >                 return e;
> > >             }
> > >             LockSupport.park ();
> > >
> > >         }
> > >     }
> > >
> > >     /**
> > >      * Retrieves and removes the head of this queue, waiting up to the
> > > specified
> > >      * wait time if necessary for an element to become available.
> > >      *
> > >      * @param timeout
> > >      *            how long to wait before giving up, in units of
> > > <tt>unit</tt>
> > >      * @param unit
> > >      *            a <tt>TimeUnit</tt> determining how to interpret the
> > >      *            <tt>timeout</tt> parameter
> > >      * @return the head of this queue, or <tt>null</tt> if the
> > > specified
> > >      *         waiting time elapses before an element is available
> > >      * @throws InterruptedException
> > >      *             if interrupted while waiting
> > >      */
> > >     public E poll(long timeout, TimeUnit unit) {
> > >
> > >         for (;;) {
> > >             E e = q.poll();
> > >
> > >             if (e != null)
> > >                 return e;
> > >             ThreadMarker m = new ThreadMarker( Thread.currentThread());
> > >
> > >             parkq.offer(m);
> > >             e = q.poll();
> > >
> > >             if (e != null) {
> > >                 m.parked = false;
> > >                 return e;
> > >             }
> > >             LockSupport.parkNanos (unit.toNanos(timeout));
> > >         }
> > >     }
> > > }
> > >
> > >
> >
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060909/7b79fc43/attachment.html 

From alarmnummer at gmail.com  Sun Sep 10 11:42:14 2006
From: alarmnummer at gmail.com (Peter Veentjer)
Date: Sun, 10 Sep 2006 17:42:14 +0200
Subject: [concurrency-interest] concurrency puzzle
Message-ID: <1466c1d60609100842w3206d82fmb181d77eff7a71b6@mail.gmail.com>

I'm playing with the Java Memory Model documentation. And to check if
my understanding is correct, I'm interested in the following example:

public class A{
   private int x = 10;

   public void foo(){
      x = 20;
      synchronized(this){
         System.out.println( x );
      }
   }
}

If this class is used in a multithreaded environment, what could the
output be? (foo is called only once to make it easy)

20: if the x=20 is written to main memory, the read for println( x )
will return 20.

10: if x=20 isn't written to main memory, but only in local memory,
the write (x=20) could get lost. This is because the local memory is
invalidated when the lock on this is acquired. When x is read for the
println, it will return the value in main memory (and that is 10).

0: the 10 value doesn't need to be visible in main memory because it
isn't published safely. So for the same reasons 10 could be read for
the println, the default value for int (that is a 0) could be read for
the println.

0: a different thread could obtain a partially initialized reference
to A because of reordening. So 0 could be the value of x when it is
read for the println statement. So this is another way the 0 value
could be found.

So my bet would be

20 or 10 or 0.

Is my understanding correct?

From jmanson at cs.umd.edu  Sun Sep 10 12:07:20 2006
From: jmanson at cs.umd.edu (Jeremy Manson)
Date: Sun, 10 Sep 2006 09:07:20 -0700
Subject: [concurrency-interest] concurrency puzzle
In-Reply-To: <1466c1d60609100842w3206d82fmb181d77eff7a71b6@mail.gmail.com>
References: <1466c1d60609100842w3206d82fmb181d77eff7a71b6@mail.gmail.com>
Message-ID: <45043838.3000704@cs.umd.edu>

I'm not sure what you mean by multithreaded environment, because all 
Java code runs in a multithreaded environment.  If the object is 
constructed and foo() runs in a single thread, this program will print 
20.  This is because the write to 20 is the last write to x that 
happens-before the print.  If another thread sneaks in a write to x, 
then it might print something else, but there is no other thread in this 
example.

If, on the other hand, you are worried about the case where one thread 
constructs the object and passes it to another thread via a data race:

Thread 1:
global.a = new A();

Thread 2:
if (global.a != null)
   global.a.foo();

Then it will either print 20 or 10.  It can't print 0 because the write 
of the default value (conceptually) happens-before the first action of 
every thread.  This means that Thread 2's write of 20 to x will 
overwrite the value of 0.

This can be avoided by making global.a volatile, although you still 
might get a NullPointerException in Thread 2.

I'm not sure which JMM documentation you have been examining, if it 
talks about local memory and invalidation.  The old one had kind-of a 
notion of local memory, but it hasn't been the "active" JMM for a good 
while.  Certainly, that JMM didn't talk about safe publication.

And speaking of safe publication - the write of 10 to x happens in the 
constructor, according to the semantics of the Java programming 
language.  If it were a final field (which is where the notion of safe 
publication is important), then it would be considered to be published 
safely, unless a reference to the object escapes an otherwise-unseen 
constructor.

Now, a /different/ thread could see 0 or 10 or 20 for x:

Thread 1:
global.a = new A();
global.a.foo();

Thread 2:
r1 = global.a.x;

r1 can be 0, 10 or 20

It can avoid this by synchronizing properly.

					Jeremy

Peter Veentjer wrote:
> I'm playing with the Java Memory Model documentation. And to check if
> my understanding is correct, I'm interested in the following example:
> 
> public class A{
>    private int x = 10;
> 
>    public void foo(){
>       x = 20;
>       synchronized(this){
>          System.out.println( x );
>       }
>    }
> }
> 
> If this class is used in a multithreaded environment, what could the
> output be? (foo is called only once to make it easy)
> 
> 20: if the x=20 is written to main memory, the read for println( x )
> will return 20.
> 
> 10: if x=20 isn't written to main memory, but only in local memory,
> the write (x=20) could get lost. This is because the local memory is
> invalidated when the lock on this is acquired. When x is read for the
> println, it will return the value in main memory (and that is 10).
> 
> 0: the 10 value doesn't need to be visible in main memory because it
> isn't published safely. So for the same reasons 10 could be read for
> the println, the default value for int (that is a 0) could be read for
> the println.
> 
> 0: a different thread could obtain a partially initialized reference
> to A because of reordening. So 0 could be the value of x when it is
> read for the println statement. So this is another way the 0 value
> could be found.
> 
> So my bet would be
> 
> 20 or 10 or 0.
> 
> Is my understanding correct?
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest


From alarmnummer at gmail.com  Sun Sep 10 14:18:45 2006
From: alarmnummer at gmail.com (Peter Veentjer)
Date: Sun, 10 Sep 2006 20:18:45 +0200
Subject: [concurrency-interest] concurrency puzzle
In-Reply-To: <45043838.3000704@cs.umd.edu>
References: <1466c1d60609100842w3206d82fmb181d77eff7a71b6@mail.gmail.com>
	<45043838.3000704@cs.umd.edu>
Message-ID: <1466c1d60609101118w5fc9785nb2afeb350bb9f6af@mail.gmail.com>

On 9/10/06, Jeremy Manson <jmanson at cs.umd.edu> wrote:
> I'm not sure what you mean by multithreaded environment, because all
> Java code runs in a multithreaded environment.

True. But I mean that the object is used by multiple threads.

> Then it will either print 20 or 10.  It can't print 0 because the write
> of the default value (conceptually) happens-before the first action of
> every thread.
I was not sure about this one. I thought the following would happen
1) a stackframe is allocated with all defaults for the fields (x=0)
2) the initilization for the fields is called (so the x=10)
3) constructor call

I have been looking through the documentation, but the initialisation
(x=10) felt the same as a constructor.

So there is a difference between:

public class A{
    private int a;
    public A(){
       a = 10;
    }
    ...
}

and

public class A{
    private int a = 10;
    ...
}

Is there a difference?

> This can be avoided by making global.a volatile, although you still
> might get a NullPointerException in Thread 2.
I know, but I'm trying to understand what is allowed by the JMM.

> I'm not sure which JMM documentation you have been examining, if it
> talks about local memory and invalidation.  The old one had kind-of a
> notion of local memory, but it hasn't been the "active" JMM for a good
> while.  Certainly, that JMM didn't talk about safe publication.
JSR133, JCIP and Concurrent and Realtime programming in Java.

> And speaking of safe publication - the write of 10 to x happens in the
> constructor, according to the semantics of the Java programming
> language.  If it were a final field (which is where the notion of safe
> publication is important), then it would be considered to be published
> safely, unless a reference to the object escapes an otherwise-unseen
> constructor.
Instruction reordering makes it possible to see a partially constructed object

reg = alloc(A)
reg.x=10
global.a = reg

This could be reordened to:

reg = alloc(A)
global.a = reg
reg.x=10

If another thread picks up a and calls foo before the ref.x=10 is
called, the println could see a zero.

> It can avoid this by synchronizing properly.
Ofcourse... but I'm trying to understand what can happen if programs
are not synchronized propertly. I want to understand how the JMM works
and that is why I made up this example.

>
>                                         Jeremy

Thanks for your reply.

>
> Peter Veentjer wrote:
> > I'm playing with the Java Memory Model documentation. And to check if
> > my understanding is correct, I'm interested in the following example:
> >
> > public class A{
> >    private int x = 10;
> >
> >    public void foo(){
> >       x = 20;
> >       synchronized(this){
> >          System.out.println( x );
> >       }
> >    }
> > }
> >
> > If this class is used in a multithreaded environment, what could the
> > output be? (foo is called only once to make it easy)
> >
> > 20: if the x=20 is written to main memory, the read for println( x )
> > will return 20.
> >
> > 10: if x=20 isn't written to main memory, but only in local memory,
> > the write (x=20) could get lost. This is because the local memory is
> > invalidated when the lock on this is acquired. When x is read for the
> > println, it will return the value in main memory (and that is 10).
> >
> > 0: the 10 value doesn't need to be visible in main memory because it
> > isn't published safely. So for the same reasons 10 could be read for
> > the println, the default value for int (that is a 0) could be read for
> > the println.
> >
> > 0: a different thread could obtain a partially initialized reference
> > to A because of reordening. So 0 could be the value of x when it is
> > read for the println statement. So this is another way the 0 value
> > could be found.
> >
> > So my bet would be
> >
> > 20 or 10 or 0.
> >
> > Is my understanding correct?
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at altair.cs.oswego.edu
> > http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>

From jmanson at cs.umd.edu  Sun Sep 10 14:50:13 2006
From: jmanson at cs.umd.edu (Jeremy Manson)
Date: Sun, 10 Sep 2006 11:50:13 -0700
Subject: [concurrency-interest] concurrency puzzle
In-Reply-To: <1466c1d60609101118w5fc9785nb2afeb350bb9f6af@mail.gmail.com>
References: <1466c1d60609100842w3206d82fmb181d77eff7a71b6@mail.gmail.com>	
	<45043838.3000704@cs.umd.edu>
	<1466c1d60609101118w5fc9785nb2afeb350bb9f6af@mail.gmail.com>
Message-ID: <45045E65.205@cs.umd.edu>

Peter Veentjer wrote:
> On 9/10/06, Jeremy Manson <jmanson at cs.umd.edu> wrote:
>> Then it will either print 20 or 10.  It can't print 0 because the write
>> of the default value (conceptually) happens-before the first action of
>> every thread.
> I was not sure about this one. I thought the following would happen
> 1) a stackframe is allocated with all defaults for the fields (x=0)
> 2) the initilization for the fields is called (so the x=10)
> 3) constructor call

[snip]

 > I have been looking through the documentation, but the initialisation
 > (x=10) felt the same as a constructor.

The initialization is the same as the constructor.  If you look at the 
bytecode generated by javac, you will see that field initializers 
actually occur in the object's constructor.  That was what I meant in my 
previous message, when I said, "the write of 10 to x happens in the
constructor, according to the semantics of the Java programming
language".

Having said that, the fact that print() can't write out 0 has nothing to 
do with the constructing thread.  Basically, there is a conceptual write 
of 0 to x that happens-before every other action in the program, at "The 
Dawn of Time" (this corresponds to the GC zeroing out memory).  Your 
program can be abstracted in the following way:

"The Dawn of Time":
// zeroes out all memory, including:
f.x = 0;

Thread 1:
// The dawn of time happens-before this:
f.x = 10;
global.a = f;

Thread 2:
// The dawn of time happens-before this:
g = global.a;
g.x = 20;
print(g.x);

The execution engine for Thread 2 will know that the write of 20 has 
overwritten the write of 0, because it knows that the write of 0 
occurred at the dawn of time.  It won't print 0.

What you said about Thread 1 is true in general, though: the accesses in 
Thread 1 can be reordered, so that the write to f.x may not be visible 
to Thread 2 when it performs its print.

I hope that is a little clearer than my last message.  It is longer, any 
way.  :)

>> I'm not sure which JMM documentation you have been examining, if it
>> talks about local memory and invalidation.  The old one had kind-of a
>> notion of local memory, but it hasn't been the "active" JMM for a good
>> while.  Certainly, that JMM didn't talk about safe publication.
> JSR133, JCIP and Concurrent and Realtime programming in Java.

JSR 133, which is the official definition, doesn't make mention of local 
memory or invalidation.  If JCiP does (I don't remember), it is only to 
explain the concepts of the official memory model.


> Instruction reordering makes it possible to see a partially constructed 
> object

Yes.  This is why there are additional semantics for final fields.  If x 
were final, even though there would be no write of 20, it would be 
impossible for Thread 2 to print 0.   This is because the correctly 
constructed value of the final field is guaranteed to be seen regardless 
of data races, cache effects and instruction reordering.

					Jeremy

From alarmnummer at gmail.com  Sun Sep 10 17:05:16 2006
From: alarmnummer at gmail.com (Peter Veentjer)
Date: Sun, 10 Sep 2006 23:05:16 +0200
Subject: [concurrency-interest] concurrency puzzle
In-Reply-To: <45045E65.205@cs.umd.edu>
References: <1466c1d60609100842w3206d82fmb181d77eff7a71b6@mail.gmail.com>
	<45043838.3000704@cs.umd.edu>
	<1466c1d60609101118w5fc9785nb2afeb350bb9f6af@mail.gmail.com>
	<45045E65.205@cs.umd.edu>
Message-ID: <1466c1d60609101405x38ad321dsee10000651957e34@mail.gmail.com>

  That was what I meant in my
> previous message, when I said, "the write of 10 to x happens in the
> constructor, according to the semantics of the Java programming
> language".
If A was used by a single thread, this would be the case. But if A is
used by multiple threads I'm not sure. This is the reason why double
checked locking doesn't work for example:

Let me create another example:

class B{
   private int y = 10;

  public void print(){
       System.out.println(y);
  }
}

What do you expect for output (if an instance of B also is used by
different threads)

I would say 0 or 10.

Maybe we can get this issue out of the way because I think it is the
cause misunderstanding.


>
> Having said that, the fact that print() can't write out 0 has nothing to
> do with the constructing thread.  Basically, there is a conceptual write
> of 0 to x that happens-before every other action in the program, at "The
> Dawn of Time" (this corresponds to the GC zeroing out memory).  Your
> program can be abstracted in the following way:
>
> "The Dawn of Time":
> // zeroes out all memory, including:
> f.x = 0;
>
> Thread 1:
> // The dawn of time happens-before this:
> f.x = 10;
> global.a = f;
>
> Thread 2:
> // The dawn of time happens-before this:
> g = global.a;
> g.x = 20;
> print(g.x);
>
> The execution engine for Thread 2 will know that the write of 20 has
> overwritten the write of 0, because it knows that the write of 0
> occurred at the dawn of time.  It won't print 0.
>
> What you said about Thread 1 is true in general, though: the accesses in
> Thread 1 can be reordered, so that the write to f.x may not be visible
> to Thread 2 when it performs its print.
>
> I hope that is a little clearer than my last message.  It is longer, any
> way.  :)
>
> >> I'm not sure which JMM documentation you have been examining, if it
> >> talks about local memory and invalidation.  The old one had kind-of a
> >> notion of local memory, but it hasn't been the "active" JMM for a good
> >> while.  Certainly, that JMM didn't talk about safe publication.
> > JSR133, JCIP and Concurrent and Realtime programming in Java.
>
> JSR 133, which is the official definition, doesn't make mention of local
> memory or invalidation.  If JCiP does (I don't remember), it is only to
> explain the concepts of the official memory model.
>
>
> > Instruction reordering makes it possible to see a partially constructed
> > object
>
> Yes.  This is why there are additional semantics for final fields.  If x
> were final, even though there would be no write of 20, it would be
> impossible for Thread 2 to print 0.   This is because the correctly
> constructed value of the final field is guaranteed to be seen regardless
> of data races, cache effects and instruction reordering.
>
>                                         Jeremy
>

From jmanson at cs.umd.edu  Sun Sep 10 17:37:00 2006
From: jmanson at cs.umd.edu (Jeremy Manson)
Date: Sun, 10 Sep 2006 14:37:00 -0700
Subject: [concurrency-interest] concurrency puzzle
In-Reply-To: <1466c1d60609101405x38ad321dsee10000651957e34@mail.gmail.com>
References: <1466c1d60609100842w3206d82fmb181d77eff7a71b6@mail.gmail.com>	
	<45043838.3000704@cs.umd.edu>	
	<1466c1d60609101118w5fc9785nb2afeb350bb9f6af@mail.gmail.com>	
	<45045E65.205@cs.umd.edu>
	<1466c1d60609101405x38ad321dsee10000651957e34@mail.gmail.com>
Message-ID: <4504857C.3090408@cs.umd.edu>

Peter Veentjer wrote:
>  That was what I meant in my
>> previous message, when I said, "the write of 10 to x happens in the
>> constructor, according to the semantics of the Java programming
>> language".
> If A was used by a single thread, this would be the case. But if A is
> used by multiple threads I'm not sure. This is the reason why double
> checked locking doesn't work for example:

We must be having some crossed communication lines somewhere, because I 
don't think we are talking about the same thing.  All I meant by the 
above was that this:

class A {
   int x = 10;
}

is the same as this:

class A {
   int x;
   public A() {
     x = 10;
   }
}

All javac does with the first is to turn it into the second.  It is 
relatively clear that this is the intention of the JLS, if you read 
Section 8.3.2.  It is what we intended when we wrote JSR-133.

None of this has anything to do with double-checked locking, which won't 
work regardless of whether the singleton object is constructed with its 
fields assigned in variable initializers or in a constructor.

So I'm pretty sure I'm not quite getting what you are saying.

> Let me create another example:
> 
> class B{
>   private int y = 10;
> 
>  public void print(){
>       System.out.println(y);
>  }
> }
> 
> What do you expect for output (if an instance of B also is used by
> different threads)
> 
> I would say 0 or 10.

If one thread constructs an object of type B, and passes a reference to 
that object to another thread via a data race, and that other thread 
invokes print(), then that print statement may emit either 0 or 10.

If, on the other hand, the print() method assigns a value to y before 
the println() statement, thus:

public void print() {
   y = 42;
   System.out.println(y);
}

then the same program would either print out 10 or 42, but not 0.  This 
is because the write of the value 0 to y is deemed to have taken place 
before the invocation of print().  In other words, the write of 0 to y 
is not in a data race with the accesses of y in the print method.  The 
write of 42 is considered to overwrite the value 0.

This is the intent of the fourth bullet point in Section 17.4.4 of the 
JLS, where we say "The write of the default value (zero, false or null) 
to each variable synchronizes-with the first action in every thread".

					Jeremy

From dcholmes at optusnet.com.au  Sun Sep 10 19:16:49 2006
From: dcholmes at optusnet.com.au (David Holmes)
Date: Mon, 11 Sep 2006 09:16:49 +1000
Subject: [concurrency-interest] concurrency puzzle
In-Reply-To: <4504857C.3090408@cs.umd.edu>
Message-ID: <NFBBKALFDCPFIDBNKAPCMEEBHCAA.dcholmes@optusnet.com.au>

Jeremy,

Now you've confused me :)

> If, on the other hand, the print() method assigns a value to y before
> the println() statement, thus:
>
> public void print() {
>    y = 42;
>    System.out.println(y);
> }
>
> then the same program would either print out 10 or 42, but not 0.

Ummm if the assignment occurs in the print method then the subsequent
println must print 42 regardless because the assignment occurred in the same
thread.

Cheers,
David Holmes


From dcholmes at optusnet.com.au  Sun Sep 10 19:22:20 2006
From: dcholmes at optusnet.com.au (David Holmes)
Date: Mon, 11 Sep 2006 09:22:20 +1000
Subject: [concurrency-interest] concurrency puzzle
In-Reply-To: <1466c1d60609100842w3206d82fmb181d77eff7a71b6@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCCEECHCAA.dcholmes@optusnet.com.au>

Peter,

Some confusion seems to have arisen.

>    public void foo(){
>       x = 20;
>       synchronized(this){
>          System.out.println( x );
>       }
>    }
> }
>
> If this class is used in a multithreaded environment, what could the
> output be? (foo is called only once to make it easy)

In the absence of any other methods that write to x then the value of x is
always 20 when the println is invoked. It is irrelevant whether the write to
x is visible to some other thread at that time as no other thread has access
to x. Any thread that executes foo() will do the assignment x=20 and then
print out the value 20 because in program order the write to x
happens-before the read of x used by the println.

It would be a different matter if there was a seperate setter method that
could be called. I think you need to reconsider your example.

Cheers,
David Holmes


From jmanson at cs.umd.edu  Sun Sep 10 19:35:18 2006
From: jmanson at cs.umd.edu (Jeremy Manson)
Date: Sun, 10 Sep 2006 16:35:18 -0700
Subject: [concurrency-interest] concurrency puzzle
In-Reply-To: <NFBBKALFDCPFIDBNKAPCMEEBHCAA.dcholmes@optusnet.com.au>
References: <NFBBKALFDCPFIDBNKAPCMEEBHCAA.dcholmes@optusnet.com.au>
Message-ID: <4504A136.3060108@cs.umd.edu>

David Holmes wrote:
> Jeremy,
> 
> Now you've confused me :)

I do seem to be doing more harm than good here...  :)

>> If, on the other hand, the print() method assigns a value to y before
>> the println() statement, thus:
>>
>> public void print() {
>>    y = 42;
>>    System.out.println(y);
>> }
>>
>> then the same program would either print out 10 or 42, but not 0.
> 
> Ummm if the assignment occurs in the print method then the subsequent
> println must print 42 regardless because the assignment occurred in the same
> thread.

This is why I don't like the way we've been phrasing this example.  We 
haven't actually been showing both threads.  I'll do it a little 
differently (Maybe this will clarify?).  If the class is this:

public class A{
    private int x = 10;
    public static A a;

    public void foo(){
       x = 20;
       synchronized(this){
          System.out.println( x );
       }
    }

    public void bar(){
       synchronized(this){
          System.out.println( x );
       }
    }

}


This is one of the examples we have been discussing:

Thread 1:
A o = new A();
A.a = o;

Thread 2:
A r1 = A.a;
assert r1 != null;
r1.foo(); // can print out 10 or 20

This is the other:

Thread 1:
A o = new A();
A.a = o;

Thread 2:
A r1 = A.a;
assert r1 != null;
A r1.bar(); // can print out 10 or 0


					Jeremy

From dcholmes at optusnet.com.au  Sun Sep 10 20:11:45 2006
From: dcholmes at optusnet.com.au (David Holmes)
Date: Mon, 11 Sep 2006 10:11:45 +1000
Subject: [concurrency-interest] concurrency puzzle
In-Reply-To: <NFBBKALFDCPFIDBNKAPCMEEBHCAA.dcholmes@optusnet.com.au>
Message-ID: <NFBBKALFDCPFIDBNKAPCOEEEHCAA.dcholmes@optusnet.com.au>

I wrote:
> Ummm if the assignment occurs in the print method then the subsequent
> println must print 42 regardless because the assignment occurred
> in the same thread.

As Jeremy has been pointing out you need to see how multiple threads come to
access the instance of the class. What I wrote above is incorrect if the
instance was unsafely published because the write in the constructor due to
the initialization statement could "appear" after the assignment within the
print method.

Sorry for adding to the confusion.

Cheers,
David Holmes


From dcholmes at optusnet.com.au  Sun Sep 10 20:40:00 2006
From: dcholmes at optusnet.com.au (David Holmes)
Date: Mon, 11 Sep 2006 10:40:00 +1000
Subject: [concurrency-interest] concurrency puzzle
In-Reply-To: <45045E65.205@cs.umd.edu>
Message-ID: <NFBBKALFDCPFIDBNKAPCAEEGHCAA.dcholmes@optusnet.com.au>

Jeremy Manson wrote:
> Having said that, the fact that print() can't write out 0 has nothing to
> do with the constructing thread.  Basically, there is a conceptual write
> of 0 to x that happens-before every other action in the program, at "The
> Dawn of Time" (this corresponds to the GC zeroing out memory).  Your
> program can be abstracted in the following way:
>
> "The Dawn of Time":
> // zeroes out all memory, including:
> f.x = 0;
>
> Thread 1:
> // The dawn of time happens-before this:
> f.x = 10;
> global.a = f;
>
> Thread 2:
> // The dawn of time happens-before this:
> g = global.a;
> g.x = 20;
> print(g.x);
>
> The execution engine for Thread 2 will know that the write of 20 has
> overwritten the write of 0, because it knows that the write of 0
> occurred at the dawn of time.  It won't print 0.

So this means that:

   class A {
       int x;
       ...
   }

is semantically different to:

   class A {
       int x = 0;
       ...
   }

because the explicit write of zero to x could "appear" later (even though it
is overwriting a zero value in main memory) just as the value 10 could
"appear" later.

Wow! That is a very subtle distinction. I'm not sure there are any practical
consequences arising from it but I find it disconcerting to have this
difference.

Cheers,
David Holmes


From yangjs at alibaba-inc.com  Sun Sep 10 21:41:31 2006
From: yangjs at alibaba-inc.com (yangjs)
Date: Mon, 11 Sep 2006 09:41:31 +0800
Subject: [concurrency-interest] concurrency puzzle
References: <NFBBKALFDCPFIDBNKAPCCEECHCAA.dcholmes@optusnet.com.au>
Message-ID: <007001c6d543$685bce50$06000200@alibabahz.com>

 >
> Some confusion seems to have arisen.
>
>>    public void foo(){
>>       x = 20;
>>       synchronized(this){
>>          System.out.println( x );
>>       }
>>    }
>> }
.According to JLS ,
    "Each action in a thread happens-before every action in that thread that 
comes later in the program order".

  so the println out only have one result :20






Best regards,


 ???(yangjs)
Tel:0571-85022088-3021
Email:yangjs at alibaba-inc.com
http://www.alibaba.com
----- Original Message ----- 
From: "David Holmes" <dcholmes at optusnet.com.au>
To: "Peter Veentjer" <alarmnummer at gmail.com>; 
<concurrency-interest at cs.oswego.edu>
Sent: Monday, September 11, 2006 7:22 AM
Subject: Re: [concurrency-interest] concurrency puzzle


> Peter,
>
> Some confusion seems to have arisen.
>
>>    public void foo(){
>>       x = 20;
>>       synchronized(this){
>>          System.out.println( x );
>>       }
>>    }
>> }
>>
>> If this class is used in a multithreaded environment, what could the
>> output be? (foo is called only once to make it easy)
>
> In the absence of any other methods that write to x then the value of x is
> always 20 when the println is invoked. It is irrelevant whether the write 
> to
> x is visible to some other thread at that time as no other thread has 
> access
> to x. Any thread that executes foo() will do the assignment x=20 and then
> print out the value 20 because in program order the write to x
> happens-before the read of x used by the println.
>
> It would be a different matter if there was a seperate setter method that
> could be called. I think you need to reconsider your example.
>
> Cheers,
> David Holmes
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
> 

From dcholmes at optusnet.com.au  Sun Sep 10 21:45:28 2006
From: dcholmes at optusnet.com.au (David Holmes)
Date: Mon, 11 Sep 2006 11:45:28 +1000
Subject: [concurrency-interest] concurrency puzzle
In-Reply-To: <007001c6d543$685bce50$06000200@alibabahz.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCMEEGHCAA.dcholmes@optusnet.com.au>

yangjs writes:
> >>    public void foo(){
> >>       x = 20;
> >>       synchronized(this){
> >>          System.out.println( x );
> >>       }
> >>    }
> >> }
> .According to JLS ,
>     "Each action in a thread happens-before every action in that
> thread that comes later in the program order".

Yes

>   so the println out only have one result :20

Not necessarily. If the instance was constructed in a different thread then
the write of x=10 from the constructor can occur after the x=20 and before
the println(x), hence the output can be 10 or 20.

David Holmes


From jmanson at cs.umd.edu  Sun Sep 10 21:48:21 2006
From: jmanson at cs.umd.edu (Jeremy Manson)
Date: Sun, 10 Sep 2006 18:48:21 -0700
Subject: [concurrency-interest] concurrency puzzle
In-Reply-To: <007001c6d543$685bce50$06000200@alibabahz.com>
References: <NFBBKALFDCPFIDBNKAPCCEECHCAA.dcholmes@optusnet.com.au>
	<007001c6d543$685bce50$06000200@alibabahz.com>
Message-ID: <4504C065.4090406@cs.umd.edu>

yangjs wrote:
>  >
>> Some confusion seems to have arisen.
>>
>>>    public void foo(){
>>>       x = 20;
>>>       synchronized(this){
>>>          System.out.println( x );
>>>       }
>>>    }
>>> }
> .According to JLS ,
>     "Each action in a thread happens-before every action in that thread that 
> comes later in the program order".
> 
>   so the println out only have one result :20
> 
> 

This is not correct.  In the examples, as I gave them, there is a write 
in the constructor of 10 to the field x.  That write is in a data race 
with this read of x.  Therefore, the read can see 10, as well.

					Jeremy

From hanson.char at gmail.com  Mon Sep 11 02:40:39 2006
From: hanson.char at gmail.com (Hanson Char)
Date: Sun, 10 Sep 2006 23:40:39 -0700
Subject: [concurrency-interest] ConcurrentLinkedBlockingQueue ?
In-Reply-To: <45044524.4070109@wizards.de>
References: <ca53c8f80609062313h502eb3f9n9217820f4d8b46a0@mail.gmail.com>
	<4500017E.4040202@cs.oswego.edu>
	<ca53c8f80609091818x71495fbbtf598233fe69c3271@mail.gmail.com>
	<ca53c8f80609092051u7f6f8b24n4882c96acbc45fb6@mail.gmail.com>
	<ca53c8f80609092321y2f585d28k5073d7e73f02416a@mail.gmail.com>
	<ca53c8f80609092358o7f9c08dcrdea39e87e71ee580@mail.gmail.com>
	<45044524.4070109@wizards.de>
Message-ID: <ca53c8f80609102340n4dce9bfcg69c51cdb9ea244bf@mail.gmail.com>

>
> Outstanding! Could you please make up-to-date versions of this class and
> its unit test available over the web in a more permanent location? That
> way you don't have to send updates by mail..
>

Latest source:

http://svn.sourceforge.net/viewvc/beanlib/trunk/beanlib/src/net/sf/beanlib/util/concurrent/ConcurrentLinkedBlockingQueue.java

Test classes:

http://svn.sourceforge.net/viewvc/beanlib/trunk/beanlib-test/src/net/sf/beanlib/util/concurrent/

Hanson
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060910/e285a0fb/attachment.html 

From jmanson at cs.umd.edu  Mon Sep 11 03:12:47 2006
From: jmanson at cs.umd.edu (Jeremy Manson)
Date: Mon, 11 Sep 2006 00:12:47 -0700
Subject: [concurrency-interest] concurrency puzzle
In-Reply-To: <NFBBKALFDCPFIDBNKAPCAEEGHCAA.dcholmes@optusnet.com.au>
References: <NFBBKALFDCPFIDBNKAPCAEEGHCAA.dcholmes@optusnet.com.au>
Message-ID: <45050C6F.8060802@cs.umd.edu>

This is off list, as it is probably a little more obscure than most 
people will want to tackle.

David Holmes wrote:

> Wow! That is a very subtle distinction. I'm not sure there are any practical
> consequences arising from it but I find it disconcerting to have this
> difference.

I think it is the least disconcerting of the possible alternatives:

If we picked the semantics of the variable initializer, and gave it to 
the default write, then the default write wouldn't be guaranteed to be 
seen by every thread that reads a reference to that object via a data 
race.  You can't see a garbage value, so what value do you see?

If we picked the semantics of the default write, and gave it to the 
variable initializer, then writes made in constructors would have to 
propagate to every thread that sees a reference to the object.  This 
would have a pretty big performance impact.

If we picked the semantics of the default write, and gave it to only the 
variable initializers that write out default values, then you have far 
worse problems, because you are
	a) creating yet another obscure corner case for Java;
	b) in practice, relying on the compiler or virtual machine to detect 
and remove writes of the default value in a variable initializer, so 
that other threads don't see them out of order.  Not only is this not 
always possible, but it is bad for the spec to require certain 
optimizations to occur; and
	c) (and possibly worst of all) changing the memory semantics based on 
which value is written out to a field.

These are all straw men, but I can't really think of a better "fifth 
way" to do this.

					Jeremy

From dcholmes at optusnet.com.au  Mon Sep 11 03:25:29 2006
From: dcholmes at optusnet.com.au (David Holmes)
Date: Mon, 11 Sep 2006 17:25:29 +1000
Subject: [concurrency-interest] concurrency puzzle
In-Reply-To: <45050C6F.8060802@cs.umd.edu>
Message-ID: <NFBBKALFDCPFIDBNKAPCCEEJHCAA.dcholmes@optusnet.com.au>

Jeremy writes:
> These are all straw men, but I can't really think of a better "fifth
> way" to do this.

I agree there is no better solution. I still find it disconcerting though.
:)

Cheers,
David


From alarmnummer at gmail.com  Mon Sep 11 03:44:34 2006
From: alarmnummer at gmail.com (Peter Veentjer)
Date: Mon, 11 Sep 2006 09:44:34 +0200
Subject: [concurrency-interest] concurrency puzzle
In-Reply-To: <45050C6F.8060802@cs.umd.edu>
References: <NFBBKALFDCPFIDBNKAPCAEEGHCAA.dcholmes@optusnet.com.au>
	<45050C6F.8060802@cs.umd.edu>
Message-ID: <1466c1d60609110044u24562271ra1f46fd0421f78bc@mail.gmail.com>

But what about the behaviour of the synchronized statement? If a lock
is obtained, local memory (for that thread) is invalidated and the
next read is going to access main memory.

That is why the assignment of x=20 could get lost. If the x=20 isn't
written to main memory (and in this case there is no need to) but only
to cache, the value is 'dropped' when the cache is invalidated.

Is this correct? I'm trying to understand the memory behaviour of the
synchronized statement: that is the reason why I made this example.

On 9/11/06, Jeremy Manson <jmanson at cs.umd.edu> wrote:
> This is off list, as it is probably a little more obscure than most
> people will want to tackle.
>
> David Holmes wrote:
>
> > Wow! That is a very subtle distinction. I'm not sure there are any practical
> > consequences arising from it but I find it disconcerting to have this
> > difference.
>
> I think it is the least disconcerting of the possible alternatives:
>
> If we picked the semantics of the variable initializer, and gave it to
> the default write, then the default write wouldn't be guaranteed to be
> seen by every thread that reads a reference to that object via a data
> race.  You can't see a garbage value, so what value do you see?
>
> If we picked the semantics of the default write, and gave it to the
> variable initializer, then writes made in constructors would have to
> propagate to every thread that sees a reference to the object.  This
> would have a pretty big performance impact.
>
> If we picked the semantics of the default write, and gave it to only the
> variable initializers that write out default values, then you have far
> worse problems, because you are
>        a) creating yet another obscure corner case for Java;
>        b) in practice, relying on the compiler or virtual machine to detect
> and remove writes of the default value in a variable initializer, so
> that other threads don't see them out of order.  Not only is this not
> always possible, but it is bad for the spec to require certain
> optimizations to occur; and
>        c) (and possibly worst of all) changing the memory semantics based on
> which value is written out to a field.
>
> These are all straw men, but I can't really think of a better "fifth
> way" to do this.
>
>                                        Jeremy
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>

From jed at atlassian.com  Mon Sep 11 04:35:28 2006
From: jed at atlassian.com (Jed Wesley-Smith)
Date: Mon, 11 Sep 2006 18:35:28 +1000
Subject: [concurrency-interest] blocking executor
Message-ID: <45051FD0.8060708@atlassian.com>

All,

I have been charged with implementing an ExecutorService that has a 
bounded capacity, and if that capacity is reached will block the caller. 
A CallerRunsPolicy is not desired here as the number of concurrent tasks 
executing must be strictly held to the number of threads in the pool, 
and the number of clients adding tasks is indeterminate.

Having looked at the code for a while, it seems a 
BlockingRejectedExectionHandler should do the trick eg.

RejectedExecutionHandler blocker = new RejectedExecutionHandler()
{
    public void rejectedExecution(Runnable r, ThreadPoolExecutor 
executor)throws RejectedExecutionException
    {
        if (executor.isShutdown())
        {
            throw new RejectedExecutionException(executor + " is 
shutdown!");
        }
        try
        {
            executor.getQueue().put(r);
        }
        catch (InterruptedException e)
        {
            Thread.currentThread().interrupt();
            throw new RejectedExecutionException(e);
        }
    }
});

The question is, are there any better ways to do this, or are there any 
obvious errors in the above.

All advice greatly appreciated.

-- 
cheers,
- jed.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060911/fe5a2cfd/attachment-0001.html 

From jed at atlassian.com  Mon Sep 11 04:59:23 2006
From: jed at atlassian.com (Jed Wesley-Smith)
Date: Mon, 11 Sep 2006 18:59:23 +1000
Subject: [concurrency-interest] blocking executor
In-Reply-To: <45051FD0.8060708@atlassian.com>
References: <45051FD0.8060708@atlassian.com>
Message-ID: <4505256B.4080707@atlassian.com>

Sorry all, hit the send button too soon. The BoundedExecutor (JCiP 
Listing 8.4) is a better choice. I'll shut up now...

Jed Wesley-Smith wrote:
> All,
>
> I have been charged with implementing an ExecutorService that has a 
> bounded capacity, and if that capacity is reached will block the 
> caller. A CallerRunsPolicy is not desired here as the number of 
> concurrent tasks executing must be strictly held to the number of 
> threads in the pool, and the number of clients adding tasks is 
> indeterminate.
>
> Having looked at the code for a while, it seems a 
> BlockingRejectedExectionHandler should do the trick eg.
>
> RejectedExecutionHandler blocker = new RejectedExecutionHandler()
> {
>     public void rejectedExecution(Runnable r, ThreadPoolExecutor 
> executor)throws RejectedExecutionException
>     {
>         if (executor.isShutdown())
>         {
>             throw new RejectedExecutionException(executor + " is 
> shutdown!");
>         }
>         try
>         {
>             executor.getQueue().put(r);
>         }
>         catch (InterruptedException e)
>         {
>             Thread.currentThread().interrupt();
>             throw new RejectedExecutionException(e);
>         }
>     }
> });
>
> The question is, are there any better ways to do this, or are there 
> any obvious errors in the above.
>
> All advice greatly appreciated.
> -- 
> cheers,
> - jed.
>   
> ------------------------------------------------------------------------
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>   

-- 
cheers,
- jed.


From vijay at saraswat.org  Mon Sep 11 07:55:53 2006
From: vijay at saraswat.org (Vijay Saraswat)
Date: Mon, 11 Sep 2006 07:55:53 -0400
Subject: [concurrency-interest] concurrency puzzle
In-Reply-To: <NFBBKALFDCPFIDBNKAPCAEEGHCAA.dcholmes@optusnet.com.au>
References: <NFBBKALFDCPFIDBNKAPCAEEGHCAA.dcholmes@optusnet.com.au>
Message-ID: <45054EC9.8000006@saraswat.org>

David Holmes wrote:

> So this means that:
>
>   class A {
>       int x;
>       ...
>   }
>
>is semantically different to:
>
>   class A {
>       int x = 0;
>       ...
>   }
>
>because the explicit write of zero to x could "appear" later (even though it
>is overwriting a zero value in main memory) just as the value 10 could
>"appear" later.
>
>Wow! That is a very subtle distinction. I'm not sure there are any practical
>consequences arising from it but I find it disconcerting to have this
>difference.
>  
>
The difference could be huge.

Question: Is there some argument for why any Java code has a race with 
the second fragment iff it has a race with the first?

That would be a very interesting global property of the Java language.

If not, then one would need some interesting results about JSR 133 rules 
to be certain that there are no practical consequences arising from this 
distinction -- since the presence of races has such a huge impact on the 
semantics. For instance one would need *at least* the following to be 
true (and in reality much more general forms of this would need to be true):

Claim: Suppose the only race in a program is between a read action and a 
write action on x that writes the same value as the hb-value of x at the 
read operation. Then all executions of the program are SC.

Unfortunately, JSR 133 rules are complex and not mathematically precise. 
So I cant figure out if this is true.

If someone can provide some precise reasoning based on JSR133 for why 
the question or the claim above is true (or false!), I would be very 
grateful.

Best,
Vijay



From jmanson at cs.umd.edu  Mon Sep 11 12:20:02 2006
From: jmanson at cs.umd.edu (Jeremy Manson)
Date: Mon, 11 Sep 2006 09:20:02 -0700
Subject: [concurrency-interest] concurrency puzzle
In-Reply-To: <1466c1d60609110044u24562271ra1f46fd0421f78bc@mail.gmail.com>
References: <NFBBKALFDCPFIDBNKAPCAEEGHCAA.dcholmes@optusnet.com.au>	
	<45050C6F.8060802@cs.umd.edu>
	<1466c1d60609110044u24562271ra1f46fd0421f78bc@mail.gmail.com>
Message-ID: <45058CB2.2010803@cs.umd.edu>

Oops!  I guess "off list" is a concept I am having trouble with today 
:).  Oh, well, if people are interested...

Peter Veentjer wrote:
> But what about the behaviour of the synchronized statement? If a lock
> is obtained, local memory (for that thread) is invalidated and the
> next read is going to access main memory.
> 
> That is why the assignment of x=20 could get lost. If the x=20 isn't
> written to main memory (and in this case there is no need to) but only
> to cache, the value is 'dropped' when the cache is invalidated.
> 
> Is this correct? I'm trying to understand the memory behaviour of the
> synchronized statement: that is the reason why I made this example.
> 

Peter,

First, the example again:

> public class A{
>     private int x = 10;
>     public static A a;
> 
>     public void foo(){
>        x = 20;
>        synchronized(this){
>           System.out.println( x );
>        }
>     }
> 
> }
> 
> Thread 1:
> A o = new A();
> A.a = o;
> 
> Thread 2:
> A r1 = A.a;
> assert r1 != null;
> r1.foo(); // can print out 10 or 20


You are still thinking about this in terms of local memory for a given 
thread, and invalidation.  A Java thread doesn't have local memory, and 
doesn't perform invalidation.  Processors have caches, and their caches 
can be invalidated.  However, if 20 was written out to x earlier in the 
same thread, there are no processors that will then allow this print 
statement to write the older value 0.  That's not how memory coherence 
works.

Even if it were how memory coherence worked in some processor (which it 
wouldn't be), it would be a violation of the Java memory model, which, 
as we have discussed, disallows a read of the value 0.  So a Java 
implementation would have to find a way around it.

					Jeremy

From conivek at gmail.com  Mon Sep 11 17:06:07 2006
From: conivek at gmail.com (Kevin Condon)
Date: Mon, 11 Sep 2006 17:06:07 -0400
Subject: [concurrency-interest] concurrency puzzle
In-Reply-To: <NFBBKALFDCPFIDBNKAPCAEEGHCAA.dcholmes@optusnet.com.au>
References: <45045E65.205@cs.umd.edu>
	<NFBBKALFDCPFIDBNKAPCAEEGHCAA.dcholmes@optusnet.com.au>
Message-ID: <2e780ac60609111406g4ef762f9i20004ca572969f21@mail.gmail.com>

> Wow! That is a very subtle distinction. I'm not sure there are any practical
> consequences arising from it but I find it disconcerting to have this
> difference.

This isn't *exactly* the same thing, but I think there is a practical
consequence for a similar situation when populating non-serialized
fields in readObject().  For some Serializable class MyObject:

  private transient Object mutex;  // needs to be volatile
  private int f;

  public MyObject() {
    initTransient();
  }

  private void readObject(ObjectInputStream in) throws ... {
    initTransient();
    synchronized (mutex) {
      in.defaultReadObject();
    }
  }

  private void initTransient() {
    // bad, no synchronization on the non-volatile field init
    mutex = new Object();
  }

  private void writeObject(ObjectOutputStream out) throws ... {
    // NPE, mutex init visiblility not guaranteed in all threads
    synchronized (mutex) {
      out.defaultWriteObject();
    }
  }

  public int getF() {
    // NPE, mutex init visiblility not guaranteed in all threads
    synchronized (mutex) {
      return f;
    }
  }

  public void setF(int f) {
    // NPE, mutex init visiblility not guaranteed in all threads
    synchronized (mutex) {
      this.f = f;
    }
  }

T1:
  volatile MyObject o;
  ...
  o = (MyObject) in.readObject();

T2:
  int v = o.getF();  // might get a NPE due to race on mutex value
  // same NPE potential with o.setF() and with serializing the object ...

If I didn't have to serialize, I'd just make mutex final.  I think
making mutex volatile fixes the problem with potential NPEs, but then
you incur performance costs for both synchronization and volatile
semantics.  Is there some other option that I'm missing?  (Please
assume I don't want to do synchronize (this), because more complex
classes might not want to expose internal locks to avoid deadlock.)

This scares me because it's so easy to forget to do (I definitely
have) and because of the performance costs imposed (even if they may
be small).  In general remembering to synchronize properly during
construction and deserialization is just too easy to forget.

Regards,
Kevin

From hanson.char at gmail.com  Mon Sep 11 17:53:33 2006
From: hanson.char at gmail.com (Hanson Char)
Date: Mon, 11 Sep 2006 14:53:33 -0700
Subject: [concurrency-interest] ConcurrentLinkedBlockingQueue ?
In-Reply-To: <ca53c8f80609102340n4dce9bfcg69c51cdb9ea244bf@mail.gmail.com>
References: <ca53c8f80609062313h502eb3f9n9217820f4d8b46a0@mail.gmail.com>
	<4500017E.4040202@cs.oswego.edu>
	<ca53c8f80609091818x71495fbbtf598233fe69c3271@mail.gmail.com>
	<ca53c8f80609092051u7f6f8b24n4882c96acbc45fb6@mail.gmail.com>
	<ca53c8f80609092321y2f585d28k5073d7e73f02416a@mail.gmail.com>
	<ca53c8f80609092358o7f9c08dcrdea39e87e71ee580@mail.gmail.com>
	<45044524.4070109@wizards.de>
	<ca53c8f80609102340n4dce9bfcg69c51cdb9ea244bf@mail.gmail.com>
Message-ID: <ca53c8f80609111453h4a090d08l4057774c4c745ed6@mail.gmail.com>

Testing on an AMD Opteron dual processor, the CLBQ is on average 72% faster
than the LBQ using the test harness.  Please see the attached pdf for more
details.

Hanson

On 9/10/06, Hanson Char <hanson.char at gmail.com> wrote:
>
> Outstanding! Could you please make up-to-date versions of this class and
> > its unit test available over the web in a more permanent location? That
> > way you don't have to send updates by mail..
> >
>
> Latest source:
>
> http://svn.sourceforge.net/viewvc/beanlib/trunk/beanlib/src/net/sf/beanlib/util/concurrent/ConcurrentLinkedBlockingQueue.java
>
>
> Test classes:
>
> http://svn.sourceforge.net/viewvc/beanlib/trunk/beanlib-test/src/net/sf/beanlib/util/concurrent/
>
>
> Hanson
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060911/be4596af/attachment-0001.html 
-------------- next part --------------
A non-text attachment was scrubbed...
Name: 060911-clbq.pdf
Type: application/pdf
Size: 26172 bytes
Desc: not available
Url : /pipermail/attachments/20060911/be4596af/attachment-0001.pdf 

From dcholmes at optusnet.com.au  Mon Sep 11 22:25:50 2006
From: dcholmes at optusnet.com.au (David Holmes)
Date: Tue, 12 Sep 2006 12:25:50 +1000
Subject: [concurrency-interest] concurrency puzzle
In-Reply-To: <1466c1d60609110044u24562271ra1f46fd0421f78bc@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCOEEMHCAA.dcholmes@optusnet.com.au>

Peter,

> But what about the behaviour of the synchronized statement? If a lock
> is obtained, local memory (for that thread) is invalidated and the
> next read is going to access main memory.

As Jeremy stated you are looking at this from the wrong perspective. The new
JMM isn't about local vs. global memory, or about invalidation - it defines
happens-before relationships and how they define the legal set of values
that a read of a variable can return. In that sense the behaviour of a
synchronized statement is quite simple:
 - releasing a lock happens-before any subsequent (in time) acquisition of
that lock

The interesting thing is forming the chain of happens-before relationships
to see how a write in one thread is related to a read in another.

> That is why the assignment of x=20 could get lost. If the x=20 isn't
> written to main memory (and in this case there is no need to) but only
> to cache, the value is 'dropped' when the cache is invalidated.

You could look at it this way but it isn't necessary to do so. In hardware
terms the write x=20 could go straight to main memory, but that could be
followed by the write of x=10 from the constructor which has been in the
write buffer on another processor. The first thread then acquires the lock
and reads the value of x and sees 10.

In terms of the memory model there is no happens-before ordering between the
write x=10 and the write x=20 so when x is read, the read can return either
of those values.

Thinking about the memory model in terms of a conceptual "cache" sometimes
serves as a useful model for descriptive purposes, but it falls apart if
people then try to apply hardware caching behaviour to the conceptual
model - it doesn't work that way.

Cheers,
David Holmes


From dcholmes at optusnet.com.au  Mon Sep 11 22:34:23 2006
From: dcholmes at optusnet.com.au (David Holmes)
Date: Tue, 12 Sep 2006 12:34:23 +1000
Subject: [concurrency-interest] concurrency puzzle
In-Reply-To: <45054EC9.8000006@saraswat.org>
Message-ID: <NFBBKALFDCPFIDBNKAPCIEENHCAA.dcholmes@optusnet.com.au>

Hi Vijay,

> > So this means that:
> >
> >   class A {
> >       int x;
> >       ...
> >   }
> >
> >is semantically different to:
> >
> >   class A {
> >       int x = 0;
> >       ...
> >   }
> >
> >because the explicit write of zero to x could "appear" later
> (even though it
> >is overwriting a zero value in main memory) just as the value 10 could
> >"appear" later.
> >
> >Wow! That is a very subtle distinction. I'm not sure there are
> any practical
> >consequences arising from it but I find it disconcerting to have this
> >difference.
> >
> >
> The difference could be huge.

I don't claim to understand the attempted formalism of this. The difference
between:

    int x;

and

    int x = 0;

is that given a thread T that is reading x from an instance that was
unsafely published, and where T first writes the value x=N, then in the
first case the legal values for the read of x is only N, while in the second
case it is 0 or N.

I guess the practical consequence is that in the second case you may
actually discover that you have a bug due to unsafe publication, while in
the first case you likely never will. Of course it would be perfectly legal
(probably not worth the effort) for the compiler to optimize away writes of
default values in the constructor, such that the runtime behaviour of both
cases is the same.

Where do you see the potential for a huge difference?

Cheers,
David Holmes


From vijay at saraswat.org  Mon Sep 11 22:49:19 2006
From: vijay at saraswat.org (Vijay Saraswat)
Date: Mon, 11 Sep 2006 22:49:19 -0400
Subject: [concurrency-interest] concurrency puzzle
In-Reply-To: <NFBBKALFDCPFIDBNKAPCIEENHCAA.dcholmes@optusnet.com.au>
References: <NFBBKALFDCPFIDBNKAPCIEENHCAA.dcholmes@optusnet.com.au>
Message-ID: <4506202F.7080206@saraswat.org>

David Holmes wrote:

>I don't claim to understand the attempted formalism of this. The difference
>between:
>
>    int x;
>
>and
>
>    int x = 0;
>
>is that given a thread T that is reading x from an instance that was
>unsafely published, and where T first writes the value x=N, then in the
>first case the legal values for the read of x is only N, while in the second
>case it is 0 or N.
>
>I guess the practical consequence is that in the second case you may
>actually discover that you have a bug due to unsafe publication, while in
>the first case you likely never will. Of course it would be perfectly legal
>(probably not worth the effort) for the compiler to optimize away writes of
>default values in the constructor, such that the runtime behaviour of both
>cases is the same.
>
>Where do you see the potential for a huge difference?
>  
>
As I said in my message, the first program fragment has no 
unsynchronized writes -- so no possibility of a race. But the second 
does. So there is a potential for a race involving a thread that reads 
the value of the variable x.


From dcholmes at optusnet.com.au  Mon Sep 11 22:54:23 2006
From: dcholmes at optusnet.com.au (David Holmes)
Date: Tue, 12 Sep 2006 12:54:23 +1000
Subject: [concurrency-interest] concurrency puzzle
In-Reply-To: <2e780ac60609111406g4ef762f9i20004ca572969f21@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCKEEOHCAA.dcholmes@optusnet.com.au>

Kevin,

I don't know if this answers your question but unsafe-publication arising
from deserialization would seem to be no different to unsafe-publication
arising from direct construction. In botrh cases bad things can happen.

Cheers,
David Holmes

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Kevin
> Condon
> Sent: Tuesday, 12 September 2006 7:06 AM
> To: dholmes at ieee.org
> Cc: concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] concurrency puzzle
>
>
> > Wow! That is a very subtle distinction. I'm not sure there are
> any practical
> > consequences arising from it but I find it disconcerting to have this
> > difference.
>
> This isn't *exactly* the same thing, but I think there is a practical
> consequence for a similar situation when populating non-serialized
> fields in readObject().  For some Serializable class MyObject:
>
>   private transient Object mutex;  // needs to be volatile
>   private int f;
>
>   public MyObject() {
>     initTransient();
>   }
>
>   private void readObject(ObjectInputStream in) throws ... {
>     initTransient();
>     synchronized (mutex) {
>       in.defaultReadObject();
>     }
>   }
>
>   private void initTransient() {
>     // bad, no synchronization on the non-volatile field init
>     mutex = new Object();
>   }
>
>   private void writeObject(ObjectOutputStream out) throws ... {
>     // NPE, mutex init visiblility not guaranteed in all threads
>     synchronized (mutex) {
>       out.defaultWriteObject();
>     }
>   }
>
>   public int getF() {
>     // NPE, mutex init visiblility not guaranteed in all threads
>     synchronized (mutex) {
>       return f;
>     }
>   }
>
>   public void setF(int f) {
>     // NPE, mutex init visiblility not guaranteed in all threads
>     synchronized (mutex) {
>       this.f = f;
>     }
>   }
>
> T1:
>   volatile MyObject o;
>   ...
>   o = (MyObject) in.readObject();
>
> T2:
>   int v = o.getF();  // might get a NPE due to race on mutex value
>   // same NPE potential with o.setF() and with serializing the object ...
>
> If I didn't have to serialize, I'd just make mutex final.  I think
> making mutex volatile fixes the problem with potential NPEs, but then
> you incur performance costs for both synchronization and volatile
> semantics.  Is there some other option that I'm missing?  (Please
> assume I don't want to do synchronize (this), because more complex
> classes might not want to expose internal locks to avoid deadlock.)
>
> This scares me because it's so easy to forget to do (I definitely
> have) and because of the performance costs imposed (even if they may
> be small).  In general remembering to synchronize properly during
> construction and deserialization is just too easy to forget.
>
> Regards,
> Kevin
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest


From dcholmes at optusnet.com.au  Mon Sep 11 23:01:40 2006
From: dcholmes at optusnet.com.au (David Holmes)
Date: Tue, 12 Sep 2006 13:01:40 +1000
Subject: [concurrency-interest] concurrency puzzle
In-Reply-To: <4506202F.7080206@saraswat.org>
Message-ID: <NFBBKALFDCPFIDBNKAPCGEEPHCAA.dcholmes@optusnet.com.au>

> >
> >    int x;
> >
> >and
> >
> >    int x = 0;
> >
> >
> As I said in my message, the first program fragment has no
> unsynchronized writes -- so no possibility of a race. But the second
> does. So there is a potential for a race involving a thread that reads
> the value of the variable x.

Sorry I don't follow. If the reading thread has not written to x then it can
only read zero. It can't tell whether it reads the "before the dawn of time"
zero, or the constructor written zero.

To me the only interesting case is when a thread does:

    foo.x = N;
    print(foo.x)

and the question is under what circumstances is the print guaranteed to
print N? If the only other write to x is the default initialization of it
then in that case the print is guaranteed to print N. Otherwise the print
can print either N or whatever other value was written to x.

Cheers,
David


From rbalamohan at sonoasystems.com  Mon Sep 11 23:38:48 2006
From: rbalamohan at sonoasystems.com (rbalamohan)
Date: Tue, 12 Sep 2006 03:38:48 +0000
Subject: [concurrency-interest] ConcurrentLinkedBlockingQueue ?
In-Reply-To: <ca53c8f80609111453h4a090d08l4057774c4c745ed6@mail.gmail.com>
References: <ca53c8f80609062313h502eb3f9n9217820f4d8b46a0@mail.gmail.com>
	<4500017E.4040202@cs.oswego.edu>
	<ca53c8f80609091818x71495fbbtf598233fe69c3271@mail.gmail.com>
	<ca53c8f80609092051u7f6f8b24n4882c96acbc45fb6@mail.gmail.com>
	<ca53c8f80609092321y2f585d28k5073d7e73f02416a@mail.gmail.com>
	<ca53c8f80609092358o7f9c08dcrdea39e87e71ee580@mail.gmail.com>
	<45044524.4070109@wizards.de>
	<ca53c8f80609102340n4dce9bfcg69c51cdb9ea244bf@mail.gmail.com>
	<ca53c8f80609111453h4a090d08l4057774c4c745ed6@mail.gmail.com>
Message-ID: <1158032328.23617.6.camel@xen-host-ce>

The results look really promising. 

On a similar note, I was wondering why we don't have "PRIORITY BASED
LINKED QUEUE"

This might be useful for creating work load with priority associated
with it and submitting to the threadpool executor.

~Rajesh.B

On Mon, 2006-09-11 at 14:53 -0700, Hanson Char wrote:

> Testing on an AMD Opteron dual processor, the CLBQ is on average 72%
> faster than the LBQ using the test harness.  Please see the attached
> pdf for more details.
> 
> Hanson
> 
> On 9/10/06, Hanson Char <hanson.char at gmail.com> wrote:
> 
>                 Outstanding! Could you please make up-to-date versions
>                 of this class and 
>                 its unit test available over the web in a more
>                 permanent location? That
>                 way you don't have to send updates by mail..
>         
>         
>         
>         Latest source: 
>         
>         http://svn.sourceforge.net/viewvc/beanlib/trunk/beanlib/src/net/sf/beanlib/util/concurrent/ConcurrentLinkedBlockingQueue.java
>         
>         Test classes:
>         
>         http://svn.sourceforge.net/viewvc/beanlib/trunk/beanlib-
>         test/src/net/sf/beanlib/util/concurrent/
>         
>         
>         Hanson
>         
>         
> 
> 
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060912/c93056d1/attachment.html 

From hanson.char at gmail.com  Tue Sep 12 00:15:25 2006
From: hanson.char at gmail.com (Hanson Char)
Date: Mon, 11 Sep 2006 21:15:25 -0700
Subject: [concurrency-interest] ConcurrentLinkedBlockingQueue ?
In-Reply-To: <1158032328.23617.6.camel@xen-host-ce>
References: <ca53c8f80609062313h502eb3f9n9217820f4d8b46a0@mail.gmail.com>
	<4500017E.4040202@cs.oswego.edu>
	<ca53c8f80609091818x71495fbbtf598233fe69c3271@mail.gmail.com>
	<ca53c8f80609092051u7f6f8b24n4882c96acbc45fb6@mail.gmail.com>
	<ca53c8f80609092321y2f585d28k5073d7e73f02416a@mail.gmail.com>
	<ca53c8f80609092358o7f9c08dcrdea39e87e71ee580@mail.gmail.com>
	<45044524.4070109@wizards.de>
	<ca53c8f80609102340n4dce9bfcg69c51cdb9ea244bf@mail.gmail.com>
	<ca53c8f80609111453h4a090d08l4057774c4c745ed6@mail.gmail.com>
	<1158032328.23617.6.camel@xen-host-ce>
Message-ID: <ca53c8f80609112115l64b69b7fy54a52e7da013d531@mail.gmail.com>

Is j.u.PriorityQueue what you are looking for ?

Hanson

On 9/11/06, rbalamohan <rbalamohan at sonoasystems.com> wrote:
>
>  The results look really promising.
>
> On a similar note, I was wondering why we don't have "PRIORITY BASED
> LINKED QUEUE"
>
> This might be useful for creating work load with priority associated with
> it and submitting to the threadpool executor.
>
> ~Rajesh.B
>
>
> On Mon, 2006-09-11 at 14:53 -0700, Hanson Char wrote:
>
> Testing on an AMD Opteron dual processor, the CLBQ is on average 72%
> faster than the LBQ using the test harness.  Please see the attached pdf for
> more details.
>
> Hanson
>
> On 9/10/06, *Hanson Char* <hanson.char at gmail.com> wrote:
>
>  Outstanding! Could you please make up-to-date versions of this class and
> its unit test available over the web in a more permanent location? That
> way you don't have to send updates by mail..
>
>
>
> Latest source:
>
>
> http://svn.sourceforge.net/viewvc/beanlib/trunk/beanlib/src/net/sf/beanlib/util/concurrent/ConcurrentLinkedBlockingQueue.java
>
> Test classes:
>
>
> http://svn.sourceforge.net/viewvc/beanlib/trunk/beanlib-test/src/net/sf/beanlib/util/concurrent/
>
>
> Hanson
>
>
>
> _______________________________________________Concurrency-interest mailing listConcurrency-interest at altair.cs.oswego.eduhttp://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060911/359e11a6/attachment-0001.html 

From hanson.char at gmail.com  Tue Sep 12 00:29:49 2006
From: hanson.char at gmail.com (Hanson Char)
Date: Mon, 11 Sep 2006 21:29:49 -0700
Subject: [concurrency-interest] PriorityLinkedQueue ?
Message-ID: <ca53c8f80609112129v5c39cca2jb7ca4f8daaa21445@mail.gmail.com>

>This might be useful for creating work load with priority associated with
it and submitting
> to the threadpool executor.

If you can determine the priority of each work item a priori, you can assign
it an expired "delay" and place it on a j.u.c.DelayQueue.  The higher the
prioirty, the further into the past a work item should be "expired".

The consumer(s) can then dequeue/process items with a priority according to
their expiration.

However, if the prioirty of each work item needs to be changed after it has
been placed on the queue, then this won't work as DelayQueue would ignore
such changes.

It would be really nice if there is a "decrease Key" operation as Tim
Peierls pointed out earlier under the email thread "Changing delays in
DelayQueue ?"

Hanson

On 9/11/06, Hanson Char <hanson.char at gmail.com> wrote:
>
> Is j.u.PriorityQueue what you are looking for ?
>
> Hanson
>
>
> On 9/11/06, rbalamohan <rbalamohan at sonoasystems.com > wrote:
> >
> >  The results look really promising.
> >
> > On a similar note, I was wondering why we don't have "PRIORITY BASED
> > LINKED QUEUE"
> >
> > This might be useful for creating work load with priority associated
> > with it and submitting to the threadpool executor.
> >
> > ~Rajesh.B
> >
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060911/6e09c62c/attachment.html 

From hanson.char at gmail.com  Tue Sep 12 00:46:31 2006
From: hanson.char at gmail.com (Hanson Char)
Date: Mon, 11 Sep 2006 21:46:31 -0700
Subject: [concurrency-interest] PriorityLinkedQueue ?
In-Reply-To: <ca53c8f80609112129v5c39cca2jb7ca4f8daaa21445@mail.gmail.com>
References: <ca53c8f80609112129v5c39cca2jb7ca4f8daaa21445@mail.gmail.com>
Message-ID: <ca53c8f80609112146r1bcda6f4if386b10856a51e2e@mail.gmail.com>

Oops, forget about the "delay" priority.  Better to directly use
j.u.c.PriorityBlockingQueue and implement Comparable for each q item.  There
is still no "decrease Key" operation  in PriorityBlockingQueue though ...

H

On 9/11/06, Hanson Char <hanson.char at gmail.com> wrote:
>
> >This might be useful for creating work load with priority associated with
> it and submitting
> > to the threadpool executor.
>
> If you can determine the priority of each work item a priori, you can
> assign it an expired "delay" and place it on a j.u.c.DelayQueue.  The
> higher the prioirty, the further into the past a work item should be
> "expired".
>
> The consumer(s) can then dequeue/process items with a priority according
> to their expiration.
>
> However, if the prioirty of each work item needs to be changed after it
> has been placed on the queue, then this won't work as DelayQueue would
> ignore such changes.
>
> It would be really nice if there is a "decrease Key" operation as Tim
> Peierls pointed out earlier under the email thread " Changing delays in
> DelayQueue ?"
>
> Hanson
>
> On 9/11/06, Hanson Char <hanson.char at gmail.com> wrote:
> >
> > Is j.u.PriorityQueue what you are looking for ?
> >
> > Hanson
> >
> >
> > On 9/11/06, rbalamohan <rbalamohan at sonoasystems.com > wrote:
> > >
> > >  The results look really promising.
> > >
> > > On a similar note, I was wondering why we don't have "PRIORITY BASED
> > > LINKED QUEUE"
> > >
> > > This might be useful for creating work load with priority associated
> > > with it and submitting to the threadpool executor.
> > >
> > > ~Rajesh.B
> > >
> >
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060911/8fcc838c/attachment.html 

From rbalamohan at sonoasystems.com  Tue Sep 12 03:44:46 2006
From: rbalamohan at sonoasystems.com (rbalamohan)
Date: Tue, 12 Sep 2006 07:44:46 +0000
Subject: [concurrency-interest] PriorityLinkedQueue ?
In-Reply-To: <ca53c8f80609112146r1bcda6f4if386b10856a51e2e@mail.gmail.com>
References: <ca53c8f80609112129v5c39cca2jb7ca4f8daaa21445@mail.gmail.com>
	<ca53c8f80609112146r1bcda6f4if386b10856a51e2e@mail.gmail.com>
Message-ID: <1158047087.23617.12.camel@xen-host-ce>

My bad in not giving the complete details. Sorry about that..

Actually we can make use of PriorityBlockingQueue. However, this is
unbounded. 

There can be requirements where we need BOUNDED PriorityBlockingQueue.

ex: Throttling number of workers in a application server or servlet
container or something like that.

1. Lots of requests come in and we need to prioritize them and hand it
over to the ThreadPoolExecutor.
2. Problem of using PriorityBlockingQueue in this case is that, it is
unbounded. However, we need to restrict the number of requests coming
in, so that the "Discard Policy" of the ThreadPoolExecutor can kick in.

~Rajesh.B

On Mon, 2006-09-11 at 21:46 -0700, Hanson Char wrote:

> Oops, forget about the "delay" priority.  Better to directly use
> j.u.c.PriorityBlockingQueue and implement Comparable for each q item.
> There is still no "decrease Key" operation  in PriorityBlockingQueue
> though ... 
> 
> H
> 
> On 9/11/06, Hanson Char <hanson.char at gmail.com> wrote:
> 
>         >This might be useful for creating work load with priority
>         associated with it and submitting 
>         > to the threadpool executor.
>         
>         If you can determine the priority of each work item a priori,
>         you can assign it an expired "delay" and place it on a
>         j.u.c.DelayQueue.  The higher the prioirty, the further into
>         the past a work item should be "expired".
>         
>         The consumer(s) can then dequeue/process items with a priority
>         according to their expiration.
>         
>         However, if the prioirty of each work item needs to be changed
>         after it has been placed on the queue, then this won't work as
>         DelayQueue would ignore such changes.
>         
>         It would be really nice if there is a "decrease Key" operation
>         as Tim Peierls pointed out earlier under the email thread
>         "Changing delays in DelayQueue ?"
>         
>         Hanson
>         
>         On 9/11/06, Hanson Char <hanson.char at gmail.com> wrote:
>         
>                 Is j.u.PriorityQueue what you are looking for ?
>                 
>                 
>                 Hanson
>                 
>                 
>                 On 9/11/06, rbalamohan <rbalamohan at sonoasystems.com>
>                 wrote:
>                 
>                         The results look really promising. 
>                         
>                         On a similar note, I was wondering why we
>                         don't have "PRIORITY BASED LINKED QUEUE"
>                         
>                         This might be useful for creating work load
>                         with priority associated with it and
>                         submitting to the threadpool executor.
>                         
>                         ~Rajesh.B
>         
>         
>         
> 
> 
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060912/49f4508a/attachment.html 

From hanson.char at gmail.com  Tue Sep 12 12:11:15 2006
From: hanson.char at gmail.com (Hanson Char)
Date: Tue, 12 Sep 2006 09:11:15 -0700
Subject: [concurrency-interest] BoundedPriorityBlockingQueue ?
Message-ID: <ca53c8f80609120911p3f2f1b40rfd4fdbb004620f96@mail.gmail.com>

Say if you had such BoundedPriorityBlockingQueue, what would be the
preferred behavior when the queue is full ?  Should the producer blocks in
enqueueing, or return immediately with a false boolean value ?

Hanson

On 9/12/06, rbalamohan <rbalamohan at sonoasystems.com> wrote:
>
>  My bad in not giving the complete details. Sorry about that..
>
> Actually we can make use of PriorityBlockingQueue. However, this is
> unbounded.
>
> There can be requirements where we need BOUNDED PriorityBlockingQueue.
>
> ex: Throttling number of workers in a application server or servlet
> container or something like that.
>
> 1. Lots of requests come in and we need to prioritize them and hand it
> over to the ThreadPoolExecutor.
> 2. Problem of using PriorityBlockingQueue in this case is that, it is
> unbounded. However, we need to restrict the number of requests coming in, so
> that the "Discard Policy" of the ThreadPoolExecutor can kick in.
>
> ~Rajesh.B
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060912/fc5fce89/attachment.html 

From kav at it.edu  Tue Sep 12 12:24:14 2006
From: kav at it.edu (Kasper Nielsen)
Date: Tue, 12 Sep 2006 18:24:14 +0200
Subject: [concurrency-interest] BoundedPriorityBlockingQueue ?
In-Reply-To: <ca53c8f80609120911p3f2f1b40rfd4fdbb004620f96@mail.gmail.com>
References: <ca53c8f80609120911p3f2f1b40rfd4fdbb004620f96@mail.gmail.com>
Message-ID: <4506DF2E.1020005@it.edu>

Hanson Char wrote:
> Say if you had such BoundedPriorityBlockingQueue, what would be the
> preferred behavior when the queue is full ?  Should the producer blocks in
> enqueueing, or return immediately with a false boolean value ?
> 
> Hanson
> 
It should obey the standard BlockingQueue contract.
offer, pool -> no blocking
offer(timeout), put, poll(timeout), take -> blocking

- Kasper

From hanson.char at gmail.com  Tue Sep 12 12:44:18 2006
From: hanson.char at gmail.com (Hanson Char)
Date: Tue, 12 Sep 2006 09:44:18 -0700
Subject: [concurrency-interest] BoundedPriorityBlockingQueue ?
In-Reply-To: <4506DF2E.1020005@it.edu>
References: <ca53c8f80609120911p3f2f1b40rfd4fdbb004620f96@mail.gmail.com>
	<4506DF2E.1020005@it.edu>
Message-ID: <ca53c8f80609120944q665e1834maec0222ef38e01a9@mail.gmail.com>

>From a cursory look at the existing implementation of the
PriorityBlockingQueue, it seems implementing a blocking put or
offer(timeout) that actually blocks upon the queue size being full (if a
queue size is made specifiable) without compromising the existing level of
concurrency is not difficult.

Or am I missing something here ?

Hanson

On 9/12/06, Kasper Nielsen <kav at it.edu> wrote:
>
> Hanson Char wrote:
> > Say if you had such BoundedPriorityBlockingQueue, what would be the
> > preferred behavior when the queue is full ?  Should the producer blocks
> in
> > enqueueing, or return immediately with a false boolean value ?
> >
> > Hanson
> >
> It should obey the standard BlockingQueue contract.
> offer, pool -> no blocking
> offer(timeout), put, poll(timeout), take -> blocking
>
> - Kasper
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060912/82a63a67/attachment.html 

From brian at quiotix.com  Tue Sep 12 13:01:49 2006
From: brian at quiotix.com (Brian Goetz)
Date: Tue, 12 Sep 2006 13:01:49 -0400
Subject: [concurrency-interest] PriorityLinkedQueue ?
In-Reply-To: <1158047087.23617.12.camel@xen-host-ce>
References: <ca53c8f80609112129v5c39cca2jb7ca4f8daaa21445@mail.gmail.com>	<ca53c8f80609112146r1bcda6f4if386b10856a51e2e@mail.gmail.com>
	<1158047087.23617.12.camel@xen-host-ce>
Message-ID: <4506E7FD.3090007@quiotix.com>

> There can be requirements where we need BOUNDED PriorityBlockingQueue.

See the examples in JCiP for using a Semaphore to bound an Executor 
(Listing 8.4) or a collection (Listing 5.14); you can use the same approach.


From brian at quiotix.com  Tue Sep 12 13:16:07 2006
From: brian at quiotix.com (Brian Goetz)
Date: Tue, 12 Sep 2006 13:16:07 -0400
Subject: [concurrency-interest] BoundedPriorityBlockingQueue ?
In-Reply-To: <4506DF2E.1020005@it.edu>
References: <ca53c8f80609120911p3f2f1b40rfd4fdbb004620f96@mail.gmail.com>
	<4506DF2E.1020005@it.edu>
Message-ID: <4506EB57.9080204@quiotix.com>

The tricky part is if you want to guarantee that the thread offering the 
element of highest priority is first to be unblocked.

Kasper Nielsen wrote:
> Hanson Char wrote:
>> Say if you had such BoundedPriorityBlockingQueue, what would be the
>> preferred behavior when the queue is full ?  Should the producer blocks in
>> enqueueing, or return immediately with a false boolean value ?
>>
>> Hanson
>>
> It should obey the standard BlockingQueue contract.
> offer, pool -> no blocking
> offer(timeout), put, poll(timeout), take -> blocking
> 
> - Kasper
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest

From alarmnummer at gmail.com  Tue Sep 12 14:12:50 2006
From: alarmnummer at gmail.com (Peter Veentjer)
Date: Tue, 12 Sep 2006 20:12:50 +0200
Subject: [concurrency-interest] concurrency puzzle
In-Reply-To: <45058CB2.2010803@cs.umd.edu>
References: <NFBBKALFDCPFIDBNKAPCAEEGHCAA.dcholmes@optusnet.com.au>
	<45050C6F.8060802@cs.umd.edu>
	<1466c1d60609110044u24562271ra1f46fd0421f78bc@mail.gmail.com>
	<45058CB2.2010803@cs.umd.edu>
Message-ID: <1466c1d60609121112r553ddfdfnf13c38ee91b388e9@mail.gmail.com>

I'm stilling thinking about the subject and I trying to think in the
happens before terms.

Another example with a happens before relation

private int a;

public void foo(){
    a = 20;
    synchronized(this){
        System.out.println(a);
    }
}

public void bar(){
    synchronized(this){
        a=10;
    }
}


In this example, could it happen that 10 is printed?

There is a happens before relation between x=10 and System.out.println
(when the lock is released at the end of the bar method, and acquired
in the foo method.

From alarmnummer at gmail.com  Tue Sep 12 14:15:45 2006
From: alarmnummer at gmail.com (Peter Veentjer)
Date: Tue, 12 Sep 2006 20:15:45 +0200
Subject: [concurrency-interest] concurrency puzzle
In-Reply-To: <1466c1d60609121112r553ddfdfnf13c38ee91b388e9@mail.gmail.com>
References: <NFBBKALFDCPFIDBNKAPCAEEGHCAA.dcholmes@optusnet.com.au>
	<45050C6F.8060802@cs.umd.edu>
	<1466c1d60609110044u24562271ra1f46fd0421f78bc@mail.gmail.com>
	<45058CB2.2010803@cs.umd.edu>
	<1466c1d60609121112r553ddfdfnf13c38ee91b388e9@mail.gmail.com>
Message-ID: <1466c1d60609121115n7de77ecapeb91c4ecdf164491@mail.gmail.com>

I know 10 could be printed if the bar method is executed just after
a=20 and just before the synchronized(this){println(a)}. But what if
bar is executed some time before the foo method is called? Could 10
still be printed?

On 9/12/06, Peter Veentjer <alarmnummer at gmail.com> wrote:
> I'm stilling thinking about the subject and I trying to think in the
> happens before terms.
>
> Another example with a happens before relation
>
> private int a;
>
> public void foo(){
>     a = 20;
>     synchronized(this){
>         System.out.println(a);
>     }
> }
>
> public void bar(){
>     synchronized(this){
>         a=10;
>     }
> }
>
>
> In this example, could it happen that 10 is printed?
>
> There is a happens before relation between x=10 and System.out.println
> (when the lock is released at the end of the bar method, and acquired
> in the foo method.
>

From hanson.char at gmail.com  Tue Sep 12 14:20:44 2006
From: hanson.char at gmail.com (Hanson Char)
Date: Tue, 12 Sep 2006 11:20:44 -0700
Subject: [concurrency-interest] BoundedPriorityBlockingQueue ?
In-Reply-To: <4506EB57.9080204@quiotix.com>
References: <ca53c8f80609120911p3f2f1b40rfd4fdbb004620f96@mail.gmail.com>
	<4506DF2E.1020005@it.edu> <4506EB57.9080204@quiotix.com>
Message-ID: <ca53c8f80609121120p6b411a8xb28846e4ca9219d4@mail.gmail.com>

>The tricky part is if you want to guarantee that the thread offering the
>element of highest priority is first to be unblocked.

That does sound tricky, but I would think unblocking them in a FIFO order of
arrival should usually be good enough ?

Hanson
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060912/5f417814/attachment.html 

From jmanson at cs.umd.edu  Tue Sep 12 14:27:20 2006
From: jmanson at cs.umd.edu (Jeremy Manson)
Date: Tue, 12 Sep 2006 11:27:20 -0700
Subject: [concurrency-interest] concurrency puzzle
In-Reply-To: <1466c1d60609121115n7de77ecapeb91c4ecdf164491@mail.gmail.com>
References: <NFBBKALFDCPFIDBNKAPCAEEGHCAA.dcholmes@optusnet.com.au>	
	<45050C6F.8060802@cs.umd.edu>	
	<1466c1d60609110044u24562271ra1f46fd0421f78bc@mail.gmail.com>	
	<45058CB2.2010803@cs.umd.edu>	
	<1466c1d60609121112r553ddfdfnf13c38ee91b388e9@mail.gmail.com>
	<1466c1d60609121115n7de77ecapeb91c4ecdf164491@mail.gmail.com>
Message-ID: <4506FC08.2010808@cs.umd.edu>

If foo() is executed in one thread, and bar() is executed in another, 
then your explanation is the right one, and this program can print 20 or 10.

About your second question -- "this program can print 20 or 10" is the 
right explanation, no matter what happened in the runtime.  This is 
because there is no way, in this example, to tell if bar is executed 
before the assignment to a in foo().

If you restructure your example so you can tell that bar() is executed 
first, then the answer to that question might change.

					Jeremy


Peter Veentjer wrote:
> I know 10 could be printed if the bar method is executed just after
> a=20 and just before the synchronized(this){println(a)}. But what if
> bar is executed some time before the foo method is called? Could 10
> still be printed?
> 
> On 9/12/06, Peter Veentjer <alarmnummer at gmail.com> wrote:
>> I'm stilling thinking about the subject and I trying to think in the
>> happens before terms.
>>
>> Another example with a happens before relation
>>
>> private int a;
>>
>> public void foo(){
>>     a = 20;
>>     synchronized(this){
>>         System.out.println(a);
>>     }
>> }
>>
>> public void bar(){
>>     synchronized(this){
>>         a=10;
>>     }
>> }
>>
>>
>> In this example, could it happen that 10 is printed?
>>
>> There is a happens before relation between x=10 and System.out.println
>> (when the lock is released at the end of the bar method, and acquired
>> in the foo method.
>>


From brian at quiotix.com  Tue Sep 12 14:34:39 2006
From: brian at quiotix.com (Brian Goetz)
Date: Tue, 12 Sep 2006 14:34:39 -0400
Subject: [concurrency-interest] BoundedPriorityBlockingQueue ?
In-Reply-To: <ca53c8f80609121120p6b411a8xb28846e4ca9219d4@mail.gmail.com>
References: <ca53c8f80609120911p3f2f1b40rfd4fdbb004620f96@mail.gmail.com>	
	<4506DF2E.1020005@it.edu> <4506EB57.9080204@quiotix.com>
	<ca53c8f80609121120p6b411a8xb28846e4ca9219d4@mail.gmail.com>
Message-ID: <4506FDBF.2000109@quiotix.com>

> That does sound tricky, but I would think unblocking them in a FIFO 
> order of arrival should usually be good enough ?

I don't think FIFO adds much (and it costs something).  The problem is, 
what if a low-priority, but long-running task wins the unblocking race 
against a high-priority task?  It could delay the high-priority task a 
lot.  The effects of this are smaller if (a) the queue size is large or 
(b) the number of workers is large; this is really only a problem if the 
worker pool AND the queue are small.


From alarmnummer at gmail.com  Tue Sep 12 15:06:03 2006
From: alarmnummer at gmail.com (Peter Veentjer)
Date: Tue, 12 Sep 2006 21:06:03 +0200
Subject: [concurrency-interest] concurrency puzzle
In-Reply-To: <4506FC08.2010808@cs.umd.edu>
References: <NFBBKALFDCPFIDBNKAPCAEEGHCAA.dcholmes@optusnet.com.au>
	<45050C6F.8060802@cs.umd.edu>
	<1466c1d60609110044u24562271ra1f46fd0421f78bc@mail.gmail.com>
	<45058CB2.2010803@cs.umd.edu>
	<1466c1d60609121112r553ddfdfnf13c38ee91b388e9@mail.gmail.com>
	<1466c1d60609121115n7de77ecapeb91c4ecdf164491@mail.gmail.com>
	<4506FC08.2010808@cs.umd.edu>
Message-ID: <1466c1d60609121206y722099e4s46c0d1e20e9f9b6f@mail.gmail.com>

I you look at the following documentation:

http://www.cs.umd.edu/~pugh/java/memoryModel/jsr-133-faq.html#volatile

They talk about cache invalidation:
if a lock is obtained, the cache is invalidated.

So occording to this documentation, the write x=20 could get lost if
it isn't flushed to main memory and this makes 0 a possible output
value.

So what should I believe?

From hanson.char at gmail.com  Tue Sep 12 15:10:15 2006
From: hanson.char at gmail.com (Hanson Char)
Date: Tue, 12 Sep 2006 12:10:15 -0700
Subject: [concurrency-interest] BoundedPriorityBlockingQueue ?
In-Reply-To: <4506FDBF.2000109@quiotix.com>
References: <ca53c8f80609120911p3f2f1b40rfd4fdbb004620f96@mail.gmail.com>
	<4506DF2E.1020005@it.edu> <4506EB57.9080204@quiotix.com>
	<ca53c8f80609121120p6b411a8xb28846e4ca9219d4@mail.gmail.com>
	<4506FDBF.2000109@quiotix.com>
Message-ID: <ca53c8f80609121210i690dacc3pcd6e2de1d28eb674@mail.gmail.com>

I see.  That is interesting.  Thanks.

Hanson

On 9/12/06, Brian Goetz <brian at quiotix.com> wrote:
>
> > That does sound tricky, but I would think unblocking them in a FIFO
> > order of arrival should usually be good enough ?
>
> I don't think FIFO adds much (and it costs something).  The problem is,
> what if a low-priority, but long-running task wins the unblocking race
> against a high-priority task?  It could delay the high-priority task a
> lot.  The effects of this are smaller if (a) the queue size is large or
> (b) the number of workers is large; this is really only a problem if the
> worker pool AND the queue are small.
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060912/b47d522a/attachment.html 

From jmanson at cs.umd.edu  Tue Sep 12 15:29:35 2006
From: jmanson at cs.umd.edu (Jeremy Manson)
Date: Tue, 12 Sep 2006 12:29:35 -0700
Subject: [concurrency-interest] concurrency puzzle
In-Reply-To: <1466c1d60609121206y722099e4s46c0d1e20e9f9b6f@mail.gmail.com>
References: <NFBBKALFDCPFIDBNKAPCAEEGHCAA.dcholmes@optusnet.com.au>	
	<45050C6F.8060802@cs.umd.edu>	
	<1466c1d60609110044u24562271ra1f46fd0421f78bc@mail.gmail.com>	
	<45058CB2.2010803@cs.umd.edu>	
	<1466c1d60609121112r553ddfdfnf13c38ee91b388e9@mail.gmail.com>	
	<1466c1d60609121115n7de77ecapeb91c4ecdf164491@mail.gmail.com>	
	<4506FC08.2010808@cs.umd.edu>
	<1466c1d60609121206y722099e4s46c0d1e20e9f9b6f@mail.gmail.com>
Message-ID: <45070A9F.9010507@cs.umd.edu>

Peter Veentjer wrote:
> I you look at the following documentation:
> 
> http://www.cs.umd.edu/~pugh/java/memoryModel/jsr-133-faq.html#volatile
> 
> They talk about cache invalidation:
> if a lock is obtained, the cache is invalidated.
> 
> So occording to this documentation, the write x=20 could get lost if
> it isn't flushed to main memory and this makes 0 a possible output
> value.
> 
> So what should I believe?

Both of us! :)

(Actually, it is only one of us, technically.  Well, two, because Brian 
co-wrote it.)

I believe that you were concerned about the following example:

x = 20;
synchronized (this) {
   print x;
}

You were worried that the write to x would be in the cache, but that the 
print would be from the value in main memory.  However, that's not how 
cache coherence works.  To simplify, what will happen is that the write 
to x will occur, and subsequent reads of x from the same processor will 
either

a) be taken from the cache (and return 20), or

b) be taken from main memory, *after the values in the cache are written 
out to it* (and return 20, or some other value that was written out 
after the 20 -- this might include the 10 from the other thread).

The paragraph to which you point is just saying that reads of volatiles 
have to be of type b), so that they see any writes to that volatile that 
might have been performed by another processor.  Otherwise, they might 
see stale cache values.

I hope that helps a little.

						Jeremy

From alarmnummer at gmail.com  Tue Sep 12 15:50:19 2006
From: alarmnummer at gmail.com (Peter Veentjer)
Date: Tue, 12 Sep 2006 21:50:19 +0200
Subject: [concurrency-interest] concurrency puzzle
In-Reply-To: <45070A9F.9010507@cs.umd.edu>
References: <NFBBKALFDCPFIDBNKAPCAEEGHCAA.dcholmes@optusnet.com.au>
	<45050C6F.8060802@cs.umd.edu>
	<1466c1d60609110044u24562271ra1f46fd0421f78bc@mail.gmail.com>
	<45058CB2.2010803@cs.umd.edu>
	<1466c1d60609121112r553ddfdfnf13c38ee91b388e9@mail.gmail.com>
	<1466c1d60609121115n7de77ecapeb91c4ecdf164491@mail.gmail.com>
	<4506FC08.2010808@cs.umd.edu>
	<1466c1d60609121206y722099e4s46c0d1e20e9f9b6f@mail.gmail.com>
	<45070A9F.9010507@cs.umd.edu>
Message-ID: <1466c1d60609121250n255355f5p83a75b4249dc3a95@mail.gmail.com>

> You were worried that the write to x would be in the cache, but that the
> print would be from the value in main memory.
That is the part I'm trying to understand.

 However, that's not how
> cache coherence works.  To simplify, what will happen is that the write
> to x will occur, and subsequent reads of x from the same processor will
> either
>
> a) be taken from the cache (and return 20), or
>
> b) be taken from main memory, *after the values in the cache are written
> out to it* (and return 20, or some other value that was written out
> after the 20 -- this might include the 10 from the other thread).
Aha.. so you are saying that before 'invalidating' cache, all values
are written to main memory? I though they were not written to main
memory, but they were 'dropped'. So if they are already in main
memory, you are lucky, if they only were in cache, you have lost your
writes.

This is exactly what I don't understand.

From alarmnummer at gmail.com  Tue Sep 12 16:04:06 2006
From: alarmnummer at gmail.com (Peter Veentjer)
Date: Tue, 12 Sep 2006 22:04:06 +0200
Subject: [concurrency-interest] concurrency puzzle
In-Reply-To: <1466c1d60609121250n255355f5p83a75b4249dc3a95@mail.gmail.com>
References: <NFBBKALFDCPFIDBNKAPCAEEGHCAA.dcholmes@optusnet.com.au>
	<45050C6F.8060802@cs.umd.edu>
	<1466c1d60609110044u24562271ra1f46fd0421f78bc@mail.gmail.com>
	<45058CB2.2010803@cs.umd.edu>
	<1466c1d60609121112r553ddfdfnf13c38ee91b388e9@mail.gmail.com>
	<1466c1d60609121115n7de77ecapeb91c4ecdf164491@mail.gmail.com>
	<4506FC08.2010808@cs.umd.edu>
	<1466c1d60609121206y722099e4s46c0d1e20e9f9b6f@mail.gmail.com>
	<45070A9F.9010507@cs.umd.edu>
	<1466c1d60609121250n255355f5p83a75b4249dc3a95@mail.gmail.com>
Message-ID: <1466c1d60609121304m545f411fy2e3a4d41488ba15c@mail.gmail.com>

It seems my understanding of cache wasn't correct.

There is a read-cache and a write cache. It isn't just a chunk of
memory that gets flushed.. only the writes are flushed and the read
memory is invalidated.

I'm going to sleep about it...

On 9/12/06, Peter Veentjer <alarmnummer at gmail.com> wrote:
> > You were worried that the write to x would be in the cache, but that the
> > print would be from the value in main memory.
> That is the part I'm trying to understand.
>
>  However, that's not how
> > cache coherence works.  To simplify, what will happen is that the write
> > to x will occur, and subsequent reads of x from the same processor will
> > either
> >
> > a) be taken from the cache (and return 20), or
> >
> > b) be taken from main memory, *after the values in the cache are written
> > out to it* (and return 20, or some other value that was written out
> > after the 20 -- this might include the 10 from the other thread).
> Aha.. so you are saying that before 'invalidating' cache, all values
> are written to main memory? I though they were not written to main
> memory, but they were 'dropped'. So if they are already in main
> memory, you are lucky, if they only were in cache, you have lost your
> writes.
>
> This is exactly what I don't understand.
>

From brian at quiotix.com  Tue Sep 12 16:15:50 2006
From: brian at quiotix.com (Brian Goetz)
Date: Tue, 12 Sep 2006 16:15:50 -0400
Subject: [concurrency-interest] concurrency puzzle
In-Reply-To: <1466c1d60609121206y722099e4s46c0d1e20e9f9b6f@mail.gmail.com>
References: <NFBBKALFDCPFIDBNKAPCAEEGHCAA.dcholmes@optusnet.com.au>	<45050C6F.8060802@cs.umd.edu>	<1466c1d60609110044u24562271ra1f46fd0421f78bc@mail.gmail.com>	<45058CB2.2010803@cs.umd.edu>	<1466c1d60609121112r553ddfdfnf13c38ee91b388e9@mail.gmail.com>	<1466c1d60609121115n7de77ecapeb91c4ecdf164491@mail.gmail.com>	<4506FC08.2010808@cs.umd.edu>
	<1466c1d60609121206y722099e4s46c0d1e20e9f9b6f@mail.gmail.com>
Message-ID: <45071576.1080405@quiotix.com>

We should update that document to not foster such confusion.

Peter Veentjer wrote:
> I you look at the following documentation:
> 
> http://www.cs.umd.edu/~pugh/java/memoryModel/jsr-133-faq.html#volatile
> 
> They talk about cache invalidation:
> if a lock is obtained, the cache is invalidated.
> 
> So occording to this documentation, the write x=20 could get lost if
> it isn't flushed to main memory and this makes 0 a possible output
> value.
> 
> So what should I believe?
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest

From hanson.char at gmail.com  Tue Sep 12 16:19:59 2006
From: hanson.char at gmail.com (Hanson Char)
Date: Tue, 12 Sep 2006 13:19:59 -0700
Subject: [concurrency-interest] BoundedPriorityBlockingQueue ?
In-Reply-To: <4506EB57.9080204@quiotix.com>
References: <ca53c8f80609120911p3f2f1b40rfd4fdbb004620f96@mail.gmail.com>
	<4506DF2E.1020005@it.edu> <4506EB57.9080204@quiotix.com>
Message-ID: <ca53c8f80609121319l55e749d5k6111e3841a70c467@mail.gmail.com>

I see blocking on put on a BoundedPriorityBlockingQueue is not such a good
idea.

How about making the put method return a false boolean value upon queue
full, or alternatively throwing something like a QueueFullException ?

Would this be useful/sensible ?

Hanson

On 9/12/06, Brian Goetz <brian at quiotix.com> wrote:
>
> The tricky part is if you want to guarantee that the thread offering the
> element of highest priority is first to be unblocked.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060912/d09ad99e/attachment.html 

From brian at quiotix.com  Tue Sep 12 17:02:50 2006
From: brian at quiotix.com (Brian Goetz)
Date: Tue, 12 Sep 2006 17:02:50 -0400
Subject: [concurrency-interest] concurrency puzzle
In-Reply-To: <1466c1d60609121304m545f411fy2e3a4d41488ba15c@mail.gmail.com>
References: <NFBBKALFDCPFIDBNKAPCAEEGHCAA.dcholmes@optusnet.com.au>	<45050C6F.8060802@cs.umd.edu>	<1466c1d60609110044u24562271ra1f46fd0421f78bc@mail.gmail.com>	<45058CB2.2010803@cs.umd.edu>	<1466c1d60609121112r553ddfdfnf13c38ee91b388e9@mail.gmail.com>	<1466c1d60609121115n7de77ecapeb91c4ecdf164491@mail.gmail.com>	<4506FC08.2010808@cs.umd.edu>	<1466c1d60609121206y722099e4s46c0d1e20e9f9b6f@mail.gmail.com>	<45070A9F.9010507@cs.umd.edu>	<1466c1d60609121250n255355f5p83a75b4249dc3a95@mail.gmail.com>
	<1466c1d60609121304m545f411fy2e3a4d41488ba15c@mail.gmail.com>
Message-ID: <4507207A.1060005@quiotix.com>

> It seems my understanding of cache wasn't correct.

Part of the problem with the old memory model was that it was cast in 
the languages of caching.  Such a mental model may well be more 
confusing than helpful, as the duration of this thread suggests.

A memory model speaks to the problem of knowing when a memory write by 
thread X is guaranteed to be visible to a memory read by thread Y. 
Talking about cache flushes doesn't fully address that problem.

There are lots of reasons why a memory write by thread X might not be 
immediately (or ever) visible to thread Y:

  - The compiler hoisted the variable into a register.
  - The compiler optimized away the assignment entirely.
  - The compiler reordered the write with other operations.
  - The processor reordered the write with other operations.
  - The value got written to a memory cache accessible to X but not Y 
(such as a per-processor cache), and it has not yet been flushed to 
where Y can see it.
  - The value got written and flushed to where Y can see it, but Y 
doesn't reload it, instead reading it from a cache holding a stale value.
  - Others.

As you can see from this list of reasons "weird things could happen", 
speaking in terms of any one of these mechanisms is not likely to be 
helpful except as a motivation for why worrying about memory visibility 
is important.  Beyond that, any understanding you have of the underlying 
hardware is probably counterproductive.  (In other words, the more you 
know about cache coherency protocols, the more likely that "hardware 
perspective" is likely to pollute your mental model and get in the way 
of thinking in terms of the JMM.)

So instead, the new JMM defines a partial ordering, happens-before, and 
a list of ways a HB ordering can be set up between a write and a read. 
If there is a HB ordering between a write operation W and a read 
operation R of that same variable, the reading thread is guaranteed to 
see a value that is at least as up-to-date as the value written by W.


From joe.bowbeer at gmail.com  Tue Sep 12 18:32:53 2006
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Tue, 12 Sep 2006 15:32:53 -0700
Subject: [concurrency-interest] concurrency puzzle
In-Reply-To: <4507207A.1060005@quiotix.com>
References: <NFBBKALFDCPFIDBNKAPCAEEGHCAA.dcholmes@optusnet.com.au>
	<45058CB2.2010803@cs.umd.edu>
	<1466c1d60609121112r553ddfdfnf13c38ee91b388e9@mail.gmail.com>
	<1466c1d60609121115n7de77ecapeb91c4ecdf164491@mail.gmail.com>
	<4506FC08.2010808@cs.umd.edu>
	<1466c1d60609121206y722099e4s46c0d1e20e9f9b6f@mail.gmail.com>
	<45070A9F.9010507@cs.umd.edu>
	<1466c1d60609121250n255355f5p83a75b4249dc3a95@mail.gmail.com>
	<1466c1d60609121304m545f411fy2e3a4d41488ba15c@mail.gmail.com>
	<4507207A.1060005@quiotix.com>
Message-ID: <31f2a7bd0609121532k18b52121o4a7b9c9c12b70a51@mail.gmail.com>

On 9/12/06, Brian Goetz <brian at quiotix.com> wrote:
>
> [...] Beyond that, any understanding you have of the underlying
> hardware is probably counterproductive.  (In other words, the more you
> know about cache coherency protocols, the more likely that "hardware
> perspective" is likely to pollute your mental model and get in the way
> of thinking in terms of the JMM.)
>

In my mental model, I cast the compiler in the role of the Nixon
administration, maintaining cautious optimism (optimization in this
case) in its single-minded pursuit, while preserving plausible
deniability about alleged illegal activities (prohibited actions) that
may have contributed to the result.  This is much more interesting
than reasoning about caches, I say, and it encourages the proper level
of respect for the compiler (well, and loathing, too).

From dcholmes at optusnet.com.au  Tue Sep 12 20:30:56 2006
From: dcholmes at optusnet.com.au (David Holmes)
Date: Wed, 13 Sep 2006 10:30:56 +1000
Subject: [concurrency-interest] concurrency puzzle
In-Reply-To: <1466c1d60609121250n255355f5p83a75b4249dc3a95@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCCEFIHCAA.dcholmes@optusnet.com.au>

Peter,

> Aha.. so you are saying that before 'invalidating' cache, all values
> are written to main memory? I though they were not written to main
> memory, but they were 'dropped'. So if they are already in main
> memory, you are lucky, if they only were in cache, you have lost your
> writes.
>
> This is exactly what I don't understand.

You really need to stop thinking about caches. The conceptual model of
"invalidating a cache" is just that - conceptual. Real caches don't work
this way. Actual cache coherency protocols are both simpler and more
complicated than this conceptual model. And this conceptual model is an
incomplete simplification. It was once a useful tool to help describe memory
model effects "imagine that the value is kept in a cache ...". The problem
is that people then go "Aha! In my MOESI-based cache system xxxx can't
happen therefore I don't have to worry about the memory model". That is very
bad because they do have to worry because in practice it isn't the hardware
cache they needed to be concerned about but the JIT compiler.

The memory model allows for things that may be impossible on a given piece
of hardware. The memory model defines the allowed behaviours. You can't
expect to map the memory models allowed behaviours onto the physical
behaviour of an actual machine. Nor should you try to
explain/rationalize/justify what the memory model allows by correlating that
with an actual machine's behaviour.

In your example a partial ordering is established (where -> means
happens-before):

  x=10 -> release_lock_by_T1 -> acquire_lock_by_T2 -> print(x)

  x=20 -> acquire_lock_by_T2 -> print(x)

So all that we can establish is that x=20 and x=10 happen before print(x).
But there is no constraint that either x=10->x=20 or that x=20->x=10. As the
two writes are unordered with respect to each other, either ordering is
possible and so the print(x) is allowed to print either 10 or 20. Whether it
will ever actually do so on your machine is a different question and
irrelevant as we always strive to write portable code.

Cheers,
David Holmes


From dcholmes at optusnet.com.au  Tue Sep 12 20:48:30 2006
From: dcholmes at optusnet.com.au (David Holmes)
Date: Wed, 13 Sep 2006 10:48:30 +1000
Subject: [concurrency-interest] BoundedPriorityBlockingQueue ?
In-Reply-To: <ca53c8f80609121319l55e749d5k6111e3841a70c467@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCAEFJHCAA.dcholmes@optusnet.com.au>

Hanson,

The basic issue here is that you use a PriorityBlockingQueue to prioritize
tasks, under the expectation that the queue will actually fill to some
extent, so the relative importance of tasks is, well, important. You then
place a bound on that queue and the result is you now have a second
(unbounded) queue of threads trying to submit prioritized tasks. So then you
wonder if those threads should be unblocked in order of priority of their
tasks? But that way madness lies ... this isn't real-time priority
preemptive systems programming. If you really think that the threads need to
be queued in priority order too then make your priority queue bigger. If you
size things out and you really must bound the size of the queue and you
really expect lots of threads to block waiting to put to the queue and you
really think the priority of the tasks those threads are submitting need to
be accounted for, then you have a problem. Basically you would have to have
each submitter check if its task is more important than the lowest priority
one in the queue and try to swap the new task for the old one; and then keep
retrying to submit the swapped out task. This is really, really messy. Throw
in priority-aging to avoid starvation and things get even worse. :)

Sometimes there is no complete solution - you have to define the boundaries
where you take into account things like priority. Sometimes the answer is to
have a completely different mechanism for things that are really important
(most queueing systems (banks, airlines etc) don't use priority ordered
queues, they use different queues for different "priorities" or allow
bypassing of the queue altogether.

Just my 2c.

Cheers,
David Holmes
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Hanson Char
  Sent: Wednesday, 13 September 2006 6:20 AM
  To: rbalamohan at sonoasystems.com
  Cc: Brian Goetz; concurrency-interest; Kasper Nielsen
  Subject: Re: [concurrency-interest] BoundedPriorityBlockingQueue ?


  I see blocking on put on a BoundedPriorityBlockingQueue is not such a good
idea.

  How about making the put method return a false boolean value upon queue
full, or alternatively throwing something like a QueueFullException ?

  Would this be useful/sensible ?

  Hanson


  On 9/12/06, Brian Goetz <brian at quiotix.com> wrote:
    The tricky part is if you want to guarantee that the thread offering the
    element of highest priority is first to be unblocked.


-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060913/f2678629/attachment.html 

From hanson.char at gmail.com  Tue Sep 12 20:58:59 2006
From: hanson.char at gmail.com (Hanson Char)
Date: Tue, 12 Sep 2006 17:58:59 -0700
Subject: [concurrency-interest] BoundedPriorityBlockingQueue ?
In-Reply-To: <NFBBKALFDCPFIDBNKAPCAEFJHCAA.dcholmes@optusnet.com.au>
References: <ca53c8f80609121319l55e749d5k6111e3841a70c467@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCAEFJHCAA.dcholmes@optusnet.com.au>
Message-ID: <ca53c8f80609121758r22758fe4p548de26a3cf7928d@mail.gmail.com>

Hi David,

Completely agree.  That's why I am proposing if there was such thing as a
Bounded PBQ, the BPBQ per se wouldn't want to deal with the madness when the
queue is full, but simply pass the onus to the user (via the return value of
put or via exception), and let the user decides what to do about it.

I don't know, however, if such behavior and such BPBQ would be useful at
all, and hence my question to the originator, rbalamohan.

Hanson

On 9/12/06, David Holmes <dcholmes at optusnet.com.au> wrote:
>
>  Hanson,
>
> The basic issue here is that you use a PriorityBlockingQueue to prioritize
> tasks, under the expectation that the queue will actually fill to some
> extent, so the relative importance of tasks is, well, important. You then
> place a bound on that queue and the result is you now have a second
> (unbounded) queue of threads trying to submit prioritized tasks. So then you
> wonder if those threads should be unblocked in order of priority of their
> tasks? But that way madness lies ... this isn't real-time priority
> preemptive systems programming. If you really think that the threads need to
> be queued in priority order too then make your priority queue bigger. If you
> size things out and you really must bound the size of the queue and you
> really expect lots of threads to block waiting to put to the queue and you
> really think the priority of the tasks those threads are submitting need to
> be accounted for, then you have a problem. Basically you would have to have
> each submitter check if its task is more important than the lowest priority
> one in the queue and try to swap the new task for the old one; and then keep
> retrying to submit the swapped out task. This is really, really messy. Throw
> in priority-aging to avoid starvation and things get even worse. :)
>
> Sometimes there is no complete solution - you have to define the
> boundaries where you take into account things like priority. Sometimes the
> answer is to have a completely different mechanism for things that are
> really important (most queueing systems (banks, airlines etc) don't use
> priority ordered queues, they use different queues for different
> "priorities" or allow bypassing of the queue altogether.
>
> Just my 2c.
>
> Cheers,
> David Holmes
>
> -----Original Message-----
> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
> concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of *Hanson Char
> *Sent:* Wednesday, 13 September 2006 6:20 AM
> *To:* rbalamohan at sonoasystems.com
> *Cc:* Brian Goetz; concurrency-interest; Kasper Nielsen
> *Subject:* Re: [concurrency-interest] BoundedPriorityBlockingQueue ?
>
> I see blocking on put on a BoundedPriorityBlockingQueue is not such a good
> idea.
>
> How about making the put method return a false boolean value upon queue
> full, or alternatively throwing something like a QueueFullException ?
>
> Would this be useful/sensible ?
>
> Hanson
>
> On 9/12/06, Brian Goetz <brian at quiotix.com> wrote:
> >
> > The tricky part is if you want to guarantee that the thread offering the
> > element of highest priority is first to be unblocked.
> >
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060912/4ca70cfb/attachment.html 

From dcholmes at optusnet.com.au  Tue Sep 12 21:09:01 2006
From: dcholmes at optusnet.com.au (David Holmes)
Date: Wed, 13 Sep 2006 11:09:01 +1000
Subject: [concurrency-interest] BoundedPriorityBlockingQueue ?
In-Reply-To: <ca53c8f80609121758r22758fe4p548de26a3cf7928d@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCIEFJHCAA.dcholmes@optusnet.com.au>

Sorry Hanson, I know you weren't the originator here.

What I failed to say, in response to your comments regarding return values
or exceptions is that BPBQ already has the necessary API's - there are
blocking and non-blocking forms of "put" and "take" so the application can
already choose to use non-blocking forms if they want to deal with a full
queue.

Cheers,
David
  -----Original Message-----
  From: Hanson Char [mailto:hanson.char at gmail.com]
  Sent: Wednesday, 13 September 2006 10:59 AM
  To: dholmes at ieee.org
  Cc: concurrency-interest
  Subject: Re: [concurrency-interest] BoundedPriorityBlockingQueue ?


  Hi David,

  Completely agree.  That's why I am proposing if there was such thing as a
Bounded PBQ, the BPBQ per se wouldn't want to deal with the madness when the
queue is full, but simply pass the onus to the user (via the return value of
put or via exception), and let the user decides what to do about it.

  I don't know, however, if such behavior and such BPBQ would be useful at
all, and hence my question to the originator, rbalamohan.

  Hanson


  On 9/12/06, David Holmes <dcholmes at optusnet.com.au> wrote:
    Hanson,

    The basic issue here is that you use a PriorityBlockingQueue to
prioritize tasks, under the expectation that the queue will actually fill to
some extent, so the relative importance of tasks is, well, important. You
then place a bound on that queue and the result is you now have a second
(unbounded) queue of threads trying to submit prioritized tasks. So then you
wonder if those threads should be unblocked in order of priority of their
tasks? But that way madness lies ... this isn't real-time priority
preemptive systems programming. If you really think that the threads need to
be queued in priority order too then make your priority queue bigger. If you
size things out and you really must bound the size of the queue and you
really expect lots of threads to block waiting to put to the queue and you
really think the priority of the tasks those threads are submitting need to
be accounted for, then you have a problem. Basically you would have to have
each submitter check if its task is more important than the lowest priority
one in the queue and try to swap the new task for the old one; and then keep
retrying to submit the swapped out task. This is really, really messy. Throw
in priority-aging to avoid starvation and things get even worse. :)

    Sometimes there is no complete solution - you have to define the
boundaries where you take into account things like priority. Sometimes the
answer is to have a completely different mechanism for things that are
really important (most queueing systems (banks, airlines etc) don't use
priority ordered queues, they use different queues for different
"priorities" or allow bypassing of the queue altogether.

    Just my 2c.

    Cheers,
    David Holmes
      -----Original Message-----
      From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Hanson Char
      Sent: Wednesday, 13 September 2006 6:20 AM
      To: rbalamohan at sonoasystems.com
      Cc: Brian Goetz; concurrency-interest; Kasper Nielsen
      Subject: Re: [concurrency-interest] BoundedPriorityBlockingQueue ?


      I see blocking on put on a BoundedPriorityBlockingQueue is not such a
good idea.

      How about making the put method return a false boolean value upon
queue full, or alternatively throwing something like a QueueFullException ?

      Would this be useful/sensible ?

      Hanson


      On 9/12/06, Brian Goetz <brian at quiotix.com> wrote:
        The tricky part is if you want to guarantee that the thread offering
the
        element of highest priority is first to be unblocked.




-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060913/19f118ba/attachment-0001.html 

From dcholmes at optusnet.com.au  Tue Sep 12 21:11:58 2006
From: dcholmes at optusnet.com.au (David Holmes)
Date: Wed, 13 Sep 2006 11:11:58 +1000
Subject: [concurrency-interest] BoundedPriorityBlockingQueue ?
In-Reply-To: <NFBBKALFDCPFIDBNKAPCIEFJHCAA.dcholmes@optusnet.com.au>
Message-ID: <NFBBKALFDCPFIDBNKAPCAEFKHCAA.dcholmes@optusnet.com.au>

Of course in the context of an executor, the non-blocking "put" is already
used so the application could deal with this via the
RejectedExecutionHandler.

Cheers,
David
  -----Original Message-----
  From: David Holmes [mailto:dcholmes at optusnet.com.au]
  Sent: Wednesday, 13 September 2006 11:09 AM
  To: Hanson Char
  Cc: concurrency-interest
  Subject: RE: [concurrency-interest] BoundedPriorityBlockingQueue ?


  Sorry Hanson, I know you weren't the originator here.

  What I failed to say, in response to your comments regarding return values
or exceptions is that BPBQ already has the necessary API's - there are
blocking and non-blocking forms of "put" and "take" so the application can
already choose to use non-blocking forms if they want to deal with a full
queue.

  Cheers,
  David
    -----Original Message-----
    From: Hanson Char [mailto:hanson.char at gmail.com]
    Sent: Wednesday, 13 September 2006 10:59 AM
    To: dholmes at ieee.org
    Cc: concurrency-interest
    Subject: Re: [concurrency-interest] BoundedPriorityBlockingQueue ?


    Hi David,

    Completely agree.  That's why I am proposing if there was such thing as
a Bounded PBQ, the BPBQ per se wouldn't want to deal with the madness when
the queue is full, but simply pass the onus to the user (via the return
value of put or via exception), and let the user decides what to do about
it.

    I don't know, however, if such behavior and such BPBQ would be useful at
all, and hence my question to the originator, rbalamohan.

    Hanson


    On 9/12/06, David Holmes <dcholmes at optusnet.com.au> wrote:
      Hanson,

      The basic issue here is that you use a PriorityBlockingQueue to
prioritize tasks, under the expectation that the queue will actually fill to
some extent, so the relative importance of tasks is, well, important. You
then place a bound on that queue and the result is you now have a second
(unbounded) queue of threads trying to submit prioritized tasks. So then you
wonder if those threads should be unblocked in order of priority of their
tasks? But that way madness lies ... this isn't real-time priority
preemptive systems programming. If you really think that the threads need to
be queued in priority order too then make your priority queue bigger. If you
size things out and you really must bound the size of the queue and you
really expect lots of threads to block waiting to put to the queue and you
really think the priority of the tasks those threads are submitting need to
be accounted for, then you have a problem. Basically you would have to have
each submitter check if its task is more important than the lowest priority
one in the queue and try to swap the new task for the old one; and then keep
retrying to submit the swapped out task. This is really, really messy. Throw
in priority-aging to avoid starvation and things get even worse. :)

      Sometimes there is no complete solution - you have to define the
boundaries where you take into account things like priority. Sometimes the
answer is to have a completely different mechanism for things that are
really important (most queueing systems (banks, airlines etc) don't use
priority ordered queues, they use different queues for different
"priorities" or allow bypassing of the queue altogether.

      Just my 2c.

      Cheers,
      David Holmes
        -----Original Message-----
        From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Hanson Char
        Sent: Wednesday, 13 September 2006 6:20 AM
        To: rbalamohan at sonoasystems.com
        Cc: Brian Goetz; concurrency-interest; Kasper Nielsen
        Subject: Re: [concurrency-interest] BoundedPriorityBlockingQueue ?


        I see blocking on put on a BoundedPriorityBlockingQueue is not such
a good idea.

        How about making the put method return a false boolean value upon
queue full, or alternatively throwing something like a QueueFullException ?

        Would this be useful/sensible ?

        Hanson


        On 9/12/06, Brian Goetz <brian at quiotix.com> wrote:
          The tricky part is if you want to guarantee that the thread
offering the
          element of highest priority is first to be unblocked.




-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060913/f2f3e85a/attachment.html 

From hanson.char at gmail.com  Tue Sep 12 21:25:22 2006
From: hanson.char at gmail.com (Hanson Char)
Date: Tue, 12 Sep 2006 18:25:22 -0700
Subject: [concurrency-interest] BoundedPriorityBlockingQueue ?
In-Reply-To: <NFBBKALFDCPFIDBNKAPCIEFJHCAA.dcholmes@optusnet.com.au>
References: <ca53c8f80609121758r22758fe4p548de26a3cf7928d@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCIEFJHCAA.dcholmes@optusnet.com.au>
Message-ID: <ca53c8f80609121825h6b0f98ak6c7c0af60a9abd77@mail.gmail.com>

Hi David,

I suppose you mean PBQ (as opposed to BPBQ which doesn't yet exist) already
has the necessary API.

True.  However, although there are both the blocking "put" and non-blocking
"offer" methods in PBQ, the blocking "put" as at now never blocks and the
non-blocking "offer" always return true (ie succeed) because PBQ is
unbounded!

What I am suggesting is that if we implemented something like a Bounded PBQ
(BPBQ), then we could "meaningfully" make "offer" return false when the
queue is full (and true otherwise).  Since the return type of "put" is void,
we can throw something like a QueueFullException when the queue is full.

"meaningful" or not, of course, depends whether such BPBQ is considered
useful, which I don't know.

Hanson

On 9/12/06, David Holmes <dcholmes at optusnet.com.au> wrote:
>
>  Sorry Hanson, I know you weren't the originator here.
>
> What I failed to say, in response to your comments regarding return values
> or exceptions is that BPBQ already has the necessary API's - there are
> blocking and non-blocking forms of "put" and "take" so the application can
> already choose to use non-blocking forms if they want to deal with a full
> queue.
>
> Cheers,
> David
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060912/c7e8ad9c/attachment.html 

From dcholmes at optusnet.com.au  Tue Sep 12 21:29:48 2006
From: dcholmes at optusnet.com.au (David Holmes)
Date: Wed, 13 Sep 2006 11:29:48 +1000
Subject: [concurrency-interest] BoundedPriorityBlockingQueue ?
In-Reply-To: <ca53c8f80609121825h6b0f98ak6c7c0af60a9abd77@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCOEFKHCAA.dcholmes@optusnet.com.au>

Hanson,

I'm confused: BoundedPriorityBlockingQueue is-a BlockingQueue and so we
would have the normal semantics for BlockingQueue methods on a bounded
implementation. So offer() would return false, and put() would block, if the
queue were full.

David
  -----Original Message-----
  From: Hanson Char [mailto:hanson.char at gmail.com]
  Sent: Wednesday, 13 September 2006 11:25 AM
  To: dholmes at ieee.org
  Cc: concurrency-interest
  Subject: Re: [concurrency-interest] BoundedPriorityBlockingQueue ?


  Hi David,

  I suppose you mean PBQ (as opposed to BPBQ which doesn't yet exist)
already has the necessary API.

  True.  However, although there are both the blocking "put" and
non-blocking "offer" methods in PBQ, the blocking "put" as at now never
blocks and the non-blocking "offer" always return true (ie succeed) because
PBQ is unbounded!

  What I am suggesting is that if we implemented something like a Bounded
PBQ (BPBQ), then we could "meaningfully" make "offer" return false when the
queue is full (and true otherwise).  Since the return type of "put" is void,
we can throw something like a QueueFullException when the queue is full.

  "meaningful" or not, of course, depends whether such BPBQ is considered
useful, which I don't know.

  Hanson


  On 9/12/06, David Holmes <dcholmes at optusnet.com.au> wrote:
    Sorry Hanson, I know you weren't the originator here.

    What I failed to say, in response to your comments regarding return
values or exceptions is that BPBQ already has the necessary API's - there
are blocking and non-blocking forms of "put" and "take" so the application
can already choose to use non-blocking forms if they want to deal with a
full queue.

    Cheers,
    David

-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060913/cfeefe80/attachment.html 

From hanson.char at gmail.com  Tue Sep 12 21:37:46 2006
From: hanson.char at gmail.com (Hanson Char)
Date: Tue, 12 Sep 2006 18:37:46 -0700
Subject: [concurrency-interest] BoundedPriorityBlockingQueue ?
In-Reply-To: <NFBBKALFDCPFIDBNKAPCOEFKHCAA.dcholmes@optusnet.com.au>
References: <ca53c8f80609121825h6b0f98ak6c7c0af60a9abd77@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCOEFKHCAA.dcholmes@optusnet.com.au>
Message-ID: <ca53c8f80609121837k22381567y16f3c26474ee329b@mail.gmail.com>

Hi David,

Sorry for the confusion.  I guess I am proposing something with sematics
slightly different than the formal BlockingQueue interface, in that the
"put" semantics need to be different in order to avoid the madness.

I guess the named BoundedPriorityBlockingQueue is pretty misleading in such
case, but instead should be something like a BoundedPriorityQueue, for the
lack of a better name, which provides blocking methods only on the
dequeueing side, but non-blocking methods for the enqueueing side.

Hanson


On 9/12/06, David Holmes <dcholmes at optusnet.com.au> wrote:
>
>  Hanson,
>
> I'm confused: BoundedPriorityBlockingQueue is-a BlockingQueue and so we
> would have the normal semantics for BlockingQueue methods on a bounded
> implementation. So offer() would return false, and put() would block, if the
> queue were full.
>
> David
>
> -----Original Message-----
> *From:* Hanson Char [mailto:hanson.char at gmail.com]
> *Sent:* Wednesday, 13 September 2006 11:25 AM
> *To:* dholmes at ieee.org
> *Cc:* concurrency-interest
> *Subject:* Re: [concurrency-interest] BoundedPriorityBlockingQueue ?
>
> Hi David,
>
> I suppose you mean PBQ (as opposed to BPBQ which doesn't yet exist)
> already has the necessary API.
>
> True.  However, although there are both the blocking "put" and
> non-blocking "offer" methods in PBQ, the blocking "put" as at now never
> blocks and the non-blocking "offer" always return true (ie succeed) because
> PBQ is unbounded!
>
> What I am suggesting is that if we implemented something like a Bounded
> PBQ (BPBQ), then we could "meaningfully" make "offer" return false when the
> queue is full (and true otherwise).  Since the return type of "put" is void,
> we can throw something like a QueueFullException when the queue is full.
>
> "meaningful" or not, of course, depends whether such BPBQ is considered
> useful, which I don't know.
>
> Hanson
>
> On 9/12/06, David Holmes <dcholmes at optusnet.com.au> wrote:
> >
> >  Sorry Hanson, I know you weren't the originator here.
> >
> > What I failed to say, in response to your comments regarding return
> > values or exceptions is that BPBQ already has the necessary API's - there
> > are blocking and non-blocking forms of "put" and "take" so the application
> > can already choose to use non-blocking forms if they want to deal with a
> > full queue.
> >
> > Cheers,
> >  David
> >
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060912/39a038db/attachment-0001.html 

From dcholmes at optusnet.com.au  Tue Sep 12 21:42:22 2006
From: dcholmes at optusnet.com.au (David Holmes)
Date: Wed, 13 Sep 2006 11:42:22 +1000
Subject: [concurrency-interest] BoundedPriorityBlockingQueue ?
In-Reply-To: <ca53c8f80609121837k22381567y16f3c26474ee329b@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCIEFLHCAA.dcholmes@optusnet.com.au>

Hanson,

I don't see why we need different "put" semantics. If the user were
concerned that blocking puts weren't ordered correctly then the user should
just use non-blocking offer() and "deal with it" when it returns false. If
you slipped such a thing into a ThreadPoolExecutor today it would just work,
and the RejectedExecutionHandler would be the way the application could
"deal with it".

Cheers,
David
  -----Original Message-----
  From: Hanson Char [mailto:hanson.char at gmail.com]
  Sent: Wednesday, 13 September 2006 11:38 AM
  To: dholmes at ieee.org
  Cc: concurrency-interest
  Subject: Re: [concurrency-interest] BoundedPriorityBlockingQueue ?


  Hi David,

  Sorry for the confusion.  I guess I am proposing something with sematics
slightly different than the formal BlockingQueue interface, in that the
"put" semantics need to be different in order to avoid the madness.

  I guess the named BoundedPriorityBlockingQueue is pretty misleading in
such case, but instead should be something like a BoundedPriorityQueue, for
the lack of a better name, which provides blocking methods only on the
dequeueing side, but non-blocking methods for the enqueueing side.

  Hanson



  On 9/12/06, David Holmes <dcholmes at optusnet.com.au> wrote:
    Hanson,

    I'm confused: BoundedPriorityBlockingQueue is-a BlockingQueue and so we
would have the normal semantics for BlockingQueue methods on a bounded
implementation. So offer() would return false, and put() would block, if the
queue were full.

    David
      -----Original Message-----
      From: Hanson Char [mailto:hanson.char at gmail.com]

      Sent: Wednesday, 13 September 2006 11:25 AM
      To: dholmes at ieee.org
      Cc: concurrency-interest
      Subject: Re: [concurrency-interest] BoundedPriorityBlockingQueue ?


    Hi David,

    I suppose you mean PBQ (as opposed to BPBQ which doesn't yet exist)
already has the necessary API.

    True.  However, although there are both the blocking "put" and
non-blocking "offer" methods in PBQ, the blocking "put" as at now never
blocks and the non-blocking "offer" always return true (ie succeed) because
PBQ is unbounded!

    What I am suggesting is that if we implemented something like a Bounded
PBQ (BPBQ), then we could "meaningfully" make "offer" return false when the
queue is full (and true otherwise).  Since the return type of "put" is void,
we can throw something like a QueueFullException when the queue is full.

    "meaningful" or not, of course, depends whether such BPBQ is considered
useful, which I don't know.

    Hanson


    On 9/12/06, David Holmes <dcholmes at optusnet.com.au> wrote:
      Sorry Hanson, I know you weren't the originator here.

      What I failed to say, in response to your comments regarding return
values or exceptions is that BPBQ already has the necessary API's - there
are blocking and non-blocking forms of "put" and "take" so the application
can already choose to use non-blocking forms if they want to deal with a
full queue.

      Cheers,
      David



-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060913/6c569216/attachment.html 

From hanson.char at gmail.com  Tue Sep 12 21:42:37 2006
From: hanson.char at gmail.com (Hanson Char)
Date: Tue, 12 Sep 2006 18:42:37 -0700
Subject: [concurrency-interest] BoundedPriorityBlockingQueue ?
In-Reply-To: <ca53c8f80609121837k22381567y16f3c26474ee329b@mail.gmail.com>
References: <ca53c8f80609121825h6b0f98ak6c7c0af60a9abd77@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCOEFKHCAA.dcholmes@optusnet.com.au>
	<ca53c8f80609121837k22381567y16f3c26474ee329b@mail.gmail.com>
Message-ID: <ca53c8f80609121842n36b4f8det130d22ea84b15ab9@mail.gmail.com>

>something like a BoundedPriorityQueue, for the lack of a better name, which
provides
>which provides blocking methods only on the dequeueing side, but
non-blocking methods
>for the enqueueing side.

That is, the enqueueing side of such "BPQ" has a "put" that can throw
exception, and an "offer" that can return false, iff the queue were full.

Hope I am not causing further confusion.

Hanson

On 9/12/06, Hanson Char <hanson.char at gmail.com> wrote:
>
> Hi David,
>
> Sorry for the confusion.  I guess I am proposing something with sematics
> slightly different than the formal BlockingQueue interface, in that the
> "put" semantics need to be different in order to avoid the madness.
>
> I guess the named BoundedPriorityBlockingQueue is pretty misleading in
> such case, but instead should be something like a BoundedPriorityQueue, for
> the lack of a better name, which provides blocking methods only on the
> dequeueing side, but non-blocking methods for the enqueueing side.
>
>
> Hanson
>
>
> On 9/12/06, David Holmes <dcholmes at optusnet.com.au> wrote:
> >
> >  Hanson,
> >
> > I'm confused: BoundedPriorityBlockingQueue is-a BlockingQueue and so we
> > would have the normal semantics for BlockingQueue methods on a bounded
> > implementation. So offer() would return false, and put() would block, if the
> > queue were full.
> >
> > David
> >
> > -----Original Message-----
> > *From:* Hanson Char [mailto:hanson.char at gmail.com]
> > *Sent:* Wednesday, 13 September 2006 11:25 AM
> > *To:* dholmes at ieee.org
> > *Cc:* concurrency-interest
> > *Subject:* Re: [concurrency-interest] BoundedPriorityBlockingQueue ?
> >
> > Hi David,
> >
> > I suppose you mean PBQ (as opposed to BPBQ which doesn't yet exist)
> > already has the necessary API.
> >
> > True.  However, although there are both the blocking "put" and
> > non-blocking "offer" methods in PBQ, the blocking "put" as at now never
> > blocks and the non-blocking "offer" always return true (ie succeed) because
> > PBQ is unbounded!
> >
> > What I am suggesting is that if we implemented something like a Bounded
> > PBQ (BPBQ), then we could "meaningfully" make "offer" return false when the
> > queue is full (and true otherwise).  Since the return type of "put" is void,
> > we can throw something like a QueueFullException when the queue is full.
> >
> > "meaningful" or not, of course, depends whether such BPBQ is considered
> > useful, which I don't know.
> >
> > Hanson
> >
> > On 9/12/06, David Holmes <dcholmes at optusnet.com.au> wrote:
> > >
> > >  Sorry Hanson, I know you weren't the originator here.
> > >
> > > What I failed to say, in response to your comments regarding return
> > > values or exceptions is that BPBQ already has the necessary API's - there
> > > are blocking and non-blocking forms of "put" and "take" so the application
> > > can already choose to use non-blocking forms if they want to deal with a
> > > full queue.
> > >
> > > Cheers,
> > >  David
> > >
> >
> >
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060912/e9ba8f41/attachment.html 

From hanson.char at gmail.com  Tue Sep 12 21:49:26 2006
From: hanson.char at gmail.com (Hanson Char)
Date: Tue, 12 Sep 2006 18:49:26 -0700
Subject: [concurrency-interest] BoundedPriorityBlockingQueue ?
In-Reply-To: <NFBBKALFDCPFIDBNKAPCIEFLHCAA.dcholmes@optusnet.com.au>
References: <ca53c8f80609121837k22381567y16f3c26474ee329b@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCIEFLHCAA.dcholmes@optusnet.com.au>
Message-ID: <ca53c8f80609121849q54fff958j780311aea3fbf67d@mail.gmail.com>

Hi David,

>If the user were concerned that blocking puts weren't ordered correctly
>then the user should just use non-blocking offer() and "deal with it"
>when it returns false.

But the "problem" is PriorityBlockingQueue as at now
1) never returns false when offer() is invoked; and
2) unbounded

In other words, the user never has a chance to deal with the queue-full
situation if PBQ is used, unless he has something like the
"BoundedPriorityQueue" beast I suggest.

Hanson

On 9/12/06, David Holmes <dcholmes at optusnet.com.au> wrote:
>
>  Hanson,
>
> I don't see why we need different "put" semantics. If the user were
> concerned that blocking puts weren't ordered correctly then the user should
> just use non-blocking offer() and "deal with it" when it returns false. If
> you slipped such a thing into a ThreadPoolExecutor today it would just work,
> and the RejectedExecutionHandler would be the way the application could
> "deal with it".
>
> Cheers,
> David
>
> -----Original Message-----
> *From:* Hanson Char [mailto:hanson.char at gmail.com]
> *Sent:* Wednesday, 13 September 2006 11:38 AM
> *To:* dholmes at ieee.org
> *Cc:* concurrency-interest
> *Subject:* Re: [concurrency-interest] BoundedPriorityBlockingQueue ?
>
> Hi David,
>
> Sorry for the confusion.  I guess I am proposing something with sematics
> slightly different than the formal BlockingQueue interface, in that the
> "put" semantics need to be different in order to avoid the madness.
>
> I guess the named BoundedPriorityBlockingQueue is pretty misleading in
> such case, but instead should be something like a BoundedPriorityQueue, for
> the lack of a better name, which provides blocking methods only on the
> dequeueing side, but non-blocking methods for the enqueueing side.
>
> Hanson
>
>
> On 9/12/06, David Holmes <dcholmes at optusnet.com.au> wrote:
> >
> >  Hanson,
> >
> > I'm confused: BoundedPriorityBlockingQueue is-a BlockingQueue and so we
> > would have the normal semantics for BlockingQueue methods on a bounded
> > implementation. So offer() would return false, and put() would block, if the
> > queue were full.
> >
> > David
> >
> >  -----Original Message-----
> > *From:* Hanson Char [mailto:hanson.char at gmail.com]
> > *Sent:* Wednesday, 13 September 2006 11:25 AM
> > *To:* dholmes at ieee.org
> > *Cc:* concurrency-interest
> > *Subject:* Re: [concurrency-interest] BoundedPriorityBlockingQueue ?
> >
> >  Hi David,
> >
> > I suppose you mean PBQ (as opposed to BPBQ which doesn't yet exist)
> > already has the necessary API.
> >
> > True.  However, although there are both the blocking "put" and
> > non-blocking "offer" methods in PBQ, the blocking "put" as at now never
> > blocks and the non-blocking "offer" always return true (ie succeed) because
> > PBQ is unbounded!
> >
> > What I am suggesting is that if we implemented something like a Bounded
> > PBQ (BPBQ), then we could "meaningfully" make "offer" return false when the
> > queue is full (and true otherwise).  Since the return type of "put" is void,
> > we can throw something like a QueueFullException when the queue is full.
> >
> > "meaningful" or not, of course, depends whether such BPBQ is considered
> > useful, which I don't know.
> >
> > Hanson
> >
> > On 9/12/06, David Holmes <dcholmes at optusnet.com.au> wrote:
> > >
> > >  Sorry Hanson, I know you weren't the originator here.
> > >
> > > What I failed to say, in response to your comments regarding return
> > > values or exceptions is that BPBQ already has the necessary API's - there
> > > are blocking and non-blocking forms of "put" and "take" so the application
> > > can already choose to use non-blocking forms if they want to deal with a
> > > full queue.
> > >
> > > Cheers,
> > >  David
> > >
> >
> >
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060912/061ba82d/attachment-0001.html 

From dcholmes at optusnet.com.au  Tue Sep 12 21:52:52 2006
From: dcholmes at optusnet.com.au (David Holmes)
Date: Wed, 13 Sep 2006 11:52:52 +1000
Subject: [concurrency-interest] BoundedPriorityBlockingQueue ?
In-Reply-To: <ca53c8f80609121849q54fff958j780311aea3fbf67d@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCCEFMHCAA.dcholmes@optusnet.com.au>

Hanson,

We are miscommunicating. If we created a bounded PBQ then it need only have
normal BQ semantics. With such a bounded PBQ the user would use put() or
offer() based on their own needs regarding what should happen when the queue
is full.

Cheers,
David
  -----Original Message-----
  From: Hanson Char [mailto:hanson.char at gmail.com]
  Sent: Wednesday, 13 September 2006 11:49 AM
  To: dholmes at ieee.org
  Cc: concurrency-interest
  Subject: Re: [concurrency-interest] BoundedPriorityBlockingQueue ?


  Hi David,

  >If the user were concerned that blocking puts weren't ordered correctly
  >then the user should just use non-blocking offer() and "deal with it"
  >when it returns false.

  But the "problem" is PriorityBlockingQueue as at now
  1) never returns false when offer() is invoked; and
  2) unbounded

  In other words, the user never has a chance to deal with the queue-full
situation if PBQ is used, unless he has something like the
"BoundedPriorityQueue" beast I suggest.

  Hanson


  On 9/12/06, David Holmes <dcholmes at optusnet.com.au> wrote:
    Hanson,

    I don't see why we need different "put" semantics. If the user were
concerned that blocking puts weren't ordered correctly then the user should
just use non-blocking offer() and "deal with it" when it returns false. If
you slipped such a thing into a ThreadPoolExecutor today it would just work,
and the RejectedExecutionHandler would be the way the application could
"deal with it".

    Cheers,
    David
      -----Original Message-----
      From: Hanson Char [mailto:hanson.char at gmail.com]

      Sent: Wednesday, 13 September 2006 11:38 AM
      To: dholmes at ieee.org
      Cc: concurrency-interest
      Subject: Re: [concurrency-interest] BoundedPriorityBlockingQueue ?


    Hi David,

    Sorry for the confusion.  I guess I am proposing something with sematics
slightly different than the formal BlockingQueue interface, in that the
"put" semantics need to be different in order to avoid the madness.

    I guess the named BoundedPriorityBlockingQueue is pretty misleading in
such case, but instead should be something like a BoundedPriorityQueue, for
the lack of a better name, which provides blocking methods only on the
dequeueing side, but non-blocking methods for the enqueueing side.

    Hanson



    On 9/12/06, David Holmes <dcholmes at optusnet.com.au> wrote:
      Hanson,

      I'm confused: BoundedPriorityBlockingQueue is-a BlockingQueue and so
we would have the normal semantics for BlockingQueue methods on a bounded
implementation. So offer() would return false, and put() would block, if the
queue were full.

      David
        -----Original Message-----
        From: Hanson Char [mailto:hanson.char at gmail.com]

        Sent: Wednesday, 13 September 2006 11:25 AM
        To: dholmes at ieee.org
        Cc: concurrency-interest
        Subject: Re: [concurrency-interest] BoundedPriorityBlockingQueue ?


      Hi David,

      I suppose you mean PBQ (as opposed to BPBQ which doesn't yet exist)
already has the necessary API.

      True.  However, although there are both the blocking "put" and
non-blocking "offer" methods in PBQ, the blocking "put" as at now never
blocks and the non-blocking "offer" always return true (ie succeed) because
PBQ is unbounded!

      What I am suggesting is that if we implemented something like a
Bounded PBQ (BPBQ), then we could "meaningfully" make "offer" return false
when the queue is full (and true otherwise).  Since the return type of "put"
is void, we can throw something like a QueueFullException when the queue is
full.

      "meaningful" or not, of course, depends whether such BPBQ is
considered useful, which I don't know.

      Hanson


      On 9/12/06, David Holmes <dcholmes at optusnet.com.au> wrote:
        Sorry Hanson, I know you weren't the originator here.

        What I failed to say, in response to your comments regarding return
values or exceptions is that BPBQ already has the necessary API's - there
are blocking and non-blocking forms of "put" and "take" so the application
can already choose to use non-blocking forms if they want to deal with a
full queue.

        Cheers,
        David





-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060913/152cecab/attachment.html 

From hanson.char at gmail.com  Tue Sep 12 21:58:49 2006
From: hanson.char at gmail.com (Hanson Char)
Date: Tue, 12 Sep 2006 18:58:49 -0700
Subject: [concurrency-interest] BoundedPriorityBlockingQueue ?
In-Reply-To: <NFBBKALFDCPFIDBNKAPCCEFMHCAA.dcholmes@optusnet.com.au>
References: <ca53c8f80609121849q54fff958j780311aea3fbf67d@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCCEFMHCAA.dcholmes@optusnet.com.au>
Message-ID: <ca53c8f80609121858m1f2b827fp7e6e5462ee317343@mail.gmail.com>

Hi David,

But didn't you point out the normal blocking BQ put() semantics in a
queue-full situation for a bounded PBQ would lead to madness ?

If so, why not avoid addressing BQ and create something that might be useful
but not a kind of BQ ?

Hanson

On 9/12/06, David Holmes <dcholmes at optusnet.com.au> wrote:
>
>  Hanson,
>
> We are miscommunicating. If we created a bounded PBQ then it need only
> have normal BQ semantics. With such a bounded PBQ the user would use put()
> or offer() based on their own needs regarding what should happen when the
> queue is full.
>
> Cheers,
> David
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060912/5c2b2f26/attachment.html 

From dcholmes at optusnet.com.au  Tue Sep 12 22:10:29 2006
From: dcholmes at optusnet.com.au (David Holmes)
Date: Wed, 13 Sep 2006 12:10:29 +1000
Subject: [concurrency-interest] BoundedPriorityBlockingQueue ?
In-Reply-To: <ca53c8f80609121858m1f2b827fp7e6e5462ee317343@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCMEFNHCAA.dcholmes@optusnet.com.au>

Hi Hanson,

Actually what I said, or meant, was the other way around. :)

If you use a blocking put() then you are accepting that you don't care about
the "priority" ordering of threads trying to submit. No problem, no
complexity and no "madness". :)

If you do care about the order of threads trying to submit then you should
not use put() but use offer() so that you can "deal with it". It is in
trying to "deal with it" that I think the "madness" lies ;-)

So the normal BQ semantics are quite sufficient for the job. No need to
introduce a new kind of semi-blocking queue with similar methods but
different semantics. The user can choose blocking put() with no
blocking-order guarantees; or else non-blocking offer() and figure it out
themselves.

Cheers,
David
  -----Original Message-----
  From: Hanson Char [mailto:hanson.char at gmail.com]
  Sent: Wednesday, 13 September 2006 11:59 AM
  To: dholmes at ieee.org
  Cc: concurrency-interest
  Subject: Re: [concurrency-interest] BoundedPriorityBlockingQueue ?


  Hi David,

  But didn't you point out the normal blocking BQ put() semantics in a
queue-full situation for a bounded PBQ would lead to madness ?

  If so, why not avoid addressing BQ and create something that might be
useful but not a kind of BQ ?

  Hanson


  On 9/12/06, David Holmes <dcholmes at optusnet.com.au> wrote:
    Hanson,

    We are miscommunicating. If we created a bounded PBQ then it need only
have normal BQ semantics. With such a bounded PBQ the user would use put()
or offer() based on their own needs regarding what should happen when the
queue is full.

    Cheers,
    David

-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060913/815ca4e3/attachment.html 

From hanson.char at gmail.com  Tue Sep 12 22:18:09 2006
From: hanson.char at gmail.com (Hanson Char)
Date: Tue, 12 Sep 2006 19:18:09 -0700
Subject: [concurrency-interest] BoundedPriorityBlockingQueue ?
In-Reply-To: <NFBBKALFDCPFIDBNKAPCMEFNHCAA.dcholmes@optusnet.com.au>
References: <ca53c8f80609121858m1f2b827fp7e6e5462ee317343@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCMEFNHCAA.dcholmes@optusnet.com.au>
Message-ID: <ca53c8f80609121918j7bbe567ci1e829452d8bbad91@mail.gmail.com>

Hi David,

I see.  This sounds good to me too.  And it doesn't seem difficult to
implement one.  Whether it's useful or worth implementing is a question for
the originator.

Hanson

On 9/12/06, David Holmes <dcholmes at optusnet.com.au> wrote:
>
>  Hi Hanson,
>
> Actually what I said, or meant, was the other way around. :)
>
> If you use a blocking put() then you are accepting that you don't care
> about the "priority" ordering of threads trying to submit. No problem, no
> complexity and no "madness". :)
>
> If you do care about the order of threads trying to submit then you should
> not use put() but use offer() so that you can "deal with it". It is in
> trying to "deal with it" that I think the "madness" lies ;-)
>
> So the normal BQ semantics are quite sufficient for the job. No need to
> introduce a new kind of semi-blocking queue with similar methods but
> different semantics. The user can choose blocking put() with no
> blocking-order guarantees; or else non-blocking offer() and figure it out
> themselves.
>
> Cheers,
> David
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060912/63d3e70a/attachment-0001.html 

From dhanji at gmail.com  Tue Sep 12 22:41:25 2006
From: dhanji at gmail.com (Dhanji R. Prasanna)
Date: Wed, 13 Sep 2006 12:41:25 +1000
Subject: [concurrency-interest] BoundedPriorityBlockingQueue ?
In-Reply-To: <NFBBKALFDCPFIDBNKAPCAEFJHCAA.dcholmes@optusnet.com.au>
References: <ca53c8f80609121319l55e749d5k6111e3841a70c467@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCAEFJHCAA.dcholmes@optusnet.com.au>
Message-ID: <aa067ea10609121941n657fb591ka5e09daf7283a22b@mail.gmail.com>

This is a very interesting discussion. I think separate queues for
different priorities are a better answer. I know that in short message
services (SMS) we tended even to use different applications based on
priorities (HA for billing messages for example).

The potential for a low priority take to slip past a high priority put
and cause unwanted delays is too great to entrust it to one
queue/threadpool. Some of it comes down to how you want to quantify
low and high priorities. If there are mission critical differences I
certainly would not trust them to the same threadpool.

On 9/13/06, David Holmes <dcholmes at optusnet.com.au> wrote:
>
>
> Hanson,
>
> The basic issue here is that you use a PriorityBlockingQueue to prioritize
> tasks, under the expectation that the queue will actually fill to some
> extent, so the relative importance of tasks is, well, important. You then
> place a bound on that queue and the result is you now have a second
> (unbounded) queue of threads trying to submit prioritized tasks. So then you
> wonder if those threads should be unblocked in order of priority of their
> tasks? But that way madness lies ... this isn't real-time priority
> preemptive systems programming. If you really think that the threads need to
> be queued in priority order too then make your priority queue bigger. If you
> size things out and you really must bound the size of the queue and you
> really expect lots of threads to block waiting to put to the queue and you
> really think the priority of the tasks those threads are submitting need to
> be accounted for, then you have a problem. Basically you would have to have
> each submitter check if its task is more important than the lowest priority
> one in the queue and try to swap the new task for the old one; and then keep
> retrying to submit the swapped out task. This is really, really messy. Throw
> in priority-aging to avoid starvation and things get even worse. :)
>
> Sometimes there is no complete solution - you have to define the boundaries
> where you take into account things like priority. Sometimes the answer is to
> have a completely different mechanism for things that are really important
> (most queueing systems (banks, airlines etc) don't use priority ordered
> queues, they use different queues for different "priorities" or allow
> bypassing of the queue altogether.
>
> Just my 2c.
>
> Cheers,
>
> David Holmes
>
>
> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On
> Behalf Of Hanson Char
> Sent: Wednesday, 13 September 2006 6:20 AM
> To: rbalamohan at sonoasystems.com
> Cc: Brian Goetz; concurrency-interest; Kasper Nielsen
> Subject: Re: [concurrency-interest] BoundedPriorityBlockingQueue ?
>
> I see blocking on put on a BoundedPriorityBlockingQueue is not such a good
> idea.
>
> How about making the put method return a false boolean value upon queue
> full, or alternatively throwing something like a QueueFullException ?
>
> Would this be useful/sensible ?
>
> Hanson
>
>
> On 9/12/06, Brian Goetz <brian at quiotix.com> wrote:
> > The tricky part is if you want to guarantee that the thread offering the
> > element of highest priority is first to be unblocked.
> >
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>

From rbalamohan at sonoasystems.com  Tue Sep 12 22:55:26 2006
From: rbalamohan at sonoasystems.com (rbalamohan)
Date: Wed, 13 Sep 2006 02:55:26 +0000
Subject: [concurrency-interest] BoundedPriorityBlockingQueue ?
In-Reply-To: <ca53c8f80609121758r22758fe4p548de26a3cf7928d@mail.gmail.com>
References: <ca53c8f80609121319l55e749d5k6111e3841a70c467@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCAEFJHCAA.dcholmes@optusnet.com.au>
	<ca53c8f80609121758r22758fe4p548de26a3cf7928d@mail.gmail.com>
Message-ID: <1158116128.32641.5.camel@xen-host-ce>

I agree with David on this. However, in certain cases like
ThreadPoolExecutor as I mentioned earlier, having BPBQ might be useful.

The way threadPoolExecutor handles execute(Runnable command) is that,
"if the task can't be submitted due to shutdown or because the capacity
has been reached, the task is handled by RejectedExecutionHandler".

Also, we pass a BlockingQueue (basically the workerQueue) in the
ThreadPoolExecutor constructor. In this case, we can pass a BPBQ and
have the priorities assigned to the tasks being submitted. Also, when
the queue is full, this situation can be handled by the
RejectionExecutionHandler. 

~Rajesh.B

On Tue, 2006-09-12 at 17:58 -0700, Hanson Char wrote:

> Hi David,
> 
> Completely agree.  That's why I am proposing if there was such thing
> as a Bounded PBQ, the BPBQ per se wouldn't want to deal with the
> madness when the queue is full, but simply pass the onus to the user
> (via the return value of put or via exception), and let the user
> decides what to do about it. 
> 
> I don't know, however, if such behavior and such BPBQ would be useful
> at all, and hence my question to the originator, rbalamohan.
> 
> Hanson
> 
> On 9/12/06, David Holmes <dcholmes at optusnet.com.au> wrote:
> 
>         Hanson,
>          
>         The basic issue here is that you use a PriorityBlockingQueue
>         to prioritize tasks, under the expectation that the queue will
>         actually fill to some extent, so the relative importance of
>         tasks is, well, important. You then place a bound on that
>         queue and the result is you now have a second (unbounded)
>         queue of threads trying to submit prioritized tasks. So then
>         you wonder if those threads should be unblocked in order of
>         priority of their tasks? But that way madness lies ... this
>         isn't real-time priority preemptive systems programming. If
>         you really think that the threads need to be queued in
>         priority order too then make your priority queue bigger. If
>         you size things out and you really must bound the size of the
>         queue and you really expect lots of threads to block waiting
>         to put to the queue and you really think the priority of the
>         tasks those threads are submitting need to be accounted for,
>         then you have a problem. Basically you would have to have each
>         submitter check if its task is more important than the lowest
>         priority one in the queue and try to swap the new task for the
>         old one; and then keep retrying to submit the swapped out
>         task. This is really, really messy. Throw in priority-aging to
>         avoid starvation and things get even worse. :)
>          
>         Sometimes there is no complete solution - you have to define
>         the boundaries where you take into account things like
>         priority. Sometimes the answer is to have a completely
>         different mechanism for things that are really important (most
>         queueing systems (banks, airlines etc) don't use priority
>         ordered queues, they use different queues for different
>         "priorities" or allow bypassing of the queue altogether.
>          
>         Just my 2c.
>          
>         Cheers,
>         David Holmes
>         
>                 -----Original Message-----
>                 From: concurrency-interest-bounces at cs.oswego.edu
>                 [mailto:concurrency-interest-bounces at cs.oswego.edu]On
>                 Behalf Of Hanson Char
>                 Sent: Wednesday, 13 September 2006 6:20 AM
>                 To: rbalamohan at sonoasystems.com
>                 Cc: Brian Goetz; concurrency-interest; Kasper Nielsen
>                 Subject: Re: [concurrency-interest]
>                 BoundedPriorityBlockingQueue ?
>                 
>                 
>                 I see blocking on put on a
>                 BoundedPriorityBlockingQueue is not such a good
>                 idea.  
>                 
>                 How about making the put method return a false boolean
>                 value upon queue full, or alternatively throwing
>                 something like a QueueFullException ? 
>                 
>                 Would this be useful/sensible ?
>                 
>                 Hanson
>                 
>                 On 9/12/06, Brian Goetz <brian at quiotix.com> wrote: 
>                 
>                         The tricky part is if you want to guarantee
>                         that the thread offering the
>                         element of highest priority is first to be
>                         unblocked. 
>                 
>                 
> 
> 
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060913/7d5d530f/attachment.html 

From tim at peierls.net  Tue Sep 12 23:06:12 2006
From: tim at peierls.net (Tim Peierls)
Date: Tue, 12 Sep 2006 23:06:12 -0400
Subject: [concurrency-interest] BoundedPriorityBlockingQueue ?
In-Reply-To: <aa067ea10609121941n657fb591ka5e09daf7283a22b@mail.gmail.com>
References: <ca53c8f80609121319l55e749d5k6111e3841a70c467@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCAEFJHCAA.dcholmes@optusnet.com.au>
	<aa067ea10609121941n657fb591ka5e09daf7283a22b@mail.gmail.com>
Message-ID: <63b4e4050609122006m364c3c06x7a0ef3eeaa0d3484@mail.gmail.com>

On 9/12/06, Dhanji R. Prasanna <dhanji at gmail.com> wrote:
>
> If there are mission critical differences I certainly would not trust them
> to the same threadpool.
>

Building that lack of trust into task submission code might also be a
mistake.

Maybe this is obvious to all, but here's a reminder: Sometimes the best
strategy is not to settle definitively on a particular strategy but to
encapsulate the execution policy within an Executor or ExecutorService. That
way, by changing the concrete type of an Executor and without affecting the
client code, you can change your mind at deploy time about whether to use
separate thread pools.

Then you can measure the effectiveness of different strategies in practice
instead of trying to reason in the abstract about which works better.

--tim
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060912/fb0bf128/attachment.html 

From tim at peierls.net  Tue Sep 12 23:08:39 2006
From: tim at peierls.net (Tim Peierls)
Date: Tue, 12 Sep 2006 23:08:39 -0400
Subject: [concurrency-interest] BoundedPriorityBlockingQueue ?
In-Reply-To: <63b4e4050609122006m364c3c06x7a0ef3eeaa0d3484@mail.gmail.com>
References: <ca53c8f80609121319l55e749d5k6111e3841a70c467@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCAEFJHCAA.dcholmes@optusnet.com.au>
	<aa067ea10609121941n657fb591ka5e09daf7283a22b@mail.gmail.com>
	<63b4e4050609122006m364c3c06x7a0ef3eeaa0d3484@mail.gmail.com>
Message-ID: <63b4e4050609122008o9493c3dnad2b13a07c654a52@mail.gmail.com>

On 9/12/06, Tim Peierls <tim at peierls.net> wrote:
>
> Sometimes the best strategy
>

Strike the word "Sometimes".

--tim
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060912/4a5eaa23/attachment.html 

From rbalamohan at sonoasystems.com  Tue Sep 12 23:20:40 2006
From: rbalamohan at sonoasystems.com (rbalamohan)
Date: Wed, 13 Sep 2006 03:20:40 +0000
Subject: [concurrency-interest] BoundedPriorityBlockingQueue ?
In-Reply-To: <ca53c8f80609121918j7bbe567ci1e829452d8bbad91@mail.gmail.com>
References: <ca53c8f80609121858m1f2b827fp7e6e5462ee317343@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCMEFNHCAA.dcholmes@optusnet.com.au>
	<ca53c8f80609121918j7bbe567ci1e829452d8bbad91@mail.gmail.com>
Message-ID: <1158117640.32641.14.camel@xen-host-ce>

Hi Hanson,

In simplistic terms, It has to follow the same semantics of BQ. David's
question is valid as well which can be rectified in a way. I am just
proposing one such thought. It might have some caveats. Please feel free
to suggest a better solution.

1. BPQ will have multiple PQs with specific priorities. (public
BoundedPriorityBlockingQueue(int priorityCount, int capacity) {})
2. However from external user point of view, it should seen as a single
queue.
3. When a job is submitted with a priority, it should get enqueued in
the appropriate PQ of the BPQ. (May be we can use some BitSet for
checking if the queues of BPQ is empty or not)
4. When a get() is called, it should always return an element from the
top priority queue in BPQ.
5. If all the PQs are empty in BPQ, we need to block block.

~Rajesh.B

On Tue, 2006-09-12 at 19:18 -0700, Hanson Char wrote:

> you use a blocking put() t
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060913/d4b4009a/attachment-0001.html 

From alarmnummer at gmail.com  Wed Sep 13 03:38:32 2006
From: alarmnummer at gmail.com (Peter Veentjer)
Date: Wed, 13 Sep 2006 09:38:32 +0200
Subject: [concurrency-interest] concurrency puzzle
In-Reply-To: <NFBBKALFDCPFIDBNKAPCCEFIHCAA.dcholmes@optusnet.com.au>
References: <1466c1d60609121250n255355f5p83a75b4249dc3a95@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCCEFIHCAA.dcholmes@optusnet.com.au>
Message-ID: <1466c1d60609130038q4b770e1bq75346c7d1f990e20@mail.gmail.com>

Ok..

1) I finally understand why the 0 is not possible.
2) I'm going to make the 'happens before' abstraction my default model
to reason with.
3) I'm going to write big blog about this.

Apart from the JCIP and the JSR133 documentation, is there other up to
date documentation about the JSR133?

Thanks for all your efforts trying to convert to stuborn guy.

On 9/13/06, David Holmes <dcholmes at optusnet.com.au> wrote:
> Peter,
>
> > Aha.. so you are saying that before 'invalidating' cache, all values
> > are written to main memory? I though they were not written to main
> > memory, but they were 'dropped'. So if they are already in main
> > memory, you are lucky, if they only were in cache, you have lost your
> > writes.
> >
> > This is exactly what I don't understand.
>
> You really need to stop thinking about caches. The conceptual model of
> "invalidating a cache" is just that - conceptual. Real caches don't work
> this way. Actual cache coherency protocols are both simpler and more
> complicated than this conceptual model. And this conceptual model is an
> incomplete simplification. It was once a useful tool to help describe memory
> model effects "imagine that the value is kept in a cache ...". The problem
> is that people then go "Aha! In my MOESI-based cache system xxxx can't
> happen therefore I don't have to worry about the memory model". That is very
> bad because they do have to worry because in practice it isn't the hardware
> cache they needed to be concerned about but the JIT compiler.
>
> The memory model allows for things that may be impossible on a given piece
> of hardware. The memory model defines the allowed behaviours. You can't
> expect to map the memory models allowed behaviours onto the physical
> behaviour of an actual machine. Nor should you try to
> explain/rationalize/justify what the memory model allows by correlating that
> with an actual machine's behaviour.
>
> In your example a partial ordering is established (where -> means
> happens-before):
>
>  x=10 -> release_lock_by_T1 -> acquire_lock_by_T2 -> print(x)
>
>  x=20 -> acquire_lock_by_T2 -> print(x)
>
> So all that we can establish is that x=20 and x=10 happen before print(x).
> But there is no constraint that either x=10->x=20 or that x=20->x=10. As the
> two writes are unordered with respect to each other, either ordering is
> possible and so the print(x) is allowed to print either 10 or 20. Whether it
> will ever actually do so on your machine is a different question and
> irrelevant as we always strive to write portable code.
>
> Cheers,
> David Holmes
>
>

From dcholmes at optusnet.com.au  Wed Sep 13 03:43:35 2006
From: dcholmes at optusnet.com.au (David Holmes)
Date: Wed, 13 Sep 2006 17:43:35 +1000
Subject: [concurrency-interest] concurrency puzzle
In-Reply-To: <1466c1d60609130038q4b770e1bq75346c7d1f990e20@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCCEGCHCAA.dcholmes@optusnet.com.au>

Peter,

> Apart from the JCIP and the JSR133 documentation, is there other up to
> date documentation about the JSR133?

Chapter 17 of the JLS Third Edition is the actual spec of course.

http://java.sun.com/docs/books/jls/third_edition/html/memory.html#17.4

I also did a reasonable summary in the threads chapter of "The Java
Programming Language", 4th edition. :)

Cheers,
David Holmes


From unmesh_joshi at hotmail.com  Wed Sep 13 04:40:42 2006
From: unmesh_joshi at hotmail.com (Unmesh joshi)
Date: Wed, 13 Sep 2006 08:40:42 +0000
Subject: [concurrency-interest] C++ Memory Model and Boost Threads
In-Reply-To: <mailman.108.1158117447.17952.concurrency-interest@altair.cs.oswego.edu>
Message-ID: <BAY121-F176346E13B431DBEBE6F02EF280@phx.gbl>

Hi,

Sorry if this is not the right place to post this message. But I am sending 
this email here becaues many of the authors of Java Memory model are also 
actively involved in C++ memory model specifications.
   What is the status of C++ memory model?. Are they any concurrency 
utilities  similar to Java being added in C++ standard library?. Is Boost 
Thread library accepted as basis of thread library added in standard c++?

Thanks,
Unmesh



From david at walend.net  Wed Sep 13 08:34:30 2006
From: david at walend.net (David Walend)
Date: Wed, 13 Sep 2006 08:34:30 -0400
Subject: [concurrency-interest] BoundedPriorityBlockingQueue
In-Reply-To: <mailman.108.1158117447.17952.concurrency-interest@altair.cs.oswego.edu>
References: <mailman.108.1158117447.17952.concurrency-interest@altair.cs.oswego.edu>
Message-ID: <E6D8400A-0CEF-40D4-92B3-E478CFAF2DD0@walend.net>


On Sep 12, 2006, at 11:17 PM, concurrency-interest- 
request at cs.oswego.edu wrote:

> Date: Tue, 12 Sep 2006 23:06:12 -0400
> From: "Tim Peierls" <tim at peierls.net>
>
> On 9/12/06, Dhanji R. Prasanna <dhanji at gmail.com> wrote:
>>
>> If there are mission critical differences I certainly would not  
>> trust them
>> to the same threadpool.
>>
>
> Building that lack of trust into task submission code might also be a
> mistake.
>
> Maybe this is obvious to all, but here's a reminder: Sometimes the  
> best
> strategy is not to settle definitively on a particular strategy but to
> encapsulate the execution policy within an Executor or  
> ExecutorService. That
> way, by changing the concrete type of an Executor and without  
> affecting the
> client code, you can change your mind at deploy time about whether  
> to use
> separate thread pools.
>
> Then you can measure the effectiveness of different strategies in  
> practice
> instead of trying to reason in the abstract about which works better.

Well said. I found that developers preferred a PriorityBlockingQueue  
with a custom Comparator to a FIFO queue ( https:// 
somnifugijms.dev.java.net/source/browse/*checkout*/somnifugijms/v3/ 
source/somnifugi/net/walend/somnifugi/juc/PriorityChannel.java ).  
MessageComparator first differentiates by priority, then by age  
(oldest first), then by something arbitrary but predictable. The  
developers prefer it because of the predictability.

In practice, we get around needing to bound the size by answering,  
"How will the system respond if the queue gets too big?"  
PriorityBlockingQueue has a size() method, so we can monitor it. When  
it gets too big or is growing too fast, we start more consumers or  
throttle back the less important producers or redesign to make the  
consumers more efficient. Throttling back the producers via blocking  
is a heavy handed option; it might save us running out of memory but  
would mean something else is broken.

Hope that helps,

Dave

David Walend
david at walend.net



From hanson.char at gmail.com  Wed Sep 13 14:22:01 2006
From: hanson.char at gmail.com (Hanson Char)
Date: Wed, 13 Sep 2006 11:22:01 -0700
Subject: [concurrency-interest] ConcurrentLinkedBlockingQueue ?
In-Reply-To: <ca53c8f80609111453h4a090d08l4057774c4c745ed6@mail.gmail.com>
References: <ca53c8f80609062313h502eb3f9n9217820f4d8b46a0@mail.gmail.com>
	<4500017E.4040202@cs.oswego.edu>
	<ca53c8f80609091818x71495fbbtf598233fe69c3271@mail.gmail.com>
	<ca53c8f80609092051u7f6f8b24n4882c96acbc45fb6@mail.gmail.com>
	<ca53c8f80609092321y2f585d28k5073d7e73f02416a@mail.gmail.com>
	<ca53c8f80609092358o7f9c08dcrdea39e87e71ee580@mail.gmail.com>
	<45044524.4070109@wizards.de>
	<ca53c8f80609102340n4dce9bfcg69c51cdb9ea244bf@mail.gmail.com>
	<ca53c8f80609111453h4a090d08l4057774c4c745ed6@mail.gmail.com>
Message-ID: <ca53c8f80609131122g44ba7a62w4fb1ea72b6a52125@mail.gmail.com>

Hi,

I've enhanced ConcurrentLinkedBlockingQueue (CLBQ) such that both the take()
and poll(long,TimeUnit) now throws an InterruptedException if interrupted
while waiting.

Source:


http://svn.sourceforge.net/viewvc/beanlib/trunk/beanlib/src/net/sf/beanlib/util/concurrent/ConcurrentLinkedBlockingQueue.java

However, since CLBQ does not implement the BlockingQueue interface, the name
CLBQ is pretty misleading!  Any suggestion for a better name ?  Or should
these extra take() and poll(long,TimeUnit) should really be
added/incorporated to the existing j.u.c.ConcurrentLinkedQueue ?

Hanson

(Previous test result before the enhancement:
http://beanlib.sourceforge.net/pdf/060911/060911-clbq.pdf)

On 9/11/06, Hanson Char <hanson.char at gmail.com> wrote:
>
> Testing on an AMD Opteron dual processor, the CLBQ is on average 72%
> faster than the LBQ using the test harness.  Please see the attached pdf for
> more details.
>
> Hanson
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060913/ccc684a3/attachment.html 

From hans.boehm at hp.com  Wed Sep 13 15:02:49 2006
From: hans.boehm at hp.com (Boehm, Hans)
Date: Wed, 13 Sep 2006 14:02:49 -0500
Subject: [concurrency-interest] C++ Memory Model and Boost Threads
In-Reply-To: <BAY121-F176346E13B431DBEBE6F02EF280@phx.gbl>
Message-ID: <BDA38860DCFD334EAEA905E44EE8E7EF14AAD2@G3W0067.americas.hpqcorp.net>

An appropriate web page and a pointer to the mailing list is at
http://www.hpl.hp.com/personal/Hans_Boehm/c++mm/ .

We are aiming to include something vaguely along the lines of
java.util.concurrent.atomic.  The rest of java.util.concurrent may or
may not be addressed in the first round of standardization, though there
are commercial products such as Intel's Threading Building Blocks in
roughly that space.

As you will see if you look at the mailing list, there is lots of
ongoing discussion of the threading API.  I think it is likely to
resemble Boost to some extent, but the details are unclear.

Hans

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu 
> [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf 
> Of Unmesh joshi
> Sent: Wednesday, September 13, 2006 1:41 AM
> To: concurrency-interest at cs.oswego.edu
> Subject: [concurrency-interest] C++ Memory Model and Boost Threads
> 
> Hi,
> 
> Sorry if this is not the right place to post this message. 
> But I am sending this email here becaues many of the authors 
> of Java Memory model are also actively involved in C++ memory 
> model specifications.
>    What is the status of C++ memory model?. Are they any 
> concurrency utilities  similar to Java being added in C++ 
> standard library?. Is Boost Thread library accepted as basis 
> of thread library added in standard c++?
> 
> Thanks,
> Unmesh
> 
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
> 


From dhanji at gmail.com  Wed Sep 13 18:19:03 2006
From: dhanji at gmail.com (Dhanji R. Prasanna)
Date: Thu, 14 Sep 2006 08:19:03 +1000
Subject: [concurrency-interest] concurrency puzzle
In-Reply-To: <31f2a7bd0609121532k18b52121o4a7b9c9c12b70a51@mail.gmail.com>
References: <NFBBKALFDCPFIDBNKAPCAEEGHCAA.dcholmes@optusnet.com.au>
	<1466c1d60609121112r553ddfdfnf13c38ee91b388e9@mail.gmail.com>
	<1466c1d60609121115n7de77ecapeb91c4ecdf164491@mail.gmail.com>
	<4506FC08.2010808@cs.umd.edu>
	<1466c1d60609121206y722099e4s46c0d1e20e9f9b6f@mail.gmail.com>
	<45070A9F.9010507@cs.umd.edu>
	<1466c1d60609121250n255355f5p83a75b4249dc3a95@mail.gmail.com>
	<1466c1d60609121304m545f411fy2e3a4d41488ba15c@mail.gmail.com>
	<4507207A.1060005@quiotix.com>
	<31f2a7bd0609121532k18b52121o4a7b9c9c12b70a51@mail.gmail.com>
Message-ID: <aa067ea10609131519q427e66eh86d7090669097194@mail.gmail.com>

On 9/13/06, Joe Bowbeer <joe.bowbeer at gmail.com> wrote:
> In my mental model, I cast the compiler in the role of the Nixon
> administration, maintaining cautious optimism (optimization in this
> case) in its single-minded pursuit, while preserving plausible
> deniability about alleged illegal activities (prohibited actions) that
> may have contributed to the result.  This is much more interesting
> than reasoning about caches, I say, and it encourages the proper level
> of respect for the compiler (well, and loathing, too).

haha, I love it! Does this also mean the compiler will some day be
written in (go to) china? ;)

I think you could extend the analogy somewhat and say that Nixon (Java
VM) took us (developers everywhere) off the gold standard
(platform-dependency) allowing the American economy unprecedented
growth (bigger paychecks for those of us with the letters J2EE on our
resumes) for years to come (hopefully)!

On a more serious note, is the JVM capable of deciding where threads
are run in an SMP environment? For instance, could it say that two
threads passing through the same critical section of code several
times (hotspot) can run on the same processor and thus its volatile
members work faster (i.e. cached in L2 before going to main memory or
something like that)? Or is this a impl/platform dependent issue?

> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>

From conivek at gmail.com  Thu Sep 14 11:32:42 2006
From: conivek at gmail.com (Kevin Condon)
Date: Thu, 14 Sep 2006 11:32:42 -0400
Subject: [concurrency-interest] Concurrent Serialization Pattern
Message-ID: <2e780ac60609140832o522e2f2fp57cf4b0d94feb6de@mail.gmail.com>

The Concurrency Puzzle thread inspired some questions in my mind about
serializing and deserializing objects to safely handle concurrency
issues.  I'd like to hear what patterns others are following, as well
as correcting any incorrect notions I may have developed.  Here's my
simple (and naive) example class, which I'd guess isn't far from a lot
of existing production code:

public class Serial implements Serializable {
  private static final long serialVersionUID = 2006091400L;
  private final ReentrantReadWriteLock lock = new ReentrantReadWriteLock();
  private int x;

  public int getX() {
    lock.readLock().lock();
    try {
      return x;
    } finally {
      lock.readLock().unlock();
    }
  }

  public void setX(int x) {
    lock.writeLock().lock();
    try {
      // imagine some long running op here ...
      this.x = x;
    } finally {
      lock.writeLock().unlock();
    }
  }
}

While this example handles ordinary run-time concurrency fine, it does
not guarantee that the most current value of x would be serialized.
So I'd need to add:

  private void writeObject(ObjectOutputStream out) throws IOException {
    lock.readLock().lock();
    try {
      out.defaultWriteObject();
    } finally {
      lock.readLock().unlock();
    }
  }

Easy enough.  But upon deserialization, the deserialized value of x
would not be guaranteed to be visible in threads other than the thread
invoking in.readObject().  This is because there is no happens-before
relationship between the deserializing thread and concurrent threads
invoking getX() without using lock.  To fix this, I'll need to add a
readObject() method that uses lock.  That will require additional
supporting changes, too:

  private transient int x;  // emit using custom serialization
  ...

  private void writeObject(ObjectOutputStream out) throws IOException {
    out.defaultWriteObject();
    lock.readLock().lock();
    try {
      out.writeInt(x);
    } finally {
      lock.readLock().unlock();
    }
  }

  private void readObject(ObjectInputStream in)
      throws IOException, ClassNotFoundException {
    in.defaultReadObject();
    lock.writeLock().lock();
    try {
      x = in.readInt();
    } finally {
      lock.writeLock().unlock();
    }
  }

And viola!  There's the pattern.  I'd summarize the principles this way:

1. Make locks immutable fields, using the final modifier and rely on
the default serialization handling to create the necessary
happens-before relationships for lock field value visibility.  (See
JLS 17.5.3.)

2. Use a custom serialization form (see Effective Java, Item 55) and
make sure to create happens-before relationships on all
serialized/deserialized fields by using the same locking required for
ordinary run-time access.

Note that the concurrent lock could be replaces with a class Mutex
implements Serializable {} to use simple Object monitor locking
instead.

One question does loom in my mind here:  Is it safe to serialize and
deserialize a ReentrantReadWriteLock object?  My tests with
serializing one while the lock was held didn't create any blocking
upon deserialization, but I'd like to know if there's any potential
issue with this.  I have the same question with using Object monitor
locking.

Any thoughts on this pattern?  Is anyone using something different or
better?  Does anyone else feel a little queezy about code they've
written prior to thinking through this? ;)

Regards,
Kevin Condon

From tackline at tackline.plus.com  Thu Sep 14 14:23:12 2006
From: tackline at tackline.plus.com (Thomas Hawtin)
Date: Thu, 14 Sep 2006 19:23:12 +0100
Subject: [concurrency-interest] Concurrent Serialization Pattern
In-Reply-To: <2e780ac60609140832o522e2f2fp57cf4b0d94feb6de@mail.gmail.com>
References: <2e780ac60609140832o522e2f2fp57cf4b0d94feb6de@mail.gmail.com>
Message-ID: <45099E10.1070100@tackline.plus.com>

Kevin Condon wrote:
> 
> And viola!  There's the pattern.  I'd summarize the principles this way:
> 
> 1. Make locks immutable fields, using the final modifier and rely on
> the default serialization handling to create the necessary
> happens-before relationships for lock field value visibility.  (See
> JLS 17.5.3.)
> 
> 2. Use a custom serialization form (see Effective Java, Item 55) and
> make sure to create happens-before relationships on all
> serialized/deserialized fields by using the same locking required for
> ordinary run-time access.

In 1.6, java.util.Random (resetSeed method) uses sun.misc.Unsafe to poke 
it's final, (effectively) transient seed field. Not something I would 
generally recommend.

If you don't have to worry about a serialisable base class, then it 
might be simpler just to introduce a non-serialisable, package private 
base class.

abstract class SerialBase { // Must not implement Serializable
     final ReadWriteLock lock = new ReentrantReadWriteLock();

     SerialBase() {
     }
}

public class Serial extends SerialBase implements Serializable {
     private static final long serialVersionUID = 2006091401L;

     private int x;

     private void writeObject(
         ObjectOutputStream out
     ) throws IOException {
         lock.readLock().lock();
         try {
             out.defaultWriteObject();
         } finally {
             lock.readLock().unlock();
         }
     }
     private void readObject(
         ObjectInputStream in
     ) throws IOException, ClassNotFoundException {
         lock.writeLock().lock();
         try {
             in.defaultReadObject();
         } finally {
             lock.writeLock().unlock();
         }
     }
}

Tom Hawtin

From conivek at gmail.com  Thu Sep 14 14:46:14 2006
From: conivek at gmail.com (Kevin Condon)
Date: Thu, 14 Sep 2006 14:46:14 -0400
Subject: [concurrency-interest] Concurrent Serialization Pattern
In-Reply-To: <45099E10.1070100@tackline.plus.com>
References: <2e780ac60609140832o522e2f2fp57cf4b0d94feb6de@mail.gmail.com>
	<45099E10.1070100@tackline.plus.com>
Message-ID: <2e780ac60609141146g2ae843bet8c5758cc25bc774d@mail.gmail.com>

Hi Tom,

On 9/14/06, Thomas Hawtin <tackline at tackline.plus.com> wrote:
> In 1.6, java.util.Random (resetSeed method) uses sun.misc.Unsafe to poke
> it's final, (effectively) transient seed field. Not something I would
> generally recommend.

Agreed.  I'd much rather rely on the default serialization "magic" to
ensure that the final fields never publish anything other than the
deserialized field values.  JLS certainly discourages non-wizards from
exercising such magic.  :)

> If you don't have to worry about a serialisable base class, then it
> might be simpler just to introduce a non-serialisable, package private
> base class.

Ahh, that would be a clever way to avoid implementing the custom
serialization.  I've actually made it a habit to make serializable
state fields transient and doing the custom serialize/deserialize code
(based on the recommendations in Efffective Java), but I can see why
someone might not think the extra work required was worthwhile in
every case.

Kevin

From conivek at gmail.com  Thu Sep 14 19:46:43 2006
From: conivek at gmail.com (Kevin Condon)
Date: Thu, 14 Sep 2006 19:46:43 -0400
Subject: [concurrency-interest] Concurrent Serialization Pattern
In-Reply-To: <2e780ac60609141146g2ae843bet8c5758cc25bc774d@mail.gmail.com>
References: <2e780ac60609140832o522e2f2fp57cf4b0d94feb6de@mail.gmail.com>
	<45099E10.1070100@tackline.plus.com>
	<2e780ac60609141146g2ae843bet8c5758cc25bc774d@mail.gmail.com>
Message-ID: <2e780ac60609141646x1325f17dr7433e0c3518f4987@mail.gmail.com>

Sorry, I just realized that there's a simpler solution.  Really just
need to make lock a transient final.  Then it doesn't have to be
serialized and you're not forced to use custom serialization if you
don't want to.  I didn't realize that the final sematics had
precedence over the transient qualifier, so I incorrectly thought that
lock would have the default null value after deserialization.

public class Serial implements Serializable {
 private static final long serialVersionUID = 2006091400L;
 private transient final ReentrantReadWriteLock lock =
    new ReentrantReadWriteLock();
 private int x;

 public int getX() {
   lock.readLock().lock();
   try {
     return x;
   } finally {
     lock.readLock().unlock();
   }
 }

 public void setX(int x) {
   lock.writeLock().lock();
   try {
     // imagine some long running op here ...
     this.x = x;
   } finally {
     lock.writeLock().unlock();
   }
 }

 private void writeObject(ObjectOutputStream out) throws IOException {
   lock.readLock().lock();
   try {
     out.defaultWriteObject();
   } finally {
     lock.readLock().unlock();
   }
 }

 private void readObject(ObjectInputStream in)
     throws IOException, ClassNotFoundException {
   lock.writeLock().lock();
   try {
     in.defaultReadObject();
   } finally {
     lock.writeLock().unlock();
   }
 }
}

Are there any problems with this simplified pattern?  Especially, is
there any risk of readObject() seeing the default null value for lock
and getting an NPE?  (I don't think there is, but that's at the heart
the whole concurrency puzzle thread.)  Are there other alternatives?

Regards,
Kevin

From crazybob at crazybob.org  Thu Sep 14 20:12:37 2006
From: crazybob at crazybob.org (Bob Lee)
Date: Thu, 14 Sep 2006 17:12:37 -0700
Subject: [concurrency-interest] Concurrent Serialization Pattern
In-Reply-To: <2e780ac60609141646x1325f17dr7433e0c3518f4987@mail.gmail.com>
References: <2e780ac60609140832o522e2f2fp57cf4b0d94feb6de@mail.gmail.com>
	<45099E10.1070100@tackline.plus.com>
	<2e780ac60609141146g2ae843bet8c5758cc25bc774d@mail.gmail.com>
	<2e780ac60609141646x1325f17dr7433e0c3518f4987@mail.gmail.com>
Message-ID: <a74683f90609141712s5e2addaer348594dca937791@mail.gmail.com>

On 9/14/06, Kevin Condon <conivek at gmail.com> wrote:
> Really just
> need to make lock a transient final.  Then it doesn't have to be
> serialized and you're not forced to use custom serialization if you
> don't want to.  I didn't realize that the final sematics had
> precedence over the transient qualifier, so I incorrectly thought that
> lock would have the default null value after deserialization.

Holy cow, I always incorrectly thought that, too! Off to clean up some code...

Bob

From dcholmes at optusnet.com.au  Thu Sep 14 21:19:51 2006
From: dcholmes at optusnet.com.au (David Holmes)
Date: Fri, 15 Sep 2006 11:19:51 +1000
Subject: [concurrency-interest] Concurrent Serialization Pattern
In-Reply-To: <2e780ac60609141646x1325f17dr7433e0c3518f4987@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCIEGJHCAA.dcholmes@optusnet.com.au>

Kevin,

> I didn't realize that the final sematics had
> precedence over the transient qualifier, so I incorrectly thought that
> lock would have the default null value after deserialization.

According to my reading of the serialization spec it *should* have a null
value. transient fields don't get serialized, unless you custom serialize
them. So a transient final field should have its default initialized value
when readObject is invoked.

Something seems amiss here.

David Holmes


From crazybob at crazybob.org  Fri Sep 15 02:16:34 2006
From: crazybob at crazybob.org (Bob Lee)
Date: Thu, 14 Sep 2006 23:16:34 -0700
Subject: [concurrency-interest] Concurrent Serialization Pattern
In-Reply-To: <NFBBKALFDCPFIDBNKAPCIEGJHCAA.dcholmes@optusnet.com.au>
References: <2e780ac60609141646x1325f17dr7433e0c3518f4987@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCIEGJHCAA.dcholmes@optusnet.com.au>
Message-ID: <a74683f90609142316h5e9faaf7r1fc0b004840aaec5@mail.gmail.com>

It only works for constants. :(

On 9/14/06, David Holmes <dcholmes at optusnet.com.au> wrote:
> Kevin,
>
> > I didn't realize that the final sematics had
> > precedence over the transient qualifier, so I incorrectly thought that
> > lock would have the default null value after deserialization.
>
> According to my reading of the serialization spec it *should* have a null
> value. transient fields don't get serialized, unless you custom serialize
> them. So a transient final field should have its default initialized value
> when readObject is invoked.
>
> Something seems amiss here.
>
> David Holmes
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>

From dcholmes at optusnet.com.au  Fri Sep 15 02:29:55 2006
From: dcholmes at optusnet.com.au (David Holmes)
Date: Fri, 15 Sep 2006 16:29:55 +1000
Subject: [concurrency-interest] Concurrent Serialization Pattern
In-Reply-To: <a74683f90609142316h5e9faaf7r1fc0b004840aaec5@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCIEGMHCAA.dcholmes@optusnet.com.au>

Well not quite. If the final field is a compile-time constant then the
bytecode doesn't contain any field access expressions it simply uses the
compile-time constant value directly. The actual
serialization/deserialization is unaffected. As a test, serialize an
instance when the constant has value A, recompile the class with the
constant set to B, read the old serialized instance back in and "access" the
final field - you will see B.

David Holmes

> -----Original Message-----
> From: crazyboblee at gmail.com [mailto:crazyboblee at gmail.com]On Behalf Of
> Bob Lee
> Sent: Friday, 15 September 2006 4:17 PM
> To: dholmes at ieee.org
> Cc: Kevin Condon; concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] Concurrent Serialization Pattern
>
>
> It only works for constants. :(
>
> On 9/14/06, David Holmes <dcholmes at optusnet.com.au> wrote:
> > Kevin,
> >
> > > I didn't realize that the final sematics had
> > > precedence over the transient qualifier, so I incorrectly thought that
> > > lock would have the default null value after deserialization.
> >
> > According to my reading of the serialization spec it *should*
> have a null
> > value. transient fields don't get serialized, unless you custom
> serialize
> > them. So a transient final field should have its default
> initialized value
> > when readObject is invoked.
> >
> > Something seems amiss here.
> >
> > David Holmes
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at altair.cs.oswego.edu
> > http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
> >


From conivek at gmail.com  Fri Sep 15 09:54:00 2006
From: conivek at gmail.com (Kevin Condon)
Date: Fri, 15 Sep 2006 09:54:00 -0400
Subject: [concurrency-interest] Concurrent Serialization Pattern
In-Reply-To: <NFBBKALFDCPFIDBNKAPCIEGMHCAA.dcholmes@optusnet.com.au>
References: <a74683f90609142316h5e9faaf7r1fc0b004840aaec5@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCIEGMHCAA.dcholmes@optusnet.com.au>
Message-ID: <2e780ac60609150654xd7d7f67lc6dbd0913254771b@mail.gmail.com>

David,

On 9/15/06, David Holmes <dcholmes at optusnet.com.au> wrote:
> As a test, serialize an
> instance when the constant has value A, recompile the class with the
> constant set to B, read the old serialized instance back in and "access" the
> final field - you will see B.

That was exactly the test I used when I empirically discovered the
unexpected behavior.

> > > According to my reading of the serialization spec it *should*
> > have a null
> > > value. transient fields don't get serialized, unless you custom
> > serialize
> > > them. So a transient final field should have its default
> > initialized value
> > > when readObject is invoked.

I'm not entirely content with empirical discovery for understanding
JMM issues, and the serialization spec *does* seem to give an
expectation of seeing the default value for the transient final field:
 "The values of every field of the object whether transient or not,
static or not are set to the default value for the fields type"  (sec
3.4).  Though it doesn't explicitly mention "final" fields.  Since
"Fields declared as transient or static are ignored by the
deserialization process" according to the ObjectInputStream javadoc, I
guess it makes sense for "ignored" to mean that transient fields are
left in the "completely initialized" state that Class.newInstance()
leaves them in (JLS 17.5).  So the guarantee for never seeing the
transient final field's default value is provided by JLS 16 and 17.5.
Maybe an explicit statement about transient final fields would help in
the serialization spec and in the ObjectInputStream API javadoc?

I wonder why there's such an expectation that transient final fields
would have their default value after deserializing.  Do older
compilers or JVMs behave differently?  Or is it just the clarity of
the docs, esp. the serialization spec sec 3.4?

Regards,
Kevin

From conivek at gmail.com  Fri Sep 15 10:08:08 2006
From: conivek at gmail.com (Kevin Condon)
Date: Fri, 15 Sep 2006 10:08:08 -0400
Subject: [concurrency-interest] Concurrent Serialization Pattern
In-Reply-To: <2e780ac60609150654xd7d7f67lc6dbd0913254771b@mail.gmail.com>
References: <a74683f90609142316h5e9faaf7r1fc0b004840aaec5@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCIEGMHCAA.dcholmes@optusnet.com.au>
	<2e780ac60609150654xd7d7f67lc6dbd0913254771b@mail.gmail.com>
Message-ID: <2e780ac60609150708n50fd64a2nc991448629dfa2df@mail.gmail.com>

On 9/15/06, Kevin Condon <conivek at gmail.com> wrote:
> I wonder why there's such an expectation that transient final fields
> would have their default value after deserializing.  Do older
> compilers or JVMs behave differently?  Or is it just the clarity of
> the docs, esp. the serialization spec sec 3.4?

I just tried compiling and running my tests with Sun's 1.1.8, 1.3.1,
and 1.4.1 JVMs, and got the same results as with 1.5.  Guess it's a
matter of interpreting the docs.

Kevin

From peter.jones at sun.com  Fri Sep 15 11:27:09 2006
From: peter.jones at sun.com (Peter Jones)
Date: Fri, 15 Sep 2006 11:27:09 -0400
Subject: [concurrency-interest] Concurrent Serialization Pattern
In-Reply-To: <2e780ac60609150654xd7d7f67lc6dbd0913254771b@mail.gmail.com>
References: <a74683f90609142316h5e9faaf7r1fc0b004840aaec5@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCIEGMHCAA.dcholmes@optusnet.com.au>
	<2e780ac60609150654xd7d7f67lc6dbd0913254771b@mail.gmail.com>
Message-ID: <20060915152709.GB15157@east>

>>>> According to my reading of the serialization spec it *should*
>>>> have a null value. transient fields don't get serialized, unless
>>>> you custom serialize them. So a transient final field should have
>>>> its default initialized value when readObject is invoked.
> 
> I'm not entirely content with empirical discovery for understanding
> JMM issues, and the serialization spec *does* seem to give an
> expectation of seeing the default value for the transient final
> field: "The. values of every field of the object whether transient
> or not, static or not are set to the default value for the fields
> type" (sec 3.4). Though it doesn't explicitly mention "final"
> fields.

That assertion is accurate even for final fields.  If you reflect upon
a transient final instance field of a deserialized object-- and it
hasn't been set through privileged magic-- you will see the default
value (null, 0, or false).

The case David cited is that if a final field's initializer is a
compile-time constant expression (JLS3 15.28), so the field is a
constant variable (JLS3 4.12.4), a field access expression for it will
be compiled to the compile-time constant value (i.e. inlined) rather
than to a bytecode to access the field at runtime (JLS3 13.4.9)-- so a
non-reflective field access won't "see" the default value that it has.

> Since "Fields declared as transient or static are ignored by the
> deserialization process" according to the ObjectInputStream javadoc,
> I guess it makes sense for "ignored" to mean that transient fields
> are left in the "completely initialized" state that
> Class.newInstance() leaves them in (JLS 17.5).

No, Class.newInstance() invokes the default constructor and thus
executes instance field initializers, unlike deserialization.

> So the guarantee for never seeing the transient final field's
> default value is provided by JLS 16 and 17.5.  Maybe an explicit
> statement about transient final fields would help in the
> serialization spec and in the ObjectInputStream API javadoc?
> 
> I wonder why there's such an expectation that transient final fields
> would have their default value after deserializing.  Do older
> compilers or JVMs behave differently?  Or is it just the clarity of
> the docs, esp. the serialization spec sec 3.4?

There is no such guarantee-- it just can seem that way with
constant-initialized final fields.  Documentation could certainly be
improved to reduce confusion in this area; there is an open RFE about
related points:

	http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6252102

-- Peter

From peter.jones at sun.com  Fri Sep 15 11:45:32 2006
From: peter.jones at sun.com (Peter Jones)
Date: Fri, 15 Sep 2006 11:45:32 -0400
Subject: [concurrency-interest] Concurrent Serialization Pattern
In-Reply-To: <2e780ac60609140832o522e2f2fp57cf4b0d94feb6de@mail.gmail.com>
References: <2e780ac60609140832o522e2f2fp57cf4b0d94feb6de@mail.gmail.com>
Message-ID: <20060915154531.GC15157@east>

>   private transient int x;  // emit using custom serialization
>   ...
> 
>   private void writeObject(ObjectOutputStream out) throws IOException {
>     out.defaultWriteObject();
>     lock.readLock().lock();
>     try {
>       out.writeInt(x);
>     } finally {
>       lock.readLock().unlock();
>     }
>   }
> 
>   private void readObject(ObjectInputStream in)
>       throws IOException, ClassNotFoundException {
>     in.defaultReadObject();
>     lock.writeLock().lock();
>     try {
>       x = in.readInt();
>     } finally {
>       lock.writeLock().unlock();
>     }
>   }

[snip]

> Any thoughts on this pattern?

I would probably move the invocations on the streams "out"/"in"
outside the lock/unlock, so that you're not holding the lock during a
potentially blocking I/O operation on an arbitrary stream (at least
for the writing case, when some other thread might want the lock).
(And then I think you would have blocks of code similar to what's in
getX/setX, which you might want to share in private methods.)

If you would rather not write "x" as custom serialized form data, you
could use the ObjectOutputStream.putFields/PutField/writeFields and
ObjectInputStream.readFields/GetField APIs to still write it as the
serializable field named "x"-- for what it's worth.

-- Peter

From conivek at gmail.com  Fri Sep 15 16:28:24 2006
From: conivek at gmail.com (Kevin Condon)
Date: Fri, 15 Sep 2006 16:28:24 -0400
Subject: [concurrency-interest] Concurrent Serialization Pattern
In-Reply-To: <20060915152709.GB15157@east>
References: <a74683f90609142316h5e9faaf7r1fc0b004840aaec5@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCIEGMHCAA.dcholmes@optusnet.com.au>
	<2e780ac60609150654xd7d7f67lc6dbd0913254771b@mail.gmail.com>
	<20060915152709.GB15157@east>
Message-ID: <2e780ac60609151328t3dae6b90kf269debec37d4b6e@mail.gmail.com>

Oops, sorry -- my tests were with a primitive (transient final int),
but a transient final *object* is left at the default null value after
defaultReadObject() returns.  (Note to self:  Test *exactly* the case
you're trying to prove from now on.)  So none of my previous examples
actually work, since they all rely on the transient final
ReentrantReadWriteLock = ..., which is going to be the default null.
:(

Thomas Hawtin's earlier suggestion of extending a non-serializable
base class that declares the lock as a final variable is the only
working solution thus far, and at times isn't even possible due to
single-inheritance limitations.

Here's a solution using readResolve() that I tested successfully.  It
uses the correct synchronization, and provides a lot of flexibility.
(Credit to coworker Michael Smith who mentioned off-the-cuff, "maybe
you have to use readResolve".)

public class Serial2 implements Serializable {
  private static final long serialVersionUID = 2006091500L;
  private transient final ReentrantReadWriteLock lock =
    new ReentrantReadWriteLock();
  private transient int x;

  public int getX() {
    lock.readLock().lock();
    try {
      return x;
    } finally {
      lock.readLock().unlock();
    }
  }

  public void setX(int x) {
    lock.writeLock().lock();
    try {
      this.x = x;
    } finally {
      lock.writeLock().unlock();
    }
  }

  private void writeObject(ObjectOutputStream out) throws IOException {
    out.defaultWriteObject();
    out.writeInt(getX());
  }

  private void readObject(ObjectInputStream in)
      throws IOException, ClassNotFoundException {
    in.defaultReadObject();
    // lock is null, so don't invoke this.setX();
    // program order will ensure visibility
    x = in.readInt();
  }

  private Object readResolve() throws ObjectStreamException {
    Serial2 s = new Serial2();
    // lock is null, so don't invoke this.getX();
    // program order will ensure visibility
    s.setX(x);
    return s;
  }
}

Seems a bit complex and nuanced for serializing such a simple class.

Regards,
Kevin

From dcholmes at optusnet.com.au  Sat Sep 16 00:48:05 2006
From: dcholmes at optusnet.com.au (David Holmes)
Date: Sat, 16 Sep 2006 14:48:05 +1000
Subject: [concurrency-interest] Concurrent Serialization Pattern
In-Reply-To: <2e780ac60609151328t3dae6b90kf269debec37d4b6e@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCMEHAHCAA.dcholmes@optusnet.com.au>

Kevin,

> Here's a solution using readResolve() that I tested successfully.

I've somewhat lost the original problem that was being solved :) The issue
with final fields and serialization is that if you need to perform custom
deserialization of the final field then you can't do it directly. You then
have two alternatives that I am aware of:

1. You use custom deserialization, reflection and setAccessible(true) to set
the final field to the desired value; or
2. You use readresolve to replace the deserialized object with an equivalent
one that you can construct with all the right final field values

The issue I believe you were trying to address in your serialization code
was the possibility of unsafe-publication of the deserialized object. As you
indicated originally if the deserialization process uses the same lock that
is used to access the object's state then there can be no races. Hence
readObject deserializes while holding the writeLock. writeObject also uses
the readLock to ensure a consistent snapshot of the object's state (though
the idea that you would serialize an object while it is being actively
mutated is a little worrying :) ). So your pattern made the object immune to
unsafe-publication - though as we have discussed in the context of
constructors, generally it is not worth the effort to achieve this level of
thread safety - just ensure you do publish safely.

Now somewhere a long the way you decided that your final Lock field needed
to be transient. That is the part I don't understand as
ReentrantReadWriteLock is Serializable.

Cheers,
David Holmes


From conivek at gmail.com  Sat Sep 16 21:02:17 2006
From: conivek at gmail.com (Kevin Condon)
Date: Sat, 16 Sep 2006 21:02:17 -0400
Subject: [concurrency-interest] Concurrent Serialization Pattern
In-Reply-To: <NFBBKALFDCPFIDBNKAPCMEHAHCAA.dcholmes@optusnet.com.au>
References: <2e780ac60609151328t3dae6b90kf269debec37d4b6e@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCMEHAHCAA.dcholmes@optusnet.com.au>
Message-ID: <2e780ac60609161802r28371f9fte976ed869b59367d@mail.gmail.com>

Hi David,

On 9/16/06, David Holmes <dcholmes at optusnet.com.au> wrote:
[snip]
> So your pattern made the object immune to
> unsafe-publication - though as we have discussed in the context of
> constructors, generally it is not worth the effort to achieve this level of
> thread safety - just ensure you do publish safely.

Maybe I've misunderstood the JMM.  I was under the impression that
non-final field values set during deserialization and construction
would be in a data race unless a happens-before trigger is introduced.
 This would be true even if the object were "safely published", which
I take to mean that no reference to the object is made accessible to
other threads until construction/deserialization is complete.  Wasn't
that the gist of the concurrency puzzle thread and the possibility of
seeing the default value w/o a hb?

> Now somewhere a long the way you decided that your final Lock field needed
> to be transient. That is the part I don't understand as
> ReentrantReadWriteLock is Serializable.

If I've correctly understood the JMM on this (and I guess the verdict
is still out on that), then the same locking for "normal" access must
be used during construction, serialization, and deserialization.  Thus
my quandry, since I can't use a serialized lock object until after
it's been deserialized.  But then it's too late to use it as the hb
trigger for the other values already deserialized with it.  That's the
problem I'm trying to solve.

I've thought of 2 possibly better ways to address this than the
readResolve approach, but I'll hold off until it's confirmed that the
problem I'm solving is even real.

Regards,
Kevin

From dcholmes at optusnet.com.au  Sun Sep 17 20:00:22 2006
From: dcholmes at optusnet.com.au (David Holmes)
Date: Mon, 18 Sep 2006 10:00:22 +1000
Subject: [concurrency-interest] Concurrent Serialization Pattern
In-Reply-To: <2e780ac60609161802r28371f9fte976ed869b59367d@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCGEHEHCAA.dcholmes@optusnet.com.au>

Hi Kevin,

> Maybe I've misunderstood the JMM.  I was under the impression that
> non-final field values set during deserialization and construction
> would be in a data race unless a happens-before trigger is introduced.
>  This would be true even if the object were "safely published", which
> I take to mean that no reference to the object is made accessible to
> other threads until construction/deserialization is complete.

Safe publication means more than construction/deserialization being
"complete" it means that you establish a happens-before relationship between
the construction/deserialization and any subsequent use.

> If I've correctly understood the JMM on this (and I guess the verdict
> is still out on that), then the same locking for "normal" access must
> be used during construction, serialization, and deserialization.  Thus
> my quandry, since I can't use a serialized lock object until after
> it's been deserialized.  But then it's too late to use it as the hb
> trigger for the other values already deserialized with it.  That's the
> problem I'm trying to solve.

You must set up the following:

construction/deserialization happens-before publication happens-before
subsequent-use

If the "publication" phase involves no synchronization (ie is unsafe
publication) then it is as you have stated - the same synchronization
mechanism needs to be used in deserialization and subsequent use so that the
publisher and user have "synchronized" appropriately. So yes, even if the
lock is serialized you would still need to use custom deserialization to
actually use the lock. So your readObject could be:

    defaultReadObject();
    lock.writeLock().acquire();
    lock.writeLock().release();

However if you use safe-publication then the custom deserialization is not
needed.

Does the above clarify things?

Cheers,
David Holmes


From syg6 at yahoo.com  Mon Sep 18 06:40:42 2006
From: syg6 at yahoo.com (syg6)
Date: Mon, 18 Sep 2006 03:40:42 -0700 (PDT)
Subject: [concurrency-interest] Using concurrent to write a load balancer
Message-ID: <6361357.post@talk.nabble.com>


Hello all.

I am trying to write a load balancer using the new Java 1.5 concurrent
classes. I am currently trudging through all of the examples, trying to see
what I might use. I wanted to bounce my idea off the group.

I will have a more-or-less fixed pool of a small number of threads (it might
grow or shrink every now an then but not often). Each thread will have the
same Task associated with it, a task that sends out a 'ping' (using
java.net's isReachable, which is another can of worms, but anyway ...)

So every time a request comes in I'd like to use a CyclicBarrier (I think)
to launch all of the threads and when the first thread answers (with the ip
of the machine pinged), cancel all of the other Tasks, reset and wait for
the next request.

I *think* the best way to do this is with a CyclicBarrier, because I need a
reusable way to launch all of my threads/tasks at once. CountDownLatch isn't
reusable and it I were to use a ExecutorCompletionService I'd have to call
the submit() method for each Task, one after the other, which means the
first Task would probably almost always be the first to respond.

The only possible problem I see with CyclicBarrier is, what happens when,
say, a request comes in, I launch my 5 (for example) threads, one responds
but before the other 4 can respond I get another request? Then I'll have to
wait until they all respond to re-launch. Ideally I'd like to re-launch
immediately with the 1 thread I have available, but I don't know if this is
possible ...

Does this sound doable? Anyone have any code that resembles this?

Many thanks,
Bob

ps. I posted on this last week on the java.sun.com forums, but decided to
move over to Nabble ... Here's the 
http://forum.java.sun.com/thread.jspa?threadID=768022&tstart=0 link :


-- 
View this message in context: http://www.nabble.com/Using-concurrent-to-write-a-load-balancer-tf2290433.html#a6361357
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.


From kav at it.edu  Mon Sep 18 07:23:44 2006
From: kav at it.edu (Kasper Nielsen)
Date: Mon, 18 Sep 2006 13:23:44 +0200
Subject: [concurrency-interest] Using concurrent to write a load balancer
In-Reply-To: <6361357.post@talk.nabble.com>
References: <6361357.post@talk.nabble.com>
Message-ID: <450E81C0.3010708@it.edu>

syg6 wrote:
> Hello all.
> 
> I am trying to write a load balancer using the new Java 1.5 concurrent
> classes. I am currently trudging through all of the examples, trying to see
> what I might use. I wanted to bounce my idea off the group.
> 
> I will have a more-or-less fixed pool of a small number of threads (it might
> grow or shrink every now an then but not often). Each thread will have the
> same Task associated with it, a task that sends out a 'ping' (using
> java.net's isReachable, which is another can of worms, but anyway ...)
> 
> So every time a request comes in I'd like to use a CyclicBarrier (I think)
> to launch all of the threads and when the first thread answers (with the ip
> of the machine pinged), cancel all of the other Tasks, reset and wait for
> the next request.
> 
> I *think* the best way to do this is with a CyclicBarrier, because I need a
> reusable way to launch all of my threads/tasks at once. CountDownLatch isn't
> reusable and it I were to use a ExecutorCompletionService I'd have to call
> the submit() method for each Task, one after the other, which means the
> first Task would probably almost always be the first to respond.
> 
Hi Bob,

I think you might want to have a look at
the invokeAny method of ExecutorService
http://java.sun.com/j2se/1.5.0/docs/api/java/util/concurrent/ExecutorService.html#invokeAny(java.util.Collection)

Should be much simpler then using a CyclicBarrier.

- Kasper

From conivek at gmail.com  Mon Sep 18 07:50:48 2006
From: conivek at gmail.com (Kevin Condon)
Date: Mon, 18 Sep 2006 07:50:48 -0400
Subject: [concurrency-interest] Concurrent Serialization Pattern
In-Reply-To: <NFBBKALFDCPFIDBNKAPCGEHEHCAA.dcholmes@optusnet.com.au>
References: <2e780ac60609161802r28371f9fte976ed869b59367d@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCGEHEHCAA.dcholmes@optusnet.com.au>
Message-ID: <2e780ac60609180450x5cb88de6n5f733fb872ba299a@mail.gmail.com>

Hi David,

I don't know how I missed that.  I was so busy thinking about the
internals of the class that I failed to notice that synchronizing
references to the object would create the happens-before for values
set during construction/deserialization.  Now I understand why you
said a couple of posts ago, "generally it is not worth the effort to
achieve this level of thread safety - just ensure you do publish
safely."

Clarification complete.  :)

Thanks,
Kevin

From dcholmes at optusnet.com.au  Mon Sep 18 07:53:24 2006
From: dcholmes at optusnet.com.au (David Holmes)
Date: Mon, 18 Sep 2006 21:53:24 +1000
Subject: [concurrency-interest] Using concurrent to write a load balancer
In-Reply-To: <6361357.post@talk.nabble.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCEEHHHCAA.dcholmes@optusnet.com.au>

Hello again Bob :)

Welcome to concurrency-interest.

> The only possible problem I see with CyclicBarrier is, what happens when,
> say, a request comes in, I launch my 5 (for example) threads, one responds
> but before the other 4 can respond I get another request? Then
> I'll have to wait until they all respond to re-launch. Ideally I'd like to
re-launch
> immediately with the 1 thread I have available, but I don't know
> if this is possible ...

This seems to be the crux of your problem - you can't interrupt the PING so
if requests come in faster than your ping timeout then your threads are
going to be delayed. In that sense a CountDownLatch is more suitable than a
barrier - despite having to keep recreating it - because you want to be able
to "open" it immediately. But even then having your threads tied up on a
previous ping isn't going to help you with the next one.

Presuming your machines reply to this "ping" based on actual load then you
might be able to use the order of responses to maintain a list of "next host
to use", and you only need issue an actual ping when the list is empty
again. So the basic operation would be like this:
    // "ping" task
    while (!stopped) {
        wait-for-request
        do_ping();
        queue.add(host id);
    }

    // request processing thread
    while (!stopped) {
        Request req = getNextRequesr();
        HostID host;
        if (queue.isEmpty())
           signal-request-waiting
        host = queue.take();
        process(req, host);
    }

Hmmm looking at it now I think perhaps CyclicBarrier will work - assuming
the model I outlined suites what you want to do.

Cheers,
David Holmes


From syg6 at yahoo.com  Mon Sep 18 08:03:27 2006
From: syg6 at yahoo.com (Robert Bowen)
Date: Mon, 18 Sep 2006 05:03:27 -0700 (PDT)
Subject: [concurrency-interest] Using concurrent to write a load balancer
In-Reply-To: <450E81C0.3010708@it.edu>
Message-ID: <20060918120327.86373.qmail@web37710.mail.mud.yahoo.com>

Hi Kasper, thanks for the response.

Yea, I was meaning to look at invokeAny(). My only problem with ExecutiveService is, I still need a mechanism that allows me to launch every thread / task in my pool at the same time, instead of looping through each thread, one at a time, and calling its start() method, and then having it run.

I thought using CyclicBarrier would be the answer but I'm having issues with that as well -- A program that uses CyclicBarrier does the following:

1. Create the barrier, say with 5 threads.
2. Loops through all of the threads calling each thread's start() method.
3. When all of the threads are started the barrier is 'breached' and they all execute.

Fine. But what I want to do is start all the threads and have the barrier wait for some *other* event, in my case, an incoming request. And upon receiving it, launch all of the threads, grab the result from the 1st one, cancel the rest, and go back to waiting. 

It would seem I can't do this with CyclicBarrier, because it doesn't wait for external events but rather other threads to be started. One they are all started it triggers. It would seem a CountDownLatch would make more sense but they are not reusable! 

In summary, I'd love to use the ExecutiveService.invokeAny() but I still need a way to continually (with each request) launch all of my threads at the same time.

Can I do this with ExecutiveService?

Thanks again,
Bob

----- Original Message ----
From: Kasper Nielsen <kav at it.edu>
To: syg6 <syg6 at yahoo.com>
Cc: concurrency-interest at cs.oswego.edu
Sent: Monday, September 18, 2006 1:23:44 PM
Subject: Re: [concurrency-interest] Using concurrent to write a load balancer

syg6 wrote:
> Hello all.
> 
> I am trying to write a load balancer using the new Java 1.5 concurrent
> classes. I am currently trudging through all of the examples, trying to see
> what I might use. I wanted to bounce my idea off the group.
> 
> I will have a more-or-less fixed pool of a small number of threads (it might
> grow or shrink every now an then but not often). Each thread will have the
> same Task associated with it, a task that sends out a 'ping' (using
> java.net's isReachable, which is another can of worms, but anyway ...)
> 
> So every time a request comes in I'd like to use a CyclicBarrier (I think)
> to launch all of the threads and when the first thread answers (with the ip
> of the machine pinged), cancel all of the other Tasks, reset and wait for
> the next request.
> 
> I *think* the best way to do this is with a CyclicBarrier, because I need a
> reusable way to launch all of my threads/tasks at once. CountDownLatch isn't
> reusable and it I were to use a ExecutorCompletionService I'd have to call
> the submit() method for each Task, one after the other, which means the
> first Task would probably almost always be the first to respond.
> 
Hi Bob,

I think you might want to have a look at
the invokeAny method of ExecutorService
http://java.sun.com/j2se/1.5.0/docs/api/java/util/concurrent/ExecutorService.html#invokeAny(java.util.Collection)

Should be much simpler then using a CyclicBarrier.

- Kasper




-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060918/fbe88b06/attachment.html 

From Stefan.Skoglund at it-huset.se  Mon Sep 18 08:05:27 2006
From: Stefan.Skoglund at it-huset.se (Stefan Skoglund)
Date: Mon, 18 Sep 2006 14:05:27 +0200
Subject: [concurrency-interest] Using concurrent to write a load balancer
References: <6361357.post@talk.nabble.com>
Message-ID: <2FDD14551F535345BE46817D94D8DBEC406E6B@ripley.it-huset.se>

Hello Bob
 
CyclicBarrier has all or nothing guarantees. The idea is to let exactly, in this case 5, threads fall through. It will never let more or less than 5 threads fall through at once. This is a tricky thing to implement, and the concurrent guys has done it for you. Still, it is a rather tricky to use. 
It seems that this isn't the thing you are looking for though. It sounds like you need a powerful thread pool. Take a look at the ExecutorService, also in concurrent.
 
/Stefan

________________________________

Fr?n: concurrency-interest-bounces at cs.oswego.edu genom syg6
Skickat: m? 2006-09-18 12:40
Till: concurrency-interest at cs.oswego.edu
?mne: [concurrency-interest] Using concurrent to write a load balancer




Hello all.

I am trying to write a load balancer using the new Java 1.5 concurrent
classes. I am currently trudging through all of the examples, trying to see
what I might use. I wanted to bounce my idea off the group.

I will have a more-or-less fixed pool of a small number of threads (it might
grow or shrink every now an then but not often). Each thread will have the
same Task associated with it, a task that sends out a 'ping' (using
java.net's isReachable, which is another can of worms, but anyway ...)

So every time a request comes in I'd like to use a CyclicBarrier (I think)
to launch all of the threads and when the first thread answers (with the ip
of the machine pinged), cancel all of the other Tasks, reset and wait for
the next request.

I *think* the best way to do this is with a CyclicBarrier, because I need a
reusable way to launch all of my threads/tasks at once. CountDownLatch isn't
reusable and it I were to use a ExecutorCompletionService I'd have to call
the submit() method for each Task, one after the other, which means the
first Task would probably almost always be the first to respond.

The only possible problem I see with CyclicBarrier is, what happens when,
say, a request comes in, I launch my 5 (for example) threads, one responds
but before the other 4 can respond I get another request? Then I'll have to
wait until they all respond to re-launch. Ideally I'd like to re-launch
immediately with the 1 thread I have available, but I don't know if this is
possible ...

Does this sound doable? Anyone have any code that resembles this?

Many thanks,
Bob

ps. I posted on this last week on the java.sun.com forums, but decided to
move over to Nabble ... Here's the
http://forum.java.sun.com/thread.jspa?threadID=768022&tstart=0 link :


--
View this message in context: http://www.nabble.com/Using-concurrent-to-write-a-load-balancer-tf2290433.html#a6361357
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at altair.cs.oswego.edu
http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest


-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060918/4183acfd/attachment-0001.html 

From kav at it.edu  Mon Sep 18 08:41:29 2006
From: kav at it.edu (Kasper Nielsen)
Date: Mon, 18 Sep 2006 14:41:29 +0200
Subject: [concurrency-interest] Using concurrent to write a load balancer
In-Reply-To: <20060918120327.86373.qmail@web37710.mail.mud.yahoo.com>
References: <20060918120327.86373.qmail@web37710.mail.mud.yahoo.com>
Message-ID: <450E93F9.5000700@it.edu>

Robert Bowen wrote:
> Hi Kasper, thanks for the response.
> 
> Yea, I was meaning to look at invokeAny(). My only problem with ExecutiveService is, I still need a mechanism that allows me to launch every thread / task in my pool at the same time, instead of looping through each thread, one at a time, and calling its start() method, and then having it run.

If you use ThreadPoolExecutor you can just prestart all the core threads 
and set the keep alive time to a high value.


-Kasper

From syg6 at yahoo.com  Mon Sep 18 11:56:20 2006
From: syg6 at yahoo.com (Robert Bowen)
Date: Mon, 18 Sep 2006 08:56:20 -0700 (PDT)
Subject: [concurrency-interest] Using concurrent to write a load balancer
In-Reply-To: <450E81C0.3010708@it.edu>
Message-ID: <20060918155620.44513.qmail@web37701.mail.mud.yahoo.com>

Well I have something up and running using ExecutorService. Basically it's like this:

class Ping implements Callable<String> {

    public String call() throws Exception {
        InetAddress iNetAddress = InetAddress.getByName(address);
       if (iNetAddress.isReachable(3000)) return address;
        else return null;
    }
}

public class ThreadPoolExecutorTest {

    public static void main(String[] args) {
        ThreadPoolExecutor tpe = new ThreadPoolExecutor(...);
        tpe.prestartAllCoreThreads();

        ArrayList<Callable> tasks = new ArrayList<Callable>();
        Ping ping = new Ping("192.168.0.27");
        tasks.add(ping);
        ping = new Ping("192.168.0.28");
        tasks.add(ping);

        while (true) {
            String result = (String)tpe.invokeAny(tasks);
            Thread.sleep(1000);
        }
    }
}

It's just a demo but it seems to work very well, many thanks for all of the tips! Working with threads is confusing to start with in Java and with this new API it's *much* easier, but even so there's a bunch of different classes that have similar functionality, it's a question of navigating through all the docs and (of course) speaking to people more clever than yourself on the forums.

My only other question is how to cancel the other tasks that finish after the first one, so they are available again for the next request? I know I can cancel tasks using Future.cancel() but if I use a FutureTask instead of a Callable, then I can't use invokeAny() ...

Anyway, I'm further along than I was with your help!

Bob


----- Original Message ----
From: Kasper Nielsen <kav at it.edu>
To: syg6 <syg6 at yahoo.com>
Cc: concurrency-interest at cs.oswego.edu
Sent: Monday, September 18, 2006 1:23:44 PM
Subject: Re: [concurrency-interest] Using concurrent to write a load balancer

syg6 wrote:
> Hello all.
> 
> I am trying to write a load balancer using the new Java 1.5 concurrent
> classes. I am currently trudging through all of the examples, trying to see
> what I might use. I wanted to bounce my idea off the group.
> 
> I will have a more-or-less fixed pool of a small number of threads (it might
> grow or shrink every now an then but not often). Each thread will have the
> same Task associated with it, a task that sends out a 'ping' (using
> java.net's isReachable, which is another can of worms, but anyway ...)
> 
> So every time a request comes in I'd like to use a CyclicBarrier (I think)
> to launch all of the threads and when the first thread answers (with the ip
> of the machine pinged), cancel all of the other Tasks, reset and wait for
> the next request.
> 
> I *think* the best way to do this is with a CyclicBarrier, because I need a
> reusable way to launch all of my threads/tasks at once. CountDownLatch isn't
> reusable and it I were to use a ExecutorCompletionService I'd have to call
> the submit() method for each Task, one after the other, which means the
> first Task would probably almost always be the first to respond.
> 
Hi Bob,

I think you might want to have a look at
the invokeAny method of ExecutorService
http://java.sun.com/j2se/1.5.0/docs/api/java/util/concurrent/ExecutorService.html#invokeAny(java.util.Collection)

Should be much simpler then using a CyclicBarrier.

- Kasper
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at altair.cs.oswego.edu
http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest






-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060918/1e1ac4e9/attachment.html 

From kav at it.edu  Mon Sep 18 12:29:31 2006
From: kav at it.edu (Kasper Nielsen)
Date: Mon, 18 Sep 2006 18:29:31 +0200
Subject: [concurrency-interest] Using concurrent to write a load balancer
In-Reply-To: <20060918155620.44513.qmail@web37701.mail.mud.yahoo.com>
References: <20060918155620.44513.qmail@web37701.mail.mud.yahoo.com>
Message-ID: <450EC96B.3010807@it.edu>

Robert Bowen wrote:
> My only other question is how to cancel the other tasks that finish after the first one, so they are available again for the next request? I know I can cancel tasks using Future.cancel() but if I use a FutureTask instead of a Callable, then I can't use invokeAny() ...
> 
invokeAny automatically interrupts all threads that have not finished 
before returning the result. However, for some reason isReachable() does 
not respond to interrupt signals. So not much you can do about that.

If it turns out to be a problem you would probably need to add some more 
threads to the pool (don't worry to much about the overhead) or use 
java.lang.Runtime to execute an external ping application

You might also want to checkout this previous thread
http://altair.cs.oswego.edu/pipermail/concurrency-interest/2006-May/002574.html

- Kasper

From hanson.char at gmail.com  Mon Sep 18 13:40:08 2006
From: hanson.char at gmail.com (Hanson Char)
Date: Mon, 18 Sep 2006 10:40:08 -0700
Subject: [concurrency-interest] ConcurrentLinkedBlockingQueue ?
In-Reply-To: <ca53c8f80609131122g44ba7a62w4fb1ea72b6a52125@mail.gmail.com>
References: <ca53c8f80609062313h502eb3f9n9217820f4d8b46a0@mail.gmail.com>
	<4500017E.4040202@cs.oswego.edu>
	<ca53c8f80609091818x71495fbbtf598233fe69c3271@mail.gmail.com>
	<ca53c8f80609092051u7f6f8b24n4882c96acbc45fb6@mail.gmail.com>
	<ca53c8f80609092321y2f585d28k5073d7e73f02416a@mail.gmail.com>
	<ca53c8f80609092358o7f9c08dcrdea39e87e71ee580@mail.gmail.com>
	<45044524.4070109@wizards.de>
	<ca53c8f80609102340n4dce9bfcg69c51cdb9ea244bf@mail.gmail.com>
	<ca53c8f80609111453h4a090d08l4057774c4c745ed6@mail.gmail.com>
	<ca53c8f80609131122g44ba7a62w4fb1ea72b6a52125@mail.gmail.com>
Message-ID: <ca53c8f80609181040l3acc005bt642ab4a06144fb2b@mail.gmail.com>

Just in case anyone is still interested in the CLBQ, it was further
"micro-optimized" a couple of days ago.

Cheers,
Hanson

On 9/13/06, Hanson Char <hanson.char at gmail.com> wrote:
>
> Hi,
>
> I've enhanced ConcurrentLinkedBlockingQueue (CLBQ) such that both the
> take() and poll(long,TimeUnit) now throws an InterruptedException if
> interrupted while waiting.
>
> Source:
>
>
>
> http://svn.sourceforge.net/viewvc/beanlib/trunk/beanlib/src/net/sf/beanlib/util/concurrent/ConcurrentLinkedBlockingQueue.java
>
> However, since CLBQ does not implement the BlockingQueue interface, the
> name CLBQ is pretty misleading!  Any suggestion for a better name ?  Or
> should these extra take() and poll(long,TimeUnit) should really be
> added/incorporated to the existing j.u.c.ConcurrentLinkedQueue ?
>
> Hanson
>
> (Previous test result before the enhancement: http://beanlib.sourceforge.net/pdf/060911/060911-clbq.pdf
> )
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060918/8d90bae9/attachment.html 

From dcholmes at optusnet.com.au  Tue Sep 19 00:49:34 2006
From: dcholmes at optusnet.com.au (David Holmes)
Date: Tue, 19 Sep 2006 14:49:34 +1000
Subject: [concurrency-interest] Using concurrent to write a load balancer
In-Reply-To: <20060918155620.44513.qmail@web37701.mail.mud.yahoo.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCAEHNHCAA.dcholmes@optusnet.com.au>

Bob,

This approach might be viable for you but it doesn't deal with two things
you initially indicated were an issue:

a) there will be a skew toward the first Ping always being the first to
respond because of the time taken to add all the Ping tasks to the
executor - especially in light of how invokeAny works. Nowhere near as bad
as starting new threads but still might be an issue. Of course there is no
instantaneous release mechanism - even using a CyclicBarrier requires the
released threads to serialize through the internal lock, one at a time.

b) No cancellation. But as I've said before the ping isn't cancellable - so
you really have no choice here but to live with that and just use more
threads.

Cheers,
David
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Robert Bowen
  Sent: Tuesday, 19 September 2006 1:56 AM
  To: Kasper Nielsen
  Cc: concurrency-interest at cs.oswego.edu
  Subject: Re: [concurrency-interest] Using concurrent to write a load
balancer


  Well I have something up and running using ExecutorService. Basically it's
like this:

  class Ping implements Callable<String> {

      public String call() throws Exception {
         InetAddress iNetAddress = InetAddress.getByName(address);
         if (iNetAddress.isReachable(3000)) return address;
         else return null;
      }
  }

  public class ThreadPoolExecutorTest {

      public static void main(String[] args) {
          ThreadPoolExecutor tpe = new ThreadPoolExecutor(...);
          tpe.prestartAllCoreThreads();

          ArrayList<Callable> tasks = new ArrayList<Callable>();
          Ping ping = new Ping("192.168.0.27");
          tasks.add(ping);
          ping = new Ping("192.168.0.28");
          tasks.add(ping);

          while (true) {
              String result = (String)tpe.invokeAny(tasks);
              Thread.sleep(1000);
          }
      }
  }


  It's just a demo but it seems to work very well, many thanks for all of
the tips! Working with threads is confusing to start with in Java and with
this new API it's *much* easier, but even so there's a bunch of different
classes that have similar functionality, it's a question of navigating
through all the docs and (of course) speaking to people more clever than
yourself on the forums.

  My only other question is how to cancel the other tasks that finish after
the first one, so they are available again for the next request? I know I
can cancel tasks using Future.cancel() but if I use a FutureTask instead of
a Callable, then I can't use invokeAny() ...

  Anyway, I'm further along than I was with your help!

  Bob



  ----- Original Message ----
  From: Kasper Nielsen <kav at it.edu>
  To: syg6 <syg6 at yahoo.com>
  Cc: concurrency-interest at cs.oswego.edu
  Sent: Monday, September 18, 2006 1:23:44 PM
  Subject: Re: [concurrency-interest] Using concurrent to write a load
balancer


  syg6 wrote:
  > Hello all.
  >
  > I am trying to write a load balancer using the new Java 1.5 concurrent
  > classes. I am currently trudging through all of the examples, trying to
see
  > what I might use. I wanted to bounce my idea off the group.
  >
  > I will have a more-or-less fixed pool of a small number of threads (it
might
  > grow or shrink every now an then but not often). Each thread will have
the
  > same Task associated with it, a task that sends out a 'ping' (using
  > java.net's isReachable, which is another can of worms, but anyway ...)
  >
  > So every time a request comes in I'd like to use a CyclicBarrier (I
think)
  > to launch all of the threads and when the first thread answers (with the
ip
  > of the machine pinged), cancel all of the other Tasks, reset and wait
for
  > the next request.
  >
  > I *think* the best way to do this is with a CyclicBarrier, because I
need a
  > reusable way to launch all of my threads/tasks at once. CountDownLatch
isn't
  > reusable and it I were to use a ExecutorCompletionService I'd have to
call
  > the submit() method for each Task, one after the other, which means the
  > first Task would probably almost always be the first to respond.
  >
  Hi Bob,

  I think you might want to have a look at
  the invokeAny method of ExecutorService

http://java.sun.com/j2se/1.5.0/docs/api/java/util/concurrent/ExecutorService
.html#invokeAny(java.util.Collection)

  Should be much simpler then using a CyclicBarrier.

  - Kasper
  _______________________________________________
  Concurrency-interest mailing list
  Concurrency-interest at altair.cs.oswego.edu
  http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest




-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060919/39d5253c/attachment.html 

From studdugie at gmail.com  Tue Sep 19 04:35:02 2006
From: studdugie at gmail.com (studdugie)
Date: Tue, 19 Sep 2006 04:35:02 -0400
Subject: [concurrency-interest] micro benchmark hell
Message-ID: <5a59ce530609190135x4500973bs593dba2d30b68b88@mail.gmail.com>

Hello all. I've written a micro benchmark for a little pool class as a
JUnit testcase. I'm posting it next:

6    import com.tvtaxi.library.util.Nanos;
7    import com.tvtaxi.library.util.PrivateAccessor;
8    import junit.framework.TestCase;
9    import java.io.IOException;
10   import java.nio.channels.Selector;
11   import java.util.concurrent.BrokenBarrierException;
12   import java.util.concurrent.CountDownLatch;
13   import java.util.concurrent.CyclicBarrier;
14   import java.util.concurrent.atomic.AtomicLong;
15
16   public class TestSynchronizedSelectorPool extends TestCase
17   {
18       private static final int THREADS = 16;
19
20       public void testGet() throws Exception
21       {
22           final SynchronizedSelectorPool pool = new
SynchronizedSelectorPool();
23           final CountDownLatch start = new CountDownLatch( 1 );
24           final CyclicBarrier start2 = new CyclicBarrier( THREADS + 1 );
25           final AtomicLong sum = new AtomicLong();
26           final CountDownLatch fin = new CountDownLatch( THREADS );
27           for( int i = THREADS; i-- > 0; )
28           {
29               new Thread()
30               {
31                   public void run()
32                   {
33                       try
34                       {
35                           start2.await();
36                           long nanos = System.nanoTime();
37                           Selector selector = pool.get();
38                           assertTrue( sum.addAndGet(
System.nanoTime() - nanos ) > 0 );
39                           assertTrue( selector.isOpen() );
40                       }
41                       catch( BrokenBarrierException ex )
42                       {
43                           ex.printStackTrace();
44                       }
45                       catch( InterruptedException ignore )
46                       {
47                           ignore.printStackTrace();
48                       }
49                       catch( IOException e )
50                       {
51                           e.printStackTrace();
52                       }
53                       finally
54                       {
55                           fin.countDown();
56                       }
57                   }
58               }.start();
59           }
60           try
61           {
62               long nanos = System.nanoTime();
63               start2.await();
64               fin.await();
65               nanos = System.nanoTime() - nanos;
66               assertTrue( (Boolean)PrivateAccessor.getField( pool,
"full" ) );
67               assertEquals( 8, ((Integer)PrivateAccessor.getField(
pool, "slot" )).intValue() );
68               assertEquals( 8, ((Integer)PrivateAccessor.getField(
pool, "next" )).intValue() );
69               assertTrue( pool.get().isOpen() );
70               assertEquals( 1, ((Integer)PrivateAccessor.getField(
pool, "next" )).intValue() );
71               System.out.println( "Sum: " + new Nanos( sum.get() ) );
72               System.out.println( "Avg: " + new Nanos( sum.get() /
THREADS ) );
73               System.out.println( "Total time elapsed: " + new
Nanos( nanos ) );
74           }
75           finally
76           {
77               pool.close();
78           }
79       }
80   }

I'm running it on a 2 way Opterton w/ Linux kernel v2.6.17.11 w/ Sun
JVM 1.5.0_08.  The problem I'm having is if I execute it often enough
eventually "Sum" will be greater than "Total time elapsed". Which as
far as I'm concerned is supposed to be impossible.

Now it could be that because it's 4:30 AM and my brain is fried I'm
overlooking the obvious but I doubt it.  Any feedback would be greatly
appreciated.

Regards,

Dane Foster

From kav at it.edu  Tue Sep 19 04:48:34 2006
From: kav at it.edu (Kasper Nielsen)
Date: Tue, 19 Sep 2006 10:48:34 +0200
Subject: [concurrency-interest] micro benchmark hell
In-Reply-To: <5a59ce530609190135x4500973bs593dba2d30b68b88@mail.gmail.com>
References: <5a59ce530609190135x4500973bs593dba2d30b68b88@mail.gmail.com>
Message-ID: <450FAEE2.8050108@it.edu>

  > I'm running it on a 2 way Opterton w/ Linux kernel v2.6.17.11 w/ Sun
> JVM 1.5.0_08.  The problem I'm having is if I execute it often enough
> eventually "Sum" will be greater than "Total time elapsed". Which as
> far as I'm concerned is supposed to be impossible.
> 
> Now it could be that because it's 4:30 AM and my brain is fried I'm
> overlooking the obvious but I doubt it.  Any feedback would be greatly
> appreciated.
> 
> Regards,
> 
> Dane Foster

Hi Dane,

Lets say:

Thread1-Thread8 uses 1 s each on Processor 1
Thread9-Thread16 uses 1 s each on Processor 2

That's 16 seconds in total. But only 8 seconds in wall clock time 
(excluding time spent on starting/joining threads)

- Kasper



From studdugie at gmail.com  Tue Sep 19 10:07:05 2006
From: studdugie at gmail.com (studdugie)
Date: Tue, 19 Sep 2006 10:07:05 -0400
Subject: [concurrency-interest] micro benchmark hell
In-Reply-To: <450FAEE2.8050108@it.edu>
References: <5a59ce530609190135x4500973bs593dba2d30b68b88@mail.gmail.com>
	<450FAEE2.8050108@it.edu>
Message-ID: <5a59ce530609190707u35d1293ftf06ab70b9feb9fa@mail.gmail.com>

Thank you Kasper. It just clicked.

New rule! No coding past 2AM.

On 9/19/06, Kasper Nielsen <kav at it.edu> wrote:
>   > I'm running it on a 2 way Opterton w/ Linux kernel v2.6.17.11 w/ Sun
> > JVM 1.5.0_08.  The problem I'm having is if I execute it often enough
> > eventually "Sum" will be greater than "Total time elapsed". Which as
> > far as I'm concerned is supposed to be impossible.
> >
> > Now it could be that because it's 4:30 AM and my brain is fried I'm
> > overlooking the obvious but I doubt it.  Any feedback would be greatly
> > appreciated.
> >
> > Regards,
> >
> > Dane Foster
>
> Hi Dane,
>
> Lets say:
>
> Thread1-Thread8 uses 1 s each on Processor 1
> Thread9-Thread16 uses 1 s each on Processor 2
>
> That's 16 seconds in total. But only 8 seconds in wall clock time
> (excluding time spent on starting/joining threads)
>
> - Kasper
>
>
>

From hanson.char at gmail.com  Tue Sep 19 13:15:46 2006
From: hanson.char at gmail.com (Hanson Char)
Date: Tue, 19 Sep 2006 10:15:46 -0700
Subject: [concurrency-interest] ConcurrentLinkedBlockingQueue ?
In-Reply-To: <ca53c8f80609181040l3acc005bt642ab4a06144fb2b@mail.gmail.com>
References: <ca53c8f80609062313h502eb3f9n9217820f4d8b46a0@mail.gmail.com>
	<ca53c8f80609091818x71495fbbtf598233fe69c3271@mail.gmail.com>
	<ca53c8f80609092051u7f6f8b24n4882c96acbc45fb6@mail.gmail.com>
	<ca53c8f80609092321y2f585d28k5073d7e73f02416a@mail.gmail.com>
	<ca53c8f80609092358o7f9c08dcrdea39e87e71ee580@mail.gmail.com>
	<45044524.4070109@wizards.de>
	<ca53c8f80609102340n4dce9bfcg69c51cdb9ea244bf@mail.gmail.com>
	<ca53c8f80609111453h4a090d08l4057774c4c745ed6@mail.gmail.com>
	<ca53c8f80609131122g44ba7a62w4fb1ea72b6a52125@mail.gmail.com>
	<ca53c8f80609181040l3acc005bt642ab4a06144fb2b@mail.gmail.com>
Message-ID: <ca53c8f80609191015i773348ecyb3da3b1ca7605b5a@mail.gmail.com>

More experiments seem to indicate that when the mix is N-producer
1-consumer, CLBQ consistently outperform the LBQ . However, when the mix is
N-producer and M-consumer, it seems the reverse is true. Not exactly sure
why. Probably CLBQ incurs a higher overhead as compared to LBQ in the
M-consumer situation.

Hanson

On 9/18/06, Hanson Char <hanson.char at gmail.com> wrote:
>
> Just in case anyone is still interested in the CLBQ, it was further
> "micro-optimized" a couple of days ago.
>
> Cheers,
> Hanson
>
>
> On 9/13/06, Hanson Char <hanson.char at gmail.com> wrote:
> >
> > Hi,
> >
> > I've enhanced ConcurrentLinkedBlockingQueue (CLBQ) such that both the
> > take() and poll(long,TimeUnit) now throws an InterruptedException if
> > interrupted while waiting.
> >
> > Source:
> >
> >
> >
> > http://svn.sourceforge.net/viewvc/beanlib/trunk/beanlib/src/net/sf/beanlib/util/concurrent/ConcurrentLinkedBlockingQueue.java
> >
> > However, since CLBQ does not implement the BlockingQueue interface, the
> > name CLBQ is pretty misleading!  Any suggestion for a better name ?  Or
> > should these extra take() and poll(long,TimeUnit) should really be
> > added/incorporated to the existing j.u.c.ConcurrentLinkedQueue ?
> >
> > Hanson
> >
> > (Previous test result before the enhancement: http://beanlib.sourceforge.net/pdf/060911/060911-clbq.pdf
> > )
> >
> >
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060919/8000fa9d/attachment.html 

From yangjs at alibaba-inc.com  Thu Sep 21 02:29:55 2006
From: yangjs at alibaba-inc.com (yangjs)
Date: Thu, 21 Sep 2006 14:29:55 +0800
Subject: [concurrency-interest] ThreadPoolExecutor implement question!
Message-ID: <002f01c6dd47$5a4ed490$5544000a@alibabahz.com>

hi,all

when I read ThreadPoolExecutor code ,I found it's declare the mainLock use "final" ,

  private final ReentrantLock mainLock = new ReentrantLock();

many method use the lock  guarding state, use case as follow:

     final ReentrantLock mainLock = this.mainLock;
        mainLock.lock();
        try {
            // some code 
        } finally {
            mainLock.unlock();
        }

My question is the instance field already use "final" to mainLock, 
why every method need copy mainLock reference to final  local var.

this puzzle me,who can tell me why ? thanks.

Best regards,
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060921/699494ac/attachment.html 

From dcholmes at optusnet.com.au  Thu Sep 21 03:33:32 2006
From: dcholmes at optusnet.com.au (David Holmes)
Date: Thu, 21 Sep 2006 17:33:32 +1000
Subject: [concurrency-interest] ThreadPoolExecutor implement question!
In-Reply-To: <002f01c6dd47$5a4ed490$5544000a@alibabahz.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCEEIIHCAA.dcholmes@optusnet.com.au>

This was an optimization to work around a VM "glitch" which where the final
field is loaded on each access (as a normal field would be) rather than
loading it once. To avoid this it is copied to a local variable.

Cheers,
David Holmes
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of yangjs
  Sent: Thursday, 21 September 2006 4:30 PM
  To: concurrency-interest at cs.oswego.edu
  Subject: [concurrency-interest] ThreadPoolExecutor implement question!


  hi,all

  when I read ThreadPoolExecutor code ,I found it's declare the mainLock use
"final" ,

    private final ReentrantLock mainLock = new ReentrantLock();

  many method use the lock  guarding state, use case as follow:

       final ReentrantLock mainLock = this.mainLock;
          mainLock.lock();
          try {
              // some code
          } finally {
              mainLock.unlock();
          }

  My question is the instance field already use "final" to mainLock,
  why every method need copy mainLock reference to final  local var.

  this puzzle me,who can tell me why ? thanks.

  Best regards,
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060921/52797253/attachment.html 

From hanson.char at gmail.com  Thu Sep 21 04:03:28 2006
From: hanson.char at gmail.com (Hanson Char)
Date: Thu, 21 Sep 2006 01:03:28 -0700
Subject: [concurrency-interest] ThreadPoolExecutor implement question!
In-Reply-To: <NFBBKALFDCPFIDBNKAPCEEIIHCAA.dcholmes@optusnet.com.au>
References: <002f01c6dd47$5a4ed490$5544000a@alibabahz.com>
	<NFBBKALFDCPFIDBNKAPCEEIIHCAA.dcholmes@optusnet.com.au>
Message-ID: <ca53c8f80609210103j53a63247mc6ae446c12a44534@mail.gmail.com>

Is such optimization only applicable to final fields, but not instance
member fields in general ?  Does this mean if a final field is accessed more
than once in a method, it's always faster to assign it first to a local
variable ?

Hanson

On 9/21/06, David Holmes <dcholmes at optusnet.com.au> wrote:
>
>  This was an optimization to work around a VM "glitch" which where the
> final field is loaded on each access (as a normal field would be) rather
> than loading it once. To avoid this it is copied to a local variable.
>
> Cheers,
> David Holmes
>
> -----Original Message-----
> *From:* concurrency-interest-bounces at cs.oswego.edu [mailto:
> concurrency-interest-bounces at cs.oswego.edu]*On Behalf Of *yangjs
> *Sent:* Thursday, 21 September 2006 4:30 PM
> *To:* concurrency-interest at cs.oswego.edu
> *Subject:* [concurrency-interest] ThreadPoolExecutor implement question!
>
> hi,all
>
> when I read ThreadPoolExecutor code ,I found it's declare the mainLock use
> "final" ,
>
>   private final ReentrantLock mainLock = new ReentrantLock();
>
> many method use the lock  guarding state, use case as follow:
>
>      final ReentrantLock mainLock = this.mainLock;
>         mainLock.lock();
>         try {
>             // some code
>         } finally {
>             mainLock.unlock();
>         }
>
> My question is the instance field already use "final" to mainLock,
> why every method need copy mainLock reference to final  local var.
>
> this puzzle me,who can tell me why ? thanks.
>
> Best regards,
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060921/8f96af20/attachment.html 

From yangjs at alibaba-inc.com  Thu Sep 21 04:48:21 2006
From: yangjs at alibaba-inc.com (yangjs)
Date: Thu, 21 Sep 2006 16:48:21 +0800
Subject: [concurrency-interest] ThreadPoolExecutor implement question!
References: <002f01c6dd47$5a4ed490$5544000a@alibabahz.com>
	<NFBBKALFDCPFIDBNKAPCEEIIHCAA.dcholmes@optusnet.com.au>
	<ca53c8f80609210103j53a63247mc6ae446c12a44534@mail.gmail.com>
Message-ID: <005201c6dd5a$b122f090$5544000a@alibabahz.com>

 thx, 

  I think if it does as David said, the optimization should happend  at compile time or  VM  runtime.
 
  it is a lower level skill for application developer. is right?




 
  ----- Original Message ----- 
  From: Hanson Char 
  To: dholmes at ieee.org 
  Cc: yangjs ; concurrency-interest at cs.oswego.edu 
  Sent: Thursday, September 21, 2006 4:03 PM
  Subject: Re: [concurrency-interest] ThreadPoolExecutor implement question!


  Is such optimization only applicable to final fields, but not instance member fields in general ?  Does this mean if a final field is accessed more than once in a method, it's always faster to assign it first to a local variable ? 

  Hanson


  On 9/21/06, David Holmes <dcholmes at optusnet.com.au> wrote:
    This was an optimization to work around a VM "glitch" which where the final field is loaded on each access (as a normal field would be) rather than loading it once. To avoid this it is copied to a local variable.

    Cheers,
    David Holmes
      -----Original Message-----
      From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of yangjs
      Sent: Thursday, 21 September 2006 4:30 PM
      To: concurrency-interest at cs.oswego.edu
      Subject: [concurrency-interest] ThreadPoolExecutor implement question!


      hi,all

      when I read ThreadPoolExecutor code ,I found it's declare the mainLock use "final" ,

        private final ReentrantLock mainLock = new ReentrantLock();

      many method use the lock  guarding state, use case as follow:

           final ReentrantLock mainLock = this.mainLock;
              mainLock.lock();
              try {
                  // some code 
              } finally {
                  mainLock.unlock();
              }

      My question is the instance field already use "final" to mainLock, 
      why every method need copy mainLock reference to final  local var.

      this puzzle me,who can tell me why ? thanks.

      Best regards,

    _______________________________________________
    Concurrency-interest mailing list
    Concurrency-interest at altair.cs.oswego.edu 
    http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest 




-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060921/10537bf7/attachment-0001.html 

From dl at cs.oswego.edu  Thu Sep 21 07:15:01 2006
From: dl at cs.oswego.edu (Doug Lea)
Date: Thu, 21 Sep 2006 07:15:01 -0400
Subject: [concurrency-interest] ThreadPoolExecutor implement question!
In-Reply-To: <ca53c8f80609210103j53a63247mc6ae446c12a44534@mail.gmail.com>
References: <002f01c6dd47$5a4ed490$5544000a@alibabahz.com>	<NFBBKALFDCPFIDBNKAPCEEIIHCAA.dcholmes@optusnet.com.au>
	<ca53c8f80609210103j53a63247mc6ae446c12a44534@mail.gmail.com>
Message-ID: <45127435.2040407@cs.oswego.edu>

Hanson Char wrote:
> Is such optimization only applicable to final fields, but not instance 
> member fields in general ?  Does this mean if a final field is accessed 
> more than once in a method, it's always faster to assign it first to a 
> local variable ?
> 

JVMs are generally smart enough to do this, when they are allowed to.
They are not allowed to, for example if you have
   a = somefield;
   synchronized(this) { b = somefield; ... }
According to the JMM (Java Memory Model), b cannot in general just
use the load from a.

JVMs must completely understand and enforce all the rules of the JMM.
Which they do, but sometimes too conservatively. The performance glitch
here is that JVMs don't always understand that a final field that itself
references a lock doesn't have to be reloaded across its own lock boundary,
when it is again used to perform the corresponding unlock. This
is a fairly subtle fact that escapes the attention of at least
some JVMs.

This is a micro-optimization that matters less as JVMs get smarter,
and even so, is hardly ever worth doing (which is why we don't
talk about it much) unless you are writing library code used
by millions of people where you might as well make it as fast
as you know how.

-Doug

From dcholmes at optusnet.com.au  Thu Sep 21 08:12:37 2006
From: dcholmes at optusnet.com.au (David Holmes)
Date: Thu, 21 Sep 2006 22:12:37 +1000
Subject: [concurrency-interest] ThreadPoolExecutor implement question!
In-Reply-To: <45127435.2040407@cs.oswego.edu>
Message-ID: <NFBBKALFDCPFIDBNKAPCKEIKHCAA.dcholmes@optusnet.com.au>

Doug writes:
> This is a micro-optimization that matters less as JVMs get smarter,
> and even so, is hardly ever worth doing (which is why we don't
> talk about it much) unless you are writing library code used
> by millions of people where you might as well make it as fast
> as you know how.

For completeness I'll mention that "caching" fields into locals can have
performance benefits in specialized contexts, such as in real-time Java. In
the RTSJ field accesses can require additional checks on both reads and
writes, so reducing that to a single read/write can be beneficial. The
optimizations in such environments are still lagging.

Cheers,
David



From david at walend.net  Sat Sep 23 10:47:23 2006
From: david at walend.net (David Walend)
Date: Sat, 23 Sep 2006 10:47:23 -0400
Subject: [concurrency-interest] Special Conditions and PriorityBlockingQueue
Message-ID: <8BB9BC3C-E323-40DD-B8D0-41631B205C94@walend.net>

In the SomnifugiJMS project, I'm getting close to implementing the  
full JMS specification. The last big technical hurdle is implementing  
message selectors for JMS Queues. (Message selectors are these odd  
bits of SQL92 that make sure a particular queue only receives  
messages with particular properties represented as key/value pairs.)

Currently, JMS Queues are wrapped around BlockingQueues, usually  
PriorityBlockingQueues. I just looked at the PriorityBlockingQueue  
code, which wraps a ReentrantLock and a notEmpty Condition around a  
PriorityQueue.

Message selectors in Topics were fairly easy. Each subscriber to a  
topic gets its own BlockingQueue. If a subscriber's message selector  
doesn't match the message, the message doesn't drop into that  
subscriber's BlockingQueue.

That approach won't work for JMS Queues. All QueueReceivers currently  
work off the same BlockingQueue, and I'd like to keep that abstraction.

I have two vague ideas for how to get message selectors working for  
JMS Queues, neither of which is great. I'd like to get some feedback  
before trying either.

Multiple Conditions: Write a custom BlockingQueue with a Condition  
for each message selector. Start with the code for  
PriorityBlockingQueue. Add custom code to create a Map<JMS  
QueueReceiver with a MessageSelector,Condition>, that holds waiting  
receivers. When a message enters the queue, check the waiting  
receivers to see if any message selectors match the message; if so,  
signal that receiver's take() or poll() call. When a receiver first  
take()s or poll()s or is signaled by its Condition, iterate through  
the queue for a matching message. If there's a message, return it and  
(if needed) remove the entry from the Map. If no message matches, add  
the receiver to the Map.

Multiple BlockingQueues: Use a Map<MessageSelector,BlockingQueue>,  
with one entry for each MessageSelector plus one for all messages:  
When a new QueueReceiver with a new MessageSelector is created, add a  
new entry to the Map. When a message is sent, check all the  
MessageSelectors, and place the message in all the corresponding  
BlockingQueues. When a QueueReceiver takes() a message successfully,  
remove() it from all the BlockingQueues. Fold all this into a class  
that implements BlockingQueue.

Neither approach is without problems. The first requires creating  
Conditions dynamically, which the example code doesn't do, plus  
iterating through the queue (could be mitigated by holding onto the  
matching message). The second has this traffic jam of remove()ing  
from multiple Queues. I'll take any advice on where to start.

Thanks,

Dave

David Walend
david at walend.net



From tim at peierls.net  Sun Sep 24 14:39:11 2006
From: tim at peierls.net (Tim Peierls)
Date: Sun, 24 Sep 2006 14:39:11 -0400
Subject: [concurrency-interest] Special Conditions and
	PriorityBlockingQueue
In-Reply-To: <8BB9BC3C-E323-40DD-B8D0-41631B205C94@walend.net>
References: <8BB9BC3C-E323-40DD-B8D0-41631B205C94@walend.net>
Message-ID: <63b4e4050609241139j8201fc9sad67acea287d27bc@mail.gmail.com>

No responses! Maybe because your analysis seems to be bang-on: neither
approach is very satisfactory. Maybe the problem is inherently not
addressable (at least not nicely) by composing BlockingQueues?

I can't find a clear statement in the JMS spec of just what is required
here, otherwise I'd try to figure out another approach. If you expanded a
little on your introductory paragraph, I bet you could make it clear.

--tim

On 9/23/06, David Walend <david at walend.net> wrote:
>
> In the SomnifugiJMS project, I'm getting close to implementing the
> full JMS specification. The last big technical hurdle is implementing
> message selectors for JMS Queues. (Message selectors are these odd
> bits of SQL92 that make sure a particular queue only receives
> messages with particular properties represented as key/value pairs.)
>
> Currently, JMS Queues are wrapped around BlockingQueues, usually
> PriorityBlockingQueues. I just looked at the PriorityBlockingQueue
> code, which wraps a ReentrantLock and a notEmpty Condition around a
> PriorityQueue.
>
> Message selectors in Topics were fairly easy. Each subscriber to a
> topic gets its own BlockingQueue. If a subscriber's message selector
> doesn't match the message, the message doesn't drop into that
> subscriber's BlockingQueue.
>
> That approach won't work for JMS Queues. All QueueReceivers currently
> work off the same BlockingQueue, and I'd like to keep that abstraction.
>
> I have two vague ideas for how to get message selectors working for
> JMS Queues, neither of which is great. I'd like to get some feedback
> before trying either.
>
> Multiple Conditions: Write a custom BlockingQueue with a Condition
> for each message selector. Start with the code for
> PriorityBlockingQueue. Add custom code to create a Map<JMS
> QueueReceiver with a MessageSelector,Condition>, that holds waiting
> receivers. When a message enters the queue, check the waiting
> receivers to see if any message selectors match the message; if so,
> signal that receiver's take() or poll() call. When a receiver first
> take()s or poll()s or is signaled by its Condition, iterate through
> the queue for a matching message. If there's a message, return it and
> (if needed) remove the entry from the Map. If no message matches, add
> the receiver to the Map.
>
> Multiple BlockingQueues: Use a Map<MessageSelector,BlockingQueue>,
> with one entry for each MessageSelector plus one for all messages:
> When a new QueueReceiver with a new MessageSelector is created, add a
> new entry to the Map. When a message is sent, check all the
> MessageSelectors, and place the message in all the corresponding
> BlockingQueues. When a QueueReceiver takes() a message successfully,
> remove() it from all the BlockingQueues. Fold all this into a class
> that implements BlockingQueue.
>
> Neither approach is without problems. The first requires creating
> Conditions dynamically, which the example code doesn't do, plus
> iterating through the queue (could be mitigated by holding onto the
> matching message). The second has this traffic jam of remove()ing
> from multiple Queues. I'll take any advice on where to start.
>
> Thanks,
>
> Dave
>
> David Walend
> david at walend.net
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060924/78ff1726/attachment.html 

From dcholmes at optusnet.com.au  Sun Sep 24 19:55:15 2006
From: dcholmes at optusnet.com.au (David Holmes)
Date: Mon, 25 Sep 2006 09:55:15 +1000
Subject: [concurrency-interest] ACM OOPSLA Conference - Java Concurrency
	Tutorials
In-Reply-To: <8BB9BC3C-E323-40DD-B8D0-41631B205C94@walend.net>
Message-ID: <NFBBKALFDCPFIDBNKAPCMEJJHCAA.dcholmes@optusnet.com.au>

The annual ACM OOPSLA conference is being held in Portland, Oregon, from
October 22 - 26.

http://www.oopsla.org/2006/

As usual some familiar names will be presenting tutorials on concurrent
programming in Java:

http://www.oopsla.org/2006/submission/tutorials/intro_to_concurrent_programm
ing_in_java_5.0.html

http://www.oopsla.org/2006/submission/tutorials/effective_concurrent_program
ming_in_java.html

So tell your colleagues, tell your friends, ...

Cheers,
David Holmes


From vijay at saraswat.org  Sun Sep 24 20:10:35 2006
From: vijay at saraswat.org (Vijay Saraswat)
Date: Sun, 24 Sep 2006 20:10:35 -0400
Subject: [concurrency-interest] ACM OOPSLA Conference - Java Concurrency
 Tutorials
In-Reply-To: <NFBBKALFDCPFIDBNKAPCMEJJHCAA.dcholmes@optusnet.com.au>
References: <NFBBKALFDCPFIDBNKAPCMEJJHCAA.dcholmes@optusnet.com.au>
Message-ID: <45171E7B.1000506@saraswat.org>

And on X10 :-)!

http://www.oopsla.org/2006/submission/tutorials/concurrent_object-oriented_programming_for_modern_architectures.html

David Holmes wrote:

>The annual ACM OOPSLA conference is being held in Portland, Oregon, from
>October 22 - 26.
>
>http://www.oopsla.org/2006/
>
>As usual some familiar names will be presenting tutorials on concurrent
>programming in Java:
>
>http://www.oopsla.org/2006/submission/tutorials/intro_to_concurrent_programm
>ing_in_java_5.0.html
>
>http://www.oopsla.org/2006/submission/tutorials/effective_concurrent_program
>ming_in_java.html
>
>So tell your colleagues, tell your friends, ...
>
>Cheers,
>David Holmes
>
>_______________________________________________
>Concurrency-interest mailing list
>Concurrency-interest at altair.cs.oswego.edu
>http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
>  
>



From david at walend.net  Mon Sep 25 13:01:17 2006
From: david at walend.net (David Walend)
Date: Mon, 25 Sep 2006 13:01:17 -0400
Subject: [concurrency-interest] Special Conditions and
	PriorityBlockingQueue
In-Reply-To: <mailman.3.1159027200.1869.concurrency-interest@altair.cs.oswego.edu>
References: <mailman.3.1159027200.1869.concurrency-interest@altair.cs.oswego.edu>
Message-ID: <601D04A1-D2A2-4BAA-830F-93F98BECB76F@walend.net>

On Sep 23, 2006, at 12:00 PM, concurrency-interest- 
request at cs.oswego.edu wrote:

> From: David Walend <david at walend.net>
> Subject: [concurrency-interest] Special Conditions and
> 	PriorityBlockingQueue
>
/snip/
> Multiple Conditions: Write a custom BlockingQueue with a Condition
> for each message selector. Start with the code for
> PriorityBlockingQueue. Add custom code to create a Map<JMS
> QueueReceiver with a MessageSelector,Condition>, that holds waiting
> receivers. When a message enters the queue, check the waiting
> receivers to see if any message selectors match the message; if so,
> signal that receiver's take() or poll() call. When a receiver first
> take()s or poll()s or is signaled by its Condition, iterate through
> the queue for a matching message. If there's a message, return it and
> (if needed) remove the entry from the Map. If no message matches, add
> the receiver to the Map.

Is there a way to remove a Condition once Lock.newCondition() has  
been called? If not, how much overhead does the Condition take if  
it's never .signal()ed?

Thanks,

Dave

David Walend
david at walend.net



From dl at cs.oswego.edu  Mon Sep 25 13:15:50 2006
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon, 25 Sep 2006 13:15:50 -0400
Subject: [concurrency-interest] Special Conditions
	and	PriorityBlockingQueue
In-Reply-To: <601D04A1-D2A2-4BAA-830F-93F98BECB76F@walend.net>
References: <mailman.3.1159027200.1869.concurrency-interest@altair.cs.oswego.edu>
	<601D04A1-D2A2-4BAA-830F-93F98BECB76F@walend.net>
Message-ID: <45180EC6.7000900@cs.oswego.edu>

David Walend wrote:
> 
> Is there a way to remove a Condition once Lock.newCondition() has  
> been called? 

No.

> If not, how much overhead does the Condition take if  
> it's never .signal()ed?

Not much -- when there are no waiters, Conditions just
maintain a few bookkeeping fields.
And they are GCable once unreferenced.
And they are also pretty cheap to construct.

I don't recall anyone ever exploiting all this to build
something with lots of dynamically created Conditions, but
I can't think of any reason not to try.

-Doug

From jason_mehrens at hotmail.com  Mon Sep 25 13:31:28 2006
From: jason_mehrens at hotmail.com (Jason Mehrens)
Date: Mon, 25 Sep 2006 12:31:28 -0500
Subject: [concurrency-interest] ThreadLocal example bug.
Message-ID: <BAY105-F143F6C308FC85D2790C44B83240@phx.gbl>

Correct me if I am wrong but, it appears that the new ThreadLocal example in 
b99 code has a bug in the getCurrentThreadId method.  It returns the value 
of the counter (uniqueId) and not the value from thread local (uniqueNum).  
June archives of the c-i list has the old traffic on this issue.

Regards,

Jason Mehrens



From tackline at tackline.plus.com  Mon Sep 25 15:56:07 2006
From: tackline at tackline.plus.com (Thomas Hawtin)
Date: Mon, 25 Sep 2006 20:56:07 +0100
Subject: [concurrency-interest] ThreadLocal example bug.
In-Reply-To: <BAY105-F143F6C308FC85D2790C44B83240@phx.gbl>
References: <BAY105-F143F6C308FC85D2790C44B83240@phx.gbl>
Message-ID: <45183457.50508@tackline.plus.com>

Jason Mehrens wrote:
> Correct me if I am wrong but, it appears that the new ThreadLocal example in 
> b99 code has a bug in the getCurrentThreadId method.  It returns the value 
> of the counter (uniqueId) and not the value from thread local (uniqueNum).  
> June archives of the c-i list has the old traffic on this issue.

Nice.

I guess that blindness that comes from viewing the same thing too often.

Perhaps following the rule of thumb that variables should be declared 
with as tight a scope as possible should be applied here:

  import java.util.concurrent.atomic.AtomicInteger;

  public class UniqueThreadIdGenerator {

      private static final ThreadLocal < Integer > uniqueNum =
          new ThreadLocal < Integer > () {
              final AtomicInteger uniqueId = new AtomicInteger(0);
              @Override protected Integer initialValue() {
                  return uniqueId.getAndIncrement();
              }
          };

      public static int getCurrentThreadId() {
          return uniqueNum.get();
      }
  } // UniqueThreadIdGenerator

Tom Hawtin

From david at walend.net  Mon Sep 25 16:19:35 2006
From: david at walend.net (David Walend)
Date: Mon, 25 Sep 2006 16:19:35 -0400
Subject: [concurrency-interest] PriorityBlockingQueue question
Message-ID: <385F5ACD-A953-4C1D-B2FE-6FD1B07BA5F8@walend.net>

I'm imitating the PriorityBlockingQueue code to add Conditions  
dynamically.

I ran across a line of code I don't understand. The member variable  
lock is final. The offer() method sets up a final local variable  
named lock, set to the same value. Nothing can change the final  
(short of the hack used by Serialization). The Condition notEmpty and  
the PriorityQueue q don't get the same treatment.

Do I need to imitate that line of code, or can I skip it?

Thanks,

Dave

     private final PriorityQueue<E> q;
     private final ReentrantLock lock = new ReentrantLock(true);
     private final Condition notEmpty = lock.newCondition();

...

     public boolean offer(E o) {
         if (o == null) throw new NullPointerException(); //needs a  
message
         final ReentrantLock lock = this.lock; //why is a local  
variable needed?
         lock.lock();
         try {
             boolean ok = q.offer(o);
             assert ok;
             notEmpty.signal();
             return true;
         } finally {
             lock.unlock();
         }
     }


David Walend
david at walend.net



From kav at it.edu  Mon Sep 25 16:32:40 2006
From: kav at it.edu (Kasper Nielsen)
Date: Mon, 25 Sep 2006 22:32:40 +0200
Subject: [concurrency-interest] PriorityBlockingQueue question
In-Reply-To: <385F5ACD-A953-4C1D-B2FE-6FD1B07BA5F8@walend.net>
References: <385F5ACD-A953-4C1D-B2FE-6FD1B07BA5F8@walend.net>
Message-ID: <45183CE8.1040302@it.edu>

David Walend wrote:
> I'm imitating the PriorityBlockingQueue code to add Conditions  
> dynamically.
> 
> I ran across a line of code I don't understand. The member variable  
> lock is final. The offer() method sets up a final local variable  
> named lock, set to the same value. Nothing can change the final  
> (short of the hack used by Serialization). The Condition notEmpty and  
> the PriorityQueue q don't get the same treatment.
> 
> Do I need to imitate that line of code, or can I skip it?

Take a look at the "[concurrency-interest] ThreadPoolExecutor implement 
question!" thread that was posted a couple of days ago.

- Kasper

From gregg at cytetech.com  Mon Sep 25 18:12:34 2006
From: gregg at cytetech.com (Gregg Wonderly)
Date: Mon, 25 Sep 2006 17:12:34 -0500
Subject: [concurrency-interest] deadlock in java.util.logging
Message-ID: <45185452.4020801@cytetech.com>

I've encountered a deadlocking situation in java.util.logging where a thread has 
asserted a lock on a classloader which is logging some progress information. 
The java.util.logging.ConsoleHandler is at issue.  Below are the associated 
stack traces.

It seems to me that the primary issue is that StreamHandler.publish() method is 
synchronized.  It seems to me that only the doneHeader stuff is really at issue 
here.  If that section becomes the only synchronized(this) section, and writer 
is declared as volatile (to cover the setOutputStream() path to modification 
from another thread), would publish still be thread safe?

I'd like to find a solution to this and submit a bug report ASAP.

Gregg Wonderly

Java stack information for the threads listed above:
===================================================
"task":
         at java.util.logging.StreamHandler.publish(StreamHandler.java:174)
         - waiting to lock <0x1f6babc0> (a java.util.logging.ConsoleHandler)
         at java.util.logging.ConsoleHandler.publish(ConsoleHandler.java:88)
         at java.util.logging.Logger.log(Logger.java:428)
         at java.util.logging.Logger.doLog(Logger.java:450)
         at java.util.logging.Logger.log(Logger.java:473)
         at java.util.logging.Logger.fine(Logger.java:1024)
         at
net.jini.loader.pref.PreferredClassLoader.isPreferredResource(PreferredClassLoader.java:750)
         at
org.wonderly.jini2.browse.JiniDeskTop$12.isPreferredResource(JiniDeskTop.java:920)
         at
net.jini.loader.pref.PreferredClassLoader.loadClass(PreferredClassLoader.java:924)
         - locked <0x1f7484d8> (a org.wonderly.jini2.browse.JiniDeskTop$12)
         at org.wonderly.jini2.browse.JiniDeskTop$12.loadClass(JiniDeskTop.java:933)
         at java.lang.ClassLoader.loadClass(ClassLoader.java:299)
         - locked <0x1f87a3a0> (a net.jini.loader.pref.PreferredClassLoader)
         at
net.jini.loader.pref.PreferredClassLoader.loadClass(PreferredClassLoader.java:942)
         - locked <0x1f87a3a0> (a net.jini.loader.pref.PreferredClassLoader)
         at java.lang.ClassLoader.loadClass(ClassLoader.java:251)
         at java.lang.ClassLoader.loadClassInternal(ClassLoader.java:319)
         - locked <0x1f87a3a0> (a net.jini.loader.pref.PreferredClassLoader)
         at com.sun.jini.reggie.$Proxy2.getConstraints(Unknown Source)
         at
com.sun.jini.proxy.ConstrainableProxyUtil.verifyConsistentConstraints(ConstrainableProxyUtil.java:184)
         at
com.sun.jini.reggie.ConstrainableRegistrarProxy.readObject(ConstrainableRegistrarProxy.java:146)
         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
         at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
         at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
         at java.lang.reflect.Method.invoke(Method.java:585)
         at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:946)
         at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1818)
         at
java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1718)
         at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1304)
         at java.io.ObjectInputStream.readObject(ObjectInputStream.java:349)
         at net.jini.io.MarshalledInstance.get(MarshalledInstance.java:402)
         at
com.sun.jini.discovery.DiscoveryV1.doUnicastDiscovery(DiscoveryV1.java:397)
         at net.jini.discovery.LookupDiscovery$10.run(LookupDiscovery.java:3327)
         at java.security.AccessController.doPrivileged(Native Method)
         at
net.jini.discovery.LookupDiscovery.doUnicastDiscovery(LookupDiscovery.java:3324)
         at net.jini.discovery.LookupDiscovery.access$3900(LookupDiscovery.java:696)
         at
net.jini.discovery.LookupDiscovery$UnicastDiscoveryTask$1.performDiscovery(LookupDiscovery.java:1763)
         at
com.sun.jini.discovery.internal.MultiIPDiscovery.getSingleResponse(MultiIPDiscovery.java:152)
         at
com.sun.jini.discovery.internal.MultiIPDiscovery.getResponse(MultiIPDiscovery.java:83)
         at
net.jini.discovery.LookupDiscovery$UnicastDiscoveryTask.run(LookupDiscovery.java:1756)
         at
net.jini.discovery.LookupDiscovery$DecodeAnnouncementTask.run(LookupDiscovery.java:1599)
         at com.sun.jini.thread.TaskManager$TaskThread.run(TaskManager.java:331)
"task":
         at
net.jini.loader.pref.PreferredClassLoader.loadClass(PreferredClassLoader.java:920)
         - waiting to lock <0x1f7484d8> (a org.wonderly.jini2.browse.JiniDeskTop$12)
         at org.wonderly.jini2.browse.JiniDeskTop$12.loadClass(JiniDeskTop.java:933)
         at java.lang.ClassLoader.loadClass(ClassLoader.java:299)
         - locked <0x1f879270> (a net.jini.loader.pref.PreferredClassLoader)
         at
net.jini.loader.pref.PreferredClassLoader.loadClass(PreferredClassLoader.java:942)
         - locked <0x1f879270> (a net.jini.loader.pref.PreferredClassLoader)
         at java.lang.ClassLoader.loadClass(ClassLoader.java:251)
         at java.lang.ClassLoader.loadClassInternal(ClassLoader.java:319)
         - locked <0x1f879270> (a net.jini.loader.pref.PreferredClassLoader)
         at com.sun.jini.reggie.RegistrarProxy.toString(RegistrarProxy.java:448)
         at java.lang.String.valueOf(String.java:2615)
         at java.lang.StringBuilder.append(StringBuilder.java:116)
         at
com.sun.jini.discovery.UnicastResponse.toString(UnicastResponse.java:118)
         at java.text.MessageFormat.subformat(MessageFormat.java:1237)
         at java.text.MessageFormat.format(MessageFormat.java:828)
         at java.text.Format.format(Format.java:133)
         at java.text.MessageFormat.format(MessageFormat.java:804)
         at java.util.logging.Formatter.formatMessage(Formatter.java:130)
         - locked <0x1f6ccc30> (a com.cytetech.log.StreamFormatter)
         at com.cytetech.log.StreamFormatter.format(StreamFormatter.java:127)
         at java.util.logging.StreamHandler.publish(StreamHandler.java:179)
         - locked <0x1f6babc0> (a java.util.logging.ConsoleHandler)
         at java.util.logging.ConsoleHandler.publish(ConsoleHandler.java:88)
         at java.util.logging.Logger.log(Logger.java:428)
         at java.util.logging.Logger.doLog(Logger.java:450)
         at java.util.logging.Logger.log(Logger.java:514)
         at
com.sun.jini.discovery.DiscoveryV1.doUnicastDiscovery(DiscoveryV1.java:414)
         at net.jini.discovery.LookupDiscovery$10.run(LookupDiscovery.java:3327)
         at java.security.AccessController.doPrivileged(Native Method)
         at
net.jini.discovery.LookupDiscovery.doUnicastDiscovery(LookupDiscovery.java:3324)
         at
net.jini.discovery.LookupDiscovery.doUnicastDiscovery(LookupDiscovery.java:3355)
         at net.jini.discovery.LookupDiscovery.access$3700(LookupDiscovery.java:696)
         at
net.jini.discovery.LookupDiscovery$UnicastDiscoveryTask.run(LookupDiscovery.java:1744)
         at com.sun.jini.thread.TaskManager$TaskThread.run(TaskManager.java:331)

From dcholmes at optusnet.com.au  Mon Sep 25 20:43:19 2006
From: dcholmes at optusnet.com.au (David Holmes)
Date: Tue, 26 Sep 2006 10:43:19 +1000
Subject: [concurrency-interest] deadlock in java.util.logging
In-Reply-To: <45185452.4020801@cytetech.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCGEKBHCAA.dcholmes@optusnet.com.au>

Hi Gregg,

I'm not sure if the StreamHandler can rely on its OutputStream correctly
synchronizing writes so that messages are written atomically. However, even
if it can't I think it should be able to do the formatter related actions
outside of the synchronized block. That would prevent the cycle that causes
the deadlock here. eg something like:

    public void publish(LogRecord record) {
	if (!isLoggable(record)) {
	    return;
	}
	String msg;
	Formatter formatter = getFormatter();
	try {
 	    msg = formatter.format(record);
	} catch (Exception ex) {
	    // We don't want to throw an exception here, but we
	    // report the exception to any registered ErrorManager.
	    reportError(null, ex, ErrorManager.FORMAT_FAILURE);
	    return;
	}

	String head = formatter.getHead(this); // does this need sync?
       synchronized(this) {
	   try {
	       if (!doneHeader) {
	         writer.write(head);
		  doneHeader = true;
	       }
	       writer.write(msg);
	   } catch (Exception ex) {
	     // We don't want to throw an exception here, but we
	     // report the exception to any registered ErrorManager.
	     reportError(null, ex, ErrorManager.WRITE_FAILURE);
	   }
       }
    }



That said, the classloader isn't blameless here as it performs logging while
holding its own lock, thus violating the open-call principle. :)

Cheers,
David Holmes

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Gregg
> Wonderly
> Sent: Tuesday, 26 September 2006 8:13 AM
> To: concurrency-interest
> Subject: [concurrency-interest] deadlock in java.util.logging
>
>
> I've encountered a deadlocking situation in java.util.logging
> where a thread has
> asserted a lock on a classloader which is logging some progress
> information.
> The java.util.logging.ConsoleHandler is at issue.  Below are the
> associated
> stack traces.
>
> It seems to me that the primary issue is that
> StreamHandler.publish() method is
> synchronized.  It seems to me that only the doneHeader stuff is
> really at issue
> here.  If that section becomes the only synchronized(this)
> section, and writer
> is declared as volatile (to cover the setOutputStream() path to
> modification
> from another thread), would publish still be thread safe?
>
> I'd like to find a solution to this and submit a bug report ASAP.
>
> Gregg Wonderly
>
> Java stack information for the threads listed above:
> ===================================================
> "task":
>          at
> java.util.logging.StreamHandler.publish(StreamHandler.java:174)
>          - waiting to lock <0x1f6babc0> (a
> java.util.logging.ConsoleHandler)
>          at
> java.util.logging.ConsoleHandler.publish(ConsoleHandler.java:88)
>          at java.util.logging.Logger.log(Logger.java:428)
>          at java.util.logging.Logger.doLog(Logger.java:450)
>          at java.util.logging.Logger.log(Logger.java:473)
>          at java.util.logging.Logger.fine(Logger.java:1024)
>          at
> net.jini.loader.pref.PreferredClassLoader.isPreferredResource(Pref
erredClassLoader.java:750)
>          at
> org.wonderly.jini2.browse.JiniDeskTop$12.isPreferredResource(JiniD
> eskTop.java:920)
>          at
> net.jini.loader.pref.PreferredClassLoader.loadClass(PreferredClass
> Loader.java:924)
>          - locked <0x1f7484d8> (a
> org.wonderly.jini2.browse.JiniDeskTop$12)
>          at
> org.wonderly.jini2.browse.JiniDeskTop$12.loadClass(JiniDeskTop.java:933)
>          at java.lang.ClassLoader.loadClass(ClassLoader.java:299)
>          - locked <0x1f87a3a0> (a
> net.jini.loader.pref.PreferredClassLoader)
>          at
> net.jini.loader.pref.PreferredClassLoader.loadClass(PreferredClass
> Loader.java:942)
>          - locked <0x1f87a3a0> (a
> net.jini.loader.pref.PreferredClassLoader)
>          at java.lang.ClassLoader.loadClass(ClassLoader.java:251)
>          at java.lang.ClassLoader.loadClassInternal(ClassLoader.java:319)
>          - locked <0x1f87a3a0> (a
> net.jini.loader.pref.PreferredClassLoader)
>          at com.sun.jini.reggie.$Proxy2.getConstraints(Unknown Source)
>          at
> com.sun.jini.proxy.ConstrainableProxyUtil.verifyConsistentConstrai
> nts(ConstrainableProxyUtil.java:184)
>          at
> com.sun.jini.reggie.ConstrainableRegistrarProxy.readObject(Constra
inableRegistrarProxy.java:146)
>          at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
>          at
> sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorIm
> pl.java:39)
>          at
> sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAc
> cessorImpl.java:25)
>          at java.lang.reflect.Method.invoke(Method.java:585)
>          at
> java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:946)
>          at
> java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1818)
>          at
> java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1718)
>          at
> java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1304)
>          at
> java.io.ObjectInputStream.readObject(ObjectInputStream.java:349)
>          at
> net.jini.io.MarshalledInstance.get(MarshalledInstance.java:402)
>          at
> com.sun.jini.discovery.DiscoveryV1.doUnicastDiscovery(DiscoveryV1.
> java:397)
>          at
> net.jini.discovery.LookupDiscovery$10.run(LookupDiscovery.java:3327)
>          at java.security.AccessController.doPrivileged(Native Method)
>          at
> net.jini.discovery.LookupDiscovery.doUnicastDiscovery(LookupDiscov
> ery.java:3324)
>          at
> net.jini.discovery.LookupDiscovery.access$3900(LookupDiscovery.java:696)
>          at
> net.jini.discovery.LookupDiscovery$UnicastDiscoveryTask$1.performD
> iscovery(LookupDiscovery.java:1763)
>          at
> com.sun.jini.discovery.internal.MultiIPDiscovery.getSingleResponse
> (MultiIPDiscovery.java:152)
>          at
> com.sun.jini.discovery.internal.MultiIPDiscovery.getResponse(Multi
> IPDiscovery.java:83)
>          at
> net.jini.discovery.LookupDiscovery$UnicastDiscoveryTask.run(Lookup
> Discovery.java:1756)
>          at
> net.jini.discovery.LookupDiscovery$DecodeAnnouncementTask.run(Look
upDiscovery.java:1599)
>          at
> com.sun.jini.thread.TaskManager$TaskThread.run(TaskManager.java:331)
> "task":
>          at
> net.jini.loader.pref.PreferredClassLoader.loadClass(PreferredClass
> Loader.java:920)
>          - waiting to lock <0x1f7484d8> (a
> org.wonderly.jini2.browse.JiniDeskTop$12)
>          at
> org.wonderly.jini2.browse.JiniDeskTop$12.loadClass(JiniDeskTop.java:933)
>          at java.lang.ClassLoader.loadClass(ClassLoader.java:299)
>          - locked <0x1f879270> (a
> net.jini.loader.pref.PreferredClassLoader)
>          at
> net.jini.loader.pref.PreferredClassLoader.loadClass(PreferredClass
> Loader.java:942)
>          - locked <0x1f879270> (a
> net.jini.loader.pref.PreferredClassLoader)
>          at java.lang.ClassLoader.loadClass(ClassLoader.java:251)
>          at java.lang.ClassLoader.loadClassInternal(ClassLoader.java:319)
>          - locked <0x1f879270> (a
> net.jini.loader.pref.PreferredClassLoader)
>          at
> com.sun.jini.reggie.RegistrarProxy.toString(RegistrarProxy.java:448)
>          at java.lang.String.valueOf(String.java:2615)
>          at java.lang.StringBuilder.append(StringBuilder.java:116)
>          at
> com.sun.jini.discovery.UnicastResponse.toString(UnicastResponse.java:118)
>          at java.text.MessageFormat.subformat(MessageFormat.java:1237)
>          at java.text.MessageFormat.format(MessageFormat.java:828)
>          at java.text.Format.format(Format.java:133)
>          at java.text.MessageFormat.format(MessageFormat.java:804)
>          at java.util.logging.Formatter.formatMessage(Formatter.java:130)
>          - locked <0x1f6ccc30> (a com.cytetech.log.StreamFormatter)
>          at
> com.cytetech.log.StreamFormatter.format(StreamFormatter.java:127)
>          at
> java.util.logging.StreamHandler.publish(StreamHandler.java:179)
>          - locked <0x1f6babc0> (a java.util.logging.ConsoleHandler)
>          at
> java.util.logging.ConsoleHandler.publish(ConsoleHandler.java:88)
>          at java.util.logging.Logger.log(Logger.java:428)
>          at java.util.logging.Logger.doLog(Logger.java:450)
>          at java.util.logging.Logger.log(Logger.java:514)
>          at
> com.sun.jini.discovery.DiscoveryV1.doUnicastDiscovery(DiscoveryV1.
> java:414)
>          at
> net.jini.discovery.LookupDiscovery$10.run(LookupDiscovery.java:3327)
>          at java.security.AccessController.doPrivileged(Native Method)
>          at
> net.jini.discovery.LookupDiscovery.doUnicastDiscovery(LookupDiscov
> ery.java:3324)
>          at
> net.jini.discovery.LookupDiscovery.doUnicastDiscovery(LookupDiscov
> ery.java:3355)
>          at
> net.jini.discovery.LookupDiscovery.access$3700(LookupDiscovery.java:696)
>          at
> net.jini.discovery.LookupDiscovery$UnicastDiscoveryTask.run(Lookup
> Discovery.java:1744)
>          at
> com.sun.jini.thread.TaskManager$TaskThread.run(TaskManager.java:331)
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest


From gregg at cytetech.com  Mon Sep 25 21:35:05 2006
From: gregg at cytetech.com (Gregg Wonderly)
Date: Mon, 25 Sep 2006 20:35:05 -0500
Subject: [concurrency-interest] deadlock in java.util.logging
In-Reply-To: <NFBBKALFDCPFIDBNKAPCGEKBHCAA.dcholmes@optusnet.com.au>
References: <NFBBKALFDCPFIDBNKAPCGEKBHCAA.dcholmes@optusnet.com.au>
Message-ID: <451883C9.7030708@cytetech.com>



David Holmes wrote:
> I'm not sure if the StreamHandler can rely on its OutputStream correctly
> synchronizing writes so that messages are written atomically. However, even
> if it can't I think it should be able to do the formatter related actions
> outside of the synchronized block. That would prevent the cycle that causes
> the deadlock here. eg something like:

I've coded such a solution as you suggest to see if anything else falls out.

> That said, the classloader isn't blameless here as it performs logging while
> holding its own lock, thus violating the open-call principle. :)

Yes, that is perhaps the bigger problem.  It's my fault that it is logging with 
a lock held.  There are moments when that is unavoidable given some of the 
execution paths that expose classloading operations.  I will remove some of the 
debugging in that ClassLoader, or at least try and reduce some of the exposures.

But, I think this reveals an interesting path of circular dependencies involving 
unanticipated class loading operations.

Thanks for your views on this David!  I think I will file an enhancement request 
to at least get the call into the formating to not be synchronized.

Gregg Wonderly

From david at walend.net  Mon Sep 25 22:17:37 2006
From: david at walend.net (David Walend)
Date: Mon, 25 Sep 2006 22:17:37 -0400
Subject: [concurrency-interest] PriorityBlockingQueue question
In-Reply-To: <45183CE8.1040302@it.edu>
References: <385F5ACD-A953-4C1D-B2FE-6FD1B07BA5F8@walend.net>
	<45183CE8.1040302@it.edu>
Message-ID: <0C9DDA6E-95BD-4B39-9C67-7EDF5D0262B8@walend.net>

Thanks, everyone, for the help so far.

I've got something coded up that should work, but isn't very  
satisfying. I did find a way to signal just the conditions that  
matter, but take() and poll(timeout) still have to scan back to find  
the right message.

I know there's still things to clean up, but I'm still trying to  
convince myself this is a good approach.

I'd like a way to clean up the scanForMatchingMessages() st/ it isn't  
scanning. Any ideas?

Code is at
https://somnifugijms.dev.java.net/source/browse/somnifugijms/v3/ 
source/somnifugi/net/walend/somnifugi/juc/ 
MessageSelectingPriorityBlockingQueue.java?rev=1.1&view=auto&content- 
type=text/vnd.viewcvs-markup

Thanks again,

Dave

David Walend
david at walend.net



From jason_mehrens at hotmail.com  Tue Sep 26 09:50:24 2006
From: jason_mehrens at hotmail.com (Jason Mehrens)
Date: Tue, 26 Sep 2006 08:50:24 -0500
Subject: [concurrency-interest] deadlock in java.util.logging
Message-ID: <BAY105-F3188B3A177DDB6AA7BA91283250@phx.gbl>

I've had similar deadlock issues with the EDT/app thread logging.  One trick 
you can use is the java.util.logging.Filter interface to perform deadlock 
detection and prevention.  Or even log an alternate message to the original 
logger.  Here is another deadlock bug with RMI that was reported: 
http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4939307

>Thanks for your views on this David!  I think I will file an enhancement 
>request
>to at least get the call into the formating to not be synchronized.
I think the lock scope could be narrowed on almost all of the handlers.

Regards,

Jason Mehrens



From tim at peierls.net  Tue Sep 26 10:35:32 2006
From: tim at peierls.net (Tim Peierls)
Date: Tue, 26 Sep 2006 10:35:32 -0400
Subject: [concurrency-interest] PriorityBlockingQueue question
In-Reply-To: <0C9DDA6E-95BD-4B39-9C67-7EDF5D0262B8@walend.net>
References: <385F5ACD-A953-4C1D-B2FE-6FD1B07BA5F8@walend.net>
	<45183CE8.1040302@it.edu>
	<0C9DDA6E-95BD-4B39-9C67-7EDF5D0262B8@walend.net>
Message-ID: <63b4e4050609260735t1cce80bag54c79a2f8a55d208@mail.gmail.com>

On 9/25/06, David Walend <david at walend.net> wrote:
>
> I've got something coded up that should work, but isn't very
> satisfying. I did find a way to signal just the conditions that
> matter, but take() and poll(timeout) still have to scan back to find
> the right message.
>
> I know there's still things to clean up, but I'm still trying to
> convince myself this is a good approach.


I'm trying to understand the code first. How can take() ever work with this
code:

while(message == null)
{
    condition.await();
}

message is a local variable, so how can you wait for it to become non-null?


I'd like a way to clean up the scanForMatchingMessages() st/ it isn't
> scanning. Any ideas?


No ideas for this -- I'm thinking about the other approach.

--tim
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060926/51f85122/attachment.html 

From tim at peierls.net  Tue Sep 26 11:11:40 2006
From: tim at peierls.net (Tim Peierls)
Date: Tue, 26 Sep 2006 11:11:40 -0400
Subject: [concurrency-interest] PriorityBlockingQueue question
In-Reply-To: <63b4e4050609260735t1cce80bag54c79a2f8a55d208@mail.gmail.com>
References: <385F5ACD-A953-4C1D-B2FE-6FD1B07BA5F8@walend.net>
	<45183CE8.1040302@it.edu>
	<0C9DDA6E-95BD-4B39-9C67-7EDF5D0262B8@walend.net>
	<63b4e4050609260735t1cce80bag54c79a2f8a55d208@mail.gmail.com>
Message-ID: <63b4e4050609260811s7d82dd14o9707d799444742d2@mail.gmail.com>

On 9/26/06, Tim Peierls <tim at peierls.net> wrote:
>
> I'd like a way to clean up the scanForMatchingMessages() st/ it isn't
> > scanning. Any ideas?
>
>
> No ideas for this -- I'm thinking about the other approach.
>

What if you maintained a separate queue for each selector and atomically
marked messages when consumed? (You could use
AtomicMarkableReference.attemptMark, for example.) Then you don't have the
problem of having to remove a message from all other queues, since receivers
can simply ignore messages that someone else marked.

Then take(selector) is just "take from queue associated with selector" --
more precisely, repeatedly take until you can atomically mark an unmarked
message. I think you could use PBQs instead of PQs and a ConcurrentMap from
selector to queue, avoiding the need for a global lock.

Not sure of the desired behavior for messages that don't match any currently
waiting selector. Are they discarded? Left in their own queue for
selector-less consumption? Or do they have to be scanned for matches each
time you hear about a new selector? (In which case it might seem as though
you're almost back to the other approach, but maybe without the need for a
global lock.)

--tim
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060926/b7654c59/attachment.html 

From david at walend.net  Wed Sep 27 08:28:06 2006
From: david at walend.net (David Walend)
Date: Wed, 27 Sep 2006 08:28:06 -0400
Subject: [concurrency-interest] PriorityBlockingQueue question
In-Reply-To: <63b4e4050609260735t1cce80bag54c79a2f8a55d208@mail.gmail.com>
References: <385F5ACD-A953-4C1D-B2FE-6FD1B07BA5F8@walend.net>
	<45183CE8.1040302@it.edu>
	<0C9DDA6E-95BD-4B39-9C67-7EDF5D0262B8@walend.net>
	<63b4e4050609260735t1cce80bag54c79a2f8a55d208@mail.gmail.com>
Message-ID: <EE119428-D868-4E4D-A4D8-571BC75316E9@walend.net>

On Sep 26, 2006, at 10:35 AM, Tim Peierls wrote:
>
> I'm trying to understand the code first. How can take() ever work  
> with this code:
>
> while(message == null)
> {
>     condition.await();
> }

Arg. Missed a scan. Try

                     while(message == null)
                     {
                         condition.await();
                         message = scanForMatchingMessages 
(messageSelector);
                     }

Thanks,

Dave

David Walend
david at walend.net



From david at walend.net  Wed Sep 27 08:59:05 2006
From: david at walend.net (David Walend)
Date: Wed, 27 Sep 2006 08:59:05 -0400
Subject: [concurrency-interest] PriorityBlockingQueue question
References: <6104B8D9-7995-423C-A8F3-51810634E412@walend.net>
Message-ID: <31D704B5-B62E-4077-950E-8A77C4736E3F@walend.net>



Begin forwarded message:

> From: David Walend <david at walend.net>
> Date: September 27, 2006 8:57:56 AM EDT
> To: Tim Peierls <tim at peierls.net>
> Subject: Re: [concurrency-interest] PriorityBlockingQueue question
>
> On Sep 26, 2006, at 11:11 AM, Tim Peierls wrote:
>
>> On 9/26/06, Tim Peierls <tim at peierls.net> wrote:
>> No ideas for this -- I'm thinking about the other approach.
>>
>> What if you maintained a separate queue for each selector and  
>> atomically marked messages when consumed? (You could use  
>> AtomicMarkableReference.attemptMark, for example.) Then you don't  
>> have the problem of having to remove a message from all other  
>> queues, since receivers can simply ignore messages that someone  
>> else marked.
>>
>> Then take(selector) is just "take from queue associated with  
>> selector" -- more precisely, repeatedly take until you can  
>> atomically mark an unmarked message. I think you could use PBQs  
>> instead of PQs and a ConcurrentMap from selector to queue,  
>> avoiding the need for a global lock.
>
> Thanks, Tim. That sounds much better. I wouldn't (and didn't) find  
> AtomicMarkableReference on my own. Also, everything that can have a  
> message selector also has a close() method that I can use to drop  
> the PBQs when nothing is listening.
>
> I like the approach. It promises to be very lively, but will make a  
> lot of AtomicMarkableReferences to GC.
>
>> Not sure of the desired behavior for messages that don't match any  
>> currently waiting selector. Are they discarded? Left in their own  
>> queue for selector-less consumption? Or do they have to be scanned  
>> for matches each time you hear about a new selector? (In which  
>> case it might seem as though you're almost back to the other  
>> approach, but maybe without the need for a global lock.)
>
> That one is pretty straight forward; the Channel has to have a king  
> PBQ that represents the JMS Queue's contents (for QueueBrowsers if  
> nothing else). When something adds a new message selector, the  
> Channel will have to get a copy of that PBQ's contents into the new  
> message selector queue. Then it should be off and running. It's OK  
> if up a new QueueReceiver is relatively heavy.
>
> Thanks again for the help. I hope to code it up over the weekend.
>
> Dave
>
> David Walend
> david at walend.net
>
>

David Walend
david at walend.net



From Pete.Soper at Sun.COM  Wed Sep 27 13:20:20 2006
From: Pete.Soper at Sun.COM (Pete.Soper at Sun.COM)
Date: Wed, 27 Sep 2006 13:20:20 -0400
Subject: [concurrency-interest] ThreadLocal example bug.
In-Reply-To: <BAY105-F143F6C308FC85D2790C44B83240@phx.gbl>
References: <BAY105-F143F6C308FC85D2790C44B83240@phx.gbl>
Message-ID: <451AB2D4.4020509@Sun.COM>

Hi Jason,
             For the example code field uniqueId is a "dispenser" of the 
next available id value and that is used once per thread (but is visible 
to all threads) via the overriden initialVlaue method invocation that is 
a side effect of the first invocation of uniqueNum.get. The per-thread 
value is maintained by ThreadLocal code that you can't see and it never 
changes for a given thread. But maybe I'm missing something. Do you have 
a misbehaving test?

Regards,
Pete

  import java.util.concurrent.atomic.AtomicInteger;

  public class UniqueThreadIdGenerator {

      private static final ThreadLocal < Integer > uniqueNum =
          new ThreadLocal < Integer > () {
              final AtomicInteger uniqueId = new AtomicInteger(0);
              @Override protected Integer initialValue() {
                  return uniqueId.getAndIncrement();
              }
          };

      public static int getCurrentThreadId() {
          return uniqueNum.get();
      }
  } // UniqueThreadIdGenerator


From jason_mehrens at hotmail.com  Wed Sep 27 13:35:26 2006
From: jason_mehrens at hotmail.com (Jason Mehrens)
Date: Wed, 27 Sep 2006 12:35:26 -0500
Subject: [concurrency-interest] ThreadLocal example bug.
Message-ID: <BAY105-F7B1CFEBA957D3DB07B629831A0@phx.gbl>

Pete,

The code you have attached bellow is correct (and what i would expect to 
see) however, it is not the code out on the web. From 
http://download.java.net/jdk6/docs/api/java/lang/ThreadLocal.html

import java.util.concurrent.atomic.AtomicInteger;

public class UniqueThreadIdGenerator {

     private static final AtomicInteger uniqueId = new AtomicInteger(0);

     private static final ThreadLocal < Integer > uniqueNum =
         new ThreadLocal < Integer > () {
             @Override protected Integer initialValue() {
                 return uniqueId.getAndIncrement();
         }
     };

     public static int getCurrentThreadId() {
         return uniqueId.get();
     }
} // UniqueThreadIdGenerator


Regards,

Jason Mehrens


>From: Pete.Soper at Sun.COM
>To: Jason Mehrens <jason_mehrens at hotmail.com>
>CC: concurrency-interest at cs.oswego.edu
>Subject: Re: [concurrency-interest] ThreadLocal example bug.
>Date: Wed, 27 Sep 2006 13:20:20 -0400
>
>Hi Jason,
>             For the example code field uniqueId is a "dispenser" of the 
>next available id value and that is used once per thread (but is visible to 
>all threads) via the overriden initialVlaue method invocation that is a 
>side effect of the first invocation of uniqueNum.get. The per-thread value 
>is maintained by ThreadLocal code that you can't see and it never changes 
>for a given thread. But maybe I'm missing something. Do you have a 
>misbehaving test?
>
>Regards,
>Pete
>
>  import java.util.concurrent.atomic.AtomicInteger;
>
>  public class UniqueThreadIdGenerator {
>
>      private static final ThreadLocal < Integer > uniqueNum =
>          new ThreadLocal < Integer > () {
>              final AtomicInteger uniqueId = new AtomicInteger(0);
>              @Override protected Integer initialValue() {
>                  return uniqueId.getAndIncrement();
>              }
>          };
>
>      public static int getCurrentThreadId() {
>          return uniqueNum.get();
>      }
>  } // UniqueThreadIdGenerator
>



From Pete.Soper at Sun.COM  Wed Sep 27 14:14:43 2006
From: Pete.Soper at Sun.COM (Pete.Soper at Sun.COM)
Date: Wed, 27 Sep 2006 14:14:43 -0400
Subject: [concurrency-interest] ThreadLocal example bug.
In-Reply-To: <BAY105-F7B1CFEBA957D3DB07B629831A0@phx.gbl>
References: <BAY105-F7B1CFEBA957D3DB07B629831A0@phx.gbl>
Message-ID: <451ABF93.9060407@Sun.COM>

Oh. I thought the fragment of the class file Tom sent me was the code in 
question. Dang.

Pete

Jason Mehrens wrote:

> Pete,
>
> The code you have attached bellow is correct (and what i would expect 
> to see) however, it is not the code out on the web. From 
> http://download.java.net/jdk6/docs/api/java/lang/ThreadLocal.html
>
> import java.util.concurrent.atomic.AtomicInteger;
>
> public class UniqueThreadIdGenerator {
>
>      private static final AtomicInteger uniqueId = new AtomicInteger(0);
>
>      private static final ThreadLocal < Integer > uniqueNum =
>          new ThreadLocal < Integer > () {
>              @Override protected Integer initialValue() {
>                  return uniqueId.getAndIncrement();
>          }
>      };
>
>      public static int getCurrentThreadId() {
>          return uniqueId.get();
>      }
> } // UniqueThreadIdGenerator
>
>
> Regards,
>
> Jason Mehrens
>
>
>> From: Pete.Soper at Sun.COM
>> To: Jason Mehrens <jason_mehrens at hotmail.com>
>> CC: concurrency-interest at cs.oswego.edu
>> Subject: Re: [concurrency-interest] ThreadLocal example bug.
>> Date: Wed, 27 Sep 2006 13:20:20 -0400
>>
>> Hi Jason,
>>             For the example code field uniqueId is a "dispenser" of 
>> the next available id value and that is used once per thread (but is 
>> visible to all threads) via the overriden initialVlaue method 
>> invocation that is a side effect of the first invocation of 
>> uniqueNum.get. The per-thread value is maintained by ThreadLocal code 
>> that you can't see and it never changes for a given thread. But maybe 
>> I'm missing something. Do you have a misbehaving test?
>>
>> Regards,
>> Pete
>>
>>  import java.util.concurrent.atomic.AtomicInteger;
>>
>>  public class UniqueThreadIdGenerator {
>>
>>      private static final ThreadLocal < Integer > uniqueNum =
>>          new ThreadLocal < Integer > () {
>>              final AtomicInteger uniqueId = new AtomicInteger(0);
>>              @Override protected Integer initialValue() {
>>                  return uniqueId.getAndIncrement();
>>              }
>>          };
>>
>>      public static int getCurrentThreadId() {
>>          return uniqueNum.get();
>>      }
>>  } // UniqueThreadIdGenerator
>>
>
>
>


From josh at bloch.us  Wed Sep 27 14:59:57 2006
From: josh at bloch.us (Joshua Bloch)
Date: Wed, 27 Sep 2006 11:59:57 -0700
Subject: [concurrency-interest] ThreadLocal example bug.
In-Reply-To: <451ABF93.9060407@Sun.COM>
References: <BAY105-F7B1CFEBA957D3DB07B629831A0@phx.gbl>
	<451ABF93.9060407@Sun.COM>
Message-ID: <b097ac510609271159i5f018a1sf2417acb3ecc206e@mail.gmail.com>

I would argue that the variable names could be better (and used to be).
Also the vertical spacing conventions and comment conventions aren't exactly
standard (though not far off).  How about this?

import java.util.concurrent.atomic.AtomicInteger;

 public class ThreadId {
     // Atomic integer containing the next thread ID to be assigned
     private static final AtomicInteger nextId = new AtomicInteger(0);

     // Thread local variable containing each thread's ID
     private static final ThreadLocal < Integer > threadId =
         new ThreadLocal<Integer>() {
             @Override protected Integer initialValue() {
                 return nextId.getAndIncrement();
         }
     };

     // Returns the current thread's unique ID, assigning it if necessary
     public static int get() {
         return threadId.get();
     }
 }

Note that the client now says ThreadId.get(), rather than
ThreadIdGenerator.getCurrentThreadId().

           Josh




On 9/27/06, Pete.Soper at sun.com <Pete.Soper at sun.com> wrote:
>
> Oh. I thought the fragment of the class file Tom sent me was the code in
> question. Dang.
>
> Pete
>
> Jason Mehrens wrote:
>
> > Pete,
> >
> > The code you have attached bellow is correct (and what i would expect
> > to see) however, it is not the code out on the web. From
> > http://download.java.net/jdk6/docs/api/java/lang/ThreadLocal.html
> >
> > import java.util.concurrent.atomic.AtomicInteger;
> >
> > public class UniqueThreadIdGenerator {
> >
> >      private static final AtomicInteger uniqueId = new AtomicInteger(0);
> >
> >      private static final ThreadLocal < Integer > uniqueNum =
> >          new ThreadLocal < Integer > () {
> >              @Override protected Integer initialValue() {
> >                  return uniqueId.getAndIncrement();
> >          }
> >      };
> >
> >      public static int getCurrentThreadId() {
> >          return uniqueId.get();
> >      }
> > } // UniqueThreadIdGenerator
> >
> >
> > Regards,
> >
> > Jason Mehrens
> >
> >
> >> From: Pete.Soper at Sun.COM
> >> To: Jason Mehrens <jason_mehrens at hotmail.com>
> >> CC: concurrency-interest at cs.oswego.edu
> >> Subject: Re: [concurrency-interest] ThreadLocal example bug.
> >> Date: Wed, 27 Sep 2006 13:20:20 -0400
> >>
> >> Hi Jason,
> >>             For the example code field uniqueId is a "dispenser" of
> >> the next available id value and that is used once per thread (but is
> >> visible to all threads) via the overriden initialVlaue method
> >> invocation that is a side effect of the first invocation of
> >> uniqueNum.get. The per-thread value is maintained by ThreadLocal code
> >> that you can't see and it never changes for a given thread. But maybe
> >> I'm missing something. Do you have a misbehaving test?
> >>
> >> Regards,
> >> Pete
> >>
> >>  import java.util.concurrent.atomic.AtomicInteger;
> >>
> >>  public class UniqueThreadIdGenerator {
> >>
> >>      private static final ThreadLocal < Integer > uniqueNum =
> >>          new ThreadLocal < Integer > () {
> >>              final AtomicInteger uniqueId = new AtomicInteger(0);
> >>              @Override protected Integer initialValue() {
> >>                  return uniqueId.getAndIncrement();
> >>              }
> >>          };
> >>
> >>      public static int getCurrentThreadId() {
> >>          return uniqueNum.get();
> >>      }
> >>  } // UniqueThreadIdGenerator
> >>
> >
> >
> >
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: /pipermail/attachments/20060927/9f685110/attachment.html 

From Pete.Soper at Sun.COM  Wed Sep 27 15:22:37 2006
From: Pete.Soper at Sun.COM (Pete.Soper at Sun.COM)
Date: Wed, 27 Sep 2006 15:22:37 -0400
Subject: [concurrency-interest] ThreadLocal example bug.
In-Reply-To: <451ABF93.9060407@Sun.COM>
References: <BAY105-F7B1CFEBA957D3DB07B629831A0@phx.gbl>
	<451ABF93.9060407@Sun.COM>
Message-ID: <451ACF7D.50101@Sun.COM>

OK, this is now bug id 6475885 and starting "soon" if you google 
"UniqueThreadIdGenerator" the fix should show up too. It might help to 
visit this page in case I screwed it up too:

    
http://blogs.sun.com/microwaves/entry/java_lang_threadlocal_example_class

I have something less than 1440 minutes left to study for the hardest 
math test I'll have ever taken at that point in my life, so it's hard to 
attach much weight to the formatting right now.  The choice of names got 
quite a lot of review way back wen and would require another formal 
review by the community. I won't be in the position to facilitate that 
for a while. But I stashed Josh's msg into the bug comments.

Regards,
Pete

I wrote:

>Oh. I thought the fragment of the class file Tom sent me was the code in 
>question. Dang.
>
>Pete
>
>Jason Mehrens wrote:
>
>  
>
>>Pete,
>>
>>The code you have attached bellow is correct (and what i would expect 
>>to see) however, it is not the code out on the web. From 
>>http://download.java.net/jdk6/docs/api/java/lang/ThreadLocal.html
>>
>>import java.util.concurrent.atomic.AtomicInteger;
>>
>>public class UniqueThreadIdGenerator {
>>
>>     private static final AtomicInteger uniqueId = new AtomicInteger(0);
>>
>>     private static final ThreadLocal < Integer > uniqueNum =
>>         new ThreadLocal < Integer > () {
>>             @Override protected Integer initialValue() {
>>                 return uniqueId.getAndIncrement();
>>         }
>>     };
>>
>>     public static int getCurrentThreadId() {
>>         return uniqueId.get(); // INCORRECT
>>     }
>>} // UniqueThreadIdGenerator
>>
>>
>>Regards,
>>
>>Jason Mehrens
>>
>>
>>    
>>
>>>From: Pete.Soper at Sun.COM
>>>To: Jason Mehrens <jason_mehrens at hotmail.com>
>>>CC: concurrency-interest at cs.oswego.edu
>>>Subject: Re: [concurrency-interest] ThreadLocal example bug.
>>>Date: Wed, 27 Sep 2006 13:20:20 -0400
>>>
>>>Hi Jason,
>>>            For the example code field uniqueId is a "dispenser" of 
>>>the next available id value and that is used once per thread (but is 
>>>visible to all threads) via the overriden initialVlaue method 
>>>invocation that is a side effect of the first invocation of 
>>>uniqueNum.get. The per-thread value is maintained by ThreadLocal code 
>>>that you can't see and it never changes for a given thread. But maybe 
>>>I'm missing something. Do you have a misbehaving test?
>>>
>>>Regards,
>>>Pete
>>>
>>> import java.util.concurrent.atomic.AtomicInteger;
>>>
>>> public class UniqueThreadIdGenerator {
>>>
>>>     private static final ThreadLocal < Integer > uniqueNum =
>>>         new ThreadLocal < Integer > () {
>>>             final AtomicInteger uniqueId = new AtomicInteger(0);
>>>             @Override protected Integer initialValue() {
>>>                 return uniqueId.getAndIncrement();
>>>             }
>>>         };
>>>
>>>     public static int getCurrentThreadId() {
>>>         return uniqueNum.get(); // CORRECT
>>>     }
>>> } // UniqueThreadIdGenerator
>>>
>>>      
>>>
>>
>>    
>>
>
>_______________________________________________
>Concurrency-interest mailing list
>Concurrency-interest at altair.cs.oswego.edu
>http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>  
>


From unmesh_joshi at hotmail.com  Thu Sep 28 12:20:54 2006
From: unmesh_joshi at hotmail.com (Unmesh joshi)
Date: Thu, 28 Sep 2006 16:20:54 +0000
Subject: [concurrency-interest] Lock implementation
In-Reply-To: <mailman.1.1159459200.26577.concurrency-interest@altair.cs.oswego.edu>
Message-ID: <BAY121-F385FE62D6DE05E7FCD1BF7EF1B0@phx.gbl>

Hi,

Most of the commercial Lock implementations rely on low level atomic 
instructions like compare_and_swap. Is there any algorithm that does not 
rely on low level atomic instructions and is commerciall proven?

Thanks,
Unmesh



From dawidk at mathcs.emory.edu  Thu Sep 28 12:51:35 2006
From: dawidk at mathcs.emory.edu (Dawid Kurzyniec)
Date: Thu, 28 Sep 2006 12:51:35 -0400
Subject: [concurrency-interest] Lock implementation
In-Reply-To: <BAY121-F385FE62D6DE05E7FCD1BF7EF1B0@phx.gbl>
References: <BAY121-F385FE62D6DE05E7FCD1BF7EF1B0@phx.gbl>
Message-ID: <451BFD97.9040308@mathcs.emory.edu>

Unmesh joshi wrote:
> Hi,
>
> Most of the commercial Lock implementations rely on low level atomic 
> instructions like compare_and_swap. Is there any algorithm that does not 
> rely on low level atomic instructions and is commerciall proven?
There is Eisenberg-McGuire algorithm, which does not use atomic operations:

http://www.cs.ucr.edu/~ysong/cs160/lab4/eisenberg-mcguire.cc

but for the price of much higher overhead (about 8 read and write operations per acquire in uncontended case). I don't know whether it is commercially proven, but it is definitely mathematically proven :)

Look also here for the example distributed locking implementation based on this scheme:

http://dcl.mathcs.emory.edu/cgi-bin/viewvc.cgi/software/harness2/trunk/util/src/edu/emory/mathcs/util/remote/locks/RemoteEisMcGLock.java?revision=2416&view=markup

and here, for the JNDI-based use case:

http://dcl.mathcs.emory.edu/cgi-bin/viewvc.cgi/software/harness2/trunk/jndi/jndi-common/src/edu/emory/mathcs/jndi/locks/FairJNDILock.java?revision=2102&view=markup


Regards,
Dawid



From hans.boehm at hp.com  Thu Sep 28 15:03:28 2006
From: hans.boehm at hp.com (Boehm, Hans)
Date: Thu, 28 Sep 2006 14:03:28 -0500
Subject: [concurrency-interest] Lock implementation
In-Reply-To: <451BFD97.9040308@mathcs.emory.edu>
Message-ID: <BDA38860DCFD334EAEA905E44EE8E7EF20C767@G3W0067.americas.hpqcorp.net>

There are also other algorithms that need only O(N+M) memory for M locks
for N threads, which I suspect is a practical requirement.  See

``Implementing Multiple Locks Using Lamport's Mutual Exclusion
Algorithm'', Hans-J. Boehm, Alan Demers and Chris Uhler, ACM LOPLAS 2
(Mar-Dec 1993), pp. 46-58.

I don't think any of these are practical on most modern hardware, since
they usually require memory fences, which in turn tend to be not much
cheaper than atomic memory updates.  If you have sequentially consistent
hardware with slow or nonexistent atomic memory updates, then they might
make sense.

Hans

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu 
> [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf 
> Of Dawid Kurzyniec
> Sent: Thursday, September 28, 2006 9:52 AM
> To: Unmesh joshi
> Cc: concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] Lock implementation
> 
> Unmesh joshi wrote:
> > Hi,
> >
> > Most of the commercial Lock implementations rely on low 
> level atomic 
> > instructions like compare_and_swap. Is there any algorithm 
> that does 
> > not rely on low level atomic instructions and is commerciall proven?
> There is Eisenberg-McGuire algorithm, which does not use 
> atomic operations:
> 
> http://www.cs.ucr.edu/~ysong/cs160/lab4/eisenberg-mcguire.cc
> 
> but for the price of much higher overhead (about 8 read and 
> write operations per acquire in uncontended case). I don't 
> know whether it is commercially proven, but it is definitely 
> mathematically proven :)
> 
> Look also here for the example distributed locking 
> implementation based on this scheme:
> 
> http://dcl.mathcs.emory.edu/cgi-bin/viewvc.cgi/software/harnes
s2/trunk/util/src/edu/emory/mathcs/util/remote/locks/RemoteEisMcGLock.ja
va?revision=2416&view=markup
> 
> and here, for the JNDI-based use case:
> 
> http://dcl.mathcs.emory.edu/cgi-bin/viewvc.cgi/software/harnes
> s2/trunk/jndi/jndi-common/src/edu/emory/mathcs/jndi/locks/Fair
> JNDILock.java?revision=2102&view=markup
> 
> 
> Regards,
> Dawid
> 
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at altair.cs.oswego.edu
> http://altair.cs.oswego.edu/mailman/listinfo/concurrency-interest
> 


From dawidk at mathcs.emory.edu  Thu Sep 28 19:31:05 2006
From: dawidk at mathcs.emory.edu (Dawid Kurzyniec)
Date: Thu, 28 Sep 2006 19:31:05 -0400
Subject: [concurrency-interest] Lock implementation
In-Reply-To: <BDA38860DCFD334EAEA905E44EE8E7EF20C767@G3W0067.americas.hpqcorp.net>
References: <BDA38860DCFD334EAEA905E44EE8E7EF20C767@G3W0067.americas.hpqcorp.net>
Message-ID: <451C5B39.8090902@mathcs.emory.edu>

Boehm, Hans wrote:
> There are also other algorithms that need only O(N+M) memory for M locks
> for N threads, which I suspect is a practical requirement.  See
>
> ``Implementing Multiple Locks Using Lamport's Mutual Exclusion
> Algorithm'', Hans-J. Boehm, Alan Demers and Chris Uhler, ACM LOPLAS 2
> (Mar-Dec 1993), pp. 46-58.
>   
I guess one potential advantage of Eisenberg-McGuire over Lamport's is 
fairness - if one needs it.

> I don't think any of these are practical on most modern hardware, since
> they usually require memory fences, which in turn tend to be not much
> cheaper than atomic memory updates.  If you have sequentially consistent
> hardware with slow or nonexistent atomic memory updates, then they might
> make sense.
>   
We have experimented with such locks in distributed computing settings - 
to coordinate access to data shared in a JNDI service, without using 
external lock managers. But then, the problem of failure recovery arises 
- i.e. what do you do if somebody crashes (or gets disconnected) while 
holding the lock, and possibly after having made changes that caused the 
data to be inconsistent. It seems that most usage scenarios (except for 
very simple ones, like atomic variables, counters etc.) require true 
transactions anyway.

Does it mean that those algorithms are interesting only from a 
theoretical point of view?

Regards,
Dawid


