From oleksandr.otenko at oracle.com  Tue Nov  4 07:54:03 2014
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Tue, 04 Nov 2014 12:54:03 +0000
Subject: [concurrency-interest] lazy finals - was: Here's why
 Atomic*FieldReference access checking is broken
In-Reply-To: <5436B7DD.2050305@infinite-source.de>
References: <542EBD0C.7080301@redhat.com>	<542F6641.3090600@oracle.com>	<5431ADA5.2070904@gmail.com>	<5435062C.6020200@gmail.com>	<CANPzfU9W3nPN+eMg5fRfFLMCB5iB0g3c_oeDPT=ZC2RPHvAtNQ@mail.gmail.com>	<54352F34.1010100@univ-mlv.fr>	<5435539B.3010004@gmail.com>	<54355B31.5010009@oracle.com>	<54365AEB.5070002@gmail.com>	<5436623C.1030104@oracle.com>
	<5436A7FF.8020404@gmail.com> <5436B7DD.2050305@infinite-source.de>
Message-ID: <5458CC6B.9090606@oracle.com>

What about Recursive definitions? (How lazy is lazy)

Do we permit lazy final Stream fibs = 0:1: map (+) fibs (tail fibs)

Alex


On 09/10/2014 17:29, Aaron Grunthal wrote:
> On 09.10.2014 17:21, Peter Levart wrote:
>>
>> This would be a cool optimization for mostly read-only and rarely
>> updated fields, but do we want to transform final instance fields into
>> such thing just to support deserialization via reflection? If this is
>> not a big deal to support than perhaps it is a better approach since it
>> does not neglect null/zero values.
>
> I think the issue here may be that final is used to signal two 
> slightly distinct purposes.
>
> 1. It's to tell the compiler: "This data (almost) never changes, 
> please optimize aggressively, I'm willing to take a significant 
> performance hit if I ever have to change it via reflection"
> This might also be relevant for other language implementers, e.g. for 
> jruby class variables and constants which should remain constant but 
> can be easily changed via metaprogramming or initialized multiple 
> times by various modules.
>
> 2. It's to tell users of the class that it's immutable data, it's safe 
> to pass around, that they're doing something wrong if they want to 
> change the value, etc.
> Think value-types.
> Since immutable data objects are often ephemeral and not assigned to 
> static fields the programmer doesn't really expect the compiler to 
> perform constant folding.
> It's also where the deserialization problem commonly occurs.
>
> - Aaron
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From jzaugg at gmail.com  Tue Nov  4 20:11:21 2014
From: jzaugg at gmail.com (Jason Zaugg)
Date: Wed, 5 Nov 2014 11:11:21 +1000
Subject: [concurrency-interest] ForkJoinPool with a custom thread factory
	that limits thread creation
Message-ID: <CAG3_yeXuzVQMKDSsqf2FUkJ2XObbCGz8uZL01QJOjzE79ttQhA@mail.gmail.com>

A recent change [1] to Scala's default fork join thread pool caused
intermittent deadlocks. This is only visible in the development series
of Scala, 2.12.0-SNAPSHOT.

We changed our thread factory to place a hard limit the number of
threads created (equal to the desired parallelism.)

I have extracted a test case [3] that uses jsr166e directly, rather
than using Scala's parallel collections and abstractions on top of FJ.

In the comments of the bug, Viktor suggests this was too aggressive
and instead we ought to increase the limit to parallelism + 256
(with a system property override.)

He explained:

> The number 256 is going to be the default for the max threads for
> FJP in Java9 (down from 32k) so this change will harmonize the
> settings while making it possible to override from the outside.
>
> The cause of the deadlock is twofold:
>
> 1) The test uses ExecutionContext.global, which is not designed
>    for typical ForkJoin workloads since it has async = true
>    (FIFO instead of LIFO)
> 2) And we capped the default max number of threads to be created
>    when doing managed blocking from 32k to number of cores
>    (a tad too aggressive it seems)

I just wanted to check whether there is an implicit contract between
configured parallelism of the `ForkJoinPool` and the number of threads
that the given factory is willing to offer for a workload like the
one in the test. Alternatively, does the the hang represent a bug in
ForkJoinPool itself?

Through testing, I found that for this example I could trigger the
hang with:

    parallelismLevel | maxThreads
    -----------------------------
                   2 | <= 4
                   4 | <= 9
                   8 | <= 11
                  16 | <= 15

I have cc-ed Viktor who might be able to offer more insight into the intent
of this limit.

Jason Zaugg

[1] https://github.com/scala/scala/pull/4042
[2] https://issues.scala-lang.org/browse/SI-8955
[3] https://gist.github.com/retronym/2e14cdab6d5612562d95
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141105/a6f6d49d/attachment.html>

From dl at cs.oswego.edu  Wed Nov  5 13:50:52 2014
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed, 05 Nov 2014 13:50:52 -0500
Subject: [concurrency-interest] ForkJoinPool with a custom thread
 factory that limits thread creation
In-Reply-To: <CAG3_yeXuzVQMKDSsqf2FUkJ2XObbCGz8uZL01QJOjzE79ttQhA@mail.gmail.com>
References: <CAG3_yeXuzVQMKDSsqf2FUkJ2XObbCGz8uZL01QJOjzE79ttQhA@mail.gmail.com>
Message-ID: <545A718C.5070701@cs.oswego.edu>

On 11/04/2014 08:11 PM, Jason Zaugg wrote:
> A recent change [1] to Scala's default fork join thread pool caused
> intermittent deadlocks. This is only visible in the development series
> of Scala, 2.12.0-SNAPSHOT.
>
> We changed our thread factory to place a hard limit the number of
> threads created (equal to the desired parallelism.)

As Viktor noted in your quoted mail, this seems unwise.
There is no intrinsic hard limit for the number of threads
needed to maintain liveness for possibly-blocking tasks with
arbitrary dependencies. The best you can do is impose a limit
that will almost always allow diagnosis/recovery before the JVM
runs out of resources in programs that are most likely
misbehaving. Pragmatically, a ceiling of 256 extra threads
(beyond the given parallelism level) seems to be the
best compromise on most systems.

-Doug


> I have extracted a test case [3] that uses jsr166e directly, rather
> than using Scala's parallel collections and abstractions on top of FJ.
>
> In the comments of the bug, Viktor suggests this was too aggressive
> and instead we ought to increase the limit to parallelism + 256
> (with a system property override.)
>
> He explained:
>
>  > The number 256 is going to be the default for the max threads for
>  > FJP in Java9 (down from 32k) so this change will harmonize the
>  > settings while making it possible to override from the outside.
>  >
>  > The cause of the deadlock is twofold:
>  >
>  > 1) The test uses ExecutionContext.global, which is not designed
>  >    for typical ForkJoin workloads since it has async = true
>  >    (FIFO instead of LIFO)
>  > 2) And we capped the default max number of threads to be created
>  >    when doing managed blocking from 32k to number of cores
>  >    (a tad too aggressive it seems)
>
> I just wanted to check whether there is an implicit contract between
> configured parallelism of the `ForkJoinPool` and the number of threads
> that the given factory is willing to offer for a workload like the
> one in the test. Alternatively, does the the hang represent a bug in
> ForkJoinPool itself?
>
> Through testing, I found that for this example I could trigger the
> hang with:
>
>      parallelismLevel | maxThreads
>      -----------------------------
>                     2 | <= 4
>                     4 | <= 9
>                     8 | <= 11
>                    16 | <= 15
>
> I have cc-ed Viktor who might be able to offer more insight into the intent
> of this limit.
>
> Jason Zaugg
>
> [1] https://github.com/scala/scala/pull/4042
> [2] https://issues.scala-lang.org/browse/SI-8955
> [3] https://gist.github.com/retronym/2e14cdab6d5612562d95
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From pramalhe at gmail.com  Fri Nov  7 18:11:06 2014
From: pramalhe at gmail.com (Pedro Ramalhete)
Date: Sat, 8 Nov 2014 00:11:06 +0100
Subject: [concurrency-interest] Increasing the throughput of
	ConcurrentLinkedQueue on PowerPC (and ARM?)
Message-ID: <CAAApjO3j3g2fE-e2i5Yt-K8a8t2MBt3=zPrkNSCrCpGkwDEc2Q@mail.gmail.com>

Hello,

Some time ago, we made public a lock-free linked list that allows traversal
of the nodes with relaxed loads (non-volatile loads) which we named
CLLElectedUnlink.
http://concurrencyfreaks.com/2014/06/cllelectedunlink-lock-free-list-with.html
The algorithm used in CLLElectedUnlink has some disadvantages when compared
with other approaches (like the one in ConcurrentLinkedQueue) because in
some scenarios it can cause churn on the GC, but we were expecting some
increase in performance for operations that do list traversal when ran on
CPUs with a weak memory ordering, like PowerPC and ARM.

A few weeks ago, the RunAbove cloud made available instances with PowerPC
machines, so we ran our benchmarks to see how much of a difference it did,
and we were pleasantly surprised with the results:
http://www.phoronix.com/scan.php?page=article&item=runabove_power8_cloud&num=1
http://concurrencyfreaks.com/2014/10/relaxed-atomics-linked-list-on-powerpc.html

In fact, we found the results so encouraging (up to 15x increase in
performance) that we decided to apply the same kind of optimization to
ConcurrentLinkedQueue (CLQ), and although it was a bit trickier, we believe
it to be correct, and it can be downloaded here:
https://github.com/pramalhe/ConcurrencyFreaks/blob/master/Java/com/concurrencyfreaks/list/ConcurrentLinkedQueueRelaxed.java
The code changes are minimal and the modifications are explained in this
presentation:
https://github.com/pramalhe/ConcurrencyFreaks/raw/master/Presentations/ConcurrentLinkedQueueRelaxed.pptx

Notice that doing relaxed loads has no impact on the performance on CPU
architectures with strong memory ordering like x86, because both relaxed
loads and volatile loads generate the same instruction (a MOV in the case
of x86).

If you have an application where you do a lot of contains() or remove() on
a CLQ, and you can run it on PowerPC or ARM, we would like to know how much
of a gain you see from it. We made our own microbenchmark, but those are
never a replacement for real applications.
http://concurrencyfreaks.com/2014/11/improved-concurrentlinkedqueue-for.html

Thanks,
Pedro & Andreia
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141108/e4ed5225/attachment.html>

From martinrb at google.com  Fri Nov  7 19:02:05 2014
From: martinrb at google.com (Martin Buchholz)
Date: Fri, 7 Nov 2014 16:02:05 -0800
Subject: [concurrency-interest] Increasing the throughput of
 ConcurrentLinkedQueue on PowerPC (and ARM?)
In-Reply-To: <CAAApjO3j3g2fE-e2i5Yt-K8a8t2MBt3=zPrkNSCrCpGkwDEc2Q@mail.gmail.com>
References: <CAAApjO3j3g2fE-e2i5Yt-K8a8t2MBt3=zPrkNSCrCpGkwDEc2Q@mail.gmail.com>
Message-ID: <CA+kOe0-OPZwYHNkWOz-jWhzzA0ZFDB3nyUKP1M9r+gSufa5WsA@mail.gmail.com>

Thanks for investigating this.

Of course, humans are very bad at writing algorithms using relaxed atomics.
I have had the experience of not trusting my own analysis and backing
away from an optimization.

You can see we already have some relaxed atomics in CLQ, but they are
all about saving writes, not reads, and we still think of volatile
reads as close to free, having used x86 for too many years.

After a quick look through the code, one thing in particular makes me
nervous - reading "item" in relaxed mode.  CASing item to null is
intended to atomically delete that element, and a relaxed read may
cause a thread to believe that the element has not yet been deleted.

On Fri, Nov 7, 2014 at 3:11 PM, Pedro Ramalhete <pramalhe at gmail.com> wrote:
> Hello,
>
> Some time ago, we made public a lock-free linked list that allows traversal
> of the nodes with relaxed loads (non-volatile loads) which we named
> CLLElectedUnlink.
> http://concurrencyfreaks.com/2014/06/cllelectedunlink-lock-free-list-with.html
> The algorithm used in CLLElectedUnlink has some disadvantages when compared
> with other approaches (like the one in ConcurrentLinkedQueue) because in
> some scenarios it can cause churn on the GC, but we were expecting some
> increase in performance for operations that do list traversal when ran on
> CPUs with a weak memory ordering, like PowerPC and ARM.
>
> A few weeks ago, the RunAbove cloud made available instances with PowerPC
> machines, so we ran our benchmarks to see how much of a difference it did,
> and we were pleasantly surprised with the results:
> http://www.phoronix.com/scan.php?page=article&item=runabove_power8_cloud&num=1
> http://concurrencyfreaks.com/2014/10/relaxed-atomics-linked-list-on-powerpc.html
>
> In fact, we found the results so encouraging (up to 15x increase in
> performance) that we decided to apply the same kind of optimization to
> ConcurrentLinkedQueue (CLQ), and although it was a bit trickier, we believe
> it to be correct, and it can be downloaded here:
> https://github.com/pramalhe/ConcurrencyFreaks/blob/master/Java/com/concurrencyfreaks/list/ConcurrentLinkedQueueRelaxed.java
> The code changes are minimal and the modifications are explained in this
> presentation:
> https://github.com/pramalhe/ConcurrencyFreaks/raw/master/Presentations/ConcurrentLinkedQueueRelaxed.pptx
>
> Notice that doing relaxed loads has no impact on the performance on CPU
> architectures with strong memory ordering like x86, because both relaxed
> loads and volatile loads generate the same instruction (a MOV in the case of
> x86).
>
> If you have an application where you do a lot of contains() or remove() on a
> CLQ, and you can run it on PowerPC or ARM, we would like to know how much of
> a gain you see from it. We made our own microbenchmark, but those are never
> a replacement for real applications.
> http://concurrencyfreaks.com/2014/11/improved-concurrentlinkedqueue-for.html
>
> Thanks,
> Pedro & Andreia
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

From pramalhe at gmail.com  Fri Nov  7 19:32:31 2014
From: pramalhe at gmail.com (Pedro Ramalhete)
Date: Sat, 8 Nov 2014 01:32:31 +0100
Subject: [concurrency-interest] Increasing the throughput of
 ConcurrentLinkedQueue on PowerPC (and ARM?)
In-Reply-To: <CA+kOe0-OPZwYHNkWOz-jWhzzA0ZFDB3nyUKP1M9r+gSufa5WsA@mail.gmail.com>
References: <CAAApjO3j3g2fE-e2i5Yt-K8a8t2MBt3=zPrkNSCrCpGkwDEc2Q@mail.gmail.com>
	<CA+kOe0-OPZwYHNkWOz-jWhzzA0ZFDB3nyUKP1M9r+gSufa5WsA@mail.gmail.com>
Message-ID: <CAAApjO1ExAfUMmz6-t2uezc=r6jibYz+cqLqPYvm+he8hiy78w@mail.gmail.com>

Hi Martin,

I agree that relaxed atomics are non-trivial, but then again, lock-free
programming never is  :)
It's hard to be 100% confident of the correctness of an algorithm, but
that's partially the reason whay we're sharing the code on this list, so
that other experts can take a stab at it and find possible issues  ;)

CASitem should not be a problem because if the item is no longer
"up-to-date" the CAS will fail, just like it does today for CLQ if some
other thread changes the item between the volatile load and the CAS.
Accessing the item in a relaxed way is definitely the "trickiest" part of
these optimizations, but I would like to point out three details:
1. For both contains() and remove(), when the item is seen as null, it will
be re-read using a volatile load (as in CLQ today);
2. For contains() when a matching item is seen, it will be re-read using a
volatile load (as in CLQ today);
3. For remove() when a matching item is seen, if it has changed, the CAS
will fail (as in CLQ today when the item is modified between the volatile
load and the CAS).

Cheers,
Pedro


On Sat, Nov 8, 2014 at 1:02 AM, Martin Buchholz <martinrb at google.com> wrote:

> Thanks for investigating this.
>
> Of course, humans are very bad at writing algorithms using relaxed atomics.
> I have had the experience of not trusting my own analysis and backing
> away from an optimization.
>
> You can see we already have some relaxed atomics in CLQ, but they are
> all about saving writes, not reads, and we still think of volatile
> reads as close to free, having used x86 for too many years.
>
> After a quick look through the code, one thing in particular makes me
> nervous - reading "item" in relaxed mode.  CASing item to null is
> intended to atomically delete that element, and a relaxed read may
> cause a thread to believe that the element has not yet been deleted.
>
> On Fri, Nov 7, 2014 at 3:11 PM, Pedro Ramalhete <pramalhe at gmail.com>
> wrote:
> > Hello,
> >
> > Some time ago, we made public a lock-free linked list that allows
> traversal
> > of the nodes with relaxed loads (non-volatile loads) which we named
> > CLLElectedUnlink.
> >
> http://concurrencyfreaks.com/2014/06/cllelectedunlink-lock-free-list-with.html
> > The algorithm used in CLLElectedUnlink has some disadvantages when
> compared
> > with other approaches (like the one in ConcurrentLinkedQueue) because in
> > some scenarios it can cause churn on the GC, but we were expecting some
> > increase in performance for operations that do list traversal when ran on
> > CPUs with a weak memory ordering, like PowerPC and ARM.
> >
> > A few weeks ago, the RunAbove cloud made available instances with PowerPC
> > machines, so we ran our benchmarks to see how much of a difference it
> did,
> > and we were pleasantly surprised with the results:
> >
> http://www.phoronix.com/scan.php?page=article&item=runabove_power8_cloud&num=1
> >
> http://concurrencyfreaks.com/2014/10/relaxed-atomics-linked-list-on-powerpc.html
> >
> > In fact, we found the results so encouraging (up to 15x increase in
> > performance) that we decided to apply the same kind of optimization to
> > ConcurrentLinkedQueue (CLQ), and although it was a bit trickier, we
> believe
> > it to be correct, and it can be downloaded here:
> >
> https://github.com/pramalhe/ConcurrencyFreaks/blob/master/Java/com/concurrencyfreaks/list/ConcurrentLinkedQueueRelaxed.java
> > The code changes are minimal and the modifications are explained in this
> > presentation:
> >
> https://github.com/pramalhe/ConcurrencyFreaks/raw/master/Presentations/ConcurrentLinkedQueueRelaxed.pptx
> >
> > Notice that doing relaxed loads has no impact on the performance on CPU
> > architectures with strong memory ordering like x86, because both relaxed
> > loads and volatile loads generate the same instruction (a MOV in the
> case of
> > x86).
> >
> > If you have an application where you do a lot of contains() or remove()
> on a
> > CLQ, and you can run it on PowerPC or ARM, we would like to know how
> much of
> > a gain you see from it. We made our own microbenchmark, but those are
> never
> > a replacement for real applications.
> >
> http://concurrencyfreaks.com/2014/11/improved-concurrentlinkedqueue-for.html
> >
> > Thanks,
> > Pedro & Andreia
> >
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141108/ba01436e/attachment.html>

From martinrb at google.com  Fri Nov  7 19:48:34 2014
From: martinrb at google.com (Martin Buchholz)
Date: Fri, 7 Nov 2014 16:48:34 -0800
Subject: [concurrency-interest] Increasing the throughput of
 ConcurrentLinkedQueue on PowerPC (and ARM?)
In-Reply-To: <CAAApjO1ExAfUMmz6-t2uezc=r6jibYz+cqLqPYvm+he8hiy78w@mail.gmail.com>
References: <CAAApjO3j3g2fE-e2i5Yt-K8a8t2MBt3=zPrkNSCrCpGkwDEc2Q@mail.gmail.com>
	<CA+kOe0-OPZwYHNkWOz-jWhzzA0ZFDB3nyUKP1M9r+gSufa5WsA@mail.gmail.com>
	<CAAApjO1ExAfUMmz6-t2uezc=r6jibYz+cqLqPYvm+he8hiy78w@mail.gmail.com>
Message-ID: <CA+kOe087q-WMSpWYwiPQ_ZRtbCLwVAEPE0oiqx_zZWm-RHTyFg@mail.gmail.com>

On Fri, Nov 7, 2014 at 4:32 PM, Pedro Ramalhete <pramalhe at gmail.com> wrote:
> Hi Martin,
>
> I agree that relaxed atomics are non-trivial, but then again, lock-free
> programming never is  :)
> It's hard to be 100% confident of the correctness of an algorithm, but
> that's partially the reason whay we're sharing the code on this list, so
> that other experts can take a stab at it and find possible issues  ;)
>
> CASitem should not be a problem because if the item is no longer
> "up-to-date" the CAS will fail, just like it does today for CLQ if some
> other thread changes the item between the volatile load and the CAS.
> Accessing the item in a relaxed way is definitely the "trickiest" part of
> these optimizations, but I would like to point out three details:
> 1. For both contains() and remove(), when the item is seen as null, it will
> be re-read using a volatile load (as in CLQ today);
> 2. For contains() when a matching item is seen, it will be re-read using a
> volatile load (as in CLQ today);

Suppose the q contains element e and some thread is waiting for e to
be removed by another thread using
while (q.contains(e)) {}
it looks to me like contains() is calling only relaxed operations, so
an adversarial VM can "optimize" that to
if (q.contains(e)) { while (true) {} }

From pramalhe at gmail.com  Fri Nov  7 22:34:35 2014
From: pramalhe at gmail.com (Pedro Ramalhete)
Date: Sat, 8 Nov 2014 04:34:35 +0100
Subject: [concurrency-interest] Increasing the throughput of
 ConcurrentLinkedQueue on PowerPC (and ARM?)
In-Reply-To: <CA+kOe087q-WMSpWYwiPQ_ZRtbCLwVAEPE0oiqx_zZWm-RHTyFg@mail.gmail.com>
References: <CAAApjO3j3g2fE-e2i5Yt-K8a8t2MBt3=zPrkNSCrCpGkwDEc2Q@mail.gmail.com>
	<CA+kOe0-OPZwYHNkWOz-jWhzzA0ZFDB3nyUKP1M9r+gSufa5WsA@mail.gmail.com>
	<CAAApjO1ExAfUMmz6-t2uezc=r6jibYz+cqLqPYvm+he8hiy78w@mail.gmail.com>
	<CA+kOe087q-WMSpWYwiPQ_ZRtbCLwVAEPE0oiqx_zZWm-RHTyFg@mail.gmail.com>
Message-ID: <CAAApjO1wuEfsu2ogm9utGFXu-TTShzQ8i=ArZN3OdAniYfHdGA@mail.gmail.com>

That's an interesting scenario, and in that case there will always be at
least three volatile loads in contains(e), assuming the list has a node
with an item 'e':
- 1st volatile load occurs when reading 'head' at the beginning of the list
traversal;
- 2nd volatile load occurs when reading 'item' of the node matching 'e' (as
I mentioned on point 2. above);
- 3rd volatile load occurs when reading the 'next' of the last node on the
list;

Once the other thread removes 'e' from the list and unlinks the node (that
thread or some other thread would have to call poll()/succ() and the node
would have to be one of the first ones so as to be unlinked, but it can
happen), then there will be at least 2 volatiles loads, the 1st and 3rd
mentioned above.
No VM will be able to optimize volatile loads away, or even to reorder
instructions from within two successive calls to contains() because they
are "wrapped" in the two volatile loads at the beginning and end of the
list traversal.


If you think of other scenarios, please let us know, there could be
something we're missing.
Pedro

On Sat, Nov 8, 2014 at 1:48 AM, Martin Buchholz <martinrb at google.com> wrote:

> On Fri, Nov 7, 2014 at 4:32 PM, Pedro Ramalhete <pramalhe at gmail.com>
> wrote:
> > Hi Martin,
> >
> > I agree that relaxed atomics are non-trivial, but then again, lock-free
> > programming never is  :)
> > It's hard to be 100% confident of the correctness of an algorithm, but
> > that's partially the reason whay we're sharing the code on this list, so
> > that other experts can take a stab at it and find possible issues  ;)
> >
> > CASitem should not be a problem because if the item is no longer
> > "up-to-date" the CAS will fail, just like it does today for CLQ if some
> > other thread changes the item between the volatile load and the CAS.
> > Accessing the item in a relaxed way is definitely the "trickiest" part of
> > these optimizations, but I would like to point out three details:
> > 1. For both contains() and remove(), when the item is seen as null, it
> will
> > be re-read using a volatile load (as in CLQ today);
> > 2. For contains() when a matching item is seen, it will be re-read using
> a
> > volatile load (as in CLQ today);
>
> Suppose the q contains element e and some thread is waiting for e to
> be removed by another thread using
> while (q.contains(e)) {}
> it looks to me like contains() is calling only relaxed operations, so
> an adversarial VM can "optimize" that to
> if (q.contains(e)) { while (true) {} }
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141108/91746c28/attachment.html>

From martinrb at google.com  Fri Nov  7 23:13:15 2014
From: martinrb at google.com (Martin Buchholz)
Date: Fri, 7 Nov 2014 20:13:15 -0800
Subject: [concurrency-interest] Increasing the throughput of
 ConcurrentLinkedQueue on PowerPC (and ARM?)
In-Reply-To: <CAAApjO1wuEfsu2ogm9utGFXu-TTShzQ8i=ArZN3OdAniYfHdGA@mail.gmail.com>
References: <CAAApjO3j3g2fE-e2i5Yt-K8a8t2MBt3=zPrkNSCrCpGkwDEc2Q@mail.gmail.com>
	<CA+kOe0-OPZwYHNkWOz-jWhzzA0ZFDB3nyUKP1M9r+gSufa5WsA@mail.gmail.com>
	<CAAApjO1ExAfUMmz6-t2uezc=r6jibYz+cqLqPYvm+he8hiy78w@mail.gmail.com>
	<CA+kOe087q-WMSpWYwiPQ_ZRtbCLwVAEPE0oiqx_zZWm-RHTyFg@mail.gmail.com>
	<CAAApjO1wuEfsu2ogm9utGFXu-TTShzQ8i=ArZN3OdAniYfHdGA@mail.gmail.com>
Message-ID: <CA+kOe0_Nnkix3DeTaYvZO3P4ykbHNTRcvxy92+sq+me3bD23xw@mail.gmail.com>

I'm looking at:

E getRelaxedItem() {
E localitem = (E)UNSAFE.getObject(this, itemOffset);
// If it's null we need to re-read, this time as a volatile load
return localitem == null ? item : localitem;
}

it seems to me the other way around.  If you read null, it must be the
final (deleted) state of item.  If you read non-null, the value might
be old (pre-deleted).  So why do you re-read when item is null?
enqueuing via casNext ensure visibility of initial item, so
pre-constructor null is never seen.


On Fri, Nov 7, 2014 at 7:34 PM, Pedro Ramalhete <pramalhe at gmail.com> wrote:
> That's an interesting scenario, and in that case there will always be at
> least three volatile loads in contains(e), assuming the list has a node with
> an item 'e':
> - 1st volatile load occurs when reading 'head' at the beginning of the list
> traversal;
> - 2nd volatile load occurs when reading 'item' of the node matching 'e' (as
> I mentioned on point 2. above);
> - 3rd volatile load occurs when reading the 'next' of the last node on the
> list;
>
> Once the other thread removes 'e' from the list and unlinks the node (that
> thread or some other thread would have to call poll()/succ() and the node
> would have to be one of the first ones so as to be unlinked, but it can
> happen), then there will be at least 2 volatiles loads, the 1st and 3rd
> mentioned above.
> No VM will be able to optimize volatile loads away, or even to reorder
> instructions from within two successive calls to contains() because they are
> "wrapped" in the two volatile loads at the beginning and end of the list
> traversal.
>
>
> If you think of other scenarios, please let us know, there could be
> something we're missing.
> Pedro
>
> On Sat, Nov 8, 2014 at 1:48 AM, Martin Buchholz <martinrb at google.com> wrote:
>>
>> On Fri, Nov 7, 2014 at 4:32 PM, Pedro Ramalhete <pramalhe at gmail.com>
>> wrote:
>> > Hi Martin,
>> >
>> > I agree that relaxed atomics are non-trivial, but then again, lock-free
>> > programming never is  :)
>> > It's hard to be 100% confident of the correctness of an algorithm, but
>> > that's partially the reason whay we're sharing the code on this list, so
>> > that other experts can take a stab at it and find possible issues  ;)
>> >
>> > CASitem should not be a problem because if the item is no longer
>> > "up-to-date" the CAS will fail, just like it does today for CLQ if some
>> > other thread changes the item between the volatile load and the CAS.
>> > Accessing the item in a relaxed way is definitely the "trickiest" part
>> > of
>> > these optimizations, but I would like to point out three details:
>> > 1. For both contains() and remove(), when the item is seen as null, it
>> > will
>> > be re-read using a volatile load (as in CLQ today);
>> > 2. For contains() when a matching item is seen, it will be re-read using
>> > a
>> > volatile load (as in CLQ today);
>>
>> Suppose the q contains element e and some thread is waiting for e to
>> be removed by another thread using
>> while (q.contains(e)) {}
>> it looks to me like contains() is calling only relaxed operations, so
>> an adversarial VM can "optimize" that to
>> if (q.contains(e)) { while (true) {} }
>
>

From pramalhe at gmail.com  Sat Nov  8 00:39:25 2014
From: pramalhe at gmail.com (Pedro Ramalhete)
Date: Sat, 8 Nov 2014 06:39:25 +0100
Subject: [concurrency-interest] Increasing the throughput of
 ConcurrentLinkedQueue on PowerPC (and ARM?)
In-Reply-To: <CA+kOe0_Nnkix3DeTaYvZO3P4ykbHNTRcvxy92+sq+me3bD23xw@mail.gmail.com>
References: <CAAApjO3j3g2fE-e2i5Yt-K8a8t2MBt3=zPrkNSCrCpGkwDEc2Q@mail.gmail.com>
	<CA+kOe0-OPZwYHNkWOz-jWhzzA0ZFDB3nyUKP1M9r+gSufa5WsA@mail.gmail.com>
	<CAAApjO1ExAfUMmz6-t2uezc=r6jibYz+cqLqPYvm+he8hiy78w@mail.gmail.com>
	<CA+kOe087q-WMSpWYwiPQ_ZRtbCLwVAEPE0oiqx_zZWm-RHTyFg@mail.gmail.com>
	<CAAApjO1wuEfsu2ogm9utGFXu-TTShzQ8i=ArZN3OdAniYfHdGA@mail.gmail.com>
	<CA+kOe0_Nnkix3DeTaYvZO3P4ykbHNTRcvxy92+sq+me3bD23xw@mail.gmail.com>
Message-ID: <CAAApjO2UgDXh==r+wbPEZOfcMWg6itDY32BOM7Ej3mak3M7Vfw@mail.gmail.com>

Actually, the last sentence I wrote was not completely accurate. What I
meant to write was that: no volatile loads will be reordered, but the
relaxed atomics (non-volatile) code within them may traverse downwards. As
unlikely ans weird as this may seem, it will still be safe because only
code that is unrelated to the computation of the result of contains(x)
being true/false will be re-order, so it will have not impact in the return
value of contains().
And if such a thing is worrisome, we can always follow the same approach as
on StampedLock.validate() to prevent read-only code from "escaping" below,
by using an Unsafe.loadFence(), or even a fullFence() since it won't impact
performance because it happens only once.

Regarding what your comment, the volatile load mentioned on the previous
email as being "the 2nd volatile", is on line 4 of contains() when p.item
is read in the if() statement:
public boolean contains(Object o) {
        if (o == null) return false;
        for (Node<E> p = first(); p != null; p = succRelaxed(p)) {
            E item = p.getRelaxedItem();
            if (item != null && o.equals(item) && *p.item* != null)
                return true;
        }
        return false;
    }

The goal of re-reading 'item' with a volatile load when it is null, is to
prevent the issue described in the last few slides of the presentation:
https://github.com/pramalhe/ConcurrencyFreaks/raw/master/Presentations/ConcurrentLinkedQueueRelaxed.pptx


On Sat, Nov 8, 2014 at 5:13 AM, Martin Buchholz <martinrb at google.com> wrote:

> I'm looking at:
>
> E getRelaxedItem() {
> E localitem = (E)UNSAFE.getObject(this, itemOffset);
> // If it's null we need to re-read, this time as a volatile load
> return localitem == null ? item : localitem;
> }
>
> it seems to me the other way around.  If you read null, it must be the
> final (deleted) state of item.  If you read non-null, the value might
> be old (pre-deleted).  So why do you re-read when item is null?
> enqueuing via casNext ensure visibility of initial item, so
> pre-constructor null is never seen.
>
>
> On Fri, Nov 7, 2014 at 7:34 PM, Pedro Ramalhete <pramalhe at gmail.com>
> wrote:
> > That's an interesting scenario, and in that case there will always be at
> > least three volatile loads in contains(e), assuming the list has a node
> with
> > an item 'e':
> > - 1st volatile load occurs when reading 'head' at the beginning of the
> list
> > traversal;
> > - 2nd volatile load occurs when reading 'item' of the node matching 'e'
> (as
> > I mentioned on point 2. above);
> > - 3rd volatile load occurs when reading the 'next' of the last node on
> the
> > list;
> >
> > Once the other thread removes 'e' from the list and unlinks the node
> (that
> > thread or some other thread would have to call poll()/succ() and the node
> > would have to be one of the first ones so as to be unlinked, but it can
> > happen), then there will be at least 2 volatiles loads, the 1st and 3rd
> > mentioned above.
> > No VM will be able to optimize volatile loads away, or even to reorder
> > instructions from within two successive calls to contains() because they
> are
> > "wrapped" in the two volatile loads at the beginning and end of the list
> > traversal.
> >
> >
> > If you think of other scenarios, please let us know, there could be
> > something we're missing.
> > Pedro
> >
> > On Sat, Nov 8, 2014 at 1:48 AM, Martin Buchholz <martinrb at google.com>
> wrote:
> >>
> >> On Fri, Nov 7, 2014 at 4:32 PM, Pedro Ramalhete <pramalhe at gmail.com>
> >> wrote:
> >> > Hi Martin,
> >> >
> >> > I agree that relaxed atomics are non-trivial, but then again,
> lock-free
> >> > programming never is  :)
> >> > It's hard to be 100% confident of the correctness of an algorithm, but
> >> > that's partially the reason whay we're sharing the code on this list,
> so
> >> > that other experts can take a stab at it and find possible issues  ;)
> >> >
> >> > CASitem should not be a problem because if the item is no longer
> >> > "up-to-date" the CAS will fail, just like it does today for CLQ if
> some
> >> > other thread changes the item between the volatile load and the CAS.
> >> > Accessing the item in a relaxed way is definitely the "trickiest" part
> >> > of
> >> > these optimizations, but I would like to point out three details:
> >> > 1. For both contains() and remove(), when the item is seen as null, it
> >> > will
> >> > be re-read using a volatile load (as in CLQ today);
> >> > 2. For contains() when a matching item is seen, it will be re-read
> using
> >> > a
> >> > volatile load (as in CLQ today);
> >>
> >> Suppose the q contains element e and some thread is waiting for e to
> >> be removed by another thread using
> >> while (q.contains(e)) {}
> >> it looks to me like contains() is calling only relaxed operations, so
> >> an adversarial VM can "optimize" that to
> >> if (q.contains(e)) { while (true) {} }
> >
> >
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141108/0d638106/attachment.html>

From martinrb at google.com  Sat Nov  8 18:34:05 2014
From: martinrb at google.com (Martin Buchholz)
Date: Sat, 8 Nov 2014 15:34:05 -0800
Subject: [concurrency-interest] Increasing the throughput of
 ConcurrentLinkedQueue on PowerPC (and ARM?)
In-Reply-To: <CAAApjO2UgDXh==r+wbPEZOfcMWg6itDY32BOM7Ej3mak3M7Vfw@mail.gmail.com>
References: <CAAApjO3j3g2fE-e2i5Yt-K8a8t2MBt3=zPrkNSCrCpGkwDEc2Q@mail.gmail.com>
	<CA+kOe0-OPZwYHNkWOz-jWhzzA0ZFDB3nyUKP1M9r+gSufa5WsA@mail.gmail.com>
	<CAAApjO1ExAfUMmz6-t2uezc=r6jibYz+cqLqPYvm+he8hiy78w@mail.gmail.com>
	<CA+kOe087q-WMSpWYwiPQ_ZRtbCLwVAEPE0oiqx_zZWm-RHTyFg@mail.gmail.com>
	<CAAApjO1wuEfsu2ogm9utGFXu-TTShzQ8i=ArZN3OdAniYfHdGA@mail.gmail.com>
	<CA+kOe0_Nnkix3DeTaYvZO3P4ykbHNTRcvxy92+sq+me3bD23xw@mail.gmail.com>
	<CAAApjO2UgDXh==r+wbPEZOfcMWg6itDY32BOM7Ej3mak3M7Vfw@mail.gmail.com>
Message-ID: <CA+kOe09fM-0YrVJ9bLqsRRkpbhxATdHjc2V1h6uSr9pWmMesvQ@mail.gmail.com>

Wow, you have done a lot of work on these data structures!

We probably reason differently when writing lock-free code.  I rely
very much on the "happens-before" guarantees, and find it hard to
follow arguments based on reordering and "travelling".

When contains(o) calls
o.equals(item)
item must have been safely published, that is, the enqueue of item
must happen-before the call to equals.  Since there was only one
synchronization operation during enqueue (via casNext) it seems to me
that contains *must* do the corresponding volatile read of that
variable, i.e. the next pointer of the node pointing to the one
containing item, in order to establish the happens-before
relationship.  It is less clear to me whether we can elide the
volatile read of "item", possibly missing the deletion.

In practice your code may work perfectly because of dependency
ordering, except on DEC Alpha.

The attempts to do without volatile reads remind me of the attempts to
do double-checked locking without volatile reads
http://en.wikipedia.org/wiki/Double-checked_locking

On Fri, Nov 7, 2014 at 9:39 PM, Pedro Ramalhete <pramalhe at gmail.com> wrote:
> Actually, the last sentence I wrote was not completely accurate. What I
> meant to write was that: no volatile loads will be reordered, but the
> relaxed atomics (non-volatile) code within them may traverse downwards. As
> unlikely ans weird as this may seem, it will still be safe because only code
> that is unrelated to the computation of the result of contains(x) being
> true/false will be re-order, so it will have not impact in the return value
> of contains().
> And if such a thing is worrisome, we can always follow the same approach as
> on StampedLock.validate() to prevent read-only code from "escaping" below,
> by using an Unsafe.loadFence(), or even a fullFence() since it won't impact
> performance because it happens only once.
>
> Regarding what your comment, the volatile load mentioned on the previous
> email as being "the 2nd volatile", is on line 4 of contains() when p.item is
> read in the if() statement:
> public boolean contains(Object o) {
>         if (o == null) return false;
>         for (Node<E> p = first(); p != null; p = succRelaxed(p)) {
>             E item = p.getRelaxedItem();
>             if (item != null && o.equals(item) && p.item != null)
>                 return true;
>         }
>         return false;
>     }
>
> The goal of re-reading 'item' with a volatile load when it is null, is to
> prevent the issue described in the last few slides of the presentation:
> https://github.com/pramalhe/ConcurrencyFreaks/raw/master/Presentations/ConcurrentLinkedQueueRelaxed.pptx
>
>
> On Sat, Nov 8, 2014 at 5:13 AM, Martin Buchholz <martinrb at google.com> wrote:
>>
>> I'm looking at:
>>
>> E getRelaxedItem() {
>> E localitem = (E)UNSAFE.getObject(this, itemOffset);
>> // If it's null we need to re-read, this time as a volatile load
>> return localitem == null ? item : localitem;
>> }
>>
>> it seems to me the other way around.  If you read null, it must be the
>> final (deleted) state of item.  If you read non-null, the value might
>> be old (pre-deleted).  So why do you re-read when item is null?
>> enqueuing via casNext ensure visibility of initial item, so
>> pre-constructor null is never seen.
>>
>>
>> On Fri, Nov 7, 2014 at 7:34 PM, Pedro Ramalhete <pramalhe at gmail.com>
>> wrote:
>> > That's an interesting scenario, and in that case there will always be at
>> > least three volatile loads in contains(e), assuming the list has a node
>> > with
>> > an item 'e':
>> > - 1st volatile load occurs when reading 'head' at the beginning of the
>> > list
>> > traversal;
>> > - 2nd volatile load occurs when reading 'item' of the node matching 'e'
>> > (as
>> > I mentioned on point 2. above);
>> > - 3rd volatile load occurs when reading the 'next' of the last node on
>> > the
>> > list;
>> >
>> > Once the other thread removes 'e' from the list and unlinks the node
>> > (that
>> > thread or some other thread would have to call poll()/succ() and the
>> > node
>> > would have to be one of the first ones so as to be unlinked, but it can
>> > happen), then there will be at least 2 volatiles loads, the 1st and 3rd
>> > mentioned above.
>> > No VM will be able to optimize volatile loads away, or even to reorder
>> > instructions from within two successive calls to contains() because they
>> > are
>> > "wrapped" in the two volatile loads at the beginning and end of the list
>> > traversal.
>> >
>> >
>> > If you think of other scenarios, please let us know, there could be
>> > something we're missing.
>> > Pedro
>> >
>> > On Sat, Nov 8, 2014 at 1:48 AM, Martin Buchholz <martinrb at google.com>
>> > wrote:
>> >>
>> >> On Fri, Nov 7, 2014 at 4:32 PM, Pedro Ramalhete <pramalhe at gmail.com>
>> >> wrote:
>> >> > Hi Martin,
>> >> >
>> >> > I agree that relaxed atomics are non-trivial, but then again,
>> >> > lock-free
>> >> > programming never is  :)
>> >> > It's hard to be 100% confident of the correctness of an algorithm,
>> >> > but
>> >> > that's partially the reason whay we're sharing the code on this list,
>> >> > so
>> >> > that other experts can take a stab at it and find possible issues  ;)
>> >> >
>> >> > CASitem should not be a problem because if the item is no longer
>> >> > "up-to-date" the CAS will fail, just like it does today for CLQ if
>> >> > some
>> >> > other thread changes the item between the volatile load and the CAS.
>> >> > Accessing the item in a relaxed way is definitely the "trickiest"
>> >> > part
>> >> > of
>> >> > these optimizations, but I would like to point out three details:
>> >> > 1. For both contains() and remove(), when the item is seen as null,
>> >> > it
>> >> > will
>> >> > be re-read using a volatile load (as in CLQ today);
>> >> > 2. For contains() when a matching item is seen, it will be re-read
>> >> > using
>> >> > a
>> >> > volatile load (as in CLQ today);
>> >>
>> >> Suppose the q contains element e and some thread is waiting for e to
>> >> be removed by another thread using
>> >> while (q.contains(e)) {}
>> >> it looks to me like contains() is calling only relaxed operations, so
>> >> an adversarial VM can "optimize" that to
>> >> if (q.contains(e)) { while (true) {} }
>> >
>> >
>
>

From martinrb at google.com  Sun Nov  9 11:20:06 2014
From: martinrb at google.com (Martin Buchholz)
Date: Sun, 9 Nov 2014 08:20:06 -0800
Subject: [concurrency-interest] Increasing the throughput of
 ConcurrentLinkedQueue on PowerPC (and ARM?)
In-Reply-To: <CA+kOe09fM-0YrVJ9bLqsRRkpbhxATdHjc2V1h6uSr9pWmMesvQ@mail.gmail.com>
References: <CAAApjO3j3g2fE-e2i5Yt-K8a8t2MBt3=zPrkNSCrCpGkwDEc2Q@mail.gmail.com>
	<CA+kOe0-OPZwYHNkWOz-jWhzzA0ZFDB3nyUKP1M9r+gSufa5WsA@mail.gmail.com>
	<CAAApjO1ExAfUMmz6-t2uezc=r6jibYz+cqLqPYvm+he8hiy78w@mail.gmail.com>
	<CA+kOe087q-WMSpWYwiPQ_ZRtbCLwVAEPE0oiqx_zZWm-RHTyFg@mail.gmail.com>
	<CAAApjO1wuEfsu2ogm9utGFXu-TTShzQ8i=ArZN3OdAniYfHdGA@mail.gmail.com>
	<CA+kOe0_Nnkix3DeTaYvZO3P4ykbHNTRcvxy92+sq+me3bD23xw@mail.gmail.com>
	<CAAApjO2UgDXh==r+wbPEZOfcMWg6itDY32BOM7Ej3mak3M7Vfw@mail.gmail.com>
	<CA+kOe09fM-0YrVJ9bLqsRRkpbhxATdHjc2V1h6uSr9pWmMesvQ@mail.gmail.com>
Message-ID: <CA+kOe0_Cqf0GBdJ096yDsV_GUTQC7JXst9tAarJszTitpMCqjQ@mail.gmail.com>

Inspired by Pedro, below is an alternative, where we continue to do
volatile reads of next pointers, but try to optimize item reads:

Is this a case where we really want the equivalent of (problematic?)
C++ memory_order_consume?

memory_order_consume A load operation with this memory order performs
a consume operation on the affected memory location: prior writes to
data-dependent memory locations made by the thread that did a release
operation become visible to this thread's dependency chain.

Index: src/main/java/util/concurrent/ConcurrentLinkedQueue.java
===================================================================
RCS file: /export/home/jsr166/jsr166/jsr166/src/main/java/util/concurrent/ConcurrentLinkedQueue.java,v
retrieving revision 1.99
diff -u -U 5 -r1.99 ConcurrentLinkedQueue.java
--- src/main/java/util/concurrent/ConcurrentLinkedQueue.java 29 Oct
2014 20:23:14 -0000 1.99
+++ src/main/java/util/concurrent/ConcurrentLinkedQueue.java 9 Nov
2014 16:05:40 -0000
@@ -165,10 +165,15 @@

         boolean casItem(E cmp, E val) {
             return UNSAFE.compareAndSwapObject(this, itemOffset, cmp, val);
         }

+        @SuppressWarnings("unchecked")
+        E itemRelaxed() {
+            return (E) UNSAFE.getObject(this, itemOffset);
+        }
+
         void lazySetNext(Node<E> val) {
             UNSAFE.putOrderedObject(this, nextOffset, val);
         }

         boolean casNext(Node<E> cmp, Node<E> val) {
@@ -329,11 +334,11 @@

     public E poll() {
         restartFromHead:
         for (;;) {
             for (Node<E> h = head, p = h, q;;) {
-                E item = p.item;
+                E item = p.itemRelaxed();

                 if (item != null && p.casItem(item, null)) {
                     // Successful CAS is the linearization point
                     // for item to be removed from this queue.
                     if (p != h) // hop two nodes at a time
@@ -438,12 +443,13 @@
      * @return {@code true} if this queue contains the specified element
      */
     public boolean contains(Object o) {
         if (o == null) return false;
         for (Node<E> p = first(); p != null; p = succ(p)) {
-            E item = p.item;
-            if (item != null && o.equals(item))
+            E item = p.itemRelaxed();
+            if (item != null && o.equals(item)
+                && p.item == item) // recheck
                 return true;
         }
         return false;
     }

@@ -460,11 +466,11 @@
      */
     public boolean remove(Object o) {
         if (o == null) return false;
         Node<E> pred = null;
         for (Node<E> p = first(); p != null; p = succ(p)) {
-            E item = p.item;
+            E item = p.itemRelaxed();
             if (item != null &&
                 o.equals(item) &&
                 p.casItem(item, null)) {
                 Node<E> next = succ(p);
                 if (pred != null && next != null)

On Sat, Nov 8, 2014 at 3:34 PM, Martin Buchholz <martinrb at google.com> wrote:
> Wow, you have done a lot of work on these data structures!
>
> We probably reason differently when writing lock-free code.  I rely
> very much on the "happens-before" guarantees, and find it hard to
> follow arguments based on reordering and "travelling".
>
> When contains(o) calls
> o.equals(item)
> item must have been safely published, that is, the enqueue of item
> must happen-before the call to equals.  Since there was only one
> synchronization operation during enqueue (via casNext) it seems to me
> that contains *must* do the corresponding volatile read of that
> variable, i.e. the next pointer of the node pointing to the one
> containing item, in order to establish the happens-before
> relationship.  It is less clear to me whether we can elide the
> volatile read of "item", possibly missing the deletion.
>
> In practice your code may work perfectly because of dependency
> ordering, except on DEC Alpha.
>
> The attempts to do without volatile reads remind me of the attempts to
> do double-checked locking without volatile reads
> http://en.wikipedia.org/wiki/Double-checked_locking
>
> On Fri, Nov 7, 2014 at 9:39 PM, Pedro Ramalhete <pramalhe at gmail.com> wrote:
>> Actually, the last sentence I wrote was not completely accurate. What I
>> meant to write was that: no volatile loads will be reordered, but the
>> relaxed atomics (non-volatile) code within them may traverse downwards. As
>> unlikely ans weird as this may seem, it will still be safe because only code
>> that is unrelated to the computation of the result of contains(x) being
>> true/false will be re-order, so it will have not impact in the return value
>> of contains().
>> And if such a thing is worrisome, we can always follow the same approach as
>> on StampedLock.validate() to prevent read-only code from "escaping" below,
>> by using an Unsafe.loadFence(), or even a fullFence() since it won't impact
>> performance because it happens only once.
>>
>> Regarding what your comment, the volatile load mentioned on the previous
>> email as being "the 2nd volatile", is on line 4 of contains() when p.item is
>> read in the if() statement:
>> public boolean contains(Object o) {
>>         if (o == null) return false;
>>         for (Node<E> p = first(); p != null; p = succRelaxed(p)) {
>>             E item = p.getRelaxedItem();
>>             if (item != null && o.equals(item) && p.item != null)
>>                 return true;
>>         }
>>         return false;
>>     }
>>
>> The goal of re-reading 'item' with a volatile load when it is null, is to
>> prevent the issue described in the last few slides of the presentation:
>> https://github.com/pramalhe/ConcurrencyFreaks/raw/master/Presentations/ConcurrentLinkedQueueRelaxed.pptx
>>
>>
>> On Sat, Nov 8, 2014 at 5:13 AM, Martin Buchholz <martinrb at google.com> wrote:
>>>
>>> I'm looking at:
>>>
>>> E getRelaxedItem() {
>>> E localitem = (E)UNSAFE.getObject(this, itemOffset);
>>> // If it's null we need to re-read, this time as a volatile load
>>> return localitem == null ? item : localitem;
>>> }
>>>
>>> it seems to me the other way around.  If you read null, it must be the
>>> final (deleted) state of item.  If you read non-null, the value might
>>> be old (pre-deleted).  So why do you re-read when item is null?
>>> enqueuing via casNext ensure visibility of initial item, so
>>> pre-constructor null is never seen.
>>>
>>>
>>> On Fri, Nov 7, 2014 at 7:34 PM, Pedro Ramalhete <pramalhe at gmail.com>
>>> wrote:
>>> > That's an interesting scenario, and in that case there will always be at
>>> > least three volatile loads in contains(e), assuming the list has a node
>>> > with
>>> > an item 'e':
>>> > - 1st volatile load occurs when reading 'head' at the beginning of the
>>> > list
>>> > traversal;
>>> > - 2nd volatile load occurs when reading 'item' of the node matching 'e'
>>> > (as
>>> > I mentioned on point 2. above);
>>> > - 3rd volatile load occurs when reading the 'next' of the last node on
>>> > the
>>> > list;
>>> >
>>> > Once the other thread removes 'e' from the list and unlinks the node
>>> > (that
>>> > thread or some other thread would have to call poll()/succ() and the
>>> > node
>>> > would have to be one of the first ones so as to be unlinked, but it can
>>> > happen), then there will be at least 2 volatiles loads, the 1st and 3rd
>>> > mentioned above.
>>> > No VM will be able to optimize volatile loads away, or even to reorder
>>> > instructions from within two successive calls to contains() because they
>>> > are
>>> > "wrapped" in the two volatile loads at the beginning and end of the list
>>> > traversal.
>>> >
>>> >
>>> > If you think of other scenarios, please let us know, there could be
>>> > something we're missing.
>>> > Pedro
>>> >
>>> > On Sat, Nov 8, 2014 at 1:48 AM, Martin Buchholz <martinrb at google.com>
>>> > wrote:
>>> >>
>>> >> On Fri, Nov 7, 2014 at 4:32 PM, Pedro Ramalhete <pramalhe at gmail.com>
>>> >> wrote:
>>> >> > Hi Martin,
>>> >> >
>>> >> > I agree that relaxed atomics are non-trivial, but then again,
>>> >> > lock-free
>>> >> > programming never is  :)
>>> >> > It's hard to be 100% confident of the correctness of an algorithm,
>>> >> > but
>>> >> > that's partially the reason whay we're sharing the code on this list,
>>> >> > so
>>> >> > that other experts can take a stab at it and find possible issues  ;)
>>> >> >
>>> >> > CASitem should not be a problem because if the item is no longer
>>> >> > "up-to-date" the CAS will fail, just like it does today for CLQ if
>>> >> > some
>>> >> > other thread changes the item between the volatile load and the CAS.
>>> >> > Accessing the item in a relaxed way is definitely the "trickiest"
>>> >> > part
>>> >> > of
>>> >> > these optimizations, but I would like to point out three details:
>>> >> > 1. For both contains() and remove(), when the item is seen as null,
>>> >> > it
>>> >> > will
>>> >> > be re-read using a volatile load (as in CLQ today);
>>> >> > 2. For contains() when a matching item is seen, it will be re-read
>>> >> > using
>>> >> > a
>>> >> > volatile load (as in CLQ today);
>>> >>
>>> >> Suppose the q contains element e and some thread is waiting for e to
>>> >> be removed by another thread using
>>> >> while (q.contains(e)) {}
>>> >> it looks to me like contains() is calling only relaxed operations, so
>>> >> an adversarial VM can "optimize" that to
>>> >> if (q.contains(e)) { while (true) {} }
>>> >
>>> >
>>
>>

From pramalhe at gmail.com  Sun Nov  9 17:44:37 2014
From: pramalhe at gmail.com (Pedro Ramalhete)
Date: Sun, 9 Nov 2014 23:44:37 +0100
Subject: [concurrency-interest] Increasing the throughput of
 ConcurrentLinkedQueue on PowerPC (and ARM?)
In-Reply-To: <CA+kOe0_Cqf0GBdJ096yDsV_GUTQC7JXst9tAarJszTitpMCqjQ@mail.gmail.com>
References: <CAAApjO3j3g2fE-e2i5Yt-K8a8t2MBt3=zPrkNSCrCpGkwDEc2Q@mail.gmail.com>
	<CA+kOe0-OPZwYHNkWOz-jWhzzA0ZFDB3nyUKP1M9r+gSufa5WsA@mail.gmail.com>
	<CAAApjO1ExAfUMmz6-t2uezc=r6jibYz+cqLqPYvm+he8hiy78w@mail.gmail.com>
	<CA+kOe087q-WMSpWYwiPQ_ZRtbCLwVAEPE0oiqx_zZWm-RHTyFg@mail.gmail.com>
	<CAAApjO1wuEfsu2ogm9utGFXu-TTShzQ8i=ArZN3OdAniYfHdGA@mail.gmail.com>
	<CA+kOe0_Nnkix3DeTaYvZO3P4ykbHNTRcvxy92+sq+me3bD23xw@mail.gmail.com>
	<CAAApjO2UgDXh==r+wbPEZOfcMWg6itDY32BOM7Ej3mak3M7Vfw@mail.gmail.com>
	<CA+kOe09fM-0YrVJ9bLqsRRkpbhxATdHjc2V1h6uSr9pWmMesvQ@mail.gmail.com>
	<CA+kOe0_Cqf0GBdJ096yDsV_GUTQC7JXst9tAarJszTitpMCqjQ@mail.gmail.com>
Message-ID: <CAAApjO3HmfKqqmaNWqaGTk-d2wHA1+owqDf5uqE+HqJ=ZHoDpg@mail.gmail.com>

Thanks for the encouragement.
Yes, my choice of terminology to describe the memory model is not always
the best, I hope it doesn't cause misunderstanding  :)

The scenario you describe is very interesting. Let me provide two possible
answers and maybe one of them will be satisfactory.


*1st try*
Let us simplify the problem to the bare essentials:

Thread 1 is calling add(o):
    Node newNode = new Node(o);  // A new node is created and newNode.item
is set to 'o' using a volatile store
    prevNode.casNext(newNode);   //    The new node is inserted into the
list, after the last node, making it visible to other threads (has a
release barrier)

Notice that in this code, the store of newNode.item can _never_ occur after
the casNext(), i.e. no
re-ordering of these two stores is possible by the
compiler/hotspot/cpu/cache-coherence.

Thread 2 is calling contains(o):
    nextNode = node.getRelaxedNext();   // This is a relaxed load and can
be re-ordered
    item = nextNode.getRelaxedItem();   // This is a relaxed load and can
be re-ordered

If I understood correctly, what you say is that the JVM memory model does
_not_ give the
guarantee that the item 'o' will be visible even if the nextNode is already
visible to Thread 2, is that so?
This would imply that the second line in contains(o) can occur before the
newNode's constructor has
completed, and even worse, the volatile store to item in add(o) becomes
visible after the casNext(),
even though there is an happens-before (at least) between the store of
newNode.item and the store of casNext()...
am I understanding it correctly?

I understand that Alphas can re-order even dependent loads, but I was
hoping that the memory model
itself would give the guarantee we need just because of the two volatiles
stores in add(o),
namely the store of the newNode.item must be visible before the
casNext(newNode).



*2nd try*
Even if the above is true, then consider the following: If newNode becomes
visible to thread 2 even when
newNode.item has not yet been set to 'o', then it should be safe to assume
that the value of newNode.item
is null.
It's a bit confusing, but the state machine of the 'item' variable then
becomes:
          null                 ->            'o'               ->       null
before constructor finishes        constructor has completed        node
has been logically removed
                                   and node may be visible

When a node is seen to have an item at null, the load is re-done as a
volatile load to check if it
is really null, and if it is null then it means that it has been logically
removed (last state in the diagram above),
or the constructor has not yet completed (first state of the diagram above).
If it is the case of the constructor not yet completing, then there is an
happens-before relationship that
guarantees that casNext has _not_ yet completed, and so the newNode can not
yet be visible to contains(o),
which means that contains(o) will never be able to see the first state.

Notice that in this reasoning there is the assumption that before the
constructor of Node completes, the
initial value of the node's item is null... does the JVM initialize all
references to null by default?
If it does, then the two stores described in add(o) plus the volatile load
of the item in contains(o) create
an happens-before guarantee which provides correctness for the case you
describe.



Regarding the diff you propose, using only the relaxed item and always
doing a volatile load for the Node.next,
it is much easier to reason about and certainly does not suffer from the
issue you described with add(o)/contains(o).
In terms of performance, it gives an improvement over the current
implementation of CLQ because
it does on average one volatile load (acquire fence) per node traversed,
while the current CLQ does two volatile loads.
CLQRelaxed has a higher performance gain because it does on average close
to zero volatile loads.

I ran your changes (I named it ConcurrentLinkedqueueItem) on the RunAbove
instance Power8 S using
our microbenchmark to compare against CLQ and CLQRelaxed, and the results
are show below:

numElements=1000,  numCores=8
0.0% Writes, ConcurrentLinkedQueue, ConcurrentLinkedQueueItem,
ConcurrentLinkedQueueRelaxed
1, 57683, 111233, 387917
2, 112577, 198374, 774528
4, 188658, 348552, 1179456
8, 208068, 393500, 1131104
0.1% Writes, ConcurrentLinkedQueue, ConcurrentLinkedQueueItem,
ConcurrentLinkedQueueRelaxed
1, 57699, 111109, 397134
2, 112477, 198419, 793971
4, 188241, 350068, 1168872
8, 207919, 399379, 1126918
1.0% Writes, ConcurrentLinkedQueue, ConcurrentLinkedQueueItem,
ConcurrentLinkedQueueRelaxed
1, 58043, 111959, 418096
2, 113036, 199982, 818435
4, 190894, 354092, 1188407
8, 215828, 405752, 1162508
10.0% Writes, ConcurrentLinkedQueue, ConcurrentLinkedQueueItem,
ConcurrentLinkedQueueRelaxed
1, 65238, 127538, 361053
2, 118835, 214567, 1194862
4, 207810, 375174, 1550311
8, 223638, 431316, 1290808
100.0% Writes, ConcurrentLinkedQueue, ConcurrentLinkedQueueItem,
ConcurrentLinkedQueueRelaxed
1, 124199, 236175, 1411299
2, 239134, 450326, 2293381
4, 456139, 861773, 2220363
8, 464168, 900075, 2368460

As you can see, its performance is nearly twice as much as CLQ (from 1.75x
to 1.95x), which is pretty good!
Having into consideration the almost 2x increase in performance and that it
is a much "safer change"
than CLQRelaxed, I believe the change you propose should be taken seriously
to be added to CLQ in the newest JDK.

Thanks,
Pedro


On Sun, Nov 9, 2014 at 5:20 PM, Martin Buchholz <martinrb at google.com> wrote:

> Inspired by Pedro, below is an alternative, where we continue to do
> volatile reads of next pointers, but try to optimize item reads:
>
> Is this a case where we really want the equivalent of (problematic?)
> C++ memory_order_consume?
>
> memory_order_consume A load operation with this memory order performs
> a consume operation on the affected memory location: prior writes to
> data-dependent memory locations made by the thread that did a release
> operation become visible to this thread's dependency chain.
>
> Index: src/main/java/util/concurrent/ConcurrentLinkedQueue.java
> ===================================================================
> RCS file:
> /export/home/jsr166/jsr166/jsr166/src/main/java/util/concurrent/ConcurrentLinkedQueue.java,v
> retrieving revision 1.99
> diff -u -U 5 -r1.99 ConcurrentLinkedQueue.java
> --- src/main/java/util/concurrent/ConcurrentLinkedQueue.java 29 Oct
> 2014 20:23:14 -0000 1.99
> +++ src/main/java/util/concurrent/ConcurrentLinkedQueue.java 9 Nov
> 2014 16:05:40 -0000
> @@ -165,10 +165,15 @@
>
>          boolean casItem(E cmp, E val) {
>              return UNSAFE.compareAndSwapObject(this, itemOffset, cmp,
> val);
>          }
>
> +        @SuppressWarnings("unchecked")
> +        E itemRelaxed() {
> +            return (E) UNSAFE.getObject(this, itemOffset);
> +        }
> +
>          void lazySetNext(Node<E> val) {
>              UNSAFE.putOrderedObject(this, nextOffset, val);
>          }
>
>          boolean casNext(Node<E> cmp, Node<E> val) {
> @@ -329,11 +334,11 @@
>
>      public E poll() {
>          restartFromHead:
>          for (;;) {
>              for (Node<E> h = head, p = h, q;;) {
> -                E item = p.item;
> +                E item = p.itemRelaxed();
>
>                  if (item != null && p.casItem(item, null)) {
>                      // Successful CAS is the linearization point
>                      // for item to be removed from this queue.
>                      if (p != h) // hop two nodes at a time
> @@ -438,12 +443,13 @@
>       * @return {@code true} if this queue contains the specified element
>       */
>      public boolean contains(Object o) {
>          if (o == null) return false;
>          for (Node<E> p = first(); p != null; p = succ(p)) {
> -            E item = p.item;
> -            if (item != null && o.equals(item))
> +            E item = p.itemRelaxed();
> +            if (item != null && o.equals(item)
> +                && p.item == item) // recheck
>                  return true;
>          }
>          return false;
>      }
>
> @@ -460,11 +466,11 @@
>       */
>      public boolean remove(Object o) {
>          if (o == null) return false;
>          Node<E> pred = null;
>          for (Node<E> p = first(); p != null; p = succ(p)) {
> -            E item = p.item;
> +            E item = p.itemRelaxed();
>              if (item != null &&
>                  o.equals(item) &&
>                  p.casItem(item, null)) {
>                  Node<E> next = succ(p);
>                  if (pred != null && next != null)
>
> On Sat, Nov 8, 2014 at 3:34 PM, Martin Buchholz <martinrb at google.com>
> wrote:
> > Wow, you have done a lot of work on these data structures!
> >
> > We probably reason differently when writing lock-free code.  I rely
> > very much on the "happens-before" guarantees, and find it hard to
> > follow arguments based on reordering and "travelling".
> >
> > When contains(o) calls
> > o.equals(item)
> > item must have been safely published, that is, the enqueue of item
> > must happen-before the call to equals.  Since there was only one
> > synchronization operation during enqueue (via casNext) it seems to me
> > that contains *must* do the corresponding volatile read of that
> > variable, i.e. the next pointer of the node pointing to the one
> > containing item, in order to establish the happens-before
> > relationship.  It is less clear to me whether we can elide the
> > volatile read of "item", possibly missing the deletion.
> >
> > In practice your code may work perfectly because of dependency
> > ordering, except on DEC Alpha.
> >
> > The attempts to do without volatile reads remind me of the attempts to
> > do double-checked locking without volatile reads
> > http://en.wikipedia.org/wiki/Double-checked_locking
> >
> > On Fri, Nov 7, 2014 at 9:39 PM, Pedro Ramalhete <pramalhe at gmail.com>
> wrote:
> >> Actually, the last sentence I wrote was not completely accurate. What I
> >> meant to write was that: no volatile loads will be reordered, but the
> >> relaxed atomics (non-volatile) code within them may traverse downwards.
> As
> >> unlikely ans weird as this may seem, it will still be safe because only
> code
> >> that is unrelated to the computation of the result of contains(x) being
> >> true/false will be re-order, so it will have not impact in the return
> value
> >> of contains().
> >> And if such a thing is worrisome, we can always follow the same
> approach as
> >> on StampedLock.validate() to prevent read-only code from "escaping"
> below,
> >> by using an Unsafe.loadFence(), or even a fullFence() since it won't
> impact
> >> performance because it happens only once.
> >>
> >> Regarding what your comment, the volatile load mentioned on the previous
> >> email as being "the 2nd volatile", is on line 4 of contains() when
> p.item is
> >> read in the if() statement:
> >> public boolean contains(Object o) {
> >>         if (o == null) return false;
> >>         for (Node<E> p = first(); p != null; p = succRelaxed(p)) {
> >>             E item = p.getRelaxedItem();
> >>             if (item != null && o.equals(item) && p.item != null)
> >>                 return true;
> >>         }
> >>         return false;
> >>     }
> >>
> >> The goal of re-reading 'item' with a volatile load when it is null, is
> to
> >> prevent the issue described in the last few slides of the presentation:
> >>
> https://github.com/pramalhe/ConcurrencyFreaks/raw/master/Presentations/ConcurrentLinkedQueueRelaxed.pptx
> >>
> >>
> >> On Sat, Nov 8, 2014 at 5:13 AM, Martin Buchholz <martinrb at google.com>
> wrote:
> >>>
> >>> I'm looking at:
> >>>
> >>> E getRelaxedItem() {
> >>> E localitem = (E)UNSAFE.getObject(this, itemOffset);
> >>> // If it's null we need to re-read, this time as a volatile load
> >>> return localitem == null ? item : localitem;
> >>> }
> >>>
> >>> it seems to me the other way around.  If you read null, it must be the
> >>> final (deleted) state of item.  If you read non-null, the value might
> >>> be old (pre-deleted).  So why do you re-read when item is null?
> >>> enqueuing via casNext ensure visibility of initial item, so
> >>> pre-constructor null is never seen.
> >>>
> >>>
> >>> On Fri, Nov 7, 2014 at 7:34 PM, Pedro Ramalhete <pramalhe at gmail.com>
> >>> wrote:
> >>> > That's an interesting scenario, and in that case there will always
> be at
> >>> > least three volatile loads in contains(e), assuming the list has a
> node
> >>> > with
> >>> > an item 'e':
> >>> > - 1st volatile load occurs when reading 'head' at the beginning of
> the
> >>> > list
> >>> > traversal;
> >>> > - 2nd volatile load occurs when reading 'item' of the node matching
> 'e'
> >>> > (as
> >>> > I mentioned on point 2. above);
> >>> > - 3rd volatile load occurs when reading the 'next' of the last node
> on
> >>> > the
> >>> > list;
> >>> >
> >>> > Once the other thread removes 'e' from the list and unlinks the node
> >>> > (that
> >>> > thread or some other thread would have to call poll()/succ() and the
> >>> > node
> >>> > would have to be one of the first ones so as to be unlinked, but it
> can
> >>> > happen), then there will be at least 2 volatiles loads, the 1st and
> 3rd
> >>> > mentioned above.
> >>> > No VM will be able to optimize volatile loads away, or even to
> reorder
> >>> > instructions from within two successive calls to contains() because
> they
> >>> > are
> >>> > "wrapped" in the two volatile loads at the beginning and end of the
> list
> >>> > traversal.
> >>> >
> >>> >
> >>> > If you think of other scenarios, please let us know, there could be
> >>> > something we're missing.
> >>> > Pedro
> >>> >
> >>> > On Sat, Nov 8, 2014 at 1:48 AM, Martin Buchholz <martinrb at google.com
> >
> >>> > wrote:
> >>> >>
> >>> >> On Fri, Nov 7, 2014 at 4:32 PM, Pedro Ramalhete <pramalhe at gmail.com
> >
> >>> >> wrote:
> >>> >> > Hi Martin,
> >>> >> >
> >>> >> > I agree that relaxed atomics are non-trivial, but then again,
> >>> >> > lock-free
> >>> >> > programming never is  :)
> >>> >> > It's hard to be 100% confident of the correctness of an algorithm,
> >>> >> > but
> >>> >> > that's partially the reason whay we're sharing the code on this
> list,
> >>> >> > so
> >>> >> > that other experts can take a stab at it and find possible
> issues  ;)
> >>> >> >
> >>> >> > CASitem should not be a problem because if the item is no longer
> >>> >> > "up-to-date" the CAS will fail, just like it does today for CLQ if
> >>> >> > some
> >>> >> > other thread changes the item between the volatile load and the
> CAS.
> >>> >> > Accessing the item in a relaxed way is definitely the "trickiest"
> >>> >> > part
> >>> >> > of
> >>> >> > these optimizations, but I would like to point out three details:
> >>> >> > 1. For both contains() and remove(), when the item is seen as
> null,
> >>> >> > it
> >>> >> > will
> >>> >> > be re-read using a volatile load (as in CLQ today);
> >>> >> > 2. For contains() when a matching item is seen, it will be re-read
> >>> >> > using
> >>> >> > a
> >>> >> > volatile load (as in CLQ today);
> >>> >>
> >>> >> Suppose the q contains element e and some thread is waiting for e to
> >>> >> be removed by another thread using
> >>> >> while (q.contains(e)) {}
> >>> >> it looks to me like contains() is calling only relaxed operations,
> so
> >>> >> an adversarial VM can "optimize" that to
> >>> >> if (q.contains(e)) { while (true) {} }
> >>> >
> >>> >
> >>
> >>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141109/9915e95b/attachment-0001.html>

From martinrb at google.com  Sun Nov  9 21:19:00 2014
From: martinrb at google.com (Martin Buchholz)
Date: Sun, 9 Nov 2014 18:19:00 -0800
Subject: [concurrency-interest] Increasing the throughput of
 ConcurrentLinkedQueue on PowerPC (and ARM?)
In-Reply-To: <CAAApjO3HmfKqqmaNWqaGTk-d2wHA1+owqDf5uqE+HqJ=ZHoDpg@mail.gmail.com>
References: <CAAApjO3j3g2fE-e2i5Yt-K8a8t2MBt3=zPrkNSCrCpGkwDEc2Q@mail.gmail.com>
	<CA+kOe0-OPZwYHNkWOz-jWhzzA0ZFDB3nyUKP1M9r+gSufa5WsA@mail.gmail.com>
	<CAAApjO1ExAfUMmz6-t2uezc=r6jibYz+cqLqPYvm+he8hiy78w@mail.gmail.com>
	<CA+kOe087q-WMSpWYwiPQ_ZRtbCLwVAEPE0oiqx_zZWm-RHTyFg@mail.gmail.com>
	<CAAApjO1wuEfsu2ogm9utGFXu-TTShzQ8i=ArZN3OdAniYfHdGA@mail.gmail.com>
	<CA+kOe0_Nnkix3DeTaYvZO3P4ykbHNTRcvxy92+sq+me3bD23xw@mail.gmail.com>
	<CAAApjO2UgDXh==r+wbPEZOfcMWg6itDY32BOM7Ej3mak3M7Vfw@mail.gmail.com>
	<CA+kOe09fM-0YrVJ9bLqsRRkpbhxATdHjc2V1h6uSr9pWmMesvQ@mail.gmail.com>
	<CA+kOe0_Cqf0GBdJ096yDsV_GUTQC7JXst9tAarJszTitpMCqjQ@mail.gmail.com>
	<CAAApjO3HmfKqqmaNWqaGTk-d2wHA1+owqDf5uqE+HqJ=ZHoDpg@mail.gmail.com>
Message-ID: <CA+kOe0821iJaaAL1z4PiobJKb6vzU6GDTgBb-=Fp96uGW0OxGw@mail.gmail.com>

On Sun, Nov 9, 2014 at 2:44 PM, Pedro Ramalhete <pramalhe at gmail.com> wrote:
> 1st try
> Let us simplify the problem to the bare essentials:
>
> Thread 1 is calling add(o):
>     Node newNode = new Node(o);  // A new node is created and newNode.item
> is set to 'o' using a volatile store

Note relevant here, but it uses a relaxed store

        /**
         * Constructs a new node.  Uses relaxed write because item can
         * only be seen after publication via casNext.
         */
        Node(E item) {
            UNSAFE.putObject(this, itemOffset, item);
        }

>     prevNode.casNext(newNode);   //    The new node is inserted into the
> list, after the last node, making it visible to other threads (has a release
> barrier)
>
> Notice that in this code, the store of newNode.item can _never_ occur after
> the casNext(), i.e. no
> re-ordering of these two stores is possible by the
> compiler/hotspot/cpu/cache-coherence.

There is no global order.

> Thread 2 is calling contains(o):
>     nextNode = node.getRelaxedNext();   // This is a relaxed load and can be
> re-ordered
>     item = nextNode.getRelaxedItem();   // This is a relaxed load and can be
> re-ordered
>
> If I understood correctly, what you say is that the JVM memory model does
> _not_ give the
> guarantee that the item 'o' will be visible even if the nextNode is already
> visible to Thread 2, is that so?

The risk is that ordinary writes to e.g. the fields of item, are not
yet visible to the other thread although the reference to that object
(item) is visible.

> This would imply that the second line in contains(o) can occur before the
> newNode's constructor has
> completed, and even worse, the volatile store to item in add(o) becomes
> visible after the casNext(),
> even though there is an happens-before (at least) between the store of
> newNode.item and the store of casNext()...
> am I understanding it correctly?

You need an INTER-thread happens-before, and you have none.

"""A write to a volatile field (?8.3.1.4) happens-before every
subsequent read of that field."""
(of course, only if it's a volatile read!)

> I understand that Alphas can re-order even dependent loads, but I was hoping
> that the memory model
> itself would give the guarantee we need just because of the two volatiles
> stores in add(o),
> namely the store of the newNode.item must be visible before the
> casNext(newNode).

I don't think the memory model gives that guarantee.  But I am more
worried about the fields of item than item itself, which kills
attempts to recheck if item null as in your 2nd try.  Again, compare
with double-checked locking

http://www.cs.umd.edu/~pugh/java/memoryModel/DoubleCheckedLocking.html

> 2nd try
> Even if the above is true, then consider the following: If newNode becomes
> visible to thread 2 even when
> newNode.item has not yet been set to 'o', then it should be safe to assume
> that the value of newNode.item
> is null.
> It's a bit confusing, but the state machine of the 'item' variable then
> becomes:
>           null                 ->            'o'               ->       null
> before constructor finishes        constructor has completed        node has
> been logically removed
>                                    and node may be visible
>
> When a node is seen to have an item at null, the load is re-done as a
> volatile load to check if it
> is really null, and if it is null then it means that it has been logically
> removed (last state in the diagram above),
> or the constructor has not yet completed (first state of the diagram above).
> If it is the case of the constructor not yet completing, then there is an
> happens-before relationship that
> guarantees that casNext has _not_ yet completed, and so the newNode can not
> yet be visible to contains(o),
> which means that contains(o) will never be able to see the first state.
>
> Notice that in this reasoning there is the assumption that before the
> constructor of Node completes, the
> initial value of the node's item is null... does the JVM initialize all
> references to null by default?
> If it does, then the two stores described in add(o) plus the volatile load
> of the item in contains(o) create
> an happens-before guarantee which provides correctness for the case you
> describe.
>
>
>
> Regarding the diff you propose, using only the relaxed item and always doing
> a volatile load for the Node.next,
> it is much easier to reason about and certainly does not suffer from the
> issue you described with add(o)/contains(o).
> In terms of performance, it gives an improvement over the current
> implementation of CLQ because
> it does on average one volatile load (acquire fence) per node traversed,
> while the current CLQ does two volatile loads.
> CLQRelaxed has a higher performance gain because it does on average close to
> zero volatile loads.
>
> I ran your changes (I named it ConcurrentLinkedqueueItem) on the RunAbove
> instance Power8 S using
> our microbenchmark to compare against CLQ and CLQRelaxed, and the results
> are show below:
>
> numElements=1000,  numCores=8
> 0.0% Writes, ConcurrentLinkedQueue, ConcurrentLinkedQueueItem,
> ConcurrentLinkedQueueRelaxed
> 1, 57683, 111233, 387917
> 2, 112577, 198374, 774528
> 4, 188658, 348552, 1179456
> 8, 208068, 393500, 1131104
> 0.1% Writes, ConcurrentLinkedQueue, ConcurrentLinkedQueueItem,
> ConcurrentLinkedQueueRelaxed
> 1, 57699, 111109, 397134
> 2, 112477, 198419, 793971
> 4, 188241, 350068, 1168872
> 8, 207919, 399379, 1126918
> 1.0% Writes, ConcurrentLinkedQueue, ConcurrentLinkedQueueItem,
> ConcurrentLinkedQueueRelaxed
> 1, 58043, 111959, 418096
> 2, 113036, 199982, 818435
> 4, 190894, 354092, 1188407
> 8, 215828, 405752, 1162508
> 10.0% Writes, ConcurrentLinkedQueue, ConcurrentLinkedQueueItem,
> ConcurrentLinkedQueueRelaxed
> 1, 65238, 127538, 361053
> 2, 118835, 214567, 1194862
> 4, 207810, 375174, 1550311
> 8, 223638, 431316, 1290808
> 100.0% Writes, ConcurrentLinkedQueue, ConcurrentLinkedQueueItem,
> ConcurrentLinkedQueueRelaxed
> 1, 124199, 236175, 1411299
> 2, 239134, 450326, 2293381
> 4, 456139, 861773, 2220363
> 8, 464168, 900075, 2368460
>
> As you can see, its performance is nearly twice as much as CLQ (from 1.75x
> to 1.95x), which is pretty good!
> Having into consideration the almost 2x increase in performance and that it
> is a much "safer change"
> than CLQRelaxed, I believe the change you propose should be taken seriously
> to be added to CLQ in the newest JDK.
>
> Thanks,
> Pedro
>
>
> On Sun, Nov 9, 2014 at 5:20 PM, Martin Buchholz <martinrb at google.com> wrote:
>>
>> Inspired by Pedro, below is an alternative, where we continue to do
>> volatile reads of next pointers, but try to optimize item reads:
>>
>> Is this a case where we really want the equivalent of (problematic?)
>> C++ memory_order_consume?
>>
>> memory_order_consume A load operation with this memory order performs
>> a consume operation on the affected memory location: prior writes to
>> data-dependent memory locations made by the thread that did a release
>> operation become visible to this thread's dependency chain.
>>
>> Index: src/main/java/util/concurrent/ConcurrentLinkedQueue.java
>> ===================================================================
>> RCS file:
>> /export/home/jsr166/jsr166/jsr166/src/main/java/util/concurrent/ConcurrentLinkedQueue.java,v
>> retrieving revision 1.99
>> diff -u -U 5 -r1.99 ConcurrentLinkedQueue.java
>> --- src/main/java/util/concurrent/ConcurrentLinkedQueue.java 29 Oct
>> 2014 20:23:14 -0000 1.99
>> +++ src/main/java/util/concurrent/ConcurrentLinkedQueue.java 9 Nov
>> 2014 16:05:40 -0000
>> @@ -165,10 +165,15 @@
>>
>>          boolean casItem(E cmp, E val) {
>>              return UNSAFE.compareAndSwapObject(this, itemOffset, cmp,
>> val);
>>          }
>>
>> +        @SuppressWarnings("unchecked")
>> +        E itemRelaxed() {
>> +            return (E) UNSAFE.getObject(this, itemOffset);
>> +        }
>> +
>>          void lazySetNext(Node<E> val) {
>>              UNSAFE.putOrderedObject(this, nextOffset, val);
>>          }
>>
>>          boolean casNext(Node<E> cmp, Node<E> val) {
>> @@ -329,11 +334,11 @@
>>
>>      public E poll() {
>>          restartFromHead:
>>          for (;;) {
>>              for (Node<E> h = head, p = h, q;;) {
>> -                E item = p.item;
>> +                E item = p.itemRelaxed();
>>
>>                  if (item != null && p.casItem(item, null)) {
>>                      // Successful CAS is the linearization point
>>                      // for item to be removed from this queue.
>>                      if (p != h) // hop two nodes at a time
>> @@ -438,12 +443,13 @@
>>       * @return {@code true} if this queue contains the specified element
>>       */
>>      public boolean contains(Object o) {
>>          if (o == null) return false;
>>          for (Node<E> p = first(); p != null; p = succ(p)) {
>> -            E item = p.item;
>> -            if (item != null && o.equals(item))
>> +            E item = p.itemRelaxed();
>> +            if (item != null && o.equals(item)
>> +                && p.item == item) // recheck
>>                  return true;
>>          }
>>          return false;
>>      }
>>
>> @@ -460,11 +466,11 @@
>>       */
>>      public boolean remove(Object o) {
>>          if (o == null) return false;
>>          Node<E> pred = null;
>>          for (Node<E> p = first(); p != null; p = succ(p)) {
>> -            E item = p.item;
>> +            E item = p.itemRelaxed();
>>              if (item != null &&
>>                  o.equals(item) &&
>>                  p.casItem(item, null)) {
>>                  Node<E> next = succ(p);
>>                  if (pred != null && next != null)
>>
>> On Sat, Nov 8, 2014 at 3:34 PM, Martin Buchholz <martinrb at google.com>
>> wrote:
>> > Wow, you have done a lot of work on these data structures!
>> >
>> > We probably reason differently when writing lock-free code.  I rely
>> > very much on the "happens-before" guarantees, and find it hard to
>> > follow arguments based on reordering and "travelling".
>> >
>> > When contains(o) calls
>> > o.equals(item)
>> > item must have been safely published, that is, the enqueue of item
>> > must happen-before the call to equals.  Since there was only one
>> > synchronization operation during enqueue (via casNext) it seems to me
>> > that contains *must* do the corresponding volatile read of that
>> > variable, i.e. the next pointer of the node pointing to the one
>> > containing item, in order to establish the happens-before
>> > relationship.  It is less clear to me whether we can elide the
>> > volatile read of "item", possibly missing the deletion.
>> >
>> > In practice your code may work perfectly because of dependency
>> > ordering, except on DEC Alpha.
>> >
>> > The attempts to do without volatile reads remind me of the attempts to
>> > do double-checked locking without volatile reads
>> > http://en.wikipedia.org/wiki/Double-checked_locking
>> >
>> > On Fri, Nov 7, 2014 at 9:39 PM, Pedro Ramalhete <pramalhe at gmail.com>
>> > wrote:
>> >> Actually, the last sentence I wrote was not completely accurate. What I
>> >> meant to write was that: no volatile loads will be reordered, but the
>> >> relaxed atomics (non-volatile) code within them may traverse downwards.
>> >> As
>> >> unlikely ans weird as this may seem, it will still be safe because only
>> >> code
>> >> that is unrelated to the computation of the result of contains(x) being
>> >> true/false will be re-order, so it will have not impact in the return
>> >> value
>> >> of contains().
>> >> And if such a thing is worrisome, we can always follow the same
>> >> approach as
>> >> on StampedLock.validate() to prevent read-only code from "escaping"
>> >> below,
>> >> by using an Unsafe.loadFence(), or even a fullFence() since it won't
>> >> impact
>> >> performance because it happens only once.
>> >>
>> >> Regarding what your comment, the volatile load mentioned on the
>> >> previous
>> >> email as being "the 2nd volatile", is on line 4 of contains() when
>> >> p.item is
>> >> read in the if() statement:
>> >> public boolean contains(Object o) {
>> >>         if (o == null) return false;
>> >>         for (Node<E> p = first(); p != null; p = succRelaxed(p)) {
>> >>             E item = p.getRelaxedItem();
>> >>             if (item != null && o.equals(item) && p.item != null)
>> >>                 return true;
>> >>         }
>> >>         return false;
>> >>     }
>> >>
>> >> The goal of re-reading 'item' with a volatile load when it is null, is
>> >> to
>> >> prevent the issue described in the last few slides of the presentation:
>> >>
>> >> https://github.com/pramalhe/ConcurrencyFreaks/raw/master/Presentations/ConcurrentLinkedQueueRelaxed.pptx
>> >>
>> >>
>> >> On Sat, Nov 8, 2014 at 5:13 AM, Martin Buchholz <martinrb at google.com>
>> >> wrote:
>> >>>
>> >>> I'm looking at:
>> >>>
>> >>> E getRelaxedItem() {
>> >>> E localitem = (E)UNSAFE.getObject(this, itemOffset);
>> >>> // If it's null we need to re-read, this time as a volatile load
>> >>> return localitem == null ? item : localitem;
>> >>> }
>> >>>
>> >>> it seems to me the other way around.  If you read null, it must be the
>> >>> final (deleted) state of item.  If you read non-null, the value might
>> >>> be old (pre-deleted).  So why do you re-read when item is null?
>> >>> enqueuing via casNext ensure visibility of initial item, so
>> >>> pre-constructor null is never seen.
>> >>>
>> >>>
>> >>> On Fri, Nov 7, 2014 at 7:34 PM, Pedro Ramalhete <pramalhe at gmail.com>
>> >>> wrote:
>> >>> > That's an interesting scenario, and in that case there will always
>> >>> > be at
>> >>> > least three volatile loads in contains(e), assuming the list has a
>> >>> > node
>> >>> > with
>> >>> > an item 'e':
>> >>> > - 1st volatile load occurs when reading 'head' at the beginning of
>> >>> > the
>> >>> > list
>> >>> > traversal;
>> >>> > - 2nd volatile load occurs when reading 'item' of the node matching
>> >>> > 'e'
>> >>> > (as
>> >>> > I mentioned on point 2. above);
>> >>> > - 3rd volatile load occurs when reading the 'next' of the last node
>> >>> > on
>> >>> > the
>> >>> > list;
>> >>> >
>> >>> > Once the other thread removes 'e' from the list and unlinks the node
>> >>> > (that
>> >>> > thread or some other thread would have to call poll()/succ() and the
>> >>> > node
>> >>> > would have to be one of the first ones so as to be unlinked, but it
>> >>> > can
>> >>> > happen), then there will be at least 2 volatiles loads, the 1st and
>> >>> > 3rd
>> >>> > mentioned above.
>> >>> > No VM will be able to optimize volatile loads away, or even to
>> >>> > reorder
>> >>> > instructions from within two successive calls to contains() because
>> >>> > they
>> >>> > are
>> >>> > "wrapped" in the two volatile loads at the beginning and end of the
>> >>> > list
>> >>> > traversal.
>> >>> >
>> >>> >
>> >>> > If you think of other scenarios, please let us know, there could be
>> >>> > something we're missing.
>> >>> > Pedro
>> >>> >
>> >>> > On Sat, Nov 8, 2014 at 1:48 AM, Martin Buchholz
>> >>> > <martinrb at google.com>
>> >>> > wrote:
>> >>> >>
>> >>> >> On Fri, Nov 7, 2014 at 4:32 PM, Pedro Ramalhete
>> >>> >> <pramalhe at gmail.com>
>> >>> >> wrote:
>> >>> >> > Hi Martin,
>> >>> >> >
>> >>> >> > I agree that relaxed atomics are non-trivial, but then again,
>> >>> >> > lock-free
>> >>> >> > programming never is  :)
>> >>> >> > It's hard to be 100% confident of the correctness of an
>> >>> >> > algorithm,
>> >>> >> > but
>> >>> >> > that's partially the reason whay we're sharing the code on this
>> >>> >> > list,
>> >>> >> > so
>> >>> >> > that other experts can take a stab at it and find possible issues
>> >>> >> > ;)
>> >>> >> >
>> >>> >> > CASitem should not be a problem because if the item is no longer
>> >>> >> > "up-to-date" the CAS will fail, just like it does today for CLQ
>> >>> >> > if
>> >>> >> > some
>> >>> >> > other thread changes the item between the volatile load and the
>> >>> >> > CAS.
>> >>> >> > Accessing the item in a relaxed way is definitely the "trickiest"
>> >>> >> > part
>> >>> >> > of
>> >>> >> > these optimizations, but I would like to point out three details:
>> >>> >> > 1. For both contains() and remove(), when the item is seen as
>> >>> >> > null,
>> >>> >> > it
>> >>> >> > will
>> >>> >> > be re-read using a volatile load (as in CLQ today);
>> >>> >> > 2. For contains() when a matching item is seen, it will be
>> >>> >> > re-read
>> >>> >> > using
>> >>> >> > a
>> >>> >> > volatile load (as in CLQ today);
>> >>> >>
>> >>> >> Suppose the q contains element e and some thread is waiting for e
>> >>> >> to
>> >>> >> be removed by another thread using
>> >>> >> while (q.contains(e)) {}
>> >>> >> it looks to me like contains() is calling only relaxed operations,
>> >>> >> so
>> >>> >> an adversarial VM can "optimize" that to
>> >>> >> if (q.contains(e)) { while (true) {} }
>> >>> >
>> >>> >
>> >>
>> >>
>
>


From pramalhe at gmail.com  Mon Nov 10 15:37:41 2014
From: pramalhe at gmail.com (Pedro Ramalhete)
Date: Mon, 10 Nov 2014 21:37:41 +0100
Subject: [concurrency-interest] Increasing the throughput of
 ConcurrentLinkedQueue on PowerPC (and ARM?)
In-Reply-To: <CA+kOe0821iJaaAL1z4PiobJKb6vzU6GDTgBb-=Fp96uGW0OxGw@mail.gmail.com>
References: <CAAApjO3j3g2fE-e2i5Yt-K8a8t2MBt3=zPrkNSCrCpGkwDEc2Q@mail.gmail.com>
	<CA+kOe0-OPZwYHNkWOz-jWhzzA0ZFDB3nyUKP1M9r+gSufa5WsA@mail.gmail.com>
	<CAAApjO1ExAfUMmz6-t2uezc=r6jibYz+cqLqPYvm+he8hiy78w@mail.gmail.com>
	<CA+kOe087q-WMSpWYwiPQ_ZRtbCLwVAEPE0oiqx_zZWm-RHTyFg@mail.gmail.com>
	<CAAApjO1wuEfsu2ogm9utGFXu-TTShzQ8i=ArZN3OdAniYfHdGA@mail.gmail.com>
	<CA+kOe0_Nnkix3DeTaYvZO3P4ykbHNTRcvxy92+sq+me3bD23xw@mail.gmail.com>
	<CAAApjO2UgDXh==r+wbPEZOfcMWg6itDY32BOM7Ej3mak3M7Vfw@mail.gmail.com>
	<CA+kOe09fM-0YrVJ9bLqsRRkpbhxATdHjc2V1h6uSr9pWmMesvQ@mail.gmail.com>
	<CA+kOe0_Cqf0GBdJ096yDsV_GUTQC7JXst9tAarJszTitpMCqjQ@mail.gmail.com>
	<CAAApjO3HmfKqqmaNWqaGTk-d2wHA1+owqDf5uqE+HqJ=ZHoDpg@mail.gmail.com>
	<CA+kOe0821iJaaAL1z4PiobJKb6vzU6GDTgBb-=Fp96uGW0OxGw@mail.gmail.com>
Message-ID: <CAAApjO1vzrkeq0YFe5swO5t9tA0JUZuBKPZifGFXtP_vS0XwAQ@mail.gmail.com>

Hi Martin,
I understand now, and after discussing it at length with Andreia, we
believe
your description is correct.


Let me just try to summarize the conclusions to benefit other people
following
this thread and to make sure I got it right:

The technique of using relaxed loads on 'next' used by CLLElectedUnlink and
CLQRelaxed, relies on not having dependent loads reordered, which means it
does not follow the JVM memory model (or the C11/C++11 memory model),
yet, as long as they're ran on a target CPU that gives such a guarantee it
should be correct (apart from some yet unseen bug).
In practice, this limitation is not so bad because only DEC Alphas do such
a kind of re-ordering, but the fact that the JVM does not give such
guarantees
makes this technique unsuitable to be part of any class in JDK.
http://en.wikipedia.org/wiki/Memory_ordering

On the other hand, the diff you propose for CLQ with only the relaxed access
to 'item', is correct under the JVM's memory model, and it provides
a performance improvement of nearly 2x over the current CLQ, at least on
PowerPC, making suitable to be included in any generic code.


Thank you so much for doing a "deep dive" on these algorithms, and getting
to the bottom of it. This discussion was very fruitful, at least to me  ;)

Cheers,
Pedro



On Mon, Nov 10, 2014 at 3:19 AM, Martin Buchholz <martinrb at google.com>
wrote:

> On Sun, Nov 9, 2014 at 2:44 PM, Pedro Ramalhete <pramalhe at gmail.com>
> wrote:
> > 1st try
> > Let us simplify the problem to the bare essentials:
> >
> > Thread 1 is calling add(o):
> >     Node newNode = new Node(o);  // A new node is created and
> newNode.item
> > is set to 'o' using a volatile store
>
> Note relevant here, but it uses a relaxed store
>
>         /**
>          * Constructs a new node.  Uses relaxed write because item can
>          * only be seen after publication via casNext.
>          */
>         Node(E item) {
>             UNSAFE.putObject(this, itemOffset, item);
>         }
>
> >     prevNode.casNext(newNode);   //    The new node is inserted into the
> > list, after the last node, making it visible to other threads (has a
> release
> > barrier)
> >
> > Notice that in this code, the store of newNode.item can _never_ occur
> after
> > the casNext(), i.e. no
> > re-ordering of these two stores is possible by the
> > compiler/hotspot/cpu/cache-coherence.
>
> There is no global order.
>
> > Thread 2 is calling contains(o):
> >     nextNode = node.getRelaxedNext();   // This is a relaxed load and
> can be
> > re-ordered
> >     item = nextNode.getRelaxedItem();   // This is a relaxed load and
> can be
> > re-ordered
> >
> > If I understood correctly, what you say is that the JVM memory model does
> > _not_ give the
> > guarantee that the item 'o' will be visible even if the nextNode is
> already
> > visible to Thread 2, is that so?
>
> The risk is that ordinary writes to e.g. the fields of item, are not
> yet visible to the other thread although the reference to that object
> (item) is visible.
>
> > This would imply that the second line in contains(o) can occur before the
> > newNode's constructor has
> > completed, and even worse, the volatile store to item in add(o) becomes
> > visible after the casNext(),
> > even though there is an happens-before (at least) between the store of
> > newNode.item and the store of casNext()...
> > am I understanding it correctly?
>
> You need an INTER-thread happens-before, and you have none.
>
> """A write to a volatile field (?8.3.1.4) happens-before every
> subsequent read of that field."""
> (of course, only if it's a volatile read!)
>
> > I understand that Alphas can re-order even dependent loads, but I was
> hoping
> > that the memory model
> > itself would give the guarantee we need just because of the two volatiles
> > stores in add(o),
> > namely the store of the newNode.item must be visible before the
> > casNext(newNode).
>
> I don't think the memory model gives that guarantee.  But I am more
> worried about the fields of item than item itself, which kills
> attempts to recheck if item null as in your 2nd try.  Again, compare
> with double-checked locking
>
> http://www.cs.umd.edu/~pugh/java/memoryModel/DoubleCheckedLocking.html
>
> > 2nd try
> > Even if the above is true, then consider the following: If newNode
> becomes
> > visible to thread 2 even when
> > newNode.item has not yet been set to 'o', then it should be safe to
> assume
> > that the value of newNode.item
> > is null.
> > It's a bit confusing, but the state machine of the 'item' variable then
> > becomes:
> >           null                 ->            'o'               ->
>  null
> > before constructor finishes        constructor has completed        node
> has
> > been logically removed
> >                                    and node may be visible
> >
> > When a node is seen to have an item at null, the load is re-done as a
> > volatile load to check if it
> > is really null, and if it is null then it means that it has been
> logically
> > removed (last state in the diagram above),
> > or the constructor has not yet completed (first state of the diagram
> above).
> > If it is the case of the constructor not yet completing, then there is an
> > happens-before relationship that
> > guarantees that casNext has _not_ yet completed, and so the newNode can
> not
> > yet be visible to contains(o),
> > which means that contains(o) will never be able to see the first state.
> >
> > Notice that in this reasoning there is the assumption that before the
> > constructor of Node completes, the
> > initial value of the node's item is null... does the JVM initialize all
> > references to null by default?
> > If it does, then the two stores described in add(o) plus the volatile
> load
> > of the item in contains(o) create
> > an happens-before guarantee which provides correctness for the case you
> > describe.
> >
> >
> >
> > Regarding the diff you propose, using only the relaxed item and always
> doing
> > a volatile load for the Node.next,
> > it is much easier to reason about and certainly does not suffer from the
> > issue you described with add(o)/contains(o).
> > In terms of performance, it gives an improvement over the current
> > implementation of CLQ because
> > it does on average one volatile load (acquire fence) per node traversed,
> > while the current CLQ does two volatile loads.
> > CLQRelaxed has a higher performance gain because it does on average
> close to
> > zero volatile loads.
> >
> > I ran your changes (I named it ConcurrentLinkedqueueItem) on the RunAbove
> > instance Power8 S using
> > our microbenchmark to compare against CLQ and CLQRelaxed, and the results
> > are show below:
> >
> > numElements=1000,  numCores=8
> > 0.0% Writes, ConcurrentLinkedQueue, ConcurrentLinkedQueueItem,
> > ConcurrentLinkedQueueRelaxed
> > 1, 57683, 111233, 387917
> > 2, 112577, 198374, 774528
> > 4, 188658, 348552, 1179456
> > 8, 208068, 393500, 1131104
> > 0.1% Writes, ConcurrentLinkedQueue, ConcurrentLinkedQueueItem,
> > ConcurrentLinkedQueueRelaxed
> > 1, 57699, 111109, 397134
> > 2, 112477, 198419, 793971
> > 4, 188241, 350068, 1168872
> > 8, 207919, 399379, 1126918
> > 1.0% Writes, ConcurrentLinkedQueue, ConcurrentLinkedQueueItem,
> > ConcurrentLinkedQueueRelaxed
> > 1, 58043, 111959, 418096
> > 2, 113036, 199982, 818435
> > 4, 190894, 354092, 1188407
> > 8, 215828, 405752, 1162508
> > 10.0% Writes, ConcurrentLinkedQueue, ConcurrentLinkedQueueItem,
> > ConcurrentLinkedQueueRelaxed
> > 1, 65238, 127538, 361053
> > 2, 118835, 214567, 1194862
> > 4, 207810, 375174, 1550311
> > 8, 223638, 431316, 1290808
> > 100.0% Writes, ConcurrentLinkedQueue, ConcurrentLinkedQueueItem,
> > ConcurrentLinkedQueueRelaxed
> > 1, 124199, 236175, 1411299
> > 2, 239134, 450326, 2293381
> > 4, 456139, 861773, 2220363
> > 8, 464168, 900075, 2368460
> >
> > As you can see, its performance is nearly twice as much as CLQ (from
> 1.75x
> > to 1.95x), which is pretty good!
> > Having into consideration the almost 2x increase in performance and that
> it
> > is a much "safer change"
> > than CLQRelaxed, I believe the change you propose should be taken
> seriously
> > to be added to CLQ in the newest JDK.
> >
> > Thanks,
> > Pedro
> >
> >
> > On Sun, Nov 9, 2014 at 5:20 PM, Martin Buchholz <martinrb at google.com>
> wrote:
> >>
> >> Inspired by Pedro, below is an alternative, where we continue to do
> >> volatile reads of next pointers, but try to optimize item reads:
> >>
> >> Is this a case where we really want the equivalent of (problematic?)
> >> C++ memory_order_consume?
> >>
> >> memory_order_consume A load operation with this memory order performs
> >> a consume operation on the affected memory location: prior writes to
> >> data-dependent memory locations made by the thread that did a release
> >> operation become visible to this thread's dependency chain.
> >>
> >> Index: src/main/java/util/concurrent/ConcurrentLinkedQueue.java
> >> ===================================================================
> >> RCS file:
> >>
> /export/home/jsr166/jsr166/jsr166/src/main/java/util/concurrent/ConcurrentLinkedQueue.java,v
> >> retrieving revision 1.99
> >> diff -u -U 5 -r1.99 ConcurrentLinkedQueue.java
> >> --- src/main/java/util/concurrent/ConcurrentLinkedQueue.java 29 Oct
> >> 2014 20:23:14 -0000 1.99
> >> +++ src/main/java/util/concurrent/ConcurrentLinkedQueue.java 9 Nov
> >> 2014 16:05:40 -0000
> >> @@ -165,10 +165,15 @@
> >>
> >>          boolean casItem(E cmp, E val) {
> >>              return UNSAFE.compareAndSwapObject(this, itemOffset, cmp,
> >> val);
> >>          }
> >>
> >> +        @SuppressWarnings("unchecked")
> >> +        E itemRelaxed() {
> >> +            return (E) UNSAFE.getObject(this, itemOffset);
> >> +        }
> >> +
> >>          void lazySetNext(Node<E> val) {
> >>              UNSAFE.putOrderedObject(this, nextOffset, val);
> >>          }
> >>
> >>          boolean casNext(Node<E> cmp, Node<E> val) {
> >> @@ -329,11 +334,11 @@
> >>
> >>      public E poll() {
> >>          restartFromHead:
> >>          for (;;) {
> >>              for (Node<E> h = head, p = h, q;;) {
> >> -                E item = p.item;
> >> +                E item = p.itemRelaxed();
> >>
> >>                  if (item != null && p.casItem(item, null)) {
> >>                      // Successful CAS is the linearization point
> >>                      // for item to be removed from this queue.
> >>                      if (p != h) // hop two nodes at a time
> >> @@ -438,12 +443,13 @@
> >>       * @return {@code true} if this queue contains the specified
> element
> >>       */
> >>      public boolean contains(Object o) {
> >>          if (o == null) return false;
> >>          for (Node<E> p = first(); p != null; p = succ(p)) {
> >> -            E item = p.item;
> >> -            if (item != null && o.equals(item))
> >> +            E item = p.itemRelaxed();
> >> +            if (item != null && o.equals(item)
> >> +                && p.item == item) // recheck
> >>                  return true;
> >>          }
> >>          return false;
> >>      }
> >>
> >> @@ -460,11 +466,11 @@
> >>       */
> >>      public boolean remove(Object o) {
> >>          if (o == null) return false;
> >>          Node<E> pred = null;
> >>          for (Node<E> p = first(); p != null; p = succ(p)) {
> >> -            E item = p.item;
> >> +            E item = p.itemRelaxed();
> >>              if (item != null &&
> >>                  o.equals(item) &&
> >>                  p.casItem(item, null)) {
> >>                  Node<E> next = succ(p);
> >>                  if (pred != null && next != null)
> >>
> >> On Sat, Nov 8, 2014 at 3:34 PM, Martin Buchholz <martinrb at google.com>
> >> wrote:
> >> > Wow, you have done a lot of work on these data structures!
> >> >
> >> > We probably reason differently when writing lock-free code.  I rely
> >> > very much on the "happens-before" guarantees, and find it hard to
> >> > follow arguments based on reordering and "travelling".
> >> >
> >> > When contains(o) calls
> >> > o.equals(item)
> >> > item must have been safely published, that is, the enqueue of item
> >> > must happen-before the call to equals.  Since there was only one
> >> > synchronization operation during enqueue (via casNext) it seems to me
> >> > that contains *must* do the corresponding volatile read of that
> >> > variable, i.e. the next pointer of the node pointing to the one
> >> > containing item, in order to establish the happens-before
> >> > relationship.  It is less clear to me whether we can elide the
> >> > volatile read of "item", possibly missing the deletion.
> >> >
> >> > In practice your code may work perfectly because of dependency
> >> > ordering, except on DEC Alpha.
> >> >
> >> > The attempts to do without volatile reads remind me of the attempts to
> >> > do double-checked locking without volatile reads
> >> > http://en.wikipedia.org/wiki/Double-checked_locking
> >> >
> >> > On Fri, Nov 7, 2014 at 9:39 PM, Pedro Ramalhete <pramalhe at gmail.com>
> >> > wrote:
> >> >> Actually, the last sentence I wrote was not completely accurate.
> What I
> >> >> meant to write was that: no volatile loads will be reordered, but the
> >> >> relaxed atomics (non-volatile) code within them may traverse
> downwards.
> >> >> As
> >> >> unlikely ans weird as this may seem, it will still be safe because
> only
> >> >> code
> >> >> that is unrelated to the computation of the result of contains(x)
> being
> >> >> true/false will be re-order, so it will have not impact in the return
> >> >> value
> >> >> of contains().
> >> >> And if such a thing is worrisome, we can always follow the same
> >> >> approach as
> >> >> on StampedLock.validate() to prevent read-only code from "escaping"
> >> >> below,
> >> >> by using an Unsafe.loadFence(), or even a fullFence() since it won't
> >> >> impact
> >> >> performance because it happens only once.
> >> >>
> >> >> Regarding what your comment, the volatile load mentioned on the
> >> >> previous
> >> >> email as being "the 2nd volatile", is on line 4 of contains() when
> >> >> p.item is
> >> >> read in the if() statement:
> >> >> public boolean contains(Object o) {
> >> >>         if (o == null) return false;
> >> >>         for (Node<E> p = first(); p != null; p = succRelaxed(p)) {
> >> >>             E item = p.getRelaxedItem();
> >> >>             if (item != null && o.equals(item) && p.item != null)
> >> >>                 return true;
> >> >>         }
> >> >>         return false;
> >> >>     }
> >> >>
> >> >> The goal of re-reading 'item' with a volatile load when it is null,
> is
> >> >> to
> >> >> prevent the issue described in the last few slides of the
> presentation:
> >> >>
> >> >>
> https://github.com/pramalhe/ConcurrencyFreaks/raw/master/Presentations/ConcurrentLinkedQueueRelaxed.pptx
> >> >>
> >> >>
> >> >> On Sat, Nov 8, 2014 at 5:13 AM, Martin Buchholz <martinrb at google.com
> >
> >> >> wrote:
> >> >>>
> >> >>> I'm looking at:
> >> >>>
> >> >>> E getRelaxedItem() {
> >> >>> E localitem = (E)UNSAFE.getObject(this, itemOffset);
> >> >>> // If it's null we need to re-read, this time as a volatile load
> >> >>> return localitem == null ? item : localitem;
> >> >>> }
> >> >>>
> >> >>> it seems to me the other way around.  If you read null, it must be
> the
> >> >>> final (deleted) state of item.  If you read non-null, the value
> might
> >> >>> be old (pre-deleted).  So why do you re-read when item is null?
> >> >>> enqueuing via casNext ensure visibility of initial item, so
> >> >>> pre-constructor null is never seen.
> >> >>>
> >> >>>
> >> >>> On Fri, Nov 7, 2014 at 7:34 PM, Pedro Ramalhete <pramalhe at gmail.com
> >
> >> >>> wrote:
> >> >>> > That's an interesting scenario, and in that case there will always
> >> >>> > be at
> >> >>> > least three volatile loads in contains(e), assuming the list has a
> >> >>> > node
> >> >>> > with
> >> >>> > an item 'e':
> >> >>> > - 1st volatile load occurs when reading 'head' at the beginning of
> >> >>> > the
> >> >>> > list
> >> >>> > traversal;
> >> >>> > - 2nd volatile load occurs when reading 'item' of the node
> matching
> >> >>> > 'e'
> >> >>> > (as
> >> >>> > I mentioned on point 2. above);
> >> >>> > - 3rd volatile load occurs when reading the 'next' of the last
> node
> >> >>> > on
> >> >>> > the
> >> >>> > list;
> >> >>> >
> >> >>> > Once the other thread removes 'e' from the list and unlinks the
> node
> >> >>> > (that
> >> >>> > thread or some other thread would have to call poll()/succ() and
> the
> >> >>> > node
> >> >>> > would have to be one of the first ones so as to be unlinked, but
> it
> >> >>> > can
> >> >>> > happen), then there will be at least 2 volatiles loads, the 1st
> and
> >> >>> > 3rd
> >> >>> > mentioned above.
> >> >>> > No VM will be able to optimize volatile loads away, or even to
> >> >>> > reorder
> >> >>> > instructions from within two successive calls to contains()
> because
> >> >>> > they
> >> >>> > are
> >> >>> > "wrapped" in the two volatile loads at the beginning and end of
> the
> >> >>> > list
> >> >>> > traversal.
> >> >>> >
> >> >>> >
> >> >>> > If you think of other scenarios, please let us know, there could
> be
> >> >>> > something we're missing.
> >> >>> > Pedro
> >> >>> >
> >> >>> > On Sat, Nov 8, 2014 at 1:48 AM, Martin Buchholz
> >> >>> > <martinrb at google.com>
> >> >>> > wrote:
> >> >>> >>
> >> >>> >> On Fri, Nov 7, 2014 at 4:32 PM, Pedro Ramalhete
> >> >>> >> <pramalhe at gmail.com>
> >> >>> >> wrote:
> >> >>> >> > Hi Martin,
> >> >>> >> >
> >> >>> >> > I agree that relaxed atomics are non-trivial, but then again,
> >> >>> >> > lock-free
> >> >>> >> > programming never is  :)
> >> >>> >> > It's hard to be 100% confident of the correctness of an
> >> >>> >> > algorithm,
> >> >>> >> > but
> >> >>> >> > that's partially the reason whay we're sharing the code on this
> >> >>> >> > list,
> >> >>> >> > so
> >> >>> >> > that other experts can take a stab at it and find possible
> issues
> >> >>> >> > ;)
> >> >>> >> >
> >> >>> >> > CASitem should not be a problem because if the item is no
> longer
> >> >>> >> > "up-to-date" the CAS will fail, just like it does today for CLQ
> >> >>> >> > if
> >> >>> >> > some
> >> >>> >> > other thread changes the item between the volatile load and the
> >> >>> >> > CAS.
> >> >>> >> > Accessing the item in a relaxed way is definitely the
> "trickiest"
> >> >>> >> > part
> >> >>> >> > of
> >> >>> >> > these optimizations, but I would like to point out three
> details:
> >> >>> >> > 1. For both contains() and remove(), when the item is seen as
> >> >>> >> > null,
> >> >>> >> > it
> >> >>> >> > will
> >> >>> >> > be re-read using a volatile load (as in CLQ today);
> >> >>> >> > 2. For contains() when a matching item is seen, it will be
> >> >>> >> > re-read
> >> >>> >> > using
> >> >>> >> > a
> >> >>> >> > volatile load (as in CLQ today);
> >> >>> >>
> >> >>> >> Suppose the q contains element e and some thread is waiting for e
> >> >>> >> to
> >> >>> >> be removed by another thread using
> >> >>> >> while (q.contains(e)) {}
> >> >>> >> it looks to me like contains() is calling only relaxed
> operations,
> >> >>> >> so
> >> >>> >> an adversarial VM can "optimize" that to
> >> >>> >> if (q.contains(e)) { while (true) {} }
> >> >>> >
> >> >>> >
> >> >>
> >> >>
> >
> >
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141110/269b10ba/attachment-0001.html>

From martinrb at google.com  Mon Nov 10 17:46:55 2014
From: martinrb at google.com (Martin Buchholz)
Date: Mon, 10 Nov 2014 14:46:55 -0800
Subject: [concurrency-interest] Increasing the throughput of
 ConcurrentLinkedQueue on PowerPC (and ARM?)
In-Reply-To: <CAAApjO1vzrkeq0YFe5swO5t9tA0JUZuBKPZifGFXtP_vS0XwAQ@mail.gmail.com>
References: <CAAApjO3j3g2fE-e2i5Yt-K8a8t2MBt3=zPrkNSCrCpGkwDEc2Q@mail.gmail.com>
	<CA+kOe0-OPZwYHNkWOz-jWhzzA0ZFDB3nyUKP1M9r+gSufa5WsA@mail.gmail.com>
	<CAAApjO1ExAfUMmz6-t2uezc=r6jibYz+cqLqPYvm+he8hiy78w@mail.gmail.com>
	<CA+kOe087q-WMSpWYwiPQ_ZRtbCLwVAEPE0oiqx_zZWm-RHTyFg@mail.gmail.com>
	<CAAApjO1wuEfsu2ogm9utGFXu-TTShzQ8i=ArZN3OdAniYfHdGA@mail.gmail.com>
	<CA+kOe0_Nnkix3DeTaYvZO3P4ykbHNTRcvxy92+sq+me3bD23xw@mail.gmail.com>
	<CAAApjO2UgDXh==r+wbPEZOfcMWg6itDY32BOM7Ej3mak3M7Vfw@mail.gmail.com>
	<CA+kOe09fM-0YrVJ9bLqsRRkpbhxATdHjc2V1h6uSr9pWmMesvQ@mail.gmail.com>
	<CA+kOe0_Cqf0GBdJ096yDsV_GUTQC7JXst9tAarJszTitpMCqjQ@mail.gmail.com>
	<CAAApjO3HmfKqqmaNWqaGTk-d2wHA1+owqDf5uqE+HqJ=ZHoDpg@mail.gmail.com>
	<CA+kOe0821iJaaAL1z4PiobJKb6vzU6GDTgBb-=Fp96uGW0OxGw@mail.gmail.com>
	<CAAApjO1vzrkeq0YFe5swO5t9tA0JUZuBKPZifGFXtP_vS0XwAQ@mail.gmail.com>
Message-ID: <CA+kOe0_2fnO1hPmjWA4jezruJ8L8+DmPsJP0Vc0tbpJC_SDrVQ@mail.gmail.com>

You are in good company - many authors were advocating the broken
variant of double-checked-locking for years before it became a known
anti-pattern.

Even if we leave out the case of Alpha and assume the machine does not
reorder dependent loads, I think CLLElectedUnlink is skating on thin
ice.  When contains calls item.equals(node.item) everything that might
be accessed by the equals method needs to be visible, and I am not at
all sure that is all done via dependent loads from node.item.  I
recall having other authors advising avoiding consume atomics, being
the most subtle area of C11 atomics.  I suspect Paul McKenney is the
only human on the planet who can effectively use consume atomics.

On Mon, Nov 10, 2014 at 12:37 PM, Pedro Ramalhete <pramalhe at gmail.com> wrote:
> Hi Martin,
> I understand now, and after discussing it at length with Andreia, we believe
> your description is correct.
>
>
> Let me just try to summarize the conclusions to benefit other people
> following
> this thread and to make sure I got it right:
>
> The technique of using relaxed loads on 'next' used by CLLElectedUnlink and
> CLQRelaxed, relies on not having dependent loads reordered, which means it
> does not follow the JVM memory model (or the C11/C++11 memory model),
> yet, as long as they're ran on a target CPU that gives such a guarantee it
> should be correct (apart from some yet unseen bug).
> In practice, this limitation is not so bad because only DEC Alphas do such
> a kind of re-ordering, but the fact that the JVM does not give such
> guarantees
> makes this technique unsuitable to be part of any class in JDK.
> http://en.wikipedia.org/wiki/Memory_ordering
>
> On the other hand, the diff you propose for CLQ with only the relaxed access
> to 'item', is correct under the JVM's memory model, and it provides
> a performance improvement of nearly 2x over the current CLQ, at least on
> PowerPC, making suitable to be included in any generic code.
>
>
> Thank you so much for doing a "deep dive" on these algorithms, and getting
> to the bottom of it. This discussion was very fruitful, at least to me  ;)
>
> Cheers,
> Pedro
>
>
>
> On Mon, Nov 10, 2014 at 3:19 AM, Martin Buchholz <martinrb at google.com>
> wrote:
>>
>> On Sun, Nov 9, 2014 at 2:44 PM, Pedro Ramalhete <pramalhe at gmail.com>
>> wrote:
>> > 1st try
>> > Let us simplify the problem to the bare essentials:
>> >
>> > Thread 1 is calling add(o):
>> >     Node newNode = new Node(o);  // A new node is created and
>> > newNode.item
>> > is set to 'o' using a volatile store
>>
>> Note relevant here, but it uses a relaxed store
>>
>>         /**
>>          * Constructs a new node.  Uses relaxed write because item can
>>          * only be seen after publication via casNext.
>>          */
>>         Node(E item) {
>>             UNSAFE.putObject(this, itemOffset, item);
>>         }
>>
>> >     prevNode.casNext(newNode);   //    The new node is inserted into the
>> > list, after the last node, making it visible to other threads (has a
>> > release
>> > barrier)
>> >
>> > Notice that in this code, the store of newNode.item can _never_ occur
>> > after
>> > the casNext(), i.e. no
>> > re-ordering of these two stores is possible by the
>> > compiler/hotspot/cpu/cache-coherence.
>>
>> There is no global order.
>>
>> > Thread 2 is calling contains(o):
>> >     nextNode = node.getRelaxedNext();   // This is a relaxed load and
>> > can be
>> > re-ordered
>> >     item = nextNode.getRelaxedItem();   // This is a relaxed load and
>> > can be
>> > re-ordered
>> >
>> > If I understood correctly, what you say is that the JVM memory model
>> > does
>> > _not_ give the
>> > guarantee that the item 'o' will be visible even if the nextNode is
>> > already
>> > visible to Thread 2, is that so?
>>
>> The risk is that ordinary writes to e.g. the fields of item, are not
>> yet visible to the other thread although the reference to that object
>> (item) is visible.
>>
>> > This would imply that the second line in contains(o) can occur before
>> > the
>> > newNode's constructor has
>> > completed, and even worse, the volatile store to item in add(o) becomes
>> > visible after the casNext(),
>> > even though there is an happens-before (at least) between the store of
>> > newNode.item and the store of casNext()...
>> > am I understanding it correctly?
>>
>> You need an INTER-thread happens-before, and you have none.
>>
>> """A write to a volatile field (?8.3.1.4) happens-before every
>> subsequent read of that field."""
>> (of course, only if it's a volatile read!)
>>
>> > I understand that Alphas can re-order even dependent loads, but I was
>> > hoping
>> > that the memory model
>> > itself would give the guarantee we need just because of the two
>> > volatiles
>> > stores in add(o),
>> > namely the store of the newNode.item must be visible before the
>> > casNext(newNode).
>>
>> I don't think the memory model gives that guarantee.  But I am more
>> worried about the fields of item than item itself, which kills
>> attempts to recheck if item null as in your 2nd try.  Again, compare
>> with double-checked locking
>>
>> http://www.cs.umd.edu/~pugh/java/memoryModel/DoubleCheckedLocking.html
>>
>> > 2nd try
>> > Even if the above is true, then consider the following: If newNode
>> > becomes
>> > visible to thread 2 even when
>> > newNode.item has not yet been set to 'o', then it should be safe to
>> > assume
>> > that the value of newNode.item
>> > is null.
>> > It's a bit confusing, but the state machine of the 'item' variable then
>> > becomes:
>> >           null                 ->            'o'               ->
>> > null
>> > before constructor finishes        constructor has completed        node
>> > has
>> > been logically removed
>> >                                    and node may be visible
>> >
>> > When a node is seen to have an item at null, the load is re-done as a
>> > volatile load to check if it
>> > is really null, and if it is null then it means that it has been
>> > logically
>> > removed (last state in the diagram above),
>> > or the constructor has not yet completed (first state of the diagram
>> > above).
>> > If it is the case of the constructor not yet completing, then there is
>> > an
>> > happens-before relationship that
>> > guarantees that casNext has _not_ yet completed, and so the newNode can
>> > not
>> > yet be visible to contains(o),
>> > which means that contains(o) will never be able to see the first state.
>> >
>> > Notice that in this reasoning there is the assumption that before the
>> > constructor of Node completes, the
>> > initial value of the node's item is null... does the JVM initialize all
>> > references to null by default?
>> > If it does, then the two stores described in add(o) plus the volatile
>> > load
>> > of the item in contains(o) create
>> > an happens-before guarantee which provides correctness for the case you
>> > describe.
>> >
>> >
>> >
>> > Regarding the diff you propose, using only the relaxed item and always
>> > doing
>> > a volatile load for the Node.next,
>> > it is much easier to reason about and certainly does not suffer from the
>> > issue you described with add(o)/contains(o).
>> > In terms of performance, it gives an improvement over the current
>> > implementation of CLQ because
>> > it does on average one volatile load (acquire fence) per node traversed,
>> > while the current CLQ does two volatile loads.
>> > CLQRelaxed has a higher performance gain because it does on average
>> > close to
>> > zero volatile loads.
>> >
>> > I ran your changes (I named it ConcurrentLinkedqueueItem) on the
>> > RunAbove
>> > instance Power8 S using
>> > our microbenchmark to compare against CLQ and CLQRelaxed, and the
>> > results
>> > are show below:
>> >
>> > numElements=1000,  numCores=8
>> > 0.0% Writes, ConcurrentLinkedQueue, ConcurrentLinkedQueueItem,
>> > ConcurrentLinkedQueueRelaxed
>> > 1, 57683, 111233, 387917
>> > 2, 112577, 198374, 774528
>> > 4, 188658, 348552, 1179456
>> > 8, 208068, 393500, 1131104
>> > 0.1% Writes, ConcurrentLinkedQueue, ConcurrentLinkedQueueItem,
>> > ConcurrentLinkedQueueRelaxed
>> > 1, 57699, 111109, 397134
>> > 2, 112477, 198419, 793971
>> > 4, 188241, 350068, 1168872
>> > 8, 207919, 399379, 1126918
>> > 1.0% Writes, ConcurrentLinkedQueue, ConcurrentLinkedQueueItem,
>> > ConcurrentLinkedQueueRelaxed
>> > 1, 58043, 111959, 418096
>> > 2, 113036, 199982, 818435
>> > 4, 190894, 354092, 1188407
>> > 8, 215828, 405752, 1162508
>> > 10.0% Writes, ConcurrentLinkedQueue, ConcurrentLinkedQueueItem,
>> > ConcurrentLinkedQueueRelaxed
>> > 1, 65238, 127538, 361053
>> > 2, 118835, 214567, 1194862
>> > 4, 207810, 375174, 1550311
>> > 8, 223638, 431316, 1290808
>> > 100.0% Writes, ConcurrentLinkedQueue, ConcurrentLinkedQueueItem,
>> > ConcurrentLinkedQueueRelaxed
>> > 1, 124199, 236175, 1411299
>> > 2, 239134, 450326, 2293381
>> > 4, 456139, 861773, 2220363
>> > 8, 464168, 900075, 2368460
>> >
>> > As you can see, its performance is nearly twice as much as CLQ (from
>> > 1.75x
>> > to 1.95x), which is pretty good!
>> > Having into consideration the almost 2x increase in performance and that
>> > it
>> > is a much "safer change"
>> > than CLQRelaxed, I believe the change you propose should be taken
>> > seriously
>> > to be added to CLQ in the newest JDK.
>> >
>> > Thanks,
>> > Pedro
>> >
>> >
>> > On Sun, Nov 9, 2014 at 5:20 PM, Martin Buchholz <martinrb at google.com>
>> > wrote:
>> >>
>> >> Inspired by Pedro, below is an alternative, where we continue to do
>> >> volatile reads of next pointers, but try to optimize item reads:
>> >>
>> >> Is this a case where we really want the equivalent of (problematic?)
>> >> C++ memory_order_consume?
>> >>
>> >> memory_order_consume A load operation with this memory order performs
>> >> a consume operation on the affected memory location: prior writes to
>> >> data-dependent memory locations made by the thread that did a release
>> >> operation become visible to this thread's dependency chain.
>> >>
>> >> Index: src/main/java/util/concurrent/ConcurrentLinkedQueue.java
>> >> ===================================================================
>> >> RCS file:
>> >>
>> >> /export/home/jsr166/jsr166/jsr166/src/main/java/util/concurrent/ConcurrentLinkedQueue.java,v
>> >> retrieving revision 1.99
>> >> diff -u -U 5 -r1.99 ConcurrentLinkedQueue.java
>> >> --- src/main/java/util/concurrent/ConcurrentLinkedQueue.java 29 Oct
>> >> 2014 20:23:14 -0000 1.99
>> >> +++ src/main/java/util/concurrent/ConcurrentLinkedQueue.java 9 Nov
>> >> 2014 16:05:40 -0000
>> >> @@ -165,10 +165,15 @@
>> >>
>> >>          boolean casItem(E cmp, E val) {
>> >>              return UNSAFE.compareAndSwapObject(this, itemOffset, cmp,
>> >> val);
>> >>          }
>> >>
>> >> +        @SuppressWarnings("unchecked")
>> >> +        E itemRelaxed() {
>> >> +            return (E) UNSAFE.getObject(this, itemOffset);
>> >> +        }
>> >> +
>> >>          void lazySetNext(Node<E> val) {
>> >>              UNSAFE.putOrderedObject(this, nextOffset, val);
>> >>          }
>> >>
>> >>          boolean casNext(Node<E> cmp, Node<E> val) {
>> >> @@ -329,11 +334,11 @@
>> >>
>> >>      public E poll() {
>> >>          restartFromHead:
>> >>          for (;;) {
>> >>              for (Node<E> h = head, p = h, q;;) {
>> >> -                E item = p.item;
>> >> +                E item = p.itemRelaxed();
>> >>
>> >>                  if (item != null && p.casItem(item, null)) {
>> >>                      // Successful CAS is the linearization point
>> >>                      // for item to be removed from this queue.
>> >>                      if (p != h) // hop two nodes at a time
>> >> @@ -438,12 +443,13 @@
>> >>       * @return {@code true} if this queue contains the specified
>> >> element
>> >>       */
>> >>      public boolean contains(Object o) {
>> >>          if (o == null) return false;
>> >>          for (Node<E> p = first(); p != null; p = succ(p)) {
>> >> -            E item = p.item;
>> >> -            if (item != null && o.equals(item))
>> >> +            E item = p.itemRelaxed();
>> >> +            if (item != null && o.equals(item)
>> >> +                && p.item == item) // recheck
>> >>                  return true;
>> >>          }
>> >>          return false;
>> >>      }
>> >>
>> >> @@ -460,11 +466,11 @@
>> >>       */
>> >>      public boolean remove(Object o) {
>> >>          if (o == null) return false;
>> >>          Node<E> pred = null;
>> >>          for (Node<E> p = first(); p != null; p = succ(p)) {
>> >> -            E item = p.item;
>> >> +            E item = p.itemRelaxed();
>> >>              if (item != null &&
>> >>                  o.equals(item) &&
>> >>                  p.casItem(item, null)) {
>> >>                  Node<E> next = succ(p);
>> >>                  if (pred != null && next != null)
>> >>
>> >> On Sat, Nov 8, 2014 at 3:34 PM, Martin Buchholz <martinrb at google.com>
>> >> wrote:
>> >> > Wow, you have done a lot of work on these data structures!
>> >> >
>> >> > We probably reason differently when writing lock-free code.  I rely
>> >> > very much on the "happens-before" guarantees, and find it hard to
>> >> > follow arguments based on reordering and "travelling".
>> >> >
>> >> > When contains(o) calls
>> >> > o.equals(item)
>> >> > item must have been safely published, that is, the enqueue of item
>> >> > must happen-before the call to equals.  Since there was only one
>> >> > synchronization operation during enqueue (via casNext) it seems to me
>> >> > that contains *must* do the corresponding volatile read of that
>> >> > variable, i.e. the next pointer of the node pointing to the one
>> >> > containing item, in order to establish the happens-before
>> >> > relationship.  It is less clear to me whether we can elide the
>> >> > volatile read of "item", possibly missing the deletion.
>> >> >
>> >> > In practice your code may work perfectly because of dependency
>> >> > ordering, except on DEC Alpha.
>> >> >
>> >> > The attempts to do without volatile reads remind me of the attempts
>> >> > to
>> >> > do double-checked locking without volatile reads
>> >> > http://en.wikipedia.org/wiki/Double-checked_locking
>> >> >
>> >> > On Fri, Nov 7, 2014 at 9:39 PM, Pedro Ramalhete <pramalhe at gmail.com>
>> >> > wrote:
>> >> >> Actually, the last sentence I wrote was not completely accurate.
>> >> >> What I
>> >> >> meant to write was that: no volatile loads will be reordered, but
>> >> >> the
>> >> >> relaxed atomics (non-volatile) code within them may traverse
>> >> >> downwards.
>> >> >> As
>> >> >> unlikely ans weird as this may seem, it will still be safe because
>> >> >> only
>> >> >> code
>> >> >> that is unrelated to the computation of the result of contains(x)
>> >> >> being
>> >> >> true/false will be re-order, so it will have not impact in the
>> >> >> return
>> >> >> value
>> >> >> of contains().
>> >> >> And if such a thing is worrisome, we can always follow the same
>> >> >> approach as
>> >> >> on StampedLock.validate() to prevent read-only code from "escaping"
>> >> >> below,
>> >> >> by using an Unsafe.loadFence(), or even a fullFence() since it won't
>> >> >> impact
>> >> >> performance because it happens only once.
>> >> >>
>> >> >> Regarding what your comment, the volatile load mentioned on the
>> >> >> previous
>> >> >> email as being "the 2nd volatile", is on line 4 of contains() when
>> >> >> p.item is
>> >> >> read in the if() statement:
>> >> >> public boolean contains(Object o) {
>> >> >>         if (o == null) return false;
>> >> >>         for (Node<E> p = first(); p != null; p = succRelaxed(p)) {
>> >> >>             E item = p.getRelaxedItem();
>> >> >>             if (item != null && o.equals(item) && p.item != null)
>> >> >>                 return true;
>> >> >>         }
>> >> >>         return false;
>> >> >>     }
>> >> >>
>> >> >> The goal of re-reading 'item' with a volatile load when it is null,
>> >> >> is
>> >> >> to
>> >> >> prevent the issue described in the last few slides of the
>> >> >> presentation:
>> >> >>
>> >> >>
>> >> >> https://github.com/pramalhe/ConcurrencyFreaks/raw/master/Presentations/ConcurrentLinkedQueueRelaxed.pptx
>> >> >>
>> >> >>
>> >> >> On Sat, Nov 8, 2014 at 5:13 AM, Martin Buchholz
>> >> >> <martinrb at google.com>
>> >> >> wrote:
>> >> >>>
>> >> >>> I'm looking at:
>> >> >>>
>> >> >>> E getRelaxedItem() {
>> >> >>> E localitem = (E)UNSAFE.getObject(this, itemOffset);
>> >> >>> // If it's null we need to re-read, this time as a volatile load
>> >> >>> return localitem == null ? item : localitem;
>> >> >>> }
>> >> >>>
>> >> >>> it seems to me the other way around.  If you read null, it must be
>> >> >>> the
>> >> >>> final (deleted) state of item.  If you read non-null, the value
>> >> >>> might
>> >> >>> be old (pre-deleted).  So why do you re-read when item is null?
>> >> >>> enqueuing via casNext ensure visibility of initial item, so
>> >> >>> pre-constructor null is never seen.
>> >> >>>
>> >> >>>
>> >> >>> On Fri, Nov 7, 2014 at 7:34 PM, Pedro Ramalhete
>> >> >>> <pramalhe at gmail.com>
>> >> >>> wrote:
>> >> >>> > That's an interesting scenario, and in that case there will
>> >> >>> > always
>> >> >>> > be at
>> >> >>> > least three volatile loads in contains(e), assuming the list has
>> >> >>> > a
>> >> >>> > node
>> >> >>> > with
>> >> >>> > an item 'e':
>> >> >>> > - 1st volatile load occurs when reading 'head' at the beginning
>> >> >>> > of
>> >> >>> > the
>> >> >>> > list
>> >> >>> > traversal;
>> >> >>> > - 2nd volatile load occurs when reading 'item' of the node
>> >> >>> > matching
>> >> >>> > 'e'
>> >> >>> > (as
>> >> >>> > I mentioned on point 2. above);
>> >> >>> > - 3rd volatile load occurs when reading the 'next' of the last
>> >> >>> > node
>> >> >>> > on
>> >> >>> > the
>> >> >>> > list;
>> >> >>> >
>> >> >>> > Once the other thread removes 'e' from the list and unlinks the
>> >> >>> > node
>> >> >>> > (that
>> >> >>> > thread or some other thread would have to call poll()/succ() and
>> >> >>> > the
>> >> >>> > node
>> >> >>> > would have to be one of the first ones so as to be unlinked, but
>> >> >>> > it
>> >> >>> > can
>> >> >>> > happen), then there will be at least 2 volatiles loads, the 1st
>> >> >>> > and
>> >> >>> > 3rd
>> >> >>> > mentioned above.
>> >> >>> > No VM will be able to optimize volatile loads away, or even to
>> >> >>> > reorder
>> >> >>> > instructions from within two successive calls to contains()
>> >> >>> > because
>> >> >>> > they
>> >> >>> > are
>> >> >>> > "wrapped" in the two volatile loads at the beginning and end of
>> >> >>> > the
>> >> >>> > list
>> >> >>> > traversal.
>> >> >>> >
>> >> >>> >
>> >> >>> > If you think of other scenarios, please let us know, there could
>> >> >>> > be
>> >> >>> > something we're missing.
>> >> >>> > Pedro
>> >> >>> >
>> >> >>> > On Sat, Nov 8, 2014 at 1:48 AM, Martin Buchholz
>> >> >>> > <martinrb at google.com>
>> >> >>> > wrote:
>> >> >>> >>
>> >> >>> >> On Fri, Nov 7, 2014 at 4:32 PM, Pedro Ramalhete
>> >> >>> >> <pramalhe at gmail.com>
>> >> >>> >> wrote:
>> >> >>> >> > Hi Martin,
>> >> >>> >> >
>> >> >>> >> > I agree that relaxed atomics are non-trivial, but then again,
>> >> >>> >> > lock-free
>> >> >>> >> > programming never is  :)
>> >> >>> >> > It's hard to be 100% confident of the correctness of an
>> >> >>> >> > algorithm,
>> >> >>> >> > but
>> >> >>> >> > that's partially the reason whay we're sharing the code on
>> >> >>> >> > this
>> >> >>> >> > list,
>> >> >>> >> > so
>> >> >>> >> > that other experts can take a stab at it and find possible
>> >> >>> >> > issues
>> >> >>> >> > ;)
>> >> >>> >> >
>> >> >>> >> > CASitem should not be a problem because if the item is no
>> >> >>> >> > longer
>> >> >>> >> > "up-to-date" the CAS will fail, just like it does today for
>> >> >>> >> > CLQ
>> >> >>> >> > if
>> >> >>> >> > some
>> >> >>> >> > other thread changes the item between the volatile load and
>> >> >>> >> > the
>> >> >>> >> > CAS.
>> >> >>> >> > Accessing the item in a relaxed way is definitely the
>> >> >>> >> > "trickiest"
>> >> >>> >> > part
>> >> >>> >> > of
>> >> >>> >> > these optimizations, but I would like to point out three
>> >> >>> >> > details:
>> >> >>> >> > 1. For both contains() and remove(), when the item is seen as
>> >> >>> >> > null,
>> >> >>> >> > it
>> >> >>> >> > will
>> >> >>> >> > be re-read using a volatile load (as in CLQ today);
>> >> >>> >> > 2. For contains() when a matching item is seen, it will be
>> >> >>> >> > re-read
>> >> >>> >> > using
>> >> >>> >> > a
>> >> >>> >> > volatile load (as in CLQ today);
>> >> >>> >>
>> >> >>> >> Suppose the q contains element e and some thread is waiting for
>> >> >>> >> e
>> >> >>> >> to
>> >> >>> >> be removed by another thread using
>> >> >>> >> while (q.contains(e)) {}
>> >> >>> >> it looks to me like contains() is calling only relaxed
>> >> >>> >> operations,
>> >> >>> >> so
>> >> >>> >> an adversarial VM can "optimize" that to
>> >> >>> >> if (q.contains(e)) { while (true) {} }
>> >> >>> >
>> >> >>> >
>> >> >>
>> >> >>
>> >
>> >
>
>


From dl at cs.oswego.edu  Mon Nov 10 20:15:47 2014
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon, 10 Nov 2014 20:15:47 -0500
Subject: [concurrency-interest] Increasing the throughput of
 ConcurrentLinkedQueue on PowerPC (and ARM?)
In-Reply-To: <CAAApjO3j3g2fE-e2i5Yt-K8a8t2MBt3=zPrkNSCrCpGkwDEc2Q@mail.gmail.com>
References: <CAAApjO3j3g2fE-e2i5Yt-K8a8t2MBt3=zPrkNSCrCpGkwDEc2Q@mail.gmail.com>
Message-ID: <54616343.5000708@cs.oswego.edu>

On 11/07/2014 06:11 PM, Pedro Ramalhete wrote:
> Hello,
>
> Some time ago, we made public a lock-free linked list that allows traversal of
> the nodes with relaxed loads (non-volatile loads) ...
> In fact, we found the results so encouraging (up to 15x increase in performance)
> that we decided to apply the same kind of optimization to ConcurrentLinkedQueue
> (CLQ), and although it was a bit trickier, we believe it to be correct, and it
> can be downloaded here:
> https://github.com/pramalhe/ConcurrencyFreaks/blob/master/Java/com/concurrencyfreaks/list/ConcurrentLinkedQueueRelaxed.java


Thanks! This will be useful as "enhanced volatiles" are put into place
(JEP 193 http://openjdk.java.net/jeps/193) that will enable and
specify relaxed loads for volatile variables. At the moment, the only
way to get this effect in an undocumented way is via Unsafe.
Use (and specs) of relaxed loads can be delicate, as the C++/C11
folks discovered. But the usage contexts in CLQ look OK given
your followup answers to Martin's questions. So we'll look into
adapting them, as well as exploring other opportunities.

-Doug



From martinrb at google.com  Fri Nov 14 20:37:53 2014
From: martinrb at google.com (Martin Buchholz)
Date: Fri, 14 Nov 2014 17:37:53 -0800
Subject: [concurrency-interest] Increasing the throughput of
 ConcurrentLinkedQueue on PowerPC (and ARM?)
In-Reply-To: <54616343.5000708@cs.oswego.edu>
References: <CAAApjO3j3g2fE-e2i5Yt-K8a8t2MBt3=zPrkNSCrCpGkwDEc2Q@mail.gmail.com>
	<54616343.5000708@cs.oswego.edu>
Message-ID: <CA+kOe088JYmVw1AjANfzKL+sLbc6NFgLmUVj2uKji_-8NiwOQQ@mail.gmail.com>

Here are some actual possible improvements to CLQ to relax volatile
reads of "item" in linked Nodes:

For poll() and remove() I have mostly convinced myself that these
optimizations are completely correct - A preceding volatile read of a
next pointer ensures visibility, while a following CAS ensures that
removals are noticed.  A pure win on weak memory model hardware?!

For contains(), I believe that using relaxed reads of item will Just
Work with any real world VM implementation (due to the remaining
volatile reads during traversal), but I can imagine an omniscient,
antagonistic and perverse JVM deciding that successive call to
contains() can return true even when the element has already been
removed.  We could add another volatile read recheck before returning
true, but that would add a small penalty on x86.

Index: src/main/java/util/concurrent/ConcurrentLinkedQueue.java
===================================================================
RCS file: /export/home/jsr166/jsr166/jsr166/src/main/java/util/concurrent/ConcurrentLinkedQueue.java,v
retrieving revision 1.102
diff -u -U 4 -r1.102 ConcurrentLinkedQueue.java
--- src/main/java/util/concurrent/ConcurrentLinkedQueue.java 11 Nov
2014 22:39:42 -0000 1.102
+++ src/main/java/util/concurrent/ConcurrentLinkedQueue.java 14 Nov
2014 16:10:12 -0000
@@ -313,9 +313,13 @@
     public E poll() {
         restartFromHead:
         for (;;) {
             for (Node<E> h = head, p = h, q;;) {
-                E item = p.item;
+                // Relaxed read suffices here; the volatile read of p made
+                // item visible, and the subsequent cas ensures we notice
+                // racing removal of item.
+                @SuppressWarnings("unchecked")
+                E item = (E) U.getObject(p, ITEM); // = p.item;

                 if (item != null && casItem(p, item, null)) {
                     // Successful CAS is the linearization point
                     // for item to be removed from this queue.
@@ -428,9 +432,9 @@
      */
     public boolean contains(Object o) {
         if (o == null) return false;
         for (Node<E> p = first(); p != null; p = succ(p)) {
-            E item = p.item;
+            E item = (E) U.getObject(p, ITEM); // = p.item;
             if (item != null && o.equals(item))
                 return true;
         }
         return false;
@@ -450,9 +454,14 @@
     public boolean remove(Object o) {
         if (o == null) return false;
         Node<E> pred = null;
         for (Node<E> p = first(); p != null; p = succ(p)) {
-            E item = p.item;
+            // Relaxed read suffices here; the volatile read of p made
+            // item visible, and the subsequent cas ensures we notice
+            // racing removal of item.
+            @SuppressWarnings("unchecked")
+            E item = (E) U.getObject(p, ITEM); // = p.item;
+
             if (item != null &&
                 o.equals(item) &&
                 casItem(p, item, null)) {
                 Node<E> next = succ(p);

On Mon, Nov 10, 2014 at 5:15 PM, Doug Lea <dl at cs.oswego.edu> wrote:
> On 11/07/2014 06:11 PM, Pedro Ramalhete wrote:
>>
>> Hello,
>>
>> Some time ago, we made public a lock-free linked list that allows
>> traversal of
>> the nodes with relaxed loads (non-volatile loads) ...
>> In fact, we found the results so encouraging (up to 15x increase in
>> performance)
>> that we decided to apply the same kind of optimization to
>> ConcurrentLinkedQueue
>> (CLQ), and although it was a bit trickier, we believe it to be correct,
>> and it
>> can be downloaded here:
>>
>> https://github.com/pramalhe/ConcurrencyFreaks/blob/master/Java/com/concurrencyfreaks/list/ConcurrentLinkedQueueRelaxed.java
>
>
>
> Thanks! This will be useful as "enhanced volatiles" are put into place
> (JEP 193 http://openjdk.java.net/jeps/193) that will enable and
> specify relaxed loads for volatile variables. At the moment, the only
> way to get this effect in an undocumented way is via Unsafe.
> Use (and specs) of relaxed loads can be delicate, as the C++/C11
> folks discovered. But the usage contexts in CLQ look OK given
> your followup answers to Martin's questions. So we'll look into
> adapting them, as well as exploring other opportunities.
>
> -Doug
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

From pramalhe at gmail.com  Sat Nov 15 09:50:18 2014
From: pramalhe at gmail.com (Pedro Ramalhete)
Date: Sat, 15 Nov 2014 15:50:18 +0100
Subject: [concurrency-interest] Increasing the throughput of
 ConcurrentLinkedQueue on PowerPC (and ARM?)
In-Reply-To: <CA+kOe088JYmVw1AjANfzKL+sLbc6NFgLmUVj2uKji_-8NiwOQQ@mail.gmail.com>
References: <CAAApjO3j3g2fE-e2i5Yt-K8a8t2MBt3=zPrkNSCrCpGkwDEc2Q@mail.gmail.com>
	<54616343.5000708@cs.oswego.edu>
	<CA+kOe088JYmVw1AjANfzKL+sLbc6NFgLmUVj2uKji_-8NiwOQQ@mail.gmail.com>
Message-ID: <CAAApjO2h9bNnsX4y4_ak78B0U2Xk4Y137NPYH69KrRdyMeziSw@mail.gmail.com>

Hi Martin,

Two remarks concerning this diff.

First on correctness, I see you've removed the volatile load of p.item when
item is loaded (in getRelaxedItem()) and use a simple
    E item = (E) U.getObject(p, ITEM)
Are you sure this is correct?
Andreia and I discussed this some time ago and the conclusion we got to,
was
that having a another volatile load of 'item' when the relaxed load of the
item is null, is for sure correct.
She's leaning more on the approach you describe as also being correct, the
reasoning being that the relaxed load on item will not be re-ordered to be
after the casItem() just in case the item is not null:
    if (item != null && casItem(p, item, null)) {
I'm leaning more on the approach on this diff _not_ being correct, because
if
you look at the examples in Hans Boehm and Brian Demsky's paper
http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/42967.pdf
there could be a speculative load on 'item' for it being null when it is
not
yet null, but later it becomes so, thus breaking sequential consistency.
This is the scenario described in the last two slides of our powerpoint.
https://github.com/pramalhe/ConcurrencyFreaks/blob/master/Presentations/ConcurrentLinkedQueueRelaxed.pptx
Are we missing something obvious? What is your justification for not
needing
to reload 'item' with a volatile load when it is null?

Second on performance, it is not obvious to me that doing the relaxed load
on poll() will improve performance. Did you run any benchmarks of this diff
on x86/PowerPC/ARM ?
It seems like doing this change on poll() is just a "statistical"
trade-off,
where in one hand we are doing less volatile loads at the cost of failing a
few more casItem(), and on the other approach we do more volatile loads but
should have slightly less failures on casItem().
I would expect that the answer to which one is better depends on specific
scenarios (how many threads are contending on poll()), and which CPU
architecture is being used (on x86 doing a volatile load or relaxed load
takes mostly the same time, but the relaxed load is more likely to be
out-of-date and thus cause casItem() to fail more often).




On Sat, Nov 15, 2014 at 2:37 AM, Martin Buchholz <martinrb at google.com>
wrote:

> Here are some actual possible improvements to CLQ to relax volatile
> reads of "item" in linked Nodes:
>
> For poll() and remove() I have mostly convinced myself that these
> optimizations are completely correct - A preceding volatile read of a
> next pointer ensures visibility, while a following CAS ensures that
> removals are noticed.  A pure win on weak memory model hardware?!
>
> For contains(), I believe that using relaxed reads of item will Just
> Work with any real world VM implementation (due to the remaining
> volatile reads during traversal), but I can imagine an omniscient,
> antagonistic and perverse JVM deciding that successive call to
> contains() can return true even when the element has already been
> removed.  We could add another volatile read recheck before returning
> true, but that would add a small penalty on x86.
>
> Index: src/main/java/util/concurrent/ConcurrentLinkedQueue.java
> ===================================================================
> RCS file:
> /export/home/jsr166/jsr166/jsr166/src/main/java/util/concurrent/ConcurrentLinkedQueue.java,v
> retrieving revision 1.102
> diff -u -U 4 -r1.102 ConcurrentLinkedQueue.java
> --- src/main/java/util/concurrent/ConcurrentLinkedQueue.java 11 Nov
> 2014 22:39:42 -0000 1.102
> +++ src/main/java/util/concurrent/ConcurrentLinkedQueue.java 14 Nov
> 2014 16:10:12 -0000
> @@ -313,9 +313,13 @@
>      public E poll() {
>          restartFromHead:
>          for (;;) {
>              for (Node<E> h = head, p = h, q;;) {
> -                E item = p.item;
> +                // Relaxed read suffices here; the volatile read of p made
> +                // item visible, and the subsequent cas ensures we notice
> +                // racing removal of item.
> +                @SuppressWarnings("unchecked")
> +                E item = (E) U.getObject(p, ITEM); // = p.item;
>
>                  if (item != null && casItem(p, item, null)) {
>                      // Successful CAS is the linearization point
>                      // for item to be removed from this queue.
> @@ -428,9 +432,9 @@
>       */
>      public boolean contains(Object o) {
>          if (o == null) return false;
>          for (Node<E> p = first(); p != null; p = succ(p)) {
> -            E item = p.item;
> +            E item = (E) U.getObject(p, ITEM); // = p.item;
>              if (item != null && o.equals(item))
>                  return true;
>          }
>          return false;
> @@ -450,9 +454,14 @@
>      public boolean remove(Object o) {
>          if (o == null) return false;
>          Node<E> pred = null;
>          for (Node<E> p = first(); p != null; p = succ(p)) {
> -            E item = p.item;
> +            // Relaxed read suffices here; the volatile read of p made
> +            // item visible, and the subsequent cas ensures we notice
> +            // racing removal of item.
> +            @SuppressWarnings("unchecked")
> +            E item = (E) U.getObject(p, ITEM); // = p.item;
> +
>              if (item != null &&
>                  o.equals(item) &&
>                  casItem(p, item, null)) {
>                  Node<E> next = succ(p);
>
> On Mon, Nov 10, 2014 at 5:15 PM, Doug Lea <dl at cs.oswego.edu> wrote:
> > On 11/07/2014 06:11 PM, Pedro Ramalhete wrote:
> >>
> >> Hello,
> >>
> >> Some time ago, we made public a lock-free linked list that allows
> >> traversal of
> >> the nodes with relaxed loads (non-volatile loads) ...
> >> In fact, we found the results so encouraging (up to 15x increase in
> >> performance)
> >> that we decided to apply the same kind of optimization to
> >> ConcurrentLinkedQueue
> >> (CLQ), and although it was a bit trickier, we believe it to be correct,
> >> and it
> >> can be downloaded here:
> >>
> >>
> https://github.com/pramalhe/ConcurrencyFreaks/blob/master/Java/com/concurrencyfreaks/list/ConcurrentLinkedQueueRelaxed.java
> >
> >
> >
> > Thanks! This will be useful as "enhanced volatiles" are put into place
> > (JEP 193 http://openjdk.java.net/jeps/193) that will enable and
> > specify relaxed loads for volatile variables. At the moment, the only
> > way to get this effect in an undocumented way is via Unsafe.
> > Use (and specs) of relaxed loads can be delicate, as the C++/C11
> > folks discovered. But the usage contexts in CLQ look OK given
> > your followup answers to Martin's questions. So we'll look into
> > adapting them, as well as exploring other opportunities.
> >
> > -Doug
> >
> >
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141115/cccad9c6/attachment.html>

From pramalhe at gmail.com  Sat Nov 15 18:20:08 2014
From: pramalhe at gmail.com (Pedro Ramalhete)
Date: Sun, 16 Nov 2014 00:20:08 +0100
Subject: [concurrency-interest] Increasing the throughput of
 ConcurrentLinkedQueue on PowerPC (and ARM?)
In-Reply-To: <CA+kOe0_2fnO1hPmjWA4jezruJ8L8+DmPsJP0Vc0tbpJC_SDrVQ@mail.gmail.com>
References: <CAAApjO3j3g2fE-e2i5Yt-K8a8t2MBt3=zPrkNSCrCpGkwDEc2Q@mail.gmail.com>
	<CA+kOe0-OPZwYHNkWOz-jWhzzA0ZFDB3nyUKP1M9r+gSufa5WsA@mail.gmail.com>
	<CAAApjO1ExAfUMmz6-t2uezc=r6jibYz+cqLqPYvm+he8hiy78w@mail.gmail.com>
	<CA+kOe087q-WMSpWYwiPQ_ZRtbCLwVAEPE0oiqx_zZWm-RHTyFg@mail.gmail.com>
	<CAAApjO1wuEfsu2ogm9utGFXu-TTShzQ8i=ArZN3OdAniYfHdGA@mail.gmail.com>
	<CA+kOe0_Nnkix3DeTaYvZO3P4ykbHNTRcvxy92+sq+me3bD23xw@mail.gmail.com>
	<CAAApjO2UgDXh==r+wbPEZOfcMWg6itDY32BOM7Ej3mak3M7Vfw@mail.gmail.com>
	<CA+kOe09fM-0YrVJ9bLqsRRkpbhxATdHjc2V1h6uSr9pWmMesvQ@mail.gmail.com>
	<CA+kOe0_Cqf0GBdJ096yDsV_GUTQC7JXst9tAarJszTitpMCqjQ@mail.gmail.com>
	<CAAApjO3HmfKqqmaNWqaGTk-d2wHA1+owqDf5uqE+HqJ=ZHoDpg@mail.gmail.com>
	<CA+kOe0821iJaaAL1z4PiobJKb6vzU6GDTgBb-=Fp96uGW0OxGw@mail.gmail.com>
	<CAAApjO1vzrkeq0YFe5swO5t9tA0JUZuBKPZifGFXtP_vS0XwAQ@mail.gmail.com>
	<CA+kOe0_2fnO1hPmjWA4jezruJ8L8+DmPsJP0Vc0tbpJC_SDrVQ@mail.gmail.com>
Message-ID: <CAAApjO09U0DNdpz_kzuTvwTMPiNpno8ygNWY9aVqtmtUT4fQVQ@mail.gmail.com>

I had skimmed over std::memory_order_consume some time ago and "didn't get
it",
but after you mentioned it, I gave it a better look and it seems to be
exactly
what is needed to solve this particular problem without adding hw fences.
All we need to do  for safe list traversal is to replace the relaxed load
on 'next' with
an std::memory_order_consume, and this way guarantee load dependency without
using a fence instruction (except Alphas).

I should have looked at it the first time you mentioned, several emails ago:
http://en.cppreference.com/w/cpp/atomic/memory_order
The 'node.item.whatever' must certainly have a dependency on 'node', and
there is a matching store with release so as to create what they name a
"Release-Consume Ordering".
It seems this is another useful application of memory_order_consume
(besides RCU).
I guess I better continue this discussion on the C++ mailing list, because
it
doesn't seem likely that a sun.misc.Unsafe.getObjectConsume() will be
implemented any time soon  ;)

Thanks for pointing it out!
Pedro


PS: memory_order_consume is certainly non-trivial but many years ago they
used
to say the same thing about General Relativity: that only one man in the
world
was able to effectively understand it (Einstein). Nowadays, they teach
General
Relativity to every Physics student in college.

On Mon, Nov 10, 2014 at 11:46 PM, Martin Buchholz <martinrb at google.com>
wrote:

> You are in good company - many authors were advocating the broken
> variant of double-checked-locking for years before it became a known
> anti-pattern.
>
> Even if we leave out the case of Alpha and assume the machine does not
> reorder dependent loads, I think CLLElectedUnlink is skating on thin
> ice.  When contains calls item.equals(node.item) everything that might
> be accessed by the equals method needs to be visible, and I am not at
> all sure that is all done via dependent loads from node.item.  I
> recall having other authors advising avoiding consume atomics, being
> the most subtle area of C11 atomics.  I suspect Paul McKenney is the
> only human on the planet who can effectively use consume atomics.
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141116/4698e9bc/attachment.html>

From martinrb at google.com  Tue Nov 18 20:30:03 2014
From: martinrb at google.com (Martin Buchholz)
Date: Tue, 18 Nov 2014 17:30:03 -0800
Subject: [concurrency-interest] Increasing the throughput of
 ConcurrentLinkedQueue on PowerPC (and ARM?)
In-Reply-To: <CAAApjO09U0DNdpz_kzuTvwTMPiNpno8ygNWY9aVqtmtUT4fQVQ@mail.gmail.com>
References: <CAAApjO3j3g2fE-e2i5Yt-K8a8t2MBt3=zPrkNSCrCpGkwDEc2Q@mail.gmail.com>
	<CA+kOe0-OPZwYHNkWOz-jWhzzA0ZFDB3nyUKP1M9r+gSufa5WsA@mail.gmail.com>
	<CAAApjO1ExAfUMmz6-t2uezc=r6jibYz+cqLqPYvm+he8hiy78w@mail.gmail.com>
	<CA+kOe087q-WMSpWYwiPQ_ZRtbCLwVAEPE0oiqx_zZWm-RHTyFg@mail.gmail.com>
	<CAAApjO1wuEfsu2ogm9utGFXu-TTShzQ8i=ArZN3OdAniYfHdGA@mail.gmail.com>
	<CA+kOe0_Nnkix3DeTaYvZO3P4ykbHNTRcvxy92+sq+me3bD23xw@mail.gmail.com>
	<CAAApjO2UgDXh==r+wbPEZOfcMWg6itDY32BOM7Ej3mak3M7Vfw@mail.gmail.com>
	<CA+kOe09fM-0YrVJ9bLqsRRkpbhxATdHjc2V1h6uSr9pWmMesvQ@mail.gmail.com>
	<CA+kOe0_Cqf0GBdJ096yDsV_GUTQC7JXst9tAarJszTitpMCqjQ@mail.gmail.com>
	<CAAApjO3HmfKqqmaNWqaGTk-d2wHA1+owqDf5uqE+HqJ=ZHoDpg@mail.gmail.com>
	<CA+kOe0821iJaaAL1z4PiobJKb6vzU6GDTgBb-=Fp96uGW0OxGw@mail.gmail.com>
	<CAAApjO1vzrkeq0YFe5swO5t9tA0JUZuBKPZifGFXtP_vS0XwAQ@mail.gmail.com>
	<CA+kOe0_2fnO1hPmjWA4jezruJ8L8+DmPsJP0Vc0tbpJC_SDrVQ@mail.gmail.com>
	<CAAApjO09U0DNdpz_kzuTvwTMPiNpno8ygNWY9aVqtmtUT4fQVQ@mail.gmail.com>
Message-ID: <CA+kOe08z7G3Ck34Xqj9Jn_fZGzJxVXxz6hUg_aCmyd+L3ZnkHw@mail.gmail.com>

On Sat, Nov 15, 2014 at 3:20 PM, Pedro Ramalhete <pramalhe at gmail.com> wrote:
>
> I had skimmed over std::memory_order_consume some time ago and "didn't get
> it",
> but after you mentioned it, I gave it a better look and it seems to be
> exactly
> what is needed to solve this particular problem without adding hw fences.
> All we need to do  for safe list traversal is to replace the relaxed load on
> 'next' with
> an std::memory_order_consume, and this way guarantee load dependency without
> using a fence instruction (except Alphas).

I believe you are right that memory_order_consume (or in practice
naked reads when not running on Alpha) allows us to traverse the
linked list of Nodes and check whether item is null or not.  But it
doesn't provide enough safety to call equals on item, since that may
end up doing reads not dependent on item.

So yet another hybrid relaxation scheme would be to use relaxed
(memory_order_consume) to traverse next links and volatile reads
(memory_order_acquire) to read items.

From aleksey.shipilev at oracle.com  Wed Nov 19 14:55:50 2014
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Wed, 19 Nov 2014 22:55:50 +0300
Subject: [concurrency-interest] [jmm-dev] Will the real memory read
	barrier please stand up?
In-Reply-To: <CA+kOe096db9QVmmbN=nkf=Due87etO1iaOxFdMkDP7Amf5c6zw@mail.gmail.com>
References: <CA+kOe096db9QVmmbN=nkf=Due87etO1iaOxFdMkDP7Amf5c6zw@mail.gmail.com>
Message-ID: <546CF5C6.3090703@oracle.com>

Hi Martin,

I think your To: list is overly broad. Sending these questions to
concurrency-interest@ alone is usually enough. Dropping jmm-dev@ since
that list is about the future JMM update, and not really about the
question you are asking.

On 19.11.2014 22:12, Martin Buchholz wrote:
> I'm struggling to educate myself about memory barriers, and I
> discovered both Paul McKenney's awesome book and those newfangled
> Unsafe fences in OpenJDK.  Paul's book says
> 
> """A read barrier is a partial ordering on loads only; it is not
> required to have any effect on stores."""

From this definition, this seems to sound like a [LoadLoad] fence from
the JSR 133 Cookbook. Go and re-read the Cookbook for a fourth set of
definitions there :) Be aware that the JMM ordering rules and the actual
memory barriers, while related, are not matched exactly. JSR 133
Cookbook is a minimal conservative interpretation of JMM rules. See e.g.
here:
 http://shipilev.net/blog/2014/on-the-fence-with-dependencies/#_theory


> Meanwhile,  Unsafe.loadFence doc says,
> 
> """Ensures lack of reordering of loads before the fence with loads or
> stores after the fence."""

Unsafe.loadFence in this parlance is [LoadLoad] + [LoadStore], destined
to be used as in:

 <load>
 [LoadLoad]
 [LoadStore]
 <other loads and stores>

This effectively turns the first load into a acquiring "volatile load".


> Spelunking in the hotspot sources, Unsafe.loadFence is implemented via
> LoadFenceNode, which says:
> 
> """"Acquire" - no following ref can move before (but earlier refs can
> follow, like an early Load stalled in cache)"""

Well, volatile load has the acquire semantics, and the rest of the
explanation alludes to that. Notice how [LoadLoad]+[LoadStore] prevents
the following (subsequent) refs to move before the barrier.

Anyhow, I would not count HotSpot's interpretation of JSR 133 ruleset
(that is in itself is an interpretation of JMM rules) as a good
reference material.


> So ... that's 3 different definitions?!  Further, the doc for
> LoadFenceNode seems non-sensical to me - if following refs (load or
> store) can cross the barrier backwards, how is that different in
> effect from allowing preceding refs to cross the barrier forwards?
> Both seem to allow any reordering at all?!  Should that have read
> """but earlier Stores (perhaps stalled in cache) can follow""" ?

Yes, mentioning Stores explicitly there might be more clean for
HotSpot's conservative case.


Thanks,
-Aleksey.

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 819 bytes
Desc: OpenPGP digital signature
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141119/94d59c4d/attachment.bin>

From martinrb at google.com  Wed Nov 19 12:49:50 2014
From: martinrb at google.com (Martin Buchholz)
Date: Wed, 19 Nov 2014 09:49:50 -0800
Subject: [concurrency-interest] Increasing the throughput of
 ConcurrentLinkedQueue on PowerPC (and ARM?)
In-Reply-To: <CAAApjO2h9bNnsX4y4_ak78B0U2Xk4Y137NPYH69KrRdyMeziSw@mail.gmail.com>
References: <CAAApjO3j3g2fE-e2i5Yt-K8a8t2MBt3=zPrkNSCrCpGkwDEc2Q@mail.gmail.com>
	<54616343.5000708@cs.oswego.edu>
	<CA+kOe088JYmVw1AjANfzKL+sLbc6NFgLmUVj2uKji_-8NiwOQQ@mail.gmail.com>
	<CAAApjO2h9bNnsX4y4_ak78B0U2Xk4Y137NPYH69KrRdyMeziSw@mail.gmail.com>
Message-ID: <CA+kOe0-mGZmkoC4t7VeWBS2y4nDethgOnCc10GQyCtcjvs=8LQ@mail.gmail.com>

Pedro, thanks!

On Sat, Nov 15, 2014 at 6:50 AM, Pedro Ramalhete <pramalhe at gmail.com> wrote:
> Hi Martin,
>
> Two remarks concerning this diff.
>
> First on correctness, I see you've removed the volatile load of p.item when
> item is loaded (in getRelaxedItem()) and use a simple
>     E item = (E) U.getObject(p, ITEM)
> Are you sure this is correct?

When using relaxed atomics, one is never really sure...

> Andreia and I discussed this some time ago and the conclusion we got to, was
> that having a another volatile load of 'item' when the relaxed load of the
> item is null, is for sure correct.

I believe you are (technically) correct!

> She's leaning more on the approach you describe as also being correct, the
> reasoning being that the relaxed load on item will not be re-ordered to be
> after the casItem() just in case the item is not null:
>     if (item != null && casItem(p, item, null)) {
> I'm leaning more on the approach on this diff _not_ being correct, because
> if
> you look at the examples in Hans Boehm and Brian Demsky's paper
> http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/42967.pdf
> there could be a speculative load on 'item' for it being null when it is not
> yet null, but later it becomes so, thus breaking sequential consistency.

I believe you are (technically) correct!

> This is the scenario described in the last two slides of our powerpoint.
> https://github.com/pramalhe/ConcurrencyFreaks/blob/master/Presentations/ConcurrentLinkedQueueRelaxed.pptx
> Are we missing something obvious? What is your justification for not needing
> to reload 'item' with a volatile load when it is null?

In the real world ... our relaxed read of item returning null is
always followed by a volatile read of next pointer and in the current
JDK implementation (and all real-world implementations) a volatile
read creates a memory read barrier and if we reason at the memory
barrier level (which I am struggling to learn...) we can see that the
read of item cannot be reordered with any read after the CLQ method
returns.  Hopefully preserving real-world sequential consistency.
Since CLQ is part of the JDK, we can get away with changing the CLQ
implementation to one correct only with current JDK.

I just discovered Unsafe.loadFence.  Another possibility is for us to
rewrite CLQ entirely at the fence level, using relaxed loads
everywhere, but inserting loadFence where necessary.... hmmmm.....

(Most users probably want their CLQ to be fast, and in any case don't
care about the kind of sequential consistency we are struggling to
preserve.)

> Second on performance, it is not obvious to me that doing the relaxed load
> on poll() will improve performance. Did you run any benchmarks of this diff
> on x86/PowerPC/ARM ?

No.  You seem to have both the benchmarks and the hardware, so leaving
this to you!

> It seems like doing this change on poll() is just a "statistical" trade-off,
> where in one hand we are doing less volatile loads at the cost of failing a
> few more casItem(), and on the other approach we do more volatile loads but
> should have slightly less failures on casItem().

> I would expect that the answer to which one is better depends on specific
> scenarios (how many threads are contending on poll()), and which CPU
> architecture is being used (on x86 doing a volatile load or relaxed load
> takes mostly the same time, but the relaxed load is more likely to be
> out-of-date and thus cause casItem() to fail more often).

I'm not sure.  I still expect a performance win on all platforms and
scenarios.  Measure and prove me wrong!

From martinrb at google.com  Wed Nov 19 14:12:00 2014
From: martinrb at google.com (Martin Buchholz)
Date: Wed, 19 Nov 2014 11:12:00 -0800
Subject: [concurrency-interest] Will the real memory read barrier please
	stand up?
Message-ID: <CA+kOe096db9QVmmbN=nkf=Due87etO1iaOxFdMkDP7Amf5c6zw@mail.gmail.com>

I'm struggling to educate myself about memory barriers, and I
discovered both Paul McKenney's awesome book and those newfangled
Unsafe fences in OpenJDK.  Paul's book says

"""A read barrier is a partial ordering on loads only; it is not
required to have any effect on stores."""

Meanwhile,  Unsafe.loadFence doc says,

"""Ensures lack of reordering of loads before the fence with loads or
stores after the fence."""

Spelunking in the hotspot sources, Unsafe.loadFence is implemented via
LoadFenceNode, which says:

""""Acquire" - no following ref can move before (but earlier refs can
follow, like an early Load stalled in cache)"""

So ... that's 3 different definitions?!  Further, the doc for
LoadFenceNode seems non-sensical to me - if following refs (load or
store) can cross the barrier backwards, how is that different in
effect from allowing preceding refs to cross the barrier forwards?
Both seem to allow any reordering at all?!  Should that have read
"""but earlier Stores (perhaps stalled in cache) can follow""" ?

From boehm at acm.org  Wed Nov 19 19:38:24 2014
From: boehm at acm.org (Hans Boehm)
Date: Wed, 19 Nov 2014 16:38:24 -0800
Subject: [concurrency-interest] [jmm-dev] Will the real memory read
	barrier please stand up?
In-Reply-To: <CA+kOe096db9QVmmbN=nkf=Due87etO1iaOxFdMkDP7Amf5c6zw@mail.gmail.com>
References: <CA+kOe096db9QVmmbN=nkf=Due87etO1iaOxFdMkDP7Amf5c6zw@mail.gmail.com>
Message-ID: <CAPUmR1YHLHss5+ONSUnaJQwOH-cTh16iAjikBaEekNJ7nHMVwA@mail.gmail.com>

I agree that the third definition is nonsensical unless a LoadFenceNode is
attached to a specific load.  That seems implausible given the rest of your
description.

There doesn't seem to be any consistency about the use of definition 1 or
2.  I think a definition 1 "read barrier" has very limited utility, but
some architectures provide it.  ARMv8's load barrier is type 2, but ARM's
store barrier is the analog of type 1, i.e. it orders only stores.

Hans

On Wed, Nov 19, 2014 at 11:12 AM, Martin Buchholz <martinrb at google.com>
wrote:

> I'm struggling to educate myself about memory barriers, and I
> discovered both Paul McKenney's awesome book and those newfangled
> Unsafe fences in OpenJDK.  Paul's book says
>
> """A read barrier is a partial ordering on loads only; it is not
> required to have any effect on stores."""
>
> Meanwhile,  Unsafe.loadFence doc says,
>
> """Ensures lack of reordering of loads before the fence with loads or
> stores after the fence."""
>
> Spelunking in the hotspot sources, Unsafe.loadFence is implemented via
> LoadFenceNode, which says:
>
> """"Acquire" - no following ref can move before (but earlier refs can
> follow, like an early Load stalled in cache)"""
>
> So ... that's 3 different definitions?!  Further, the doc for
> LoadFenceNode seems non-sensical to me - if following refs (load or
> store) can cross the barrier backwards, how is that different in
> effect from allowing preceding refs to cross the barrier forwards?
> Both seem to allow any reordering at all?!  Should that have read
> """but earlier Stores (perhaps stalled in cache) can follow""" ?
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141119/b4981c49/attachment.html>

From yu.lin.86 at gmail.com  Wed Nov 19 20:03:54 2014
From: yu.lin.86 at gmail.com (Yu Lin)
Date: Wed, 19 Nov 2014 19:03:54 -0600
Subject: [concurrency-interest] CompletableFuture in Java 8
Message-ID: <CAAL-3PanDAubT2RM-DePuUv_LmSX2=OsDcy82Q+ETBaRuvEwEw@mail.gmail.com>

Hi all,

I'm a Ph.D. student at University of Illinois. I'm doing research on the
uses of Java concurrent/asynchronous tasks. A new construct introduced in
Java 8, CompletableFuture, seems to be a very nice and easy to use
concurrent task.

I'm looking for projects that use CompletableFuture. But since it's very
new, I didn't find any real-world projects use it (only found many toy
examples/demos).

Does anyone know any projects that use it?

Thanks,
Yu Lin
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141119/27e1c223/attachment-0001.html>

From davidcholmes at aapt.net.au  Wed Nov 19 20:37:50 2014
From: davidcholmes at aapt.net.au (David Holmes)
Date: Thu, 20 Nov 2014 11:37:50 +1000
Subject: [concurrency-interest] [jmm-dev] Will the real memory
	readbarrier please stand up?
In-Reply-To: <546CF5C6.3090703@oracle.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCAEOIKKAA.davidcholmes@aapt.net.au>

But also note that acquire/release can not be expressed in terms of loadload|loadStore|storeload|storestore. The former are more conceptual, while the latter tend to map to architectural definitions of "memory barriers".

"read barrier" with no context is a fairly meaningless term.

David

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Aleksey
> Shipilev
> Sent: Thursday, 20 November 2014 5:56 AM
> To: Martin Buchholz; concurrency-interest; Pedro Ramalhete; Andreia
> Craveiro Ramalhete; Paul McKenney; David Holmes; Brian Goetz
> Subject: Re: [concurrency-interest] [jmm-dev] Will the real memory
> readbarrier please stand up?
> 
> 
> Hi Martin,
> 
> I think your To: list is overly broad. Sending these questions to
> concurrency-interest@ alone is usually enough. Dropping jmm-dev@ since
> that list is about the future JMM update, and not really about the
> question you are asking.
> 
> On 19.11.2014 22:12, Martin Buchholz wrote:
> > I'm struggling to educate myself about memory barriers, and I
> > discovered both Paul McKenney's awesome book and those newfangled
> > Unsafe fences in OpenJDK.  Paul's book says
> > 
> > """A read barrier is a partial ordering on loads only; it is not
> > required to have any effect on stores."""
> 
> From this definition, this seems to sound like a [LoadLoad] fence from
> the JSR 133 Cookbook. Go and re-read the Cookbook for a fourth set of
> definitions there :) Be aware that the JMM ordering rules and the actual
> memory barriers, while related, are not matched exactly. JSR 133
> Cookbook is a minimal conservative interpretation of JMM rules. See e.g.
> here:
>  http://shipilev.net/blog/2014/on-the-fence-with-dependencies/#_theory
> 
> 
> > Meanwhile,  Unsafe.loadFence doc says,
> > 
> > """Ensures lack of reordering of loads before the fence with loads or
> > stores after the fence."""
> 
> Unsafe.loadFence in this parlance is [LoadLoad] + [LoadStore], destined
> to be used as in:
> 
>  <load>
>  [LoadLoad]
>  [LoadStore]
>  <other loads and stores>
> 
> This effectively turns the first load into a acquiring "volatile load".
> 
> 
> > Spelunking in the hotspot sources, Unsafe.loadFence is implemented via
> > LoadFenceNode, which says:
> > 
> > """"Acquire" - no following ref can move before (but earlier refs can
> > follow, like an early Load stalled in cache)"""
> 
> Well, volatile load has the acquire semantics, and the rest of the
> explanation alludes to that. Notice how [LoadLoad]+[LoadStore] prevents
> the following (subsequent) refs to move before the barrier.
> 
> Anyhow, I would not count HotSpot's interpretation of JSR 133 ruleset
> (that is in itself is an interpretation of JMM rules) as a good
> reference material.
> 
> 
> > So ... that's 3 different definitions?!  Further, the doc for
> > LoadFenceNode seems non-sensical to me - if following refs (load or
> > store) can cross the barrier backwards, how is that different in
> > effect from allowing preceding refs to cross the barrier forwards?
> > Both seem to allow any reordering at all?!  Should that have read
> > """but earlier Stores (perhaps stalled in cache) can follow""" ?
> 
> Yes, mentioning Stores explicitly there might be more clean for
> HotSpot's conservative case.
> 
> 
> Thanks,
> -Aleksey.
> 
> 



From boehm at acm.org  Wed Nov 19 21:15:20 2014
From: boehm at acm.org (Hans Boehm)
Date: Wed, 19 Nov 2014 18:15:20 -0800
Subject: [concurrency-interest] [jmm-dev] Will the real memory read
 barrier please stand up?
In-Reply-To: <546CF5C6.3090703@oracle.com>
References: <CA+kOe096db9QVmmbN=nkf=Due87etO1iaOxFdMkDP7Amf5c6zw@mail.gmail.com>
	<546CF5C6.3090703@oracle.com>
Message-ID: <CAPUmR1YEaJseBHfRw7S6CRc5C-d9pHXjW=jz4tGkZnQf_-MX5Q@mail.gmail.com>

On Wed, Nov 19, 2014 at 11:55 AM, Aleksey Shipilev <
aleksey.shipilev at oracle.com> wrote:
>
> Unsafe.loadFence in this parlance is [LoadLoad] + [LoadStore], destined
> to be used as in:
>
>  <load>
>  [LoadLoad]
>  [LoadStore]
>  <other loads and stores>
>
> This effectively turns the first load into a acquiring "volatile load".

... and a lot more.  It also orders every load preceding this code with
respect to <other loads and stores>, where a plain volatile load would not.

Hans
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141119/da8c02af/attachment.html>

From martinrb at google.com  Wed Nov 19 22:27:43 2014
From: martinrb at google.com (Martin Buchholz)
Date: Wed, 19 Nov 2014 19:27:43 -0800
Subject: [concurrency-interest] Will the real memory read barrier please
	stand up?
In-Reply-To: <CA+kOe096db9QVmmbN=nkf=Due87etO1iaOxFdMkDP7Amf5c6zw@mail.gmail.com>
References: <CA+kOe096db9QVmmbN=nkf=Due87etO1iaOxFdMkDP7Amf5c6zw@mail.gmail.com>
Message-ID: <CA+kOe0_tz_=a2DfXs7w6ByvVMeUhWr6MmEFdNrX0RMLfOu+PJw@mail.gmail.com>

Maybe enlightenment can be found at preshing.com
http://preshing.com/20130922/acquire-and-release-fences/

Is Unsafe.loadFence precisely C11 atomic_thread_fence(memory_order_acquire);
Is Unsafe.storeFence precisely C11 atomic_thread_fence(memory_order_release);

If so, it would be nice if it said so!

On Wed, Nov 19, 2014 at 11:12 AM, Martin Buchholz <martinrb at google.com> wrote:
> I'm struggling to educate myself about memory barriers, and I
> discovered both Paul McKenney's awesome book and those newfangled
> Unsafe fences in OpenJDK.  Paul's book says
>
> """A read barrier is a partial ordering on loads only; it is not
> required to have any effect on stores."""
>
> Meanwhile,  Unsafe.loadFence doc says,
>
> """Ensures lack of reordering of loads before the fence with loads or
> stores after the fence."""
>
> Spelunking in the hotspot sources, Unsafe.loadFence is implemented via
> LoadFenceNode, which says:
>
> """"Acquire" - no following ref can move before (but earlier refs can
> follow, like an early Load stalled in cache)"""
>
> So ... that's 3 different definitions?!  Further, the doc for
> LoadFenceNode seems non-sensical to me - if following refs (load or
> store) can cross the barrier backwards, how is that different in
> effect from allowing preceding refs to cross the barrier forwards?
> Both seem to allow any reordering at all?!  Should that have read
> """but earlier Stores (perhaps stalled in cache) can follow""" ?

From davidcholmes at aapt.net.au  Wed Nov 19 22:46:54 2014
From: davidcholmes at aapt.net.au (David Holmes)
Date: Thu, 20 Nov 2014 13:46:54 +1000
Subject: [concurrency-interest] Will the real memory read barrier
	pleasestand up?
In-Reply-To: <CA+kOe0_tz_=a2DfXs7w6ByvVMeUhWr6MmEFdNrX0RMLfOu+PJw@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCKEOJKKAA.davidcholmes@aapt.net.au>

Martin writes:
> Maybe enlightenment can be found at preshing.com
> http://preshing.com/20130922/acquire-and-release-fences/

Or maybe not. Those definitions are not consistent with acquire() and
release() semantics - with allow  the 'roach motel' reorderings (you can
move in but you can never move out) and can't be expressed in terms of
loadLoad etc.

I find acquire/release the most misused terms in the literature, especially
when joined with words like "barrier" or "fence", because everyone seems to
have their own defintions - which are similar to each other but not
identical. What are described at the link above are simply names given to a
combination of the more basic architectural barriers:

- acquire_fence == loadLoad | loadStore
- release_fence == LoadStore | StoreStore

David

> Is Unsafe.loadFence precisely C11
> atomic_thread_fence(memory_order_acquire);
> Is Unsafe.storeFence precisely C11
> atomic_thread_fence(memory_order_release);
>
> If so, it would be nice if it said so!
>
> On Wed, Nov 19, 2014 at 11:12 AM, Martin Buchholz
> <martinrb at google.com> wrote:
> > I'm struggling to educate myself about memory barriers, and I
> > discovered both Paul McKenney's awesome book and those newfangled
> > Unsafe fences in OpenJDK.  Paul's book says
> >
> > """A read barrier is a partial ordering on loads only; it is not
> > required to have any effect on stores."""
> >
> > Meanwhile,  Unsafe.loadFence doc says,
> >
> > """Ensures lack of reordering of loads before the fence with loads or
> > stores after the fence."""
> >
> > Spelunking in the hotspot sources, Unsafe.loadFence is implemented via
> > LoadFenceNode, which says:
> >
> > """"Acquire" - no following ref can move before (but earlier refs can
> > follow, like an early Load stalled in cache)"""
> >
> > So ... that's 3 different definitions?!  Further, the doc for
> > LoadFenceNode seems non-sensical to me - if following refs (load or
> > store) can cross the barrier backwards, how is that different in
> > effect from allowing preceding refs to cross the barrier forwards?
> > Both seem to allow any reordering at all?!  Should that have read
> > """but earlier Stores (perhaps stalled in cache) can follow""" ?
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From aleksey.shipilev at oracle.com  Thu Nov 20 03:39:00 2014
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Thu, 20 Nov 2014 11:39:00 +0300
Subject: [concurrency-interest] [jmm-dev] Will the real memory read
 barrier please stand up?
In-Reply-To: <CAPUmR1YEaJseBHfRw7S6CRc5C-d9pHXjW=jz4tGkZnQf_-MX5Q@mail.gmail.com>
References: <CA+kOe096db9QVmmbN=nkf=Due87etO1iaOxFdMkDP7Amf5c6zw@mail.gmail.com>	<546CF5C6.3090703@oracle.com>
	<CAPUmR1YEaJseBHfRw7S6CRc5C-d9pHXjW=jz4tGkZnQf_-MX5Q@mail.gmail.com>
Message-ID: <546DA8A4.40808@oracle.com>

On 20.11.2014 05:15, Hans Boehm wrote:
> On Wed, Nov 19, 2014 at 11:55 AM, Aleksey Shipilev
> <aleksey.shipilev at oracle.com <mailto:aleksey.shipilev at oracle.com>> wrote:
>>
>> Unsafe.loadFence in this parlance is [LoadLoad] + [LoadStore], destined
>> to be used as in:
>>
>>  <load>
>>  [LoadLoad]
>>  [LoadStore]
>>  <other loads and stores>
>>
>> This effectively turns the first load into a acquiring "volatile load".
> 
> ... and a lot more.  It also orders every load preceding this code with
> respect to <other loads and stores>, where a plain volatile load would not.

That is true. As I said, the mapping between JMM rules and the
Cookbook-style barriers is not exact. I agree that Cookbook rules do
much more in their conservative interpretation of JMM spec.

The three levels of abstraction: JMM rules, Cookbook rules + Unsafe
fences, and the concrete IR have some things in common, and may be
connected to each other through the conservative approach above. E.g.
you can cover the acquire/release rules with explicit barriers -- while
inaccurate and doing overkill (reordering-inhibiting) damage, it
connects both concepts to each other.

It would be a mistake to use these levels of abstraction interchangeably
-- I should have called that out -- but it's important to understand how
they are connected.

Thanks,
-Aleksey

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 819 bytes
Desc: OpenPGP digital signature
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141120/ff411c2b/attachment.bin>

From aph at redhat.com  Thu Nov 20 04:33:52 2014
From: aph at redhat.com (Andrew Haley)
Date: Thu, 20 Nov 2014 09:33:52 +0000
Subject: [concurrency-interest] [jmm-dev] Will the real memory read
	barrier please stand up?
In-Reply-To: <CAPUmR1YHLHss5+ONSUnaJQwOH-cTh16iAjikBaEekNJ7nHMVwA@mail.gmail.com>
References: <CA+kOe096db9QVmmbN=nkf=Due87etO1iaOxFdMkDP7Amf5c6zw@mail.gmail.com>
	<CAPUmR1YHLHss5+ONSUnaJQwOH-cTh16iAjikBaEekNJ7nHMVwA@mail.gmail.com>
Message-ID: <546DB580.9010602@redhat.com>

On 20/11/14 00:38, Hans Boehm wrote:
> There doesn't seem to be any consistency about the use of definition 1 or
> 2.  I think a definition 1 "read barrier" has very limited utility, but
> some architectures provide it.  ARMv8's load barrier is type 2, but ARM's
> store barrier is the analog of type 1, i.e. it orders only stores.

Are you saying that ARM's DMB LD is LoadLoad|LoadStore ?  Doug's JSR-133
Cookbook suggests otherwise.

Andrew.


From joe.bowbeer at gmail.com  Thu Nov 20 05:12:11 2014
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Thu, 20 Nov 2014 02:12:11 -0800
Subject: [concurrency-interest] [jmm-dev] Will the real memory read
 barrier please stand up?
In-Reply-To: <546DB580.9010602@redhat.com>
References: <CA+kOe096db9QVmmbN=nkf=Due87etO1iaOxFdMkDP7Amf5c6zw@mail.gmail.com>
	<CAPUmR1YHLHss5+ONSUnaJQwOH-cTh16iAjikBaEekNJ7nHMVwA@mail.gmail.com>
	<546DB580.9010602@redhat.com>
Message-ID: <CAHzJPEoHZQ3DzFE7_OVXSCdq8Jb9WkW9j6Xg8BRJoNHRUK5Wgg@mail.gmail.com>

As far as I can tell, the cookbook hasn't been updated yet for ARMv8.  It
looks like the following might be helpful in updating the recipes.

http://www.cl.cam.ac.uk/~pes20/cpp/cpp0xmappings.html

On Thu, Nov 20, 2014 at 1:33 AM, Andrew Haley <aph at redhat.com> wrote:

> On 20/11/14 00:38, Hans Boehm wrote:
> > There doesn't seem to be any consistency about the use of definition 1 or
> > 2.  I think a definition 1 "read barrier" has very limited utility, but
> > some architectures provide it.  ARMv8's load barrier is type 2, but ARM's
> > store barrier is the analog of type 1, i.e. it orders only stores.
>
> Are you saying that ARM's DMB LD is LoadLoad|LoadStore ?  Doug's JSR-133
> Cookbook suggests otherwise.
>
> Andrew.
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141120/7cd66891/attachment-0001.html>

From aph at redhat.com  Thu Nov 20 05:25:17 2014
From: aph at redhat.com (Andrew Haley)
Date: Thu, 20 Nov 2014 10:25:17 +0000
Subject: [concurrency-interest] [jmm-dev] Will the real memory read
 barrier please stand up?
In-Reply-To: <CAHzJPEoHZQ3DzFE7_OVXSCdq8Jb9WkW9j6Xg8BRJoNHRUK5Wgg@mail.gmail.com>
References: <CA+kOe096db9QVmmbN=nkf=Due87etO1iaOxFdMkDP7Amf5c6zw@mail.gmail.com>	<CAPUmR1YHLHss5+ONSUnaJQwOH-cTh16iAjikBaEekNJ7nHMVwA@mail.gmail.com>	<546DB580.9010602@redhat.com>
	<CAHzJPEoHZQ3DzFE7_OVXSCdq8Jb9WkW9j6Xg8BRJoNHRUK5Wgg@mail.gmail.com>
Message-ID: <546DC18D.7020103@redhat.com>

On 20/11/14 10:12, Joe Bowbeer wrote:
> As far as I can tell, the cookbook hasn't been updated yet for ARMv8.  It
> looks like the following might be helpful in updating the recipes.
> 
> http://www.cl.cam.ac.uk/~pes20/cpp/cpp0xmappings.html

Yes, it looks like the DMB LD is now stronger if, that page is right.

It'd be nice if someone could point me at the language in the
ARMv8 spec that this derives from.

Andrew.


From vitalyd at gmail.com  Thu Nov 20 10:07:03 2014
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Thu, 20 Nov 2014 10:07:03 -0500
Subject: [concurrency-interest] [jmm-dev] Will the real memory read
 barrier please stand up?
In-Reply-To: <546DC18D.7020103@redhat.com>
References: <CA+kOe096db9QVmmbN=nkf=Due87etO1iaOxFdMkDP7Amf5c6zw@mail.gmail.com>
	<CAPUmR1YHLHss5+ONSUnaJQwOH-cTh16iAjikBaEekNJ7nHMVwA@mail.gmail.com>
	<546DB580.9010602@redhat.com>
	<CAHzJPEoHZQ3DzFE7_OVXSCdq8Jb9WkW9j6Xg8BRJoNHRUK5Wgg@mail.gmail.com>
	<546DC18D.7020103@redhat.com>
Message-ID: <CAHjP37Fn6=FgvuCSBoh9aVqhndWvxC8=n_V6-AZKzFC8Yve7UQ@mail.gmail.com>

Quick google yielded this document, although can't vouch for the source
hosting it:
http://115.28.165.193/down/arm/arch/ARM_v8_Instruction_Set_Architecture_(Overview).pdf

That also shows DMB LD having Load-Load/Load-Store semantics.

On Thu, Nov 20, 2014 at 5:25 AM, Andrew Haley <aph at redhat.com> wrote:

> On 20/11/14 10:12, Joe Bowbeer wrote:
> > As far as I can tell, the cookbook hasn't been updated yet for ARMv8.  It
> > looks like the following might be helpful in updating the recipes.
> >
> > http://www.cl.cam.ac.uk/~pes20/cpp/cpp0xmappings.html
>
> Yes, it looks like the DMB LD is now stronger if, that page is right.
>
> It'd be nice if someone could point me at the language in the
> ARMv8 spec that this derives from.
>
> Andrew.
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141120/60644499/attachment.html>

From alex.yarmula at gmail.com  Thu Nov 20 10:40:51 2014
From: alex.yarmula at gmail.com (Alex Yarmula)
Date: Thu, 20 Nov 2014 07:40:51 -0800
Subject: [concurrency-interest] CompletableFuture in Java 8
In-Reply-To: <CAAL-3PanDAubT2RM-DePuUv_LmSX2=OsDcy82Q+ETBaRuvEwEw@mail.gmail.com>
References: <CAAL-3PanDAubT2RM-DePuUv_LmSX2=OsDcy82Q+ETBaRuvEwEw@mail.gmail.com>
Message-ID: <CACSGQn_+TTKWSacQ1svLw_TgrmvJZcCoJ4uUj-wpfSRFD10qdQ@mail.gmail.com>

You could do a search on GitHub for usages, e.g.
https://github.com/search?utf8=%E2%9C%93&q=import+java.util.concurrent.CompletableFuture&type=Code&ref=searchresults
.

On Wed, Nov 19, 2014 at 5:03 PM, Yu Lin <yu.lin.86 at gmail.com> wrote:

> Hi all,
>
> I'm a Ph.D. student at University of Illinois. I'm doing research on the
> uses of Java concurrent/asynchronous tasks. A new construct introduced in
> Java 8, CompletableFuture, seems to be a very nice and easy to use
> concurrent task.
>
> I'm looking for projects that use CompletableFuture. But since it's very
> new, I didn't find any real-world projects use it (only found many toy
> examples/demos).
>
> Does anyone know any projects that use it?
>
> Thanks,
> Yu Lin
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141120/e56164ee/attachment.html>

From stephan.diestelhorst at gmail.com  Thu Nov 20 16:09:36 2014
From: stephan.diestelhorst at gmail.com (Stephan Diestelhorst)
Date: Thu, 20 Nov 2014 21:09:36 +0000
Subject: [concurrency-interest] Transact 2015 Call for Papers
Message-ID: <1924307.ZgXa44CMzo@d-allen>

[Our apologies if you receive multiple copies of this announcement.]

----------------------------------------------
Transact 2015 Call for Papers
----------------------------------------------

10th ACM SIGPLAN Workshop on Transactional Computing (Transact 2015)
15-16 June 2015 
Portland, Oregon, USA 
http://transact2015.cse.lehigh.edu/

This year, Transact will be part of the Federated Computing Research Conference (FCRC).
General FCRC information is available at http://fcrc.acm.org.
Also note Transact will two days this year rather than just one.

------------------------------

Important Dates:

Submission Deadline: 19 February 2015 (Thursday)
Author Notification: 24 April 2015 (Friday)
Workshop: 15-16 June 2015 (Monday-Tuesday)

------------------------------

Overview

The past decade has seen an explosion of interest in programming languages, systems, and hardware to support transactions, speculation, and related alternatives to classical lock-based concurrency. Recently, transactional memory has crossed two new thresholds. First, IBM and Intel are now shipping processors with hardware support for transactional memory. Second, the C++ Standard Committee has begun investigation into transactional memory as a new language feature. These developments highlight the demand for continued high quality TM research.

Transact 2015 will provide a forum to present and discuss the latest research on all aspects of transactional computing. The tenth in the series, it will extend over two days (rather than the usual one) during the Federated Computing Research Conference (FCRC).  The scope of the workshop is intentionally broad, with the goal of encouraging interaction across the languages, architecture, systems, database, and theory communities.  Papers may address implementation techniques, foundational results, applications and workloads, or experience with working systems.  Environments of interest include the full range from multithreaded or multicore processors to high-end parallel computing.

Topics

Transact seeks papers on topics related to all areas of software and hardware for transactional computing. Specific topics of interest include but are not limited to:

	? Run-time systems
	? Hardware support
	? Applications, workloads, and test suites
	? Experience reports
	? Language mechanisms and semantics
	? Memory models
	? Formal verification
	? Speculative concurrency
	? Conflict detection and contention management
	? Debugging and tools
	? Static analysis and compiler optimizations
	? Checkpointing and failure atomicity
	? Persistence and I/O
	? Nesting and exceptions

Papers should present original research. As transactional memory spans many disciplines, papers should provide sufficient background material to make them accessible to the broader community. Papers focused on foundations should indicate how the work can be used to advance practice; papers on experiences and applications should indicate how the experiments reinforce or reflect principles.

Submissions

Papers must be submitted in PDF, and be no more than 8 pages in standard two-column SIGPLAN conference format including figures and tables but not including references.  Shorter submissions are welcome.  Submissions will be judged based on the merit of the ideas rather than the length.  Submissions must be made through the on-line submission site. Final papers will be available to participants electronically at the meeting, but to facilitate resubmission to more formal venues, no archival proceedings will be published, and papers will not be sent to the ACM Digital Library.

Authors will have the option of having their final paper accessible from the workshop website.  Authors must be familiar with and abide by SIGPLAN's republication policy, which forbids simultaneous submission to multiple venues and requires disclosing prior publication of closely related work.  At the discretion of the program committee and with the consent of the authors, particularly worthy papers may be recommended for a special journal issue.

--------------------------------------

Program Committee

Cristiana Amza, University of Toronto
Annette Bieniusa, Universitat Kaiserslautern
Luke Dalessandro, Indiana University
Dave Dice, Oracle Labs
Stephan Diestelhorst, ARM
Pascal Felber, Universite de Neuchatel
Justin Gottschlich, Intel Labs
Victor Luchangco, Oracle Labs (chair)
Alessia Milani, Bordeaux Institute of Technology
Binoy Ravindran, Virginia Tech
Torvald Riegel, Red Hat
Paolo Romano, University of Lisbon
Michael Scott, University of Rochester
Michael Spear, Lehigh University
Osman Unsal, BSC-Microsoft Research Centre

------------------------------------------

General Chair
Justin Gottschlich, Intel Labs

Program Chair
Victor Luchangco, Oracle Labs

Web Chair
Michael Spear, Lehigh University

Steering Committee

Pascal Felber, University de Neuchatel
Justin Gottschlich, Intel Labs
Dan Grossman, University of Washington
Rachid Guerraoui, EPFL
Tim Harris, Oracle Labs
Maurice Herlihy, Brown University
Eliot Moss, UMass
Jan Vitek, Purdue University
Michael Scott, University of Rochester
Tatiana Shpeisman, Intel Labs
Michael Spear, Lehigh University

-- 
You received this message because you are subscribed to the Google Groups "TM & Languages" group.
To unsubscribe from this group and stop receiving emails from it, send an email to tm-languages+unsubscribe at googlegroups.com.
To post to this group, send email to tm-languages at googlegroups.com.
Visit this group at http://groups.google.com/group/tm-languages.
For more options, visit https://groups.google.com/d/optout.


From yu.lin.86 at gmail.com  Thu Nov 20 18:47:31 2014
From: yu.lin.86 at gmail.com (Yu Lin)
Date: Thu, 20 Nov 2014 17:47:31 -0600
Subject: [concurrency-interest] CompletableFuture in Java 8
In-Reply-To: <CACSGQn_+TTKWSacQ1svLw_TgrmvJZcCoJ4uUj-wpfSRFD10qdQ@mail.gmail.com>
References: <CAAL-3PanDAubT2RM-DePuUv_LmSX2=OsDcy82Q+ETBaRuvEwEw@mail.gmail.com>
	<CACSGQn_+TTKWSacQ1svLw_TgrmvJZcCoJ4uUj-wpfSRFD10qdQ@mail.gmail.com>
Message-ID: <CAAL-3PaTKUt-R1gDkpGpz=_d=GVujkwRgCynKJyr2wFtRbyKyA@mail.gmail.com>

Thanks Alex. Yes, I searched on Github and looked at several pages, but
most of them are examples/tests/demos. I'm wondering if there're any
large-scale projects (at least thousands lines of code) use it.

Feel free to let me know if you know any, and I appreciate that. I'll also
search more on Github.

Yu

On Thu, Nov 20, 2014 at 9:40 AM, Alex Yarmula <alex.yarmula at gmail.com>
wrote:

> You could do a search on GitHub for usages, e.g.
> https://github.com/search?utf8=%E2%9C%93&q=import+java.util.concurrent.CompletableFuture&type=Code&ref=searchresults
> .
>
> On Wed, Nov 19, 2014 at 5:03 PM, Yu Lin <yu.lin.86 at gmail.com> wrote:
>
>> Hi all,
>>
>> I'm a Ph.D. student at University of Illinois. I'm doing research on the
>> uses of Java concurrent/asynchronous tasks. A new construct introduced in
>> Java 8, CompletableFuture, seems to be a very nice and easy to use
>> concurrent task.
>>
>> I'm looking for projects that use CompletableFuture. But since it's very
>> new, I didn't find any real-world projects use it (only found many toy
>> examples/demos).
>>
>> Does anyone know any projects that use it?
>>
>> Thanks,
>> Yu Lin
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141120/258ea594/attachment.html>

From benjamin.sieffert at metrigo.de  Fri Nov 21 01:01:15 2014
From: benjamin.sieffert at metrigo.de (Benjamin Sieffert)
Date: Fri, 21 Nov 2014 07:01:15 +0100
Subject: [concurrency-interest] CompletableFuture in Java 8
In-Reply-To: <CAAL-3PaTKUt-R1gDkpGpz=_d=GVujkwRgCynKJyr2wFtRbyKyA@mail.gmail.com>
References: <CAAL-3PanDAubT2RM-DePuUv_LmSX2=OsDcy82Q+ETBaRuvEwEw@mail.gmail.com>
	<CACSGQn_+TTKWSacQ1svLw_TgrmvJZcCoJ4uUj-wpfSRFD10qdQ@mail.gmail.com>
	<CAAL-3PaTKUt-R1gDkpGpz=_d=GVujkwRgCynKJyr2wFtRbyKyA@mail.gmail.com>
Message-ID: <CAJr8_qPnTXyWG5mNtNTT6-3f9j=97PJ=jrUcpsRe9Ut6By5R9Q@mail.gmail.com>

Not sure if this information is of much use to you, but all the projects
wanting CompletableFuture-like functionality have probably been using
com.google.common.util.concurrent.SettableFuture
in the past.
And since adaptation of a new Java release will only ever be so fast, it
might be a while until the first ones start to switch over. (A lot of
people are still running Java6 right now. If you're doing a serious
project, there's a lot of users you lock out by only running on the latest
Java.)

Still hope this helped a bit

On 21 November 2014 00:47, Yu Lin <yu.lin.86 at gmail.com> wrote:

> Thanks Alex. Yes, I searched on Github and looked at several pages, but
> most of them are examples/tests/demos. I'm wondering if there're any
> large-scale projects (at least thousands lines of code) use it.
>
> Feel free to let me know if you know any, and I appreciate that. I'll also
> search more on Github.
>
> Yu
>
> On Thu, Nov 20, 2014 at 9:40 AM, Alex Yarmula <alex.yarmula at gmail.com>
> wrote:
>
>> You could do a search on GitHub for usages, e.g.
>> https://github.com/search?utf8=%E2%9C%93&q=import+java.util.concurrent.CompletableFuture&type=Code&ref=searchresults
>> .
>>
>> On Wed, Nov 19, 2014 at 5:03 PM, Yu Lin <yu.lin.86 at gmail.com> wrote:
>>
>>> Hi all,
>>>
>>> I'm a Ph.D. student at University of Illinois. I'm doing research on the
>>> uses of Java concurrent/asynchronous tasks. A new construct introduced in
>>> Java 8, CompletableFuture, seems to be a very nice and easy to use
>>> concurrent task.
>>>
>>> I'm looking for projects that use CompletableFuture. But since it's very
>>> new, I didn't find any real-world projects use it (only found many toy
>>> examples/demos).
>>>
>>> Does anyone know any projects that use it?
>>>
>>> Thanks,
>>> Yu Lin
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


-- 
Benjamin Sieffert
metrigo GmbH
Sternstr. 106
20357 Hamburg

Gesch?ftsf?hrer: Christian M?ller, Tobias Schlottke, Philipp Westermeyer,
Martin Rie?
Die Gesellschaft ist eingetragen beim Registergericht Hamburg
Nr. HRB 120447.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141121/5b1e8e1f/attachment.html>

From martinrb at google.com  Fri Nov 21 13:05:10 2014
From: martinrb at google.com (Martin Buchholz)
Date: Fri, 21 Nov 2014 10:05:10 -0800
Subject: [concurrency-interest] CompletableFuture in Java 8
In-Reply-To: <CAJr8_qPnTXyWG5mNtNTT6-3f9j=97PJ=jrUcpsRe9Ut6By5R9Q@mail.gmail.com>
References: <CAAL-3PanDAubT2RM-DePuUv_LmSX2=OsDcy82Q+ETBaRuvEwEw@mail.gmail.com>
	<CACSGQn_+TTKWSacQ1svLw_TgrmvJZcCoJ4uUj-wpfSRFD10qdQ@mail.gmail.com>
	<CAAL-3PaTKUt-R1gDkpGpz=_d=GVujkwRgCynKJyr2wFtRbyKyA@mail.gmail.com>
	<CAJr8_qPnTXyWG5mNtNTT6-3f9j=97PJ=jrUcpsRe9Ut6By5R9Q@mail.gmail.com>
Message-ID: <CA+kOe09yKbo-icQV=vfAnSKtrWpw6GtiD4hF0eLvpu8aXhqaQQ@mail.gmail.com>

Right.  CompletableFuture is common low-level well-tested machinery
that will hopefully be used by other higher level frameworks and
language runtimes over the course of years.

On Thu, Nov 20, 2014 at 10:01 PM, Benjamin Sieffert
<benjamin.sieffert at metrigo.de> wrote:
> Not sure if this information is of much use to you, but all the projects
> wanting CompletableFuture-like functionality have probably been using
> com.google.common.util.concurrent.SettableFuture in the past.
> And since adaptation of a new Java release will only ever be so fast, it
> might be a while until the first ones start to switch over. (A lot of people
> are still running Java6 right now. If you're doing a serious project,
> there's a lot of users you lock out by only running on the latest Java.)
>
> Still hope this helped a bit
>
> On 21 November 2014 00:47, Yu Lin <yu.lin.86 at gmail.com> wrote:
>>
>> Thanks Alex. Yes, I searched on Github and looked at several pages, but
>> most of them are examples/tests/demos. I'm wondering if there're any
>> large-scale projects (at least thousands lines of code) use it.
>>
>> Feel free to let me know if you know any, and I appreciate that. I'll also
>> search more on Github.
>>
>> Yu
>>
>> On Thu, Nov 20, 2014 at 9:40 AM, Alex Yarmula <alex.yarmula at gmail.com>
>> wrote:
>>>
>>> You could do a search on GitHub for usages, e.g.
>>> https://github.com/search?utf8=%E2%9C%93&q=import+java.util.concurrent.CompletableFuture&type=Code&ref=searchresults.
>>>
>>> On Wed, Nov 19, 2014 at 5:03 PM, Yu Lin <yu.lin.86 at gmail.com> wrote:
>>>>
>>>> Hi all,
>>>>
>>>> I'm a Ph.D. student at University of Illinois. I'm doing research on the
>>>> uses of Java concurrent/asynchronous tasks. A new construct introduced in
>>>> Java 8, CompletableFuture, seems to be a very nice and easy to use
>>>> concurrent task.
>>>>
>>>> I'm looking for projects that use CompletableFuture. But since it's very
>>>> new, I didn't find any real-world projects use it (only found many toy
>>>> examples/demos).
>>>>
>>>> Does anyone know any projects that use it?
>>>>
>>>> Thanks,
>>>> Yu Lin
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
>
> --
> Benjamin Sieffert
> metrigo GmbH
> Sternstr. 106
> 20357 Hamburg
>
> Gesch?ftsf?hrer: Christian M?ller, Tobias Schlottke, Philipp Westermeyer,
> Martin Rie?
> Die Gesellschaft ist eingetragen beim Registergericht Hamburg
> Nr. HRB 120447.
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From akarnokd at gmail.com  Fri Nov 21 14:35:29 2014
From: akarnokd at gmail.com (=?UTF-8?Q?D=C3=A1vid_Karnok?=)
Date: Fri, 21 Nov 2014 20:35:29 +0100
Subject: [concurrency-interest] CompletableFuture in Java 8
In-Reply-To: <CAAL-3PanDAubT2RM-DePuUv_LmSX2=OsDcy82Q+ETBaRuvEwEw@mail.gmail.com>
References: <CAAL-3PanDAubT2RM-DePuUv_LmSX2=OsDcy82Q+ETBaRuvEwEw@mail.gmail.com>
Message-ID: <CAAWwtm-evqwXxNsoMLsnm5PNU-n+=O1iR_ZcptfaJ60JHg9jbg@mail.gmail.com>

Hi. CompletableFuture is great, but as a Ph.D R&D student myself, I see a
few shortcomings:- Slow adaptation of Java 8,- the feature lag in a well
known mobile platform and- the alternatives available as library for some
time.In addition, it may compose better than Future, but IMHO, it doesn't
go far enough. Therefore, I use a FOSS library (with increasing popularity)
that allows one to compose asynchronous data streams with much less worry
on concurrency and continuation than CF. I use it in 50k loc projects, but
I read/heard a well known company is switching almost all of its software
stack to the reactive/asynchronous programming idiom this library enables.
Excellent material is available (including videos and conference talk) on
this subject (and the library).
2014.11.20. 2:45 ezt ?rta ("Yu Lin" <yu.lin.86 at gmail.com>):

> Hi all,
>
> I'm a Ph.D. student at University of Illinois. I'm doing research on the
> uses of Java concurrent/asynchronous tasks. A new construct introduced in
> Java 8, CompletableFuture, seems to be a very nice and easy to use
> concurrent task.
>
> I'm looking for projects that use CompletableFuture. But since it's very
> new, I didn't find any real-world projects use it (only found many toy
> examples/demos).
>
> Does anyone know any projects that use it?
>
> Thanks,
> Yu Lin
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141121/13e61fe5/attachment.html>

From yu.lin.86 at gmail.com  Fri Nov 21 17:29:50 2014
From: yu.lin.86 at gmail.com (Yu Lin)
Date: Fri, 21 Nov 2014 16:29:50 -0600
Subject: [concurrency-interest] CompletableFuture in Java 8
In-Reply-To: <CAAWwtm-evqwXxNsoMLsnm5PNU-n+=O1iR_ZcptfaJ60JHg9jbg@mail.gmail.com>
References: <CAAL-3PanDAubT2RM-DePuUv_LmSX2=OsDcy82Q+ETBaRuvEwEw@mail.gmail.com>
	<CAAWwtm-evqwXxNsoMLsnm5PNU-n+=O1iR_ZcptfaJ60JHg9jbg@mail.gmail.com>
Message-ID: <CAAL-3PaZDBmjQ7i43+VUeW+OGh+6VAcA3rp9mzmetJar2k_aZw@mail.gmail.com>

OK, thanks for all your detailed analysis. I guess the slow adaption of a
new version is why I can't find the use of it. I myself is using Java 6?
Other similar libraries you mentioned seem to be interesting.

On Fri, Nov 21, 2014 at 1:35 PM, D?vid Karnok <akarnokd at gmail.com> wrote:

> Hi. CompletableFuture is great, but as a Ph.D R&D student myself, I see a
> few shortcomings:- Slow adaptation of Java 8,- the feature lag in a well
> known mobile platform and- the alternatives available as library for some
> time.In addition, it may compose better than Future, but IMHO, it doesn't
> go far enough. Therefore, I use a FOSS library (with increasing popularity)
> that allows one to compose asynchronous data streams with much less worry
> on concurrency and continuation than CF. I use it in 50k loc projects, but
> I read/heard a well known company is switching almost all of its software
> stack to the reactive/asynchronous programming idiom this library enables.
> Excellent material is available (including videos and conference talk) on
> this subject (and the library).
> 2014.11.20. 2:45 ezt ?rta ("Yu Lin" <yu.lin.86 at gmail.com>):
>
>> Hi all,
>>
>> I'm a Ph.D. student at University of Illinois. I'm doing research on the
>> uses of Java concurrent/asynchronous tasks. A new construct introduced in
>> Java 8, CompletableFuture, seems to be a very nice and easy to use
>> concurrent task.
>>
>> I'm looking for projects that use CompletableFuture. But since it's very
>> new, I didn't find any real-world projects use it (only found many toy
>> examples/demos).
>>
>> Does anyone know any projects that use it?
>>
>> Thanks,
>> Yu Lin
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141121/e66ebe67/attachment.html>

From radhakrishnan.mohan at gmail.com  Fri Nov 21 22:09:44 2014
From: radhakrishnan.mohan at gmail.com (Mohan Radhakrishnan)
Date: Sat, 22 Nov 2014 08:39:44 +0530
Subject: [concurrency-interest] CompletableFuture in Java 8
In-Reply-To: <CAAL-3PaZDBmjQ7i43+VUeW+OGh+6VAcA3rp9mzmetJar2k_aZw@mail.gmail.com>
References: <CAAL-3PanDAubT2RM-DePuUv_LmSX2=OsDcy82Q+ETBaRuvEwEw@mail.gmail.com>
	<CAAWwtm-evqwXxNsoMLsnm5PNU-n+=O1iR_ZcptfaJ60JHg9jbg@mail.gmail.com>
	<CAAL-3PaZDBmjQ7i43+VUeW+OGh+6VAcA3rp9mzmetJar2k_aZw@mail.gmail.com>
Message-ID: <CAOoXFP_0zq4eTzTezq88YgmEC32ubR2PxTWY0QegdS0ccs9bYQ@mail.gmail.com>

Hi,
           I don't know if David is using Netflix' RxJava but I found it
easier to use. One more book on concurreny is required to cover the latest
API so that programmers can understand it.


Thanks,
Mohan

On Sat, Nov 22, 2014 at 3:59 AM, Yu Lin <yu.lin.86 at gmail.com> wrote:

> OK, thanks for all your detailed analysis. I guess the slow adaption of a
> new version is why I can't find the use of it. I myself is using Java 6?
> Other similar libraries you mentioned seem to be interesting.
>
> On Fri, Nov 21, 2014 at 1:35 PM, D?vid Karnok <akarnokd at gmail.com> wrote:
>
>> Hi. CompletableFuture is great, but as a Ph.D R&D student myself, I see a
>> few shortcomings:- Slow adaptation of Java 8,- the feature lag in a well
>> known mobile platform and- the alternatives available as library for some
>> time.In addition, it may compose better than Future, but IMHO, it doesn't
>> go far enough. Therefore, I use a FOSS library (with increasing popularity)
>> that allows one to compose asynchronous data streams with much less worry
>> on concurrency and continuation than CF. I use it in 50k loc projects, but
>> I read/heard a well known company is switching almost all of its software
>> stack to the reactive/asynchronous programming idiom this library enables.
>> Excellent material is available (including videos and conference talk) on
>> this subject (and the library).
>> 2014.11.20. 2:45 ezt ?rta ("Yu Lin" <yu.lin.86 at gmail.com>):
>>
>>> Hi all,
>>>
>>> I'm a Ph.D. student at University of Illinois. I'm doing research on the
>>> uses of Java concurrent/asynchronous tasks. A new construct introduced in
>>> Java 8, CompletableFuture, seems to be a very nice and easy to use
>>> concurrent task.
>>>
>>> I'm looking for projects that use CompletableFuture. But since it's very
>>> new, I didn't find any real-world projects use it (only found many toy
>>> examples/demos).
>>>
>>> Does anyone know any projects that use it?
>>>
>>> Thanks,
>>> Yu Lin
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141122/5bba27d7/attachment-0001.html>

From dl at cs.oswego.edu  Sun Nov 23 18:58:50 2014
From: dl at cs.oswego.edu (Doug Lea)
Date: Sun, 23 Nov 2014 18:58:50 -0500
Subject: [concurrency-interest] [jmm-dev] Will the real memory read
	barrier please stand up?
In-Reply-To: <CA+kOe096db9QVmmbN=nkf=Due87etO1iaOxFdMkDP7Amf5c6zw@mail.gmail.com>
References: <CA+kOe096db9QVmmbN=nkf=Due87etO1iaOxFdMkDP7Amf5c6zw@mail.gmail.com>
Message-ID: <547274BA.3060201@cs.oswego.edu>


On 11/19/2014 02:12 PM, Martin Buchholz wrote:
> I'm struggling to educate myself about memory barriers,..

Catching up after being diverted with other things...
here are a few minor footnotes to other replies:

Terminology is not very standard. To maximize clarity,
I use "fence" (vs "barrier") to reduce confusion vs
garbage-collection barriers.
And "load" (vs "read") to reduce confusion vs IO.
Similarly for "store" vs "write".
And when applicable, Sparc-ese beforeAfter designations:
LoadLoad, LoadStore, StoreStore, StoreLoad and their
combinations. (Although there can be fence types that
don't nicely fall into any of these categories.)

>
> """A read barrier is a partial ordering on loads only; it is not
> required to have any effect on stores."""


> Meanwhile,  Unsafe.loadFence doc says,
>
> """Ensures lack of reordering of loads before the fence with loads or
> stores after the fence."""

This is a [LoadLoad|LoadStore] fence, also known as
a "load acquire" fence, which is sometimes (including
inside hotspot) just called a "load fence" because it is the
only kind of Load* fence that is ever wanted. The hotspot
intrinsic name name was chosen to match this existing
internal convention, at the expense of external confusion.
(It's not the only such case with intrinsics.)

>
> Spelunking in the hotspot sources, Unsafe.loadFence is implemented via
> LoadFenceNode, which says:
>
> [...confusing things...]

I think most of the internal docs/comments remain from
pre-JDK5. It might be a good idea to update them.

-Doug




From dl at cs.oswego.edu  Sun Nov 23 19:09:54 2014
From: dl at cs.oswego.edu (Doug Lea)
Date: Sun, 23 Nov 2014 19:09:54 -0500
Subject: [concurrency-interest] [jmm-dev] Will the real memory read
 barrier please stand up?
In-Reply-To: <546DC18D.7020103@redhat.com>
References: <CA+kOe096db9QVmmbN=nkf=Due87etO1iaOxFdMkDP7Amf5c6zw@mail.gmail.com>	<CAPUmR1YHLHss5+ONSUnaJQwOH-cTh16iAjikBaEekNJ7nHMVwA@mail.gmail.com>	<546DB580.9010602@redhat.com>	<CAHzJPEoHZQ3DzFE7_OVXSCdq8Jb9WkW9j6Xg8BRJoNHRUK5Wgg@mail.gmail.com>
	<546DC18D.7020103@redhat.com>
Message-ID: <54727752.5060504@cs.oswego.edu>

On 11/20/2014 05:25 AM, Andrew Haley wrote:
> On 20/11/14 10:12, Joe Bowbeer wrote:
>> As far as I can tell, the cookbook hasn't been updated yet for ARMv8.

Mainly out of conservatism given the history of revising ARM and POWER
entries multiple times over the years, sometimes based on incomplete or
wrong information. It seems worth waiting for multiple validations,
including experience from the ARMv8 hotspot port. This is cruel to Andrew
(sorry!) but still better than alternatives.

>> looks like the following might be helpful in updating the recipes.
>>
>> http://www.cl.cam.ac.uk/~pes20/cpp/cpp0xmappings.html
>
> Yes, it looks like the DMB LD is now stronger if, that page is right.
>
> It'd be nice if someone could point me at the language in the
> ARMv8 spec that this derives from.

I'm hoping that someone from ARM answers this.

-Doug


From gil at azulsystems.com  Mon Nov 24 00:54:47 2014
From: gil at azulsystems.com (Gil Tene)
Date: Mon, 24 Nov 2014 05:54:47 +0000
Subject: [concurrency-interest] A new (?) concurrency primitive:
	WriterReaderPhaser
Message-ID: <730432BA-BCC5-4BC4-8F2B-1DD9D1518536@azulsystems.com>

Yeah, Yeah, I know. A new concurrency primitive? Really?

But I think this may actually be a new, generically useful primitive.

Basically, if you ever needed to analyze or log rapidly mutating data without blocking or locking out writers, this thing is for you. It supports wait-free writers, and stable readable data sets for guaranteed-forward-progress readers. And it makes double buffered data management semi-trivial.

See blog entry explaining stuff : "WriterReaderPhaser: A story about a new (?) synchronization primitive"<http://stuff-gil-says.blogspot.com/2014/11/writerreaderphaser-story-about-new.html>. (with some interesting discussion comparing it to Left-Right, which does the opposite thing: wait free readers with blocking writers).

See a simple (and very practical) example of using the primitive at: https://github.com/HdrHistogram/HdrHistogram/blob/master/src/main/java/org/HdrHistogram/IntervalHistogramRecorder.java

And see the primitive qualities and use rules documented (in the JavaDoc) along with a working implementation at: https://github.com/HdrHistogram/HdrHistogram/blob/master/src/main/java/org/HdrHistogram/WriterReaderPhaser.java

So please rip this thing apart? Or consider if it may be a useful addition to j.u.c. It needs a home.

And if you've seen it before (i.e. it's not really new like I seem to think it is), I'd really like to know.

? Gil.

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141124/daadd082/attachment.html>

From aph at redhat.com  Mon Nov 24 04:33:23 2014
From: aph at redhat.com (Andrew Haley)
Date: Mon, 24 Nov 2014 09:33:23 +0000
Subject: [concurrency-interest] [jmm-dev] Will the real memory read
 barrier please stand up?
In-Reply-To: <54727752.5060504@cs.oswego.edu>
References: <CA+kOe096db9QVmmbN=nkf=Due87etO1iaOxFdMkDP7Amf5c6zw@mail.gmail.com>	<CAPUmR1YHLHss5+ONSUnaJQwOH-cTh16iAjikBaEekNJ7nHMVwA@mail.gmail.com>	<546DB580.9010602@redhat.com>	<CAHzJPEoHZQ3DzFE7_OVXSCdq8Jb9WkW9j6Xg8BRJoNHRUK5Wgg@mail.gmail.com>
	<546DC18D.7020103@redhat.com> <54727752.5060504@cs.oswego.edu>
Message-ID: <5472FB63.2020804@redhat.com>

On 24/11/14 00:09, Doug Lea wrote:
> On 11/20/2014 05:25 AM, Andrew Haley wrote:
>> On 20/11/14 10:12, Joe Bowbeer wrote:
>>> As far as I can tell, the cookbook hasn't been updated yet for ARMv8.
> 
> Mainly out of conservatism given the history of revising ARM and POWER
> entries multiple times over the years, sometimes based on incomplete or
> wrong information. It seems worth waiting for multiple validations,
> including experience from the ARMv8 hotspot port. This is cruel to Andrew
> (sorry!) but still better than alternatives.
> 
>>> looks like the following might be helpful in updating the recipes.
>>>
>>> http://www.cl.cam.ac.uk/~pes20/cpp/cpp0xmappings.html
>>
>> Yes, it looks like the DMB LD is now stronger if, that page is right.
>>
>> It'd be nice if someone could point me at the language in the
>> ARMv8 spec that this derives from.
> 
> I'm hoping that someone from ARM answers this.

I have communicated with ARM and (although it is implied in the the
ARMv8 Architecture Reference Manual in a roundabout way) they have
raised a ticket for clarification in a future update.  They agree that
Load-Load|Load-Store is correct.

In the meantime, there is a table of memory barriers on Page 105 of
the ARMv8 Instruction Set Overview which states this explicitly.

Andrew.

From peter.levart at gmail.com  Mon Nov 24 06:28:42 2014
From: peter.levart at gmail.com (Peter Levart)
Date: Mon, 24 Nov 2014 12:28:42 +0100
Subject: [concurrency-interest] A new (?) concurrency primitive:
	WriterReaderPhaser
In-Reply-To: <730432BA-BCC5-4BC4-8F2B-1DD9D1518536@azulsystems.com>
References: <730432BA-BCC5-4BC4-8F2B-1DD9D1518536@azulsystems.com>
Message-ID: <5473166A.7030904@gmail.com>

Hi Gil,

What a coincidence. I was thinking of writing something like that myself 
in past couple of days for a similar purpose. It comes as a gift that 
you posted this here, thanks!

My application is an asynchronous cache store implementation. A 
distributed cache (Coherence in my case) emits synchronous events when 
cache is updated from multiple threads. I want to batch updates and do 
asynchronous persistence in a background thread. Coherence already 
supports this by itself, but is rather limited in features, so I have to 
re-create this functionality and add missing features.

Regards, Peter

On 11/24/2014 06:54 AM, Gil Tene wrote:
> Yeah, Yeah, I know. A new concurrency primitive? Really?
>
> But I think this may actually be a new, generically useful primitive.
>
> Basically, if you ever needed to analyze or log rapidly mutating data without blocking or locking out writers, this thing is for you. It supports wait-free writers, and stable readable data sets for guaranteed-forward-progress readers. And it makes double buffered data management semi-trivial.
>
> See blog entry explaining stuff : "WriterReaderPhaser: A story about a new (?) synchronization primitive"<http://stuff-gil-says.blogspot.com/2014/11/writerreaderphaser-story-about-new.html>. (with some interesting discussion comparing it to Left-Right, which does the opposite thing: wait free readers with blocking writers).
>
> See a simple (and very practical) example of using the primitive at: https://github.com/HdrHistogram/HdrHistogram/blob/master/src/main/java/org/HdrHistogram/IntervalHistogramRecorder.java
>
> And see the primitive qualities and use rules documented (in the JavaDoc) along with a working implementation at: https://github.com/HdrHistogram/HdrHistogram/blob/master/src/main/java/org/HdrHistogram/WriterReaderPhaser.java
>
> So please rip this thing apart? Or consider if it may be a useful addition to j.u.c. It needs a home.
>
> And if you've seen it before (i.e. it's not really new like I seem to think it is), I'd really like to know.
>
> ? Gil.
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141124/02c4a17e/attachment.html>

From dl at cs.oswego.edu  Mon Nov 24 11:00:53 2014
From: dl at cs.oswego.edu (Doug Lea)
Date: Mon, 24 Nov 2014 11:00:53 -0500
Subject: [concurrency-interest] [jmm-dev] Will the real memory read
	barrier please stand up?
In-Reply-To: <3211150.DiDWp5U5A7@mymac-ubuntu>
References: <CA+kOe096db9QVmmbN=nkf=Due87etO1iaOxFdMkDP7Amf5c6zw@mail.gmail.com>
	<547274BA.3060201@cs.oswego.edu> <3211150.DiDWp5U5A7@mymac-ubuntu>
Message-ID: <54735635.9010707@cs.oswego.edu>


[Trimming off unneeded CCs, but since Aleksey's attempt to trim lists
didn't work, keeping both.]

On 11/24/2014 10:21 AM, Stephan Diestelhorst wrote:
> On Sunday 23 November 2014 23:58:50 Doug Lea wrote:
>> This is a [LoadLoad|LoadStore] fence, also known as
>> a "load acquire" fence, which is sometimes (including
>> inside hotspot) just called a "load fence" because it is the
>> only kind of Load* fence that is ever wanted.
>
> Doug, you are the second person to say that.  Is that just common wisdom

I guess you could call it common wisdom: I think that no current
language-level models have any cases of mappings requiring issue
of only one or the other, and no current processors have a fence
that provides LoadLoad but not LoadStore. (Although maybe some
pseudo-fences have this effect?)

-Doug




From aph at redhat.com  Mon Nov 24 11:35:50 2014
From: aph at redhat.com (Andrew Haley)
Date: Mon, 24 Nov 2014 16:35:50 +0000
Subject: [concurrency-interest] [jmm-dev] Will the real memory read
 barrier please stand up?
In-Reply-To: <8017617.Z7IkjnHZrM@mymac-ubuntu>
References: <CA+kOe096db9QVmmbN=nkf=Due87etO1iaOxFdMkDP7Amf5c6zw@mail.gmail.com>
	<54727752.5060504@cs.oswego.edu> <5472FB63.2020804@redhat.com>
	<8017617.Z7IkjnHZrM@mymac-ubuntu>
Message-ID: <54735E66.6070106@redhat.com>

On 11/24/2014 03:15 PM, Stephan Diestelhorst wrote:
>> I have communicated with ARM and (although it is implied in the the
>> > ARMv8 Architecture Reference Manual in a roundabout way) they have
>> > raised a ticket for clarification in a future update.
>
> Not sure if this is already what you had in mind or whether this is the
> "roundabout way", please let me know.  The only issue I have had with
> this is that the clarification for the restricted type happens pretty
> late. Until then one (I!) too assumed it would only be loads (due to the
> name / mnemonic type being "LD").

Quite.  two things:

Firstly, that section needs to say that the first is DMB, the second
is DMB ST, and the third is DMB LD.  It may be "obvious", but you
still have to say it.  And the page which defines DMB has to say it
too, or at least have a direct link to the page where the barrier
option names are defined.

Andrew.


From martinrb at google.com  Mon Nov 24 14:40:50 2014
From: martinrb at google.com (Martin Buchholz)
Date: Mon, 24 Nov 2014 11:40:50 -0800
Subject: [concurrency-interest] Increasing the throughput of
 ConcurrentLinkedQueue on PowerPC (and ARM?)
In-Reply-To: <CA+kOe0-mGZmkoC4t7VeWBS2y4nDethgOnCc10GQyCtcjvs=8LQ@mail.gmail.com>
References: <CAAApjO3j3g2fE-e2i5Yt-K8a8t2MBt3=zPrkNSCrCpGkwDEc2Q@mail.gmail.com>
	<54616343.5000708@cs.oswego.edu>
	<CA+kOe088JYmVw1AjANfzKL+sLbc6NFgLmUVj2uKji_-8NiwOQQ@mail.gmail.com>
	<CAAApjO2h9bNnsX4y4_ak78B0U2Xk4Y137NPYH69KrRdyMeziSw@mail.gmail.com>
	<CA+kOe0-mGZmkoC4t7VeWBS2y4nDethgOnCc10GQyCtcjvs=8LQ@mail.gmail.com>
Message-ID: <CA+kOe0-HjMM9ZexyBFT+dXa+dj2o6CQ9fMFyHdrqLhe6taiR-w@mail.gmail.com>

Yet another experimental change below that I believe to be correct,
now using Unsafe.loadFence, but am not intending to commit, unless we
see compelling performance improvements on non-x86.

Index: src/main/java/util/concurrent/ConcurrentLinkedQueue.java
===================================================================
RCS file: /export/home/jsr166/jsr166/jsr166/src/main/java/util/concurrent/ConcurrentLinkedQueue.java,v
retrieving revision 1.104
diff -u -r1.104 ConcurrentLinkedQueue.java
--- src/main/java/util/concurrent/ConcurrentLinkedQueue.java 23 Nov
2014 18:20:26 -0000 1.104
+++ src/main/java/util/concurrent/ConcurrentLinkedQueue.java 24 Nov
2014 19:32:11 -0000
@@ -314,7 +314,12 @@
         restartFromHead:
         for (;;) {
             for (Node<E> h = head, p = h, q;;) {
-                E item = p.item;
+                // Relaxed read suffices here;
+                // - the volatile read of p made item visible
+                // - the subsequent cas ensures we notice racing
removal of item
+                // - loadFence ensures consistency with concurrent removal
+                @SuppressWarnings("unchecked")
+                E item = (E) U.getObject(p, ITEM); // = p.item;

                 if (item != null && casItem(p, item, null)) {
                     // Successful CAS is the linearization point
@@ -325,6 +330,7 @@
                 }
                 else if ((q = p.next) == null) {
                     updateHead(h, p);
+                    U.loadFence();
                     return null;
                 }
                 else if (p == q)
@@ -429,11 +435,13 @@
     public boolean contains(Object o) {
         if (o != null) {
             for (Node<E> p = first(); p != null; p = succ(p)) {
-                E item = p.item;
+                @SuppressWarnings("unchecked")
+                E item = (E) U.getObject(p, ITEM); // = p.item;
                 if (item != null && o.equals(item))
                     return true;
             }
         }
+        U.loadFence();
         return false;
     }

@@ -452,7 +460,13 @@
         if (o != null) {
             Node<E> pred = null;
             for (Node<E> p = first(); p != null; p = succ(p)) {
-                E item = p.item;
+                // Relaxed read suffices here;
+                // - the volatile read of p made item visible
+                // - the subsequent cas ensures we notice racing
removal of item
+                // - loadFence ensures consistency with concurrent removal
+                @SuppressWarnings("unchecked")
+                E item = (E) U.getObject(p, ITEM); // = p.item;
+
                 if (item != null &&
                     o.equals(item) &&
                     casItem(p, item, null)) {
@@ -464,6 +478,7 @@
                 pred = p;
             }
         }
+        U.loadFence();
         return false;
     }

From pramalhe at gmail.com  Mon Nov 24 15:28:09 2014
From: pramalhe at gmail.com (Pedro Ramalhete)
Date: Mon, 24 Nov 2014 21:28:09 +0100
Subject: [concurrency-interest] A new (?) concurrency primitive:
	WriterReaderPhaser
Message-ID: <CAAApjO0RAC48TZsEjAoC0Pbk8hBpDrHPDfQYhQZEwdiJ9MfJwA@mail.gmail.com>

Hi Peter,

If I was you I wouldn't bother with it.
As I tried to explain to Gil, the WriterReaderPhaser uses the same
concurrency control algorithm as the Left-Right, and as such it is a
variant of the Left-Right (used "backwards") that uses a (new)
ReadIndicator with a single ingress combined with versionIndex.
This variant is not as good for scalability under high contention as the
one you yourself have implemented some time ago, with the ReadIndicator of
ingress/egress with LongAdders.
You're better off using your own implementation, and just do the mutations
in lrSet.read() and the read-only operation in lrSet.modify(), but of
course, feel free to try both and let us your results   ;)
http://cr.openjdk.java.net/~plevart/misc/LeftRight/EnterExitWait.java
http://cr.openjdk.java.net/~plevart/misc/LeftRight/LongAdderEEW.java
http://cr.openjdk.java.net/~plevart/misc/LeftRight/LRTest.java
http://concurrencyfreaks.com/2014/11/left-right-gt-variant.html

Cheers,
Pedro
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141124/d490d060/attachment.html>

From martinrb at google.com  Mon Nov 24 15:56:15 2014
From: martinrb at google.com (Martin Buchholz)
Date: Mon, 24 Nov 2014 12:56:15 -0800
Subject: [concurrency-interest] RFR: 8065804: JEP 171:
	Clarifications/corrections for fence intrinsics
Message-ID: <CA+kOe08RfnkJ4Dit8ng7HQaSuDer_pA0i4t568uAGMg9dzujhg@mail.gmail.com>

Hi folks,

Review carefully - I am trying to learn about fences by explaining them!
I have borrowed some wording from my reviewers!


https://bugs.openjdk.java.net/browse/JDK-8065804
http://cr.openjdk.java.net/~martin/webrevs/openjdk9/fence-intrinsics/

From aleksey.shipilev at oracle.com  Mon Nov 24 16:29:40 2014
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Tue, 25 Nov 2014 00:29:40 +0300
Subject: [concurrency-interest] RFR: 8065804: JEP 171:
	Clarifications/corrections for fence intrinsics
In-Reply-To: <CA+kOe08RfnkJ4Dit8ng7HQaSuDer_pA0i4t568uAGMg9dzujhg@mail.gmail.com>
References: <CA+kOe08RfnkJ4Dit8ng7HQaSuDer_pA0i4t568uAGMg9dzujhg@mail.gmail.com>
Message-ID: <5473A344.8090302@oracle.com>

Hi Martin,

On 11/24/2014 11:56 PM, Martin Buchholz wrote:
> Review carefully - I am trying to learn about fences by explaining them!
> I have borrowed some wording from my reviewers!
> 
> https://bugs.openjdk.java.net/browse/JDK-8065804
> http://cr.openjdk.java.net/~martin/webrevs/openjdk9/fence-intrinsics/

I think "implies the effect of C++11" is too strong wording. "related"
might be more appropriate.

See also comments here for connection with "volatiles":
 https://bugs.openjdk.java.net/browse/JDK-8038978

Take note the Hans' correction that fences generally imply more than
volatile load/store, but since you are listing the related things in the
docs, I think the "native" Java example is good to have.

-Aleksey.


-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 819 bytes
Desc: OpenPGP digital signature
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141125/f1f88d2a/attachment.bin>

From martinrb at google.com  Mon Nov 24 19:47:19 2014
From: martinrb at google.com (Martin Buchholz)
Date: Mon, 24 Nov 2014 16:47:19 -0800
Subject: [concurrency-interest] RFR: 8065804: JEP 171:
	Clarifications/corrections for fence intrinsics
In-Reply-To: <5473A344.8090302@oracle.com>
References: <CA+kOe08RfnkJ4Dit8ng7HQaSuDer_pA0i4t568uAGMg9dzujhg@mail.gmail.com>
	<5473A344.8090302@oracle.com>
Message-ID: <CA+kOe0-KiKKNM-LyYvjZiXOGMF7c4FsHifcDq-ZZ65Bb1J=LLA@mail.gmail.com>

OK, I worked in some wording for comparison with volatiles.
I believe you when you say that the semantics of the corresponding C++
fences are slightly different, but it's rather subtle - can we say
anything more than "closely related to"?

On Mon, Nov 24, 2014 at 1:29 PM, Aleksey Shipilev
<aleksey.shipilev at oracle.com> wrote:
> Hi Martin,
>
> On 11/24/2014 11:56 PM, Martin Buchholz wrote:
>> Review carefully - I am trying to learn about fences by explaining them!
>> I have borrowed some wording from my reviewers!
>>
>> https://bugs.openjdk.java.net/browse/JDK-8065804
>> http://cr.openjdk.java.net/~martin/webrevs/openjdk9/fence-intrinsics/
>
> I think "implies the effect of C++11" is too strong wording. "related"
> might be more appropriate.
>
> See also comments here for connection with "volatiles":
>  https://bugs.openjdk.java.net/browse/JDK-8038978
>
> Take note the Hans' correction that fences generally imply more than
> volatile load/store, but since you are listing the related things in the
> docs, I think the "native" Java example is good to have.
>
> -Aleksey.
>
>

From gil at azulsystems.com  Tue Nov 25 02:39:49 2014
From: gil at azulsystems.com (Gil Tene)
Date: Tue, 25 Nov 2014 07:39:49 +0000
Subject: [concurrency-interest] A new (?) concurrency
	primitive:	WriterReaderPhaser
In-Reply-To: <5473166A.7030904@gmail.com>
References: <730432BA-BCC5-4BC4-8F2B-1DD9D1518536@azulsystems.com>
	<5473166A.7030904@gmail.com>
Message-ID: <4D0323E3-8E8E-4867-B6DD-117BB48879A7@azulsystems.com>

Pedro, I think you are confusing specific under-the-hood implementation choices (which are similar) with what the primitive is. I'm flattered at your naming of Left-Right GT, but Left-Right GT (and the LRTreeSetGT example) is a Left-Right variant with a different underlying (attributed to me) arrive-depart technique. It is not a WriterReaderPhaser.

WriterReaderPhaser captures (in a clean synchronization primitive API form) a pattern I've had to use and re-invent myself several times, and I'm pretty sure many others that have faced the "I want to periodically report/analyze an actively updating data set" have too. The key here is capturing the guarantees and prescribed use such that end-users can use the primitive without needing to understand the underlying logic of an implementation. I do that in my blog entry (and include a definition and use example below).

A WriterReaderPhaser is neither a ReadWriteLock nor a ReadWriteLock used backwards. It's also not Left-Right, or Left-Right used backwards. The qualities and guarantees a WriterReaderPhaser provides are not provided by reversing the meaning of "writer" and "reader" in those primitives. Even if you ignore the confusion that such upside-down use may cause the user, there are specific guarantees that the other primitives do not provide, and that a write-heavy double-buffered use case needs and gets from this primitive.

And yes, there are several possible ways to implement a well behaving WriterReaderPhaser, one of which is listed in the links I provided. We can implement it with three atomic words and some clever combos of CAS and GetAndAdd ops, or in other ways. The implementation is not what makes the primitive what it is to it's users. It's the API and the behavior guarantees that do that. And I'm pretty sure these behavior guarantees are not spelled out or provided (even backwards) in Left-Right and variants. Some of them (like the data stability guarantee for readers even in the presence of wait-free write activity) would be un-natural to provide in reverse for writers (since readers are generally not expected to change stuff). 

Left-Right is cool (really cool), but it focuses purely on wait-free readers and blocking writers. While that use case may appear to be "the opposite" of wait-free writers with blocking readers, there are specific non-mirroring qualities that make that duality invalid. Here are specific differences between the two mechanisms that make "backwards" use inapplicable:: 

- WriterReaderPhaser provides specific data stability guarantees to readers (after a flip while under a readLock), even in the presence of concurrent writer activity. Left-Right does not provide such a guarantee to writers "backwards". E.g. if Left-Right readers happened to write into the Left-Right protected data structure (as they would need to in a "backwards use" attempt like this), Left-Right says nothing about what writers can expect from that data structure in terms of data consistency or stability. Note that I'm not saying that no Left-Right implementation could accidentally provide this behavior without stating it. I'm saying that the Left-Right mechanism, as described and documented in Left-Right paper and the various APIs for it's existing variants makes no such guarantee to the caller, and that a valid Left-Right implementation may or may not provide this behavior. As such, the user cannot rely on it. And this is the main guarantee a typical WriterReaderPhaser user will be looking for.

- Left-Right specifically prohibits readers from writing. E.g. "...To access in read-only mode do something like this:..." is stated in the documentation for a LeftRightGuard variants.  In contrast, WriterReaderPhaser allows it's writers (which would be the readers in a backwards mapping attempt) to, um, write...

- Writers that use Left-Right are required to to write their updates twice (on the two sides of a "writerToggle" or equivalent "flip"). e.g. "...The exact same operations must be done on the instance before and after guard.writeToggle()." is stated in the documentation for a LeftRightGuard variants. In contrast, WriterReaderPhaser does not require reading twice or writing twice. The two APIs do not mirror each other in this critical aspect.

- Left-Right (even when used to replace a Reader-Writer lock) manages the active and inactive data structures internally (leftInstance and rightInstance, or firstInstance and secondInstance), and users of Left-right [must] operate on data structures returned from Left-Right operations. In contrast, WriterReaderPhaser does not manage the active and inactive data structures in any way, leaving that job to readers and writers that operate directly on the shared data structures.

To be specific, let me detail what a WriterReaderPhaser is (taken from an updated blog entry that now includes a definition):

-----------------------------------------------------------------
Definition of WriterReaderPhaser:

A WriterReaderPhaser provides a means for wait free writers to coordinate with blocking (but guaranteed forward progress) readers sharing a set of data structures. 

A WriterReaderPhaser instance provides the following 5 operations:

	? writerCriticalSectionEnter
	? writerCriticalSectionExit
	? readerLock
	? readerUnlock
	? flipPhase

When a WriterReaderPhaser  instance is used to protect an active [set of or single] data structure involving [potentially multiple] writers and [potentially multiple] readers , the assumptions on how readers and writers act are:

	? There are two sets of data structures (an "active" set and an "inactive" set)
	? Writing is done to the perceived active version (as perceived by the writer), and only within critical sections delineated by writerCriticalSectionEnter and writerCriticalSectionExit operations.
	? Only readers switch the perceived roles of the active and inactive data structures. They do so only while holding the readerLock, and only before execution a flipPhase.

	? Readers do not hold onto readerLock indefinitely. Only readers perform readerLock and readerUnlock.
	? Writers do not remain in their critical sections indefinitely. Only writers perform writerCriticalSectionEnter and writerCriticalSectionExit.
	? Only readers perform flipPhase operations, and only while holding the readerLock.

When the above assumptions are met, WriterReaderPhaser guarantees that the inactive data structures are not being modified by any writers while being read while under readerLock protection after a flipPhase operation.

The following progress guarantees are provided to writers and readers that adhere to the above stated assumptions: 
	? Writers operations (writerCriticalSectionEnter and  writerCriticalSectionExit) are wait free (on architectures that support wait-free atomic increment operations).
	? flipPhase operations are guaranteed to make forward progress, and will only be blocked by writers whose critical sections were entered prior to the start of the reader's flipPhase operation, and have not yet exited their critical sections.
	? readerLock only block for other readers that are holding the readerLock.

-----------------------------------------------------------------
Example use:

Imagine a simple use case where a large set of rapidly updated counter is being modified by writers, and a reader needs to gain access to stable interval samples of those counters for reporting and other analysis purposes. 

The counters are represented in a volatile array of values (it is the array reference that is volatile, not the value cells within it):

volatile long counts[];
...

A writer updates a specific count (n) in the set of counters:

writerCriticalSectionEnter
   counts[n]++;
writerCriticalSectionExit

A reader gain access to a stable set of counts collected during an interval, reports on it, and accumulates it:

long intervalCounts[];
long accumulated_counts[];

...
readerLock
   reset(interval_counts);
   long tmp[] = counts;
   counts = interval_counts;
   interval_counts = tmp;
flipPhase
   report_interval_counts(interval_counts);
   accumulated_counts.add(interval_counts);
readerUnlock
-----------------------------------------------------------------


Bottom line: the above is defines what a WriterReaderPhaser primitive is, and shows a simple example of using it. While I provide an example implementation, many are possible, and I'm sure another 17 will pop up. To avoid wrongly taking credit for this as a new primitive, I'm looking to see if there have been previously described primitives that explicitly provide these (or equivalent) qualities to their users. "Explicitly" being a key word (since no sane user would rely on an accidental implicit behavior of a specific implementation of a primitive that does not actually guarantee the given behavior).

-- Gil.

> On Nov 24, 2014, at 3:28 PM, Pedro Ramalhete wrote:
> Hi Peter, If I was you I wouldn't bother with it. As I tried to explain to Gil, the WriterReaderPhaser uses the same concurrency control algorithm as the Left-Right, and as such it is a variant of the Left-Right (used "backwards") that uses a (new) ReadIndicator with a single ingress combined with versionIndex. This variant is not as good for scalability under high contention as the one you yourself have implemented some time ago, with the ReadIndicator of ingress/egress with LongAdders. You're better off using your own implementation, and just do the mutations in lrSet.read() and the read-only operation in lrSet.modify(), but of course, feel free to try both and let us your results ;) 
> http://cr.openjdk.java.net/~plevart/misc/LeftRight/EnterExitWait.java
> http://cr.openjdk.java.net/~plevart/misc/LeftRight/LongAdderEEW.java
> http://cr.openjdk.java.net/~plevart/misc/LeftRight/LRTest.java
> http://concurrencyfreaks.com/2014/11/left-right-gt-variant.html Cheers, Pedro
> 
>> On Nov 24, 2014, at 3:28 AM, Peter Levart wrote:
>> 
>> Hi Gil,
>> 
>> What a coincidence. I was thinking of writing something like that myself in past couple of days for a similar purpose. It comes as a gift that you posted this here, thanks!
>> 
>> My application is an asynchronous cache store implementation. A distributed cache (Coherence in my case) emits synchronous events when cache is updated from multiple threads. I want to batch updates and do asynchronous persistence in a background thread. Coherence already supports this by itself, but is rather limited in features, so I have to re-create this functionality and add missing features.
>> 
>> Regards, Peter
>> 
>> On 11/24/2014 06:54 AM, Gil Tene wrote:
>>> Yeah, Yeah, I know. A new concurrency primitive? Really? But I think this may actually be a new, generically useful primitive. Basically, if you ever needed to analyze or log rapidly mutating data without blocking or locking out writers, this thing is for you. It supports wait-free writers, and stable readable data sets for guaranteed-forward-progress readers. And it makes double buffered data management semi-trivial. See blog entry explaining stuff : "WriterReaderPhaser: A story about a new (?) synchronization primitive"<http://stuff-gil-says.blogspot.com/2014/11/writerreaderphaser-story-about-new.html>. (with some interesting discussion comparing it to Left-Right, which does the opposite thing: wait free readers with blocking writers). See a simple (and very practical) example of using the primitive at: https://github.com/HdrHistogram/HdrHistogram/blob/master/src/main/java/org/HdrHistogram/IntervalHistogramRecorder.java And see the primitive qualities and use rules documented (in the JavaDoc) along with a working implementation at: https://github.com/HdrHistogram/HdrHistogram/blob/master/src/main/java/org/HdrHistogram/WriterReaderPhaser.java So please rip this thing apart? Or consider if it may be a useful addition to j.u.c. It needs a home. And if you've seen it before (i.e. it's not really new like I seem to think it is), I'd really like to know. ? Gil. 
>>> _______________________________________________ Concurrency-interest mailing list 
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 


From paul.sandoz at oracle.com  Tue Nov 25 09:04:28 2014
From: paul.sandoz at oracle.com (Paul Sandoz)
Date: Tue, 25 Nov 2014 15:04:28 +0100
Subject: [concurrency-interest] RFR: 8065804: JEP 171:
	Clarifications/corrections for fence intrinsics
In-Reply-To: <CA+kOe0-KiKKNM-LyYvjZiXOGMF7c4FsHifcDq-ZZ65Bb1J=LLA@mail.gmail.com>
References: <CA+kOe08RfnkJ4Dit8ng7HQaSuDer_pA0i4t568uAGMg9dzujhg@mail.gmail.com>
	<5473A344.8090302@oracle.com>
	<CA+kOe0-KiKKNM-LyYvjZiXOGMF7c4FsHifcDq-ZZ65Bb1J=LLA@mail.gmail.com>
Message-ID: <B84AA20B-5714-4FF0-AB17-E918395C9DDB@oracle.com>

Hi Martin,

Thanks for looking into this.

1141      * Currently hotspot's implementation of a Java language-level volatile
1142      * store has the same effect as a storeFence followed by a relaxed store,
1143      * although that may be a little stronger than needed.

IIUC to emulate hotpot's volatile store you will need to say that a fullFence immediately follows the relaxed store.

The bit that always confuses me about release and acquire is ordering is restricted to one direction, as talked about in orderAccess.hpp [1]. So for a release, accesses prior to the release cannot move below it, but accesses succeeding the release can move above it. And that seems to apply to Unsafe.storeFence [2] (acting like a monitor exit). Is that contrary to C++ release fences where ordering is restricted both to prior and succeeding accesses? [3]

So what about the following?

  a = r1; // Cannot move below the fence
  Unsafe.storeFence();
  b = r2; // Can move above the fence?

Paul.

[1] In orderAccess.hpp
// Execution by a processor of release makes the effect of all memory
// accesses issued by it previous to the release visible to all
// processors *before* the release completes.  The effect of subsequent
// memory accesses issued by it *may* be made visible *before* the
// release.  I.e., subsequent memory accesses may float above the
// release, but prior ones may not float below it.

[2] In memnode.hpp
// "Release" - no earlier ref can move after (but later refs can move
// up, like a speculative pipelined cache-hitting Load).  Requires
// multi-cpu visibility.  Inserted independent of any store, as required
// for intrinsic sun.misc.Unsafe.storeFence().
class StoreFenceNode: public MemBarNode {
public:
  StoreFenceNode(Compile* C, int alias_idx, Node* precedent)
    : MemBarNode(C, alias_idx, precedent) {}
  virtual int Opcode() const;
};

[3] http://preshing.com/20131125/acquire-and-release-fences-dont-work-the-way-youd-expect/

On Nov 25, 2014, at 1:47 AM, Martin Buchholz <martinrb at google.com> wrote:

> OK, I worked in some wording for comparison with volatiles.
> I believe you when you say that the semantics of the corresponding C++
> fences are slightly different, but it's rather subtle - can we say
> anything more than "closely related to"?
> 
> On Mon, Nov 24, 2014 at 1:29 PM, Aleksey Shipilev
> <aleksey.shipilev at oracle.com> wrote:
>> Hi Martin,
>> 
>> On 11/24/2014 11:56 PM, Martin Buchholz wrote:
>>> Review carefully - I am trying to learn about fences by explaining them!
>>> I have borrowed some wording from my reviewers!
>>> 
>>> https://bugs.openjdk.java.net/browse/JDK-8065804
>>> http://cr.openjdk.java.net/~martin/webrevs/openjdk9/fence-intrinsics/
>> 
>> I think "implies the effect of C++11" is too strong wording. "related"
>> might be more appropriate.
>> 
>> See also comments here for connection with "volatiles":
>> https://bugs.openjdk.java.net/browse/JDK-8038978
>> 
>> Take note the Hans' correction that fences generally imply more than
>> volatile load/store, but since you are listing the related things in the
>> docs, I think the "native" Java example is good to have.
>> 
>> -Aleksey.
>> 
>> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 841 bytes
Desc: Message signed with OpenPGP using GPGMail
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141125/953aa163/attachment.bin>

From peter.levart at gmail.com  Tue Nov 25 11:21:44 2014
From: peter.levart at gmail.com (Peter Levart)
Date: Tue, 25 Nov 2014 17:21:44 +0100
Subject: [concurrency-interest] A new (?) concurrency primitive:
	WriterReaderPhaser
In-Reply-To: <4D0323E3-8E8E-4867-B6DD-117BB48879A7@azulsystems.com>
References: <730432BA-BCC5-4BC4-8F2B-1DD9D1518536@azulsystems.com>	<5473166A.7030904@gmail.com>
	<4D0323E3-8E8E-4867-B6DD-117BB48879A7@azulsystems.com>
Message-ID: <5474AC98.5020106@gmail.com>

Hi Gil,

I think Pedro is right in claiming that WriteReaderPhaser is a kind of 
Left-Right, but he's wrong in explaining that it is a Left-Right used 
backwards. In Left-Right terminology, WriteReaderPhaser is not 
protecting the mutable structure (mutated from multiple threads), but 
just coordinating access to the "active pointer" to the structure". From 
Left-Right perspective, multiple threads are just readers of the "active 
pointer" (what they do with the underlying structure is not relevant 
here - the underlying structure has it's own synchronization). The 
single thread (at a time) that wants to get access to the snapshot is 
the sole writer  (or "swapper") of the "active pointer". Of course, the 
sole writer or "swapper" of the active pointer also exploits the fact 
that the "inactive pointer" is not being accessed by any current readers 
of the "active pointer" and so, the underlying structure is not touched 
by the "readers".

I agree with all your statements below about WriteReaderPhaser and I can 
see how WriteReaderPhaser API is more suited to the pointer flipping 
scenarios, but WriteReaderPhaser and Left-Right can be used 
interchangeably. They are, in a sense equivalent.

To illustrate, I'll use the lambda-enabled Left-Right API:

http://cr.openjdk.java.net/~plevart/misc/LeftRight/LeftRight.java

...and try to re-create your example from below:

public class LRCounters {

     static class ArrayRef {
         long[] array;

         ArrayRef(long[] array) {
             this.array = array;
         }
     }

     private final LeftRight<ArrayRef> counts;
     private long intervalCounts[];
     private final long accumulatedCounts[];

     public LRCounters(int size) {
         intervalCounts = new long[size];
         accumulatedCounts = new long[size];
         counts = new LeftRight<>(
             // this is the initially active ArrayRef
             new ArrayRef(new long[size]),
             // and this is the initially inactive one
             new ArrayRef(null)
         );
     }

     public void incrementCount(int iTh) {
         counts.read(iTh, (i, arrayRef) -> {
             long a[] = arrayRef.array; // this is the read operation
             return ++a[i]; // never mind the racy increment (should do 
it with atomics)
         });
     }

     public long[] getCounts() {
         long[][] result = new long[1][];

         counts.modify((arrayRef) -> {
             if (arrayRef.array == null) {
                 // we've got the previously inactive ArrayRef
                 arrayRef.array = intervalCounts; // this is the 1st 
write operation
             } else {
                 // we've got the previously active ArrayRef
                 // that has just been deactivated
                 intervalCounts = arrayRef.array;
                 arrayRef.array = null; // this is the "mirror" write 
operation
                 // add interval counts to accumulatedCounts
                 for (int i = 0; i < intervalCounts.length; i++) {
                     accumulatedCounts[i] += intervalCounts[i];
                     intervalCounts[i] = 0;
                 }
                 // return result
                 result[0] = accumulatedCounts.clone();
             }
         });

         return result[0];
     }
}


Likewise, let's take an example that is more suited to LeftRight API:

public class LRMap<K, V> {

     private final LeftRight<Map<K, V>> lrMap = new LeftRight<>(new 
HashMap<>(), new HashMap<>());

     public V get(K key) {
         return lrMap.read(m -> m.get(key));
     }

     public void put(K key, V value) {
         lrMap.modify(m -> m.put(key, value));
     }
}

...and try to implement is using WriteReaderPhaser:

public class WRPMap<K, V> {

     private final WriterReaderPhaser wrp = new WriterReaderPhaser();
     private volatile Map<K, V> activeMap = new HashMap<>();
     private volatile Map<K, V> inactiveMap = new HashMap<>();

     public V get(K key) {
         long stamp = wrp.writerCriticalSectionEnter();
         try {
             return activeMap.get(key);
         } finally {
             wrp.writerCriticalSectionExit(stamp);
         }
     }

     public void put(K key, V value) {
         wrp.readerLock();
         try {
             Map<K, V> m1 = inactiveMap;
             Map<K, V> m2 = activeMap;
             m1.put(key, value); // 1st write to inactive
             // swap active <-> inactive
             activeMap = m1;
             inactiveMap = m2;

             wrp.flipPhase();

             m2.put(key, value); // mirror write to just deactivated
         } finally {
             wrp.readerUnlock();
         }
     }
}

Does this make any sense?

Regards, Peter

On 11/25/2014 08:39 AM, Gil Tene wrote:
> Pedro, I think you are confusing specific under-the-hood implementation choices (which are similar) with what the primitive is. I'm flattered at your naming of Left-Right GT, but Left-Right GT (and the LRTreeSetGT example) is a Left-Right variant with a different underlying (attributed to me) arrive-depart technique. It is not a WriterReaderPhaser.
>
> WriterReaderPhaser captures (in a clean synchronization primitive API form) a pattern I've had to use and re-invent myself several times, and I'm pretty sure many others that have faced the "I want to periodically report/analyze an actively updating data set" have too. The key here is capturing the guarantees and prescribed use such that end-users can use the primitive without needing to understand the underlying logic of an implementation. I do that in my blog entry (and include a definition and use example below).
>
> A WriterReaderPhaser is neither a ReadWriteLock nor a ReadWriteLock used backwards. It's also not Left-Right, or Left-Right used backwards. The qualities and guarantees a WriterReaderPhaser provides are not provided by reversing the meaning of "writer" and "reader" in those primitives. Even if you ignore the confusion that such upside-down use may cause the user, there are specific guarantees that the other primitives do not provide, and that a write-heavy double-buffered use case needs and gets from this primitive.
>
> And yes, there are several possible ways to implement a well behaving WriterReaderPhaser, one of which is listed in the links I provided. We can implement it with three atomic words and some clever combos of CAS and GetAndAdd ops, or in other ways. The implementation is not what makes the primitive what it is to it's users. It's the API and the behavior guarantees that do that. And I'm pretty sure these behavior guarantees are not spelled out or provided (even backwards) in Left-Right and variants. Some of them (like the data stability guarantee for readers even in the presence of wait-free write activity) would be un-natural to provide in reverse for writers (since readers are generally not expected to change stuff).
>
> Left-Right is cool (really cool), but it focuses purely on wait-free readers and blocking writers. While that use case may appear to be "the opposite" of wait-free writers with blocking readers, there are specific non-mirroring qualities that make that duality invalid. Here are specific differences between the two mechanisms that make "backwards" use inapplicable::
>
> - WriterReaderPhaser provides specific data stability guarantees to readers (after a flip while under a readLock), even in the presence of concurrent writer activity. Left-Right does not provide such a guarantee to writers "backwards". E.g. if Left-Right readers happened to write into the Left-Right protected data structure (as they would need to in a "backwards use" attempt like this), Left-Right says nothing about what writers can expect from that data structure in terms of data consistency or stability. Note that I'm not saying that no Left-Right implementation could accidentally provide this behavior without stating it. I'm saying that the Left-Right mechanism, as described and documented in Left-Right paper and the various APIs for it's existing variants makes no such guarantee to the caller, and that a valid Left-Right implementation may or may not provide this behavior. As such, the user cannot rely on it. And this is the main guarantee a typical WriterReaderPhaser user will be looking for.
>
> - Left-Right specifically prohibits readers from writing. E.g. "...To access in read-only mode do something like this:..." is stated in the documentation for a LeftRightGuard variants.  In contrast, WriterReaderPhaser allows it's writers (which would be the readers in a backwards mapping attempt) to, um, write...
>
> - Writers that use Left-Right are required to to write their updates twice (on the two sides of a "writerToggle" or equivalent "flip"). e.g. "...The exact same operations must be done on the instance before and after guard.writeToggle()." is stated in the documentation for a LeftRightGuard variants. In contrast, WriterReaderPhaser does not require reading twice or writing twice. The two APIs do not mirror each other in this critical aspect.
>
> - Left-Right (even when used to replace a Reader-Writer lock) manages the active and inactive data structures internally (leftInstance and rightInstance, or firstInstance and secondInstance), and users of Left-right [must] operate on data structures returned from Left-Right operations. In contrast, WriterReaderPhaser does not manage the active and inactive data structures in any way, leaving that job to readers and writers that operate directly on the shared data structures.
>
> To be specific, let me detail what a WriterReaderPhaser is (taken from an updated blog entry that now includes a definition):
>
> -----------------------------------------------------------------
> Definition of WriterReaderPhaser:
>
> A WriterReaderPhaser provides a means for wait free writers to coordinate with blocking (but guaranteed forward progress) readers sharing a set of data structures.
>
> A WriterReaderPhaser instance provides the following 5 operations:
>
> 	? writerCriticalSectionEnter
> 	? writerCriticalSectionExit
> 	? readerLock
> 	? readerUnlock
> 	? flipPhase
>
> When a WriterReaderPhaser  instance is used to protect an active [set of or single] data structure involving [potentially multiple] writers and [potentially multiple] readers , the assumptions on how readers and writers act are:
>
> 	? There are two sets of data structures (an "active" set and an "inactive" set)
> 	? Writing is done to the perceived active version (as perceived by the writer), and only within critical sections delineated by writerCriticalSectionEnter and writerCriticalSectionExit operations.
> 	? Only readers switch the perceived roles of the active and inactive data structures. They do so only while holding the readerLock, and only before execution a flipPhase.
>
> 	? Readers do not hold onto readerLock indefinitely. Only readers perform readerLock and readerUnlock.
> 	? Writers do not remain in their critical sections indefinitely. Only writers perform writerCriticalSectionEnter and writerCriticalSectionExit.
> 	? Only readers perform flipPhase operations, and only while holding the readerLock.
>
> When the above assumptions are met, WriterReaderPhaser guarantees that the inactive data structures are not being modified by any writers while being read while under readerLock protection after a flipPhase operation.
>
> The following progress guarantees are provided to writers and readers that adhere to the above stated assumptions:
> 	? Writers operations (writerCriticalSectionEnter and  writerCriticalSectionExit) are wait free (on architectures that support wait-free atomic increment operations).
> 	? flipPhase operations are guaranteed to make forward progress, and will only be blocked by writers whose critical sections were entered prior to the start of the reader's flipPhase operation, and have not yet exited their critical sections.
> 	? readerLock only block for other readers that are holding the readerLock.
>
> -----------------------------------------------------------------
> Example use:
>
> Imagine a simple use case where a large set of rapidly updated counter is being modified by writers, and a reader needs to gain access to stable interval samples of those counters for reporting and other analysis purposes.
>
> The counters are represented in a volatile array of values (it is the array reference that is volatile, not the value cells within it):
>
> volatile long counts[];
> ...
>
> A writer updates a specific count (n) in the set of counters:
>
> writerCriticalSectionEnter
>     counts[n]++;
> writerCriticalSectionExit
>
> A reader gain access to a stable set of counts collected during an interval, reports on it, and accumulates it:
>
> long intervalCounts[];
> long accumulated_counts[];
>
> ...
> readerLock
>     reset(interval_counts);
>     long tmp[] = counts;
>     counts = interval_counts;
>     interval_counts = tmp;
> flipPhase
>     report_interval_counts(interval_counts);
>     accumulated_counts.add(interval_counts);
> readerUnlock
> -----------------------------------------------------------------
>
>
> Bottom line: the above is defines what a WriterReaderPhaser primitive is, and shows a simple example of using it. While I provide an example implementation, many are possible, and I'm sure another 17 will pop up. To avoid wrongly taking credit for this as a new primitive, I'm looking to see if there have been previously described primitives that explicitly provide these (or equivalent) qualities to their users. "Explicitly" being a key word (since no sane user would rely on an accidental implicit behavior of a specific implementation of a primitive that does not actually guarantee the given behavior).
>
> -- Gil.
>
>> On Nov 24, 2014, at 3:28 PM, Pedro Ramalhete wrote:
>> Hi Peter, If I was you I wouldn't bother with it. As I tried to explain to Gil, the WriterReaderPhaser uses the same concurrency control algorithm as the Left-Right, and as such it is a variant of the Left-Right (used "backwards") that uses a (new) ReadIndicator with a single ingress combined with versionIndex. This variant is not as good for scalability under high contention as the one you yourself have implemented some time ago, with the ReadIndicator of ingress/egress with LongAdders. You're better off using your own implementation, and just do the mutations in lrSet.read() and the read-only operation in lrSet.modify(), but of course, feel free to try both and let us your results ;)
>> http://cr.openjdk.java.net/~plevart/misc/LeftRight/EnterExitWait.java
>> http://cr.openjdk.java.net/~plevart/misc/LeftRight/LongAdderEEW.java
>> http://cr.openjdk.java.net/~plevart/misc/LeftRight/LRTest.java
>> http://concurrencyfreaks.com/2014/11/left-right-gt-variant.html Cheers, Pedro
>>
>>> On Nov 24, 2014, at 3:28 AM, Peter Levart wrote:
>>>
>>> Hi Gil,
>>>
>>> What a coincidence. I was thinking of writing something like that myself in past couple of days for a similar purpose. It comes as a gift that you posted this here, thanks!
>>>
>>> My application is an asynchronous cache store implementation. A distributed cache (Coherence in my case) emits synchronous events when cache is updated from multiple threads. I want to batch updates and do asynchronous persistence in a background thread. Coherence already supports this by itself, but is rather limited in features, so I have to re-create this functionality and add missing features.
>>>
>>> Regards, Peter
>>>
>>> On 11/24/2014 06:54 AM, Gil Tene wrote:
>>>> Yeah, Yeah, I know. A new concurrency primitive? Really? But I think this may actually be a new, generically useful primitive. Basically, if you ever needed to analyze or log rapidly mutating data without blocking or locking out writers, this thing is for you. It supports wait-free writers, and stable readable data sets for guaranteed-forward-progress readers. And it makes double buffered data management semi-trivial. See blog entry explaining stuff : "WriterReaderPhaser: A story about a new (?) synchronization primitive"<http://stuff-gil-says.blogspot.com/2014/11/writerreaderphaser-story-about-new.html>. (with some interesting discussion comparing it to Left-Right, which does the opposite thing: wait free readers with blocking writers). See a simple (and very practical) example of using the primitive at: https://github.com/HdrHistogram/HdrHistogram/blob/master/src/main/java/org/HdrHistogram/IntervalHistogramRecorder.java And see the primitive qualities and use rules documented (in the JavaDoc) along with a working implementation at: https://github.com/HdrHistogram/HdrHistogram/blob/master/src/main/java/org/HdrHistogram/WriterReaderPhaser.java So please rip this thing apart? Or consider if it may be a useful addition to j.u.c. It needs a home. And if you've seen it before (i.e. it's not really new like I seem to think it is), I'd really like to know. ? Gil.
>>>> _______________________________________________ Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest




From pramalhe at gmail.com  Tue Nov 25 11:52:46 2014
From: pramalhe at gmail.com (Pedro Ramalhete)
Date: Tue, 25 Nov 2014 17:52:46 +0100
Subject: [concurrency-interest] A new (?) concurrency primitive:
	WriterReaderPhaser
In-Reply-To: <5474AC98.5020106@gmail.com>
References: <730432BA-BCC5-4BC4-8F2B-1DD9D1518536@azulsystems.com>
	<5473166A.7030904@gmail.com>
	<4D0323E3-8E8E-4867-B6DD-117BB48879A7@azulsystems.com>
	<5474AC98.5020106@gmail.com>
Message-ID: <CAAApjO1mBZiD+AANfHVxXTGf_VyCPmPf+5c9RWksS0cGR070kA@mail.gmail.com>

Yes Peter, it makes absolute sense!
To make it completely clear just "how equivalent" these two methods are,
let me add a "translation table" for the APIs, from WriterReaderPhaser to
Left-Right:
        ? writerCriticalSectionEnter   -> readIndicator.arrive()
        ? writerCriticalSectionExit      -> readIndicator.depart()
        ? readerLock                          -> writersMutex.lock()
        ? readerUnlock                      -> writersMutex.unlock()
        ? flipPhase                             -> toggleVersionAndScan()

Thanks,
Pedro

On Tue, Nov 25, 2014 at 5:21 PM, Peter Levart <peter.levart at gmail.com>
wrote:

> Hi Gil,
>
> I think Pedro is right in claiming that WriteReaderPhaser is a kind of
> Left-Right, but he's wrong in explaining that it is a Left-Right used
> backwards. In Left-Right terminology, WriteReaderPhaser is not protecting
> the mutable structure (mutated from multiple threads), but just
> coordinating access to the "active pointer" to the structure". From
> Left-Right perspective, multiple threads are just readers of the "active
> pointer" (what they do with the underlying structure is not relevant here -
> the underlying structure has it's own synchronization). The single thread
> (at a time) that wants to get access to the snapshot is the sole writer
> (or "swapper") of the "active pointer". Of course, the sole writer or
> "swapper" of the active pointer also exploits the fact that the "inactive
> pointer" is not being accessed by any current readers of the "active
> pointer" and so, the underlying structure is not touched by the "readers".
>
> I agree with all your statements below about WriteReaderPhaser and I can
> see how WriteReaderPhaser API is more suited to the pointer flipping
> scenarios, but WriteReaderPhaser and Left-Right can be used
> interchangeably. They are, in a sense equivalent.
>
> To illustrate, I'll use the lambda-enabled Left-Right API:
>
> http://cr.openjdk.java.net/~plevart/misc/LeftRight/LeftRight.java
>
> ...and try to re-create your example from below:
>
> public class LRCounters {
>
>     static class ArrayRef {
>         long[] array;
>
>         ArrayRef(long[] array) {
>             this.array = array;
>         }
>     }
>
>     private final LeftRight<ArrayRef> counts;
>     private long intervalCounts[];
>     private final long accumulatedCounts[];
>
>     public LRCounters(int size) {
>         intervalCounts = new long[size];
>         accumulatedCounts = new long[size];
>         counts = new LeftRight<>(
>             // this is the initially active ArrayRef
>             new ArrayRef(new long[size]),
>             // and this is the initially inactive one
>             new ArrayRef(null)
>         );
>     }
>
>     public void incrementCount(int iTh) {
>         counts.read(iTh, (i, arrayRef) -> {
>             long a[] = arrayRef.array; // this is the read operation
>             return ++a[i]; // never mind the racy increment (should do it
> with atomics)
>         });
>     }
>
>     public long[] getCounts() {
>         long[][] result = new long[1][];
>
>         counts.modify((arrayRef) -> {
>             if (arrayRef.array == null) {
>                 // we've got the previously inactive ArrayRef
>                 arrayRef.array = intervalCounts; // this is the 1st write
> operation
>             } else {
>                 // we've got the previously active ArrayRef
>                 // that has just been deactivated
>                 intervalCounts = arrayRef.array;
>                 arrayRef.array = null; // this is the "mirror" write
> operation
>                 // add interval counts to accumulatedCounts
>                 for (int i = 0; i < intervalCounts.length; i++) {
>                     accumulatedCounts[i] += intervalCounts[i];
>                     intervalCounts[i] = 0;
>                 }
>                 // return result
>                 result[0] = accumulatedCounts.clone();
>             }
>         });
>
>         return result[0];
>     }
> }
>
>
> Likewise, let's take an example that is more suited to LeftRight API:
>
> public class LRMap<K, V> {
>
>     private final LeftRight<Map<K, V>> lrMap = new LeftRight<>(new
> HashMap<>(), new HashMap<>());
>
>     public V get(K key) {
>         return lrMap.read(m -> m.get(key));
>     }
>
>     public void put(K key, V value) {
>         lrMap.modify(m -> m.put(key, value));
>     }
> }
>
> ...and try to implement is using WriteReaderPhaser:
>
> public class WRPMap<K, V> {
>
>     private final WriterReaderPhaser wrp = new WriterReaderPhaser();
>     private volatile Map<K, V> activeMap = new HashMap<>();
>     private volatile Map<K, V> inactiveMap = new HashMap<>();
>
>     public V get(K key) {
>         long stamp = wrp.writerCriticalSectionEnter();
>         try {
>             return activeMap.get(key);
>         } finally {
>             wrp.writerCriticalSectionExit(stamp);
>         }
>     }
>
>     public void put(K key, V value) {
>         wrp.readerLock();
>         try {
>             Map<K, V> m1 = inactiveMap;
>             Map<K, V> m2 = activeMap;
>             m1.put(key, value); // 1st write to inactive
>             // swap active <-> inactive
>             activeMap = m1;
>             inactiveMap = m2;
>
>             wrp.flipPhase();
>
>             m2.put(key, value); // mirror write to just deactivated
>         } finally {
>             wrp.readerUnlock();
>         }
>     }
> }
>
> Does this make any sense?
>
> Regards, Peter
>
> On 11/25/2014 08:39 AM, Gil Tene wrote:
>
>> Pedro, I think you are confusing specific under-the-hood implementation
>> choices (which are similar) with what the primitive is. I'm flattered at
>> your naming of Left-Right GT, but Left-Right GT (and the LRTreeSetGT
>> example) is a Left-Right variant with a different underlying (attributed to
>> me) arrive-depart technique. It is not a WriterReaderPhaser.
>>
>> WriterReaderPhaser captures (in a clean synchronization primitive API
>> form) a pattern I've had to use and re-invent myself several times, and I'm
>> pretty sure many others that have faced the "I want to periodically
>> report/analyze an actively updating data set" have too. The key here is
>> capturing the guarantees and prescribed use such that end-users can use the
>> primitive without needing to understand the underlying logic of an
>> implementation. I do that in my blog entry (and include a definition and
>> use example below).
>>
>> A WriterReaderPhaser is neither a ReadWriteLock nor a ReadWriteLock used
>> backwards. It's also not Left-Right, or Left-Right used backwards. The
>> qualities and guarantees a WriterReaderPhaser provides are not provided by
>> reversing the meaning of "writer" and "reader" in those primitives. Even if
>> you ignore the confusion that such upside-down use may cause the user,
>> there are specific guarantees that the other primitives do not provide, and
>> that a write-heavy double-buffered use case needs and gets from this
>> primitive.
>>
>> And yes, there are several possible ways to implement a well behaving
>> WriterReaderPhaser, one of which is listed in the links I provided. We can
>> implement it with three atomic words and some clever combos of CAS and
>> GetAndAdd ops, or in other ways. The implementation is not what makes the
>> primitive what it is to it's users. It's the API and the behavior
>> guarantees that do that. And I'm pretty sure these behavior guarantees are
>> not spelled out or provided (even backwards) in Left-Right and variants.
>> Some of them (like the data stability guarantee for readers even in the
>> presence of wait-free write activity) would be un-natural to provide in
>> reverse for writers (since readers are generally not expected to change
>> stuff).
>>
>> Left-Right is cool (really cool), but it focuses purely on wait-free
>> readers and blocking writers. While that use case may appear to be "the
>> opposite" of wait-free writers with blocking readers, there are specific
>> non-mirroring qualities that make that duality invalid. Here are specific
>> differences between the two mechanisms that make "backwards" use
>> inapplicable::
>>
>> - WriterReaderPhaser provides specific data stability guarantees to
>> readers (after a flip while under a readLock), even in the presence of
>> concurrent writer activity. Left-Right does not provide such a guarantee to
>> writers "backwards". E.g. if Left-Right readers happened to write into the
>> Left-Right protected data structure (as they would need to in a "backwards
>> use" attempt like this), Left-Right says nothing about what writers can
>> expect from that data structure in terms of data consistency or stability.
>> Note that I'm not saying that no Left-Right implementation could
>> accidentally provide this behavior without stating it. I'm saying that the
>> Left-Right mechanism, as described and documented in Left-Right paper and
>> the various APIs for it's existing variants makes no such guarantee to the
>> caller, and that a valid Left-Right implementation may or may not provide
>> this behavior. As such, the user cannot rely on it. And this is the main
>> guarantee a typical WriterReaderPhaser user will be looking for.
>>
>> - Left-Right specifically prohibits readers from writing. E.g. "...To
>> access in read-only mode do something like this:..." is stated in the
>> documentation for a LeftRightGuard variants.  In contrast,
>> WriterReaderPhaser allows it's writers (which would be the readers in a
>> backwards mapping attempt) to, um, write...
>>
>> - Writers that use Left-Right are required to to write their updates
>> twice (on the two sides of a "writerToggle" or equivalent "flip"). e.g.
>> "...The exact same operations must be done on the instance before and after
>> guard.writeToggle()." is stated in the documentation for a LeftRightGuard
>> variants. In contrast, WriterReaderPhaser does not require reading twice or
>> writing twice. The two APIs do not mirror each other in this critical
>> aspect.
>>
>> - Left-Right (even when used to replace a Reader-Writer lock) manages the
>> active and inactive data structures internally (leftInstance and
>> rightInstance, or firstInstance and secondInstance), and users of
>> Left-right [must] operate on data structures returned from Left-Right
>> operations. In contrast, WriterReaderPhaser does not manage the active and
>> inactive data structures in any way, leaving that job to readers and
>> writers that operate directly on the shared data structures.
>>
>> To be specific, let me detail what a WriterReaderPhaser is (taken from an
>> updated blog entry that now includes a definition):
>>
>> -----------------------------------------------------------------
>> Definition of WriterReaderPhaser:
>>
>> A WriterReaderPhaser provides a means for wait free writers to coordinate
>> with blocking (but guaranteed forward progress) readers sharing a set of
>> data structures.
>>
>> A WriterReaderPhaser instance provides the following 5 operations:
>>
>>         ? writerCriticalSectionEnter
>>         ? writerCriticalSectionExit
>>         ? readerLock
>>         ? readerUnlock
>>         ? flipPhase
>>
>> When a WriterReaderPhaser  instance is used to protect an active [set of
>> or single] data structure involving [potentially multiple] writers and
>> [potentially multiple] readers , the assumptions on how readers and writers
>> act are:
>>
>>         ? There are two sets of data structures (an "active" set and an
>> "inactive" set)
>>         ? Writing is done to the perceived active version (as perceived
>> by the writer), and only within critical sections delineated by
>> writerCriticalSectionEnter and writerCriticalSectionExit operations.
>>         ? Only readers switch the perceived roles of the active and
>> inactive data structures. They do so only while holding the readerLock, and
>> only before execution a flipPhase.
>>
>>         ? Readers do not hold onto readerLock indefinitely. Only readers
>> perform readerLock and readerUnlock.
>>         ? Writers do not remain in their critical sections indefinitely.
>> Only writers perform writerCriticalSectionEnter and
>> writerCriticalSectionExit.
>>         ? Only readers perform flipPhase operations, and only while
>> holding the readerLock.
>>
>> When the above assumptions are met, WriterReaderPhaser guarantees that
>> the inactive data structures are not being modified by any writers while
>> being read while under readerLock protection after a flipPhase operation.
>>
>> The following progress guarantees are provided to writers and readers
>> that adhere to the above stated assumptions:
>>         ? Writers operations (writerCriticalSectionEnter and
>> writerCriticalSectionExit) are wait free (on architectures that support
>> wait-free atomic increment operations).
>>         ? flipPhase operations are guaranteed to make forward progress,
>> and will only be blocked by writers whose critical sections were entered
>> prior to the start of the reader's flipPhase operation, and have not yet
>> exited their critical sections.
>>         ? readerLock only block for other readers that are holding the
>> readerLock.
>>
>> -----------------------------------------------------------------
>> Example use:
>>
>> Imagine a simple use case where a large set of rapidly updated counter is
>> being modified by writers, and a reader needs to gain access to stable
>> interval samples of those counters for reporting and other analysis
>> purposes.
>>
>> The counters are represented in a volatile array of values (it is the
>> array reference that is volatile, not the value cells within it):
>>
>> volatile long counts[];
>> ...
>>
>> A writer updates a specific count (n) in the set of counters:
>>
>> writerCriticalSectionEnter
>>     counts[n]++;
>> writerCriticalSectionExit
>>
>> A reader gain access to a stable set of counts collected during an
>> interval, reports on it, and accumulates it:
>>
>> long intervalCounts[];
>> long accumulated_counts[];
>>
>> ...
>> readerLock
>>     reset(interval_counts);
>>     long tmp[] = counts;
>>     counts = interval_counts;
>>     interval_counts = tmp;
>> flipPhase
>>     report_interval_counts(interval_counts);
>>     accumulated_counts.add(interval_counts);
>> readerUnlock
>> -----------------------------------------------------------------
>>
>>
>> Bottom line: the above is defines what a WriterReaderPhaser primitive is,
>> and shows a simple example of using it. While I provide an example
>> implementation, many are possible, and I'm sure another 17 will pop up. To
>> avoid wrongly taking credit for this as a new primitive, I'm looking to see
>> if there have been previously described primitives that explicitly provide
>> these (or equivalent) qualities to their users. "Explicitly" being a key
>> word (since no sane user would rely on an accidental implicit behavior of a
>> specific implementation of a primitive that does not actually guarantee the
>> given behavior).
>>
>> -- Gil.
>>
>>  On Nov 24, 2014, at 3:28 PM, Pedro Ramalhete wrote:
>>> Hi Peter, If I was you I wouldn't bother with it. As I tried to explain
>>> to Gil, the WriterReaderPhaser uses the same concurrency control algorithm
>>> as the Left-Right, and as such it is a variant of the Left-Right (used
>>> "backwards") that uses a (new) ReadIndicator with a single ingress combined
>>> with versionIndex. This variant is not as good for scalability under high
>>> contention as the one you yourself have implemented some time ago, with the
>>> ReadIndicator of ingress/egress with LongAdders. You're better off using
>>> your own implementation, and just do the mutations in lrSet.read() and the
>>> read-only operation in lrSet.modify(), but of course, feel free to try both
>>> and let us your results ;)
>>> http://cr.openjdk.java.net/~plevart/misc/LeftRight/EnterExitWait.java
>>> http://cr.openjdk.java.net/~plevart/misc/LeftRight/LongAdderEEW.java
>>> http://cr.openjdk.java.net/~plevart/misc/LeftRight/LRTest.java
>>> http://concurrencyfreaks.com/2014/11/left-right-gt-variant.html Cheers,
>>> Pedro
>>>
>>>  On Nov 24, 2014, at 3:28 AM, Peter Levart wrote:
>>>>
>>>> Hi Gil,
>>>>
>>>> What a coincidence. I was thinking of writing something like that
>>>> myself in past couple of days for a similar purpose. It comes as a gift
>>>> that you posted this here, thanks!
>>>>
>>>> My application is an asynchronous cache store implementation. A
>>>> distributed cache (Coherence in my case) emits synchronous events when
>>>> cache is updated from multiple threads. I want to batch updates and do
>>>> asynchronous persistence in a background thread. Coherence already supports
>>>> this by itself, but is rather limited in features, so I have to re-create
>>>> this functionality and add missing features.
>>>>
>>>> Regards, Peter
>>>>
>>>> On 11/24/2014 06:54 AM, Gil Tene wrote:
>>>>
>>>>> Yeah, Yeah, I know. A new concurrency primitive? Really? But I think
>>>>> this may actually be a new, generically useful primitive. Basically, if you
>>>>> ever needed to analyze or log rapidly mutating data without blocking or
>>>>> locking out writers, this thing is for you. It supports wait-free writers,
>>>>> and stable readable data sets for guaranteed-forward-progress readers. And
>>>>> it makes double buffered data management semi-trivial. See blog entry
>>>>> explaining stuff : "WriterReaderPhaser: A story about a new (?)
>>>>> synchronization primitive"<http://stuff-gil-says.blogspot.com/2014/11/
>>>>> writerreaderphaser-story-about-new.html>. (with some interesting
>>>>> discussion comparing it to Left-Right, which does the opposite thing: wait
>>>>> free readers with blocking writers). See a simple (and very practical)
>>>>> example of using the primitive at: https://github.com/
>>>>> HdrHistogram/HdrHistogram/blob/master/src/main/java/org/HdrHistogram/
>>>>> IntervalHistogramRecorder.java And see the primitive qualities and
>>>>> use rules documented (in the JavaDoc) along with a working implementation
>>>>> at: https://github.com/HdrHistogram/HdrHistogram/
>>>>> blob/master/src/main/java/org/HdrHistogram/WriterReaderPhaser.java So
>>>>> please rip this thing apart? Or consider if it may be a useful addition to
>>>>> j.u.c. It needs a home. And if you've seen it before (i.e. it's not really
>>>>> new like I seem to think it is), I'd really like to know. ? Gil.
>>>>> _______________________________________________ Concurrency-interest
>>>>> mailing list
>>>>> Concurrency-interest at cs.oswego.edu
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141125/69c15c22/attachment-0001.html>

From dt at flyingtroika.com  Tue Nov 25 13:16:22 2014
From: dt at flyingtroika.com (DT)
Date: Tue, 25 Nov 2014 10:16:22 -0800
Subject: [concurrency-interest] RFR: 8065804: JEP 171:
	Clarifications/corrections for fence intrinsics
In-Reply-To: <B84AA20B-5714-4FF0-AB17-E918395C9DDB@oracle.com>
References: <CA+kOe08RfnkJ4Dit8ng7HQaSuDer_pA0i4t568uAGMg9dzujhg@mail.gmail.com>
	<5473A344.8090302@oracle.com>
	<CA+kOe0-KiKKNM-LyYvjZiXOGMF7c4FsHifcDq-ZZ65Bb1J=LLA@mail.gmail.com>
	<B84AA20B-5714-4FF0-AB17-E918395C9DDB@oracle.com>
Message-ID: <21DC5F06-9597-4CCB-8A25-5CC354A0BCE5@flyingtroika.com>

I see time to time comments in the jvm sources referencing membars and fences. Would you say that they are used interchangeably ? Having the same meaning but for different CPU arch.

Sent from my iPhone

> On Nov 25, 2014, at 6:04 AM, Paul Sandoz <paul.sandoz at oracle.com> wrote:
> 
> Hi Martin,
> 
> Thanks for looking into this.
> 
> 1141      * Currently hotspot's implementation of a Java language-level volatile
> 1142      * store has the same effect as a storeFence followed by a relaxed store,
> 1143      * although that may be a little stronger than needed.
> 
> IIUC to emulate hotpot's volatile store you will need to say that a fullFence immediately follows the relaxed store.
> 
> The bit that always confuses me about release and acquire is ordering is restricted to one direction, as talked about in orderAccess.hpp [1]. So for a release, accesses prior to the release cannot move below it, but accesses succeeding the release can move above it. And that seems to apply to Unsafe.storeFence [2] (acting like a monitor exit). Is that contrary to C++ release fences where ordering is restricted both to prior and succeeding accesses? [3]
> 
> So what about the following?
> 
>  a = r1; // Cannot move below the fence
>  Unsafe.storeFence();
>  b = r2; // Can move above the fence?
> 
> Paul.
> 
> [1] In orderAccess.hpp
> // Execution by a processor of release makes the effect of all memory
> // accesses issued by it previous to the release visible to all
> // processors *before* the release completes.  The effect of subsequent
> // memory accesses issued by it *may* be made visible *before* the
> // release.  I.e., subsequent memory accesses may float above the
> // release, but prior ones may not float below it.
> 
> [2] In memnode.hpp
> // "Release" - no earlier ref can move after (but later refs can move
> // up, like a speculative pipelined cache-hitting Load).  Requires
> // multi-cpu visibility.  Inserted independent of any store, as required
> // for intrinsic sun.misc.Unsafe.storeFence().
> class StoreFenceNode: public MemBarNode {
> public:
>  StoreFenceNode(Compile* C, int alias_idx, Node* precedent)
>    : MemBarNode(C, alias_idx, precedent) {}
>  virtual int Opcode() const;
> };
> 
> [3] http://preshing.com/20131125/acquire-and-release-fences-dont-work-the-way-youd-expect/
> 
>> On Nov 25, 2014, at 1:47 AM, Martin Buchholz <martinrb at google.com> wrote:
>> 
>> OK, I worked in some wording for comparison with volatiles.
>> I believe you when you say that the semantics of the corresponding C++
>> fences are slightly different, but it's rather subtle - can we say
>> anything more than "closely related to"?
>> 
>> On Mon, Nov 24, 2014 at 1:29 PM, Aleksey Shipilev
>> <aleksey.shipilev at oracle.com> wrote:
>>> Hi Martin,
>>> 
>>>> On 11/24/2014 11:56 PM, Martin Buchholz wrote:
>>>> Review carefully - I am trying to learn about fences by explaining them!
>>>> I have borrowed some wording from my reviewers!
>>>> 
>>>> https://bugs.openjdk.java.net/browse/JDK-8065804
>>>> http://cr.openjdk.java.net/~martin/webrevs/openjdk9/fence-intrinsics/
>>> 
>>> I think "implies the effect of C++11" is too strong wording. "related"
>>> might be more appropriate.
>>> 
>>> See also comments here for connection with "volatiles":
>>> https://bugs.openjdk.java.net/browse/JDK-8038978
>>> 
>>> Take note the Hans' correction that fences generally imply more than
>>> volatile load/store, but since you are listing the related things in the
>>> docs, I think the "native" Java example is good to have.
>>> 
>>> -Aleksey.
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From Sebastian.Millies at softwareag.com  Tue Nov 25 13:31:30 2014
From: Sebastian.Millies at softwareag.com (Millies, Sebastian)
Date: Tue, 25 Nov 2014 18:31:30 +0000
Subject: [concurrency-interest] How deal with RejectedExecutionException in
 CompletableFuture#supplyAsync ?
Message-ID: <32F15738E8E5524DA4F01A0FA4A8E490FD8854C2@HQMBX5.eur.ad.sag>

Hello,

consider code like this:

eventsource.stream()
  .map(event ->
     CompletableFuture.supplyAsync(()->react(event), pool)
     .thenApplyAsync(function, pool)
     .exceptionally(t -> defaultValue))
  .collect(toList());

The pool has a bounded queue (an ArrayBlockingQueue, or even a SynchronizedQueue). I?d like to return that default value even if the pool throws a RejectedExecutionException, and continue processing the rest of the stream. But of course it doesn?t work that way, supplyAsync() fails and there is not even a CompletableFuture that could complete exceptionally.

For me, it doesn?t really matter why I cannot process that element, I?d like to treat the RejectedExecutionException just like any old exception. And this might turn out a common situation for me, that the queue in front of my pool is full, and I just want to keep submitting tasks, because soon there will again be a processing opportunity, and I don?t care much about the stuff missing in the middle.

What is the best idea here? Supply a RejectedExecutionHandler that removes the head of the queue and tries to re-insert the Runnable at the end (giving me in effect a kind of circular buffer, and of course it wouldn?t work with a SynchronizedQueue)? Or is there a better way?

Regards,
Sebastian

Sebastian Millies
PPM, Saarbr?cken C1.67, +49 681 210 3221


Software AG ? Sitz/Registered office: Uhlandstra?e 12, 64297 Darmstadt, Germany ? Registergericht/Commercial register: Darmstadt HRB 1562 - Vorstand/Management Board: Karl-Heinz Streibich (Vorsitzender/Chairman), Eric Duffaut, Dr. Wolfram Jost, Arnd Zinnhardt; - Aufsichtsratsvorsitzender/Chairman of the Supervisory Board: Dr. Andreas Bereczky - http://www.softwareag.com

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141125/2af95493/attachment.html>

From Sebastian.Millies at softwareag.com  Tue Nov 25 13:50:02 2014
From: Sebastian.Millies at softwareag.com (Millies, Sebastian)
Date: Tue, 25 Nov 2014 18:50:02 +0000
Subject: [concurrency-interest] How deal with RejectedExecutionException
 in CompletableFuture#supplyAsync ?
Message-ID: <32F15738E8E5524DA4F01A0FA4A8E490FD8854E5@HQMBX5.eur.ad.sag>

To suggest an answer to my own question, I have written the following utility, which I use to wrap the construction of my futures:

/**
   * Supplies a future that completes exceptionally if the underlying thread pool throws a RejectedExecutionException.
   * @param futureSupplier supplier of a future
   * @return the future supplied by futureSupplier, or another future that has completed exceptionally
   */
  public static <T> CompletableFuture<T> supplyFuture(Supplier<CompletableFuture<T>> futureSupplier) {
    try {
      return futureSupplier.get();
    } catch (RejectedExecutionException e) {
      CompletableFuture<T> future = new CompletableFuture<>();
      future.completeExceptionally(e);
      return future;
    }
  }


n  Sebastian

From: Sebastian.Millies at softwareag.com
Sent: Tuesday, November 25, 2014 7:31 PM
To: concurrency-interest at cs.oswego.edu
Subject: How deal with RejectedExecutionException in CompletableFuture#supplyAsync ?

Hello,

consider code like this:

eventsource.stream()
  .map(event ->
     CompletableFuture.supplyAsync(()->react(event), pool)
     .thenApplyAsync(function, pool)
     .exceptionally(t -> defaultValue))
  .collect(toList());

The pool has a bounded queue (an ArrayBlockingQueue, or even a SynchronizedQueue). I?d like to return that default value even if the pool throws a RejectedExecutionException, and continue processing the rest of the stream. But of course it doesn?t work that way, supplyAsync() fails and there is not even a CompletableFuture that could complete exceptionally.

For me, it doesn?t really matter why I cannot process that element, I?d like to treat the RejectedExecutionException just like any old exception. And this might turn out a common situation for me, that the queue in front of my pool is full, and I just want to keep submitting tasks, because soon there will again be a processing opportunity, and I don?t care much about the stuff missing in the middle.

What is the best idea here? Supply a RejectedExecutionHandler that removes the head of the queue and tries to re-insert the Runnable at the end (giving me in effect a kind of circular buffer, and of course it wouldn?t work with a SynchronizedQueue)? Or is there a better way?

Regards,
Sebastian

Sebastian Millies
PPM, Saarbr?cken C1.67, +49 681 210 3221


Software AG ? Sitz/Registered office: Uhlandstra?e 12, 64297 Darmstadt, Germany ? Registergericht/Commercial register: Darmstadt HRB 1562 - Vorstand/Management Board: Karl-Heinz Streibich (Vorsitzender/Chairman), Eric Duffaut, Dr. Wolfram Jost, Arnd Zinnhardt; - Aufsichtsratsvorsitzender/Chairman of the Supervisory Board: Dr. Andreas Bereczky - http://www.softwareag.com

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141125/08d3b1d2/attachment-0001.html>

From boehm at acm.org  Tue Nov 25 14:15:36 2014
From: boehm at acm.org (Hans Boehm)
Date: Tue, 25 Nov 2014 11:15:36 -0800
Subject: [concurrency-interest] RFR: 8065804: JEP 171:
 Clarifications/corrections for fence intrinsics
In-Reply-To: <21DC5F06-9597-4CCB-8A25-5CC354A0BCE5@flyingtroika.com>
References: <CA+kOe08RfnkJ4Dit8ng7HQaSuDer_pA0i4t568uAGMg9dzujhg@mail.gmail.com>
	<5473A344.8090302@oracle.com>
	<CA+kOe0-KiKKNM-LyYvjZiXOGMF7c4FsHifcDq-ZZ65Bb1J=LLA@mail.gmail.com>
	<B84AA20B-5714-4FF0-AB17-E918395C9DDB@oracle.com>
	<21DC5F06-9597-4CCB-8A25-5CC354A0BCE5@flyingtroika.com>
Message-ID: <CAPUmR1aZNSdk5znSfp4ei1M+ZK1M06uEYujH3Z2_B1wtOqMMhw@mail.gmail.com>

It seems to me that a (dubiuously named) loadFence is intended to have
essentially the same semantics as the (perhaps slightly less dubiously
named) C++ atomic_thread_fence(memory_order_acquire), and a storeFence
matches atomic_thread_fence(memory_order_release).  The C++ standard and,
even more so, Mark Batty's work have a precise definition of what those
mean in terms of implied "synchronizes with" relationships.

It looks to me like this whole implementation model for volatiles in terms
of fences is fundamentally doomed, and it probably makes more sense to get
rid of it rather than spending time on renaming it (though we just did the
latter in Android to avoid similar confusion about semantics).  It's
fundamentally incompatible with the way volatiles/atomics are intended to
be implemented on ARMv8 (and Itanium).  Which I think fundamentally get
this much closer to right than traditional fence-based ISAs.

I'm no hardware architect, but fundamentally it seems to me that

load x
acquire_fence

imposes a much more stringent constraint than

load_acquire x

Consider the case in which the load from x is an L1 hit, but a preceding
load (from say y) is a long-latency miss.  If we enforce ordering by just
waiting for completion of prior operation, the former has to wait for the
load from y to complete; while the latter doesn't.  I find it hard to
believe that this doesn't leave an appreciable amount of performance on the
table, at least for some interesting microarchitectures.

Along similar lines, it seems to me that Doug's JSR cookbook was a great
contribution originally, in that it finally pinned down understandable
rules for implementors, while providing a lot of useful intuition.  At this
point, it is still quite useful for intuition, but
http://www.cl.cam.ac.uk/~pes20/cpp/cpp0xmappings.html (remembering that
Java volatile = C++ SC atomic) is a much better guide for implementors,
especially on Power.  The SPARC-like fence primitives used in the cookbook
are no longer reflective of the most widely used architectures.  And they
do not reflect the fence types actually needed by Java.  In addition, for
better or worse, fencing requirements on at least Power are actually driven
as much by store atomicity issues, as by the ordering issues discussed in
the cookbook.  This was not understood in 2005, and unfortunately doesn't
seem to be amenable to the kind of straightforward explanation as in Doug's
cookbook.

Hans

On Tue, Nov 25, 2014 at 10:16 AM, DT <dt at flyingtroika.com> wrote:

> I see time to time comments in the jvm sources referencing membars and
> fences. Would you say that they are used interchangeably ? Having the same
> meaning but for different CPU arch.
>
> Sent from my iPhone
>
> > On Nov 25, 2014, at 6:04 AM, Paul Sandoz <paul.sandoz at oracle.com> wrote:
> >
> > Hi Martin,
> >
> > Thanks for looking into this.
> >
> > 1141      * Currently hotspot's implementation of a Java language-level
> volatile
> > 1142      * store has the same effect as a storeFence followed by a
> relaxed store,
> > 1143      * although that may be a little stronger than needed.
> >
> > IIUC to emulate hotpot's volatile store you will need to say that a
> fullFence immediately follows the relaxed store.
> >
> > The bit that always confuses me about release and acquire is ordering is
> restricted to one direction, as talked about in orderAccess.hpp [1]. So for
> a release, accesses prior to the release cannot move below it, but accesses
> succeeding the release can move above it. And that seems to apply to
> Unsafe.storeFence [2] (acting like a monitor exit). Is that contrary to C++
> release fences where ordering is restricted both to prior and succeeding
> accesses? [3]
> >
> > So what about the following?
> >
> >  a = r1; // Cannot move below the fence
> >  Unsafe.storeFence();
> >  b = r2; // Can move above the fence?
> >
> > Paul.
> >
> > [1] In orderAccess.hpp
> > // Execution by a processor of release makes the effect of all memory
> > // accesses issued by it previous to the release visible to all
> > // processors *before* the release completes.  The effect of subsequent
> > // memory accesses issued by it *may* be made visible *before* the
> > // release.  I.e., subsequent memory accesses may float above the
> > // release, but prior ones may not float below it.
> >
> > [2] In memnode.hpp
> > // "Release" - no earlier ref can move after (but later refs can move
> > // up, like a speculative pipelined cache-hitting Load).  Requires
> > // multi-cpu visibility.  Inserted independent of any store, as required
> > // for intrinsic sun.misc.Unsafe.storeFence().
> > class StoreFenceNode: public MemBarNode {
> > public:
> >  StoreFenceNode(Compile* C, int alias_idx, Node* precedent)
> >    : MemBarNode(C, alias_idx, precedent) {}
> >  virtual int Opcode() const;
> > };
> >
> > [3]
> http://preshing.com/20131125/acquire-and-release-fences-dont-work-the-way-youd-expect/
> >
> >> On Nov 25, 2014, at 1:47 AM, Martin Buchholz <martinrb at google.com>
> wrote:
> >>
> >> OK, I worked in some wording for comparison with volatiles.
> >> I believe you when you say that the semantics of the corresponding C++
> >> fences are slightly different, but it's rather subtle - can we say
> >> anything more than "closely related to"?
> >>
> >> On Mon, Nov 24, 2014 at 1:29 PM, Aleksey Shipilev
> >> <aleksey.shipilev at oracle.com> wrote:
> >>> Hi Martin,
> >>>
> >>>> On 11/24/2014 11:56 PM, Martin Buchholz wrote:
> >>>> Review carefully - I am trying to learn about fences by explaining
> them!
> >>>> I have borrowed some wording from my reviewers!
> >>>>
> >>>> https://bugs.openjdk.java.net/browse/JDK-8065804
> >>>> http://cr.openjdk.java.net/~martin/webrevs/openjdk9/fence-intrinsics/
> >>>
> >>> I think "implies the effect of C++11" is too strong wording. "related"
> >>> might be more appropriate.
> >>>
> >>> See also comments here for connection with "volatiles":
> >>> https://bugs.openjdk.java.net/browse/JDK-8038978
> >>>
> >>> Take note the Hans' correction that fences generally imply more than
> >>> volatile load/store, but since you are listing the related things in
> the
> >>> docs, I think the "native" Java example is good to have.
> >>>
> >>> -Aleksey.
> >> _______________________________________________
> >> Concurrency-interest mailing list
> >> Concurrency-interest at cs.oswego.edu
> >> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141125/dd4baf7e/attachment.html>

From pramalhe at gmail.com  Tue Nov 25 16:01:43 2014
From: pramalhe at gmail.com (Pedro Ramalhete)
Date: Tue, 25 Nov 2014 22:01:43 +0100
Subject: [concurrency-interest] Increasing the throughput of
 ConcurrentLinkedQueue on PowerPC (and ARM?)
In-Reply-To: <CA+kOe0-HjMM9ZexyBFT+dXa+dj2o6CQ9fMFyHdrqLhe6taiR-w@mail.gmail.com>
References: <CAAApjO3j3g2fE-e2i5Yt-K8a8t2MBt3=zPrkNSCrCpGkwDEc2Q@mail.gmail.com>
	<54616343.5000708@cs.oswego.edu>
	<CA+kOe088JYmVw1AjANfzKL+sLbc6NFgLmUVj2uKji_-8NiwOQQ@mail.gmail.com>
	<CAAApjO2h9bNnsX4y4_ak78B0U2Xk4Y137NPYH69KrRdyMeziSw@mail.gmail.com>
	<CA+kOe0-mGZmkoC4t7VeWBS2y4nDethgOnCc10GQyCtcjvs=8LQ@mail.gmail.com>
	<CA+kOe0-HjMM9ZexyBFT+dXa+dj2o6CQ9fMFyHdrqLhe6taiR-w@mail.gmail.com>
Message-ID: <CAAApjO1RE+OOBYr=tRhTHp1CFTGkL=5s-cWWKOx3-nhEAu_dog@mail.gmail.com>

Hi Martin,
I've got some good news, some bad news, and some good news...

The first good news is that running your diff with our benchmarks on an
opteron (x86) with 32 cores has no impact on performance, and running it on
the RunAbove cloud instances with PowerPC 8 cores or 176 VPCs provides
performance increases ranging from 1.5x to 2x.
Notice that our benchmarks only use contains(), add(), and remove(), so
there is no way to know from them if the optimization in poll() is
effective or not.

When I examined your latest diff I though it was correct. Without the
loadFence() at the end of each method we could end up having speculative
loads that would "exit" the method, and thus break sequential consistency.
Using a loadFence() is a really cool trick, and the only other useful
application of loadFence() I've seen besides optimistic locks like
StampedLock!
The bad news is that Andreia found a scenario for which it breaks.
We were working on our own idea and came across an issue that shows that it
is not enough to do a load with acquire on the last node. The issue happens
at the end of the list, to maintain sequential consistency we have to
guarantee that when node.next is null, all previous loads have to be
executed before or at least the loads of the items. The volatile acquire on
next doesn't guarantee that previous loads are executed before, the only
way is to add a loadFence() and check again if node.next is still null with
an acquire.

The scenario is as follows:
Consider that the list starts by containing item Z.
Thread A : is executing clq.contains(Z) to the point where node.next is
null (but all the loads of item where reordered after that point, to be
immediately before the loadFence())
Thread B : executed clq.add(Z) and after clq.remove(Z). Notice that the
remove() will act on the first node found with item Z, not the node that
has just been added
Thread A : At this point, Thread A has reached the last node on the list,
and now it will start to execute the loads of item, and will not find Z
because at this point the first Z was already removed. It will return false
when, at all times, there was at least one instance of Z in the list, thus
breaking sequential consistency.

The good news is that there is a solution, we just need to guarantee that
all item's loads have been done before deciding that there are no more
nodes in the list.
One way to do it, is to remove the U.loadFence() from contains() and
remove(), and add it in succ() followed by a load in case node.next is null:

     final Node<E> succLoadFence(Node<E> p) {
        Node<E> next = p.next;
        if (next == null) {
            U.loadFence();
            next = p.next;
        }
        return (p == next) ? head : next;
    }

... and with this modification we get the same performance improvements of
1.5x to 2x on PowerPC.

Hans Boem was right to say that we should not forget that whatever changes
we do, CLQ still has to provide sequentially consistent behavior to its
clients   ;)

On Mon, Nov 24, 2014 at 8:40 PM, Martin Buchholz <martinrb at google.com>
wrote:

> Yet another experimental change below that I believe to be correct,
> now using Unsafe.loadFence, but am not intending to commit, unless we
> see compelling performance improvements on non-x86.
>
> Index: src/main/java/util/concurrent/ConcurrentLinkedQueue.java
> ===================================================================
> RCS file:
> /export/home/jsr166/jsr166/jsr166/src/main/java/util/concurrent/ConcurrentLinkedQueue.java,v
> retrieving revision 1.104
> diff -u -r1.104 ConcurrentLinkedQueue.java
> --- src/main/java/util/concurrent/ConcurrentLinkedQueue.java 23 Nov
> 2014 18:20:26 -0000 1.104
> +++ src/main/java/util/concurrent/ConcurrentLinkedQueue.java 24 Nov
> 2014 19:32:11 -0000
> @@ -314,7 +314,12 @@
>          restartFromHead:
>          for (;;) {
>              for (Node<E> h = head, p = h, q;;) {
> -                E item = p.item;
> +                // Relaxed read suffices here;
> +                // - the volatile read of p made item visible
> +                // - the subsequent cas ensures we notice racing
> removal of item
> +                // - loadFence ensures consistency with concurrent removal
> +                @SuppressWarnings("unchecked")
> +                E item = (E) U.getObject(p, ITEM); // = p.item;
>
>                  if (item != null && casItem(p, item, null)) {
>                      // Successful CAS is the linearization point
> @@ -325,6 +330,7 @@
>                  }
>                  else if ((q = p.next) == null) {
>                      updateHead(h, p);
> +                    U.loadFence();
>                      return null;
>                  }
>                  else if (p == q)
> @@ -429,11 +435,13 @@
>      public boolean contains(Object o) {
>          if (o != null) {
>              for (Node<E> p = first(); p != null; p = succ(p)) {
> -                E item = p.item;
> +                @SuppressWarnings("unchecked")
> +                E item = (E) U.getObject(p, ITEM); // = p.item;
>                  if (item != null && o.equals(item))
>                      return true;
>              }
>          }
> +        U.loadFence();
>          return false;
>      }
>
> @@ -452,7 +460,13 @@
>          if (o != null) {
>              Node<E> pred = null;
>              for (Node<E> p = first(); p != null; p = succ(p)) {
> -                E item = p.item;
> +                // Relaxed read suffices here;
> +                // - the volatile read of p made item visible
> +                // - the subsequent cas ensures we notice racing
> removal of item
> +                // - loadFence ensures consistency with concurrent removal
> +                @SuppressWarnings("unchecked")
> +                E item = (E) U.getObject(p, ITEM); // = p.item;
> +
>                  if (item != null &&
>                      o.equals(item) &&
>                      casItem(p, item, null)) {
> @@ -464,6 +478,7 @@
>                  pred = p;
>              }
>          }
> +        U.loadFence();
>          return false;
>      }
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141125/5b1acc21/attachment-0001.html>

From gil at azulsystems.com  Tue Nov 25 16:38:13 2014
From: gil at azulsystems.com (Gil Tene)
Date: Tue, 25 Nov 2014 21:38:13 +0000
Subject: [concurrency-interest] A new (?) concurrency primitive:
 WriterReaderPhaser
In-Reply-To: <CAAApjO1mBZiD+AANfHVxXTGf_VyCPmPf+5c9RWksS0cGR070kA@mail.gmail.com>
References: <730432BA-BCC5-4BC4-8F2B-1DD9D1518536@azulsystems.com>
	<5473166A.7030904@gmail.com>
	<4D0323E3-8E8E-4867-B6DD-117BB48879A7@azulsystems.com>
	<5474AC98.5020106@gmail.com>
	<CAAApjO1mBZiD+AANfHVxXTGf_VyCPmPf+5c9RWksS0cGR070kA@mail.gmail.com>
Message-ID: <0C2AA13D-E34D-4474-9E68-90F5AD5C1F3B@azulsystems.com>

Peter, Pedro, 

I could draw a similar equivalence between a ReadWriteLock and a counting Semaphore. After all, you can implement the ReadWriteLock-equivalent "pattern" directly using the APIs of a counting semaphore that supports a P(n) operation (e.g. Java's Semaphore). I could draw a translation table for that too. So maybe that means ReadWriteLock is not a separate synchronization primitive? After all, if we can map the calls in some way, it is simply equivalent to and a variant of a semaphore, isn't it?

The reason that's not the case is that ReadWriteLock describes expected use assumptions and provides guarantees that are met when those use assumptions are followed, and those are useful for users that do not want to reason about generic semaphores when what they need is a ReadWriteLock. It is also not the case because ReadWriteLock can be implemented using something other than a semaphore, and still meet it's required behaviors.

The same is true for WriterReaderPhaser and Left-Right. While WriterReaderPhaser *could* be implemented using parts of Left-Right, it does not present to it's user as Left-Right any more than it presents as a common phased epoch pair (which would be simpler). It describes expected use assumptions and provides guarantees that are met when those use assumptions are followed. And those are useful for users that do not want to reason about generic phased epochs or inside-out Left-Right parts. Furthermore, using Left-Right under the hood is not the preferred implementation of WriterReaderPhaser - it doesn't need the extra complexity and levels of internal abstraction. It certainly doesn't need the internally tracked data structure or the required double-write rules that come with the prescribed use of Left-Right.

The equivalence that you try to draw between Left-Right and WriterReaderPhaser is based on looking under the hood of specific implementations and trying to deduce (unstated) guarantees from each, and new valid ways of using the construct that contradict it's declared and prescribed use (but may still be "ok" if you know what the implementation actually does). A user that tries to do that may as well make direct use of epoch counters and skip the higher level abstractions. Mapping the calls with a translation table does not map the guarantees and use rules. It also doesn't erase the "now we are doing something the instructions tell us to never do" problem either. You'd have to list a new set of expected use assumptions and guarantees provided by the calls (when only those new use assumptions are followed) to make the mapping useful. And by the time you've added that as an alternative valid use Left-right under certain rules (which would contradict it's currently stated use rules, like the requirement to write the same data twice on both sides of a toggle), you'll probably find that you've documented a new and orthogonal use case that is basically WriterReaderPhaser.

For an example of how far you have to stretch to isolate Left-Right's desired parts from it's required use rules, take a look at how convoluted the lambda expression (in your getCounters() example, the one that is passed into LeftRight.modify) ends up being, just so that it can break the rules "just right". The lambda-friendly Left-Right API clearly expects you to perform the same modification operation twice (once on each or the left and right data structures). That's what the lambda expression is for: to avoid having you actually write the modification code twice. But your lambda expression carefully checks to see which side it was asked to work on, and performs a very different operation on each of the "sides". That's quite clever, but is exactly what Left-Right tells you not to do. It works because you've followed the code down to the bottom, and you know how it's being used, and in what order. This extra logic also shows you why thinking of this as "Left-Right protecting the active pointer" doesn't work in your example. Had that been the use case, you would have been able to validly update the "active pointer" to the same value in both calls into the lambda expression (which would obviously break the required WriterReaderPhaser behavior).

Rather than go for such a convoluted application of Left-Right, below is simpler way to implement the same double buffered counts thing. This one is neither Left-Right nor WriterReaderPhaser. It's just direct use of phased epochs with no other primitives involved (unless someone wants to claim that any use of phased epochs is a "variant" of Left-Right, which would be amusing). This direct use of epochs works. It is correct. I've used this or variants of it many times myself. But it requires the author to reason about why this stuff actually works each time, and to carefully examine and protect their logic against concurrency effects on the epochs. It is missing a basic primitive that would provide well stated guarantees and would save the work (and bugs) involved in reasoning through this each time. WriterReaderPhaser captures the API and associated behavior expectations and guarantees. That's what the primitive is all about.

-------------------------------------------
Direct use of phased epochs:

public class DoubleBufferedCountsUsingEpochs {
    private AtomicLong startEpoch = new AtomicLong(0);
    private AtomicLong evenEndEpoch = new AtomicLong(0);
    private AtomicLong oddEndEpoch = new AtomicLong(Long.MIN_VALUE);

    private long oddCounts[];
    private long evenCounts[];

    private final long accumulatedCounts[];

    public DoubleBufferedCountsUsingEpochs(int size) {
        oddCounts = new long[size];
        evenCounts = new long[size];
        accumulatedCounts = new long[size];
    }

    public void incrementCount(int iTh) {
        boolean phaseIsOdd = (startEpoch.getAndIncrement() < 0);
        if (phaseIsOdd) {
            oddCounts[iTh]++;
            oddEndEpoch.getAndIncrement();
        } else {
            evenCounts[iTh]++;
            evenEndEpoch.getAndIncrement();
        }
    }

    public synchronized long[] getCounts() {
        long sourceArray[];
        long startValueAtFlip;

        // Clear currently unused [next] phase end epoch and set new startEpoch value:
        boolean nextPhaseIsEven = (startEpoch.get() < 0); // Current phase is odd...
        if (nextPhaseIsEven) {
            evenEndEpoch.set(0);
            startValueAtFlip = startEpoch.getAndSet(0);
            sourceArray = oddCounts;
        } else {
            oddEndEpoch.set(Long.MIN_VALUE);
            startValueAtFlip = startEpoch.getAndSet(Long.MIN_VALUE);
            sourceArray = evenCounts;
        }

        // Spin until previous phase end epoch value catches up with start value at flip:
        while ((nextPhaseIsEven && (oddEndEpoch.get() != startValueAtFlip)) ||
                (!nextPhaseIsEven && (evenEndEpoch.get() != startValueAtFlip))) {
            Thread.yield();
        }

        // sourceArray is stable. Use it:
        for (int i = 0; i < sourceArray.length; i++) {
            accumulatedCounts[i] += sourceArray[i];
            sourceArray[i] = 0;
        }

        return accumulatedCounts.clone();
    }
}


Yes. You can use Left-Right per your description below (for LRCounters) with the strange rule-breaking lambda expression, and you can use this simple phased epoch approach above (or many other variants). But *do you want to*? Does using it make it easier or harder to reason about? To me (and to most people, I think) the direct use of epochs is much more readable and easier to reason about for double buffered case than using Left-Right while standing on your head (using it to protect an internal fields, where all roles are reversed from their documented meanings). But neither one of them relieves me of the need to figure out and derive the concurrency behavior for myself, leaving me with unneeded effort and lots of potential bug tails. Much like a ReaderWriterLock saves unneeded effort, pain and bugs when compared to direct use of counting semaphores for the same purpose, WriterReaderPhaser save this effort and pain for double buffered uses looking for wait free writers.

The code below is not only easier to read, it is much easier to *write*, design, and reason about. The author simply follows the recipe spelled out in the WriterReaderPhaser JavaDoc. By carefully following the rules (and without needing to understand why and how) the author knows that the access patterns are guaranteed to be safe.

----------------------------------
Direct use of WriterReaderPhaser:

public class DoubleBufferedCountsUsingWRP {
    WriterReaderPhaser phaser = new WriterReaderPhaser();
    private long activeCounts[];
    private long inactiveCounts[];

    private final long accumulatedCounts[];

    public DoubleBufferedCountsUsingWRP(int size) {
        activeCounts = new long[size];
        inactiveCounts = new long[size];
        accumulatedCounts = new long[size];
    }

    public void incrementCount(int iTh) {
        long criticalValue = phaser.writerCriticalSectionEnter();
        try {
            activeCounts[iTh]++;
        } finally {
            phaser.writerCriticalSectionExit(criticalValue);
        }
    }

    public synchronized long[] getCounts() {
        try {
            phaser.readerLock();

            // switch active and inactive data structures:
            long tmp[] = activeCounts;
            activeCounts = inactiveCounts;
            inactiveCounts = tmp;

            // flip WriterReaderPhaser phase:
            phaser.flipPhase();

            // use stable (newly inactive) data:
            for (int i = 0; i < inactiveCounts.length; i++) {
                accumulatedCounts[i] += inactiveCounts[i];
                inactiveCounts[i] = 0;
            }
        } finally {
            phaser.readerUnlock();
        }
        return accumulatedCounts.clone();
    }
}

> On Nov 25, 2014, at 8:52 AM, Pedro Ramalhete <pramalhe at gmail.com> wrote:
> 
> Yes Peter, it makes absolute sense!
> To make it completely clear just "how equivalent" these two methods are, let me add a "translation table" for the APIs, from WriterReaderPhaser to Left-Right:
>         ? writerCriticalSectionEnter   -> readIndicator.arrive()
>         ? writerCriticalSectionExit      -> readIndicator.depart()
>         ? readerLock                          -> writersMutex.lock()
>         ? readerUnlock                      -> writersMutex.unlock()
>         ? flipPhase                             -> toggleVersionAndScan()
> 
> Thanks,
> Pedro
> 
> On Tue, Nov 25, 2014 at 5:21 PM, Peter Levart <peter.levart at gmail.com> wrote:
> Hi Gil,
> 
> I think Pedro is right in claiming that WriteReaderPhaser is a kind of Left-Right, but he's wrong in explaining that it is a Left-Right used backwards. In Left-Right terminology, WriteReaderPhaser is not protecting the mutable structure (mutated from multiple threads), but just coordinating access to the "active pointer" to the structure". From Left-Right perspective, multiple threads are just readers of the "active pointer" (what they do with the underlying structure is not relevant here - the underlying structure has it's own synchronization). The single thread (at a time) that wants to get access to the snapshot is the sole writer  (or "swapper") of the "active pointer". Of course, the sole writer or "swapper" of the active pointer also exploits the fact that the "inactive pointer" is not being accessed by any current readers of the "active pointer" and so, the underlying structure is not touched by the "readers".
> 
> I agree with all your statements below about WriteReaderPhaser and I can see how WriteReaderPhaser API is more suited to the pointer flipping scenarios, but WriteReaderPhaser and Left-Right can be used interchangeably. They are, in a sense equivalent.
> 
> To illustrate, I'll use the lambda-enabled Left-Right API:
> 
> http://cr.openjdk.java.net/~plevart/misc/LeftRight/LeftRight.java
> 
> ...and try to re-create your example from below:
> 
> public class LRCounters {
> 
>     static class ArrayRef {
>         long[] array;
> 
>         ArrayRef(long[] array) {
>             this.array = array;
>         }
>     }
> 
>     private final LeftRight<ArrayRef> counts;
>     private long intervalCounts[];
>     private final long accumulatedCounts[];
> 
>     public LRCounters(int size) {
>         intervalCounts = new long[size];
>         accumulatedCounts = new long[size];
>         counts = new LeftRight<>(
>             // this is the initially active ArrayRef
>             new ArrayRef(new long[size]),
>             // and this is the initially inactive one
>             new ArrayRef(null)
>         );
>     }
> 
>     public void incrementCount(int iTh) {
>         counts.read(iTh, (i, arrayRef) -> {
>             long a[] = arrayRef.array; // this is the read operation
>             return ++a[i]; // never mind the racy increment (should do it with atomics)
>         });
>     }
> 
>     public long[] getCounts() {
>         long[][] result = new long[1][];
> 
>         counts.modify((arrayRef) -> {
>             if (arrayRef.array == null) {
>                 // we've got the previously inactive ArrayRef
>                 arrayRef.array = intervalCounts; // this is the 1st write operation
>             } else {
>                 // we've got the previously active ArrayRef
>                 // that has just been deactivated
>                 intervalCounts = arrayRef.array;
>                 arrayRef.array = null; // this is the "mirror" write operation
>                 // add interval counts to accumulatedCounts
>                 for (int i = 0; i < intervalCounts.length; i++) {
>                     accumulatedCounts[i] += intervalCounts[i];
>                     intervalCounts[i] = 0;
>                 }
>                 // return result
>                 result[0] = accumulatedCounts.clone();
>             }
>         });
> 
>         return result[0];
>     }
> }
> 
> 
> Likewise, let's take an example that is more suited to LeftRight API:
> 
> public class LRMap<K, V> {
> 
>     private final LeftRight<Map<K, V>> lrMap = new LeftRight<>(new HashMap<>(), new HashMap<>());
> 
>     public V get(K key) {
>         return lrMap.read(m -> m.get(key));
>     }
> 
>     public void put(K key, V value) {
>         lrMap.modify(m -> m.put(key, value));
>     }
> }
> 
> ...and try to implement is using WriteReaderPhaser:
> 
> public class WRPMap<K, V> {
> 
>     private final WriterReaderPhaser wrp = new WriterReaderPhaser();
>     private volatile Map<K, V> activeMap = new HashMap<>();
>     private volatile Map<K, V> inactiveMap = new HashMap<>();
> 
>     public V get(K key) {
>         long stamp = wrp.writerCriticalSectionEnter();
>         try {
>             return activeMap.get(key);
>         } finally {
>             wrp.writerCriticalSectionExit(stamp);
>         }
>     }
> 
>     public void put(K key, V value) {
>         wrp.readerLock();
>         try {
>             Map<K, V> m1 = inactiveMap;
>             Map<K, V> m2 = activeMap;
>             m1.put(key, value); // 1st write to inactive
>             // swap active <-> inactive
>             activeMap = m1;
>             inactiveMap = m2;
> 
>             wrp.flipPhase();
> 
>             m2.put(key, value); // mirror write to just deactivated
>         } finally {
>             wrp.readerUnlock();
>         }
>     }
> }
> 
> Does this make any sense?
> 
> Regards, Peter
> 
> On 11/25/2014 08:39 AM, Gil Tene wrote:
> Pedro, I think you are confusing specific under-the-hood implementation choices (which are similar) with what the primitive is. I'm flattered at your naming of Left-Right GT, but Left-Right GT (and the LRTreeSetGT example) is a Left-Right variant with a different underlying (attributed to me) arrive-depart technique. It is not a WriterReaderPhaser.
> 
> WriterReaderPhaser captures (in a clean synchronization primitive API form) a pattern I've had to use and re-invent myself several times, and I'm pretty sure many others that have faced the "I want to periodically report/analyze an actively updating data set" have too. The key here is capturing the guarantees and prescribed use such that end-users can use the primitive without needing to understand the underlying logic of an implementation. I do that in my blog entry (and include a definition and use example below).
> 
> A WriterReaderPhaser is neither a ReadWriteLock nor a ReadWriteLock used backwards. It's also not Left-Right, or Left-Right used backwards. The qualities and guarantees a WriterReaderPhaser provides are not provided by reversing the meaning of "writer" and "reader" in those primitives. Even if you ignore the confusion that such upside-down use may cause the user, there are specific guarantees that the other primitives do not provide, and that a write-heavy double-buffered use case needs and gets from this primitive.
> 
> And yes, there are several possible ways to implement a well behaving WriterReaderPhaser, one of which is listed in the links I provided. We can implement it with three atomic words and some clever combos of CAS and GetAndAdd ops, or in other ways. The implementation is not what makes the primitive what it is to it's users. It's the API and the behavior guarantees that do that. And I'm pretty sure these behavior guarantees are not spelled out or provided (even backwards) in Left-Right and variants. Some of them (like the data stability guarantee for readers even in the presence of wait-free write activity) would be un-natural to provide in reverse for writers (since readers are generally not expected to change stuff).
> 
> Left-Right is cool (really cool), but it focuses purely on wait-free readers and blocking writers. While that use case may appear to be "the opposite" of wait-free writers with blocking readers, there are specific non-mirroring qualities that make that duality invalid. Here are specific differences between the two mechanisms that make "backwards" use inapplicable::
> 
> - WriterReaderPhaser provides specific data stability guarantees to readers (after a flip while under a readLock), even in the presence of concurrent writer activity. Left-Right does not provide such a guarantee to writers "backwards". E.g. if Left-Right readers happened to write into the Left-Right protected data structure (as they would need to in a "backwards use" attempt like this), Left-Right says nothing about what writers can expect from that data structure in terms of data consistency or stability. Note that I'm not saying that no Left-Right implementation could accidentally provide this behavior without stating it. I'm saying that the Left-Right mechanism, as described and documented in Left-Right paper and the various APIs for it's existing variants makes no such guarantee to the caller, and that a valid Left-Right implementation may or may not provide this behavior. As such, the user cannot rely on it. And this is the main guarantee a typical WriterReaderPhaser user will be looking for.
> 
> - Left-Right specifically prohibits readers from writing. E.g. "...To access in read-only mode do something like this:..." is stated in the documentation for a LeftRightGuard variants.  In contrast, WriterReaderPhaser allows it's writers (which would be the readers in a backwards mapping attempt) to, um, write...
> 
> - Writers that use Left-Right are required to to write their updates twice (on the two sides of a "writerToggle" or equivalent "flip"). e.g. "...The exact same operations must be done on the instance before and after guard.writeToggle()." is stated in the documentation for a LeftRightGuard variants. In contrast, WriterReaderPhaser does not require reading twice or writing twice. The two APIs do not mirror each other in this critical aspect.
> 
> - Left-Right (even when used to replace a Reader-Writer lock) manages the active and inactive data structures internally (leftInstance and rightInstance, or firstInstance and secondInstance), and users of Left-right [must] operate on data structures returned from Left-Right operations. In contrast, WriterReaderPhaser does not manage the active and inactive data structures in any way, leaving that job to readers and writers that operate directly on the shared data structures.
> 
> To be specific, let me detail what a WriterReaderPhaser is (taken from an updated blog entry that now includes a definition):
> 
> -----------------------------------------------------------------
> Definition of WriterReaderPhaser:
> 
> A WriterReaderPhaser provides a means for wait free writers to coordinate with blocking (but guaranteed forward progress) readers sharing a set of data structures.
> 
> A WriterReaderPhaser instance provides the following 5 operations:
> 
>         ? writerCriticalSectionEnter
>         ? writerCriticalSectionExit
>         ? readerLock
>         ? readerUnlock
>         ? flipPhase
> 
> When a WriterReaderPhaser  instance is used to protect an active [set of or single] data structure involving [potentially multiple] writers and [potentially multiple] readers , the assumptions on how readers and writers act are:
> 
>         ? There are two sets of data structures (an "active" set and an "inactive" set)
>         ? Writing is done to the perceived active version (as perceived by the writer), and only within critical sections delineated by writerCriticalSectionEnter and writerCriticalSectionExit operations.
>         ? Only readers switch the perceived roles of the active and inactive data structures. They do so only while holding the readerLock, and only before execution a flipPhase.
> 
>         ? Readers do not hold onto readerLock indefinitely. Only readers perform readerLock and readerUnlock.
>         ? Writers do not remain in their critical sections indefinitely. Only writers perform writerCriticalSectionEnter and writerCriticalSectionExit.
>         ? Only readers perform flipPhase operations, and only while holding the readerLock.
> 
> When the above assumptions are met, WriterReaderPhaser guarantees that the inactive data structures are not being modified by any writers while being read while under readerLock protection after a flipPhase operation.
> 
> The following progress guarantees are provided to writers and readers that adhere to the above stated assumptions:
>         ? Writers operations (writerCriticalSectionEnter and  writerCriticalSectionExit) are wait free (on architectures that support wait-free atomic increment operations).
>         ? flipPhase operations are guaranteed to make forward progress, and will only be blocked by writers whose critical sections were entered prior to the start of the reader's flipPhase operation, and have not yet exited their critical sections.
>         ? readerLock only block for other readers that are holding the readerLock.
> 
> -----------------------------------------------------------------
> Example use:
> 
> Imagine a simple use case where a large set of rapidly updated counter is being modified by writers, and a reader needs to gain access to stable interval samples of those counters for reporting and other analysis purposes.
> 
> The counters are represented in a volatile array of values (it is the array reference that is volatile, not the value cells within it):
> 
> volatile long counts[];
> ...
> 
> A writer updates a specific count (n) in the set of counters:
> 
> writerCriticalSectionEnter
>     counts[n]++;
> writerCriticalSectionExit
> 
> A reader gain access to a stable set of counts collected during an interval, reports on it, and accumulates it:
> 
> long intervalCounts[];
> long accumulated_counts[];
> 
> ...
> readerLock
>     reset(interval_counts);
>     long tmp[] = counts;
>     counts = interval_counts;
>     interval_counts = tmp;
> flipPhase
>     report_interval_counts(interval_counts);
>     accumulated_counts.add(interval_counts);
> readerUnlock
> -----------------------------------------------------------------
> 
> 
> Bottom line: the above is defines what a WriterReaderPhaser primitive is, and shows a simple example of using it. While I provide an example implementation, many are possible, and I'm sure another 17 will pop up. To avoid wrongly taking credit for this as a new primitive, I'm looking to see if there have been previously described primitives that explicitly provide these (or equivalent) qualities to their users. "Explicitly" being a key word (since no sane user would rely on an accidental implicit behavior of a specific implementation of a primitive that does not actually guarantee the given behavior).
> 
> -- Gil.
> 
> On Nov 24, 2014, at 3:28 PM, Pedro Ramalhete wrote:
> Hi Peter, If I was you I wouldn't bother with it. As I tried to explain to Gil, the WriterReaderPhaser uses the same concurrency control algorithm as the Left-Right, and as such it is a variant of the Left-Right (used "backwards") that uses a (new) ReadIndicator with a single ingress combined with versionIndex. This variant is not as good for scalability under high contention as the one you yourself have implemented some time ago, with the ReadIndicator of ingress/egress with LongAdders. You're better off using your own implementation, and just do the mutations in lrSet.read() and the read-only operation in lrSet.modify(), but of course, feel free to try both and let us your results ;)
> http://cr.openjdk.java.net/~plevart/misc/LeftRight/EnterExitWait.java
> http://cr.openjdk.java.net/~plevart/misc/LeftRight/LongAdderEEW.java
> http://cr.openjdk.java.net/~plevart/misc/LeftRight/LRTest.java
> http://concurrencyfreaks.com/2014/11/left-right-gt-variant.html Cheers, Pedro
> 
> On Nov 24, 2014, at 3:28 AM, Peter Levart wrote:
> 
> Hi Gil,
> 
> What a coincidence. I was thinking of writing something like that myself in past couple of days for a similar purpose. It comes as a gift that you posted this here, thanks!
> 
> My application is an asynchronous cache store implementation. A distributed cache (Coherence in my case) emits synchronous events when cache is updated from multiple threads. I want to batch updates and do asynchronous persistence in a background thread. Coherence already supports this by itself, but is rather limited in features, so I have to re-create this functionality and add missing features.
> 
> Regards, Peter
> 
> On 11/24/2014 06:54 AM, Gil Tene wrote:
> Yeah, Yeah, I know. A new concurrency primitive? Really? But I think this may actually be a new, generically useful primitive. Basically, if you ever needed to analyze or log rapidly mutating data without blocking or locking out writers, this thing is for you. It supports wait-free writers, and stable readable data sets for guaranteed-forward-progress readers. And it makes double buffered data management semi-trivial. See blog entry explaining stuff : "WriterReaderPhaser: A story about a new (?) synchronization primitive"<http://stuff-gil-says.blogspot.com/2014/11/writerreaderphaser-story-about-new.html>. (with some interesting discussion comparing it to Left-Right, which does the opposite thing: wait free readers with blocking writers). See a simple (and very practical) example of using the primitive at: https://github.com/HdrHistogram/HdrHistogram/blob/master/src/main/java/org/HdrHistogram/IntervalHistogramRecorder.java And see the primitive qualities and use rules documented (in the JavaDoc) along with a working implementation at: https://github.com/HdrHistogram/HdrHistogram/blob/master/src/main/java/org/HdrHistogram/WriterReaderPhaser.java So please rip this thing apart? Or consider if it may be a useful addition to j.u.c. It needs a home. And if you've seen it before (i.e. it's not really new like I seem to think it is), I'd really like to know. ? Gil.
> _______________________________________________ Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> 
> 
> 
> 



From aph at redhat.com  Tue Nov 25 16:41:46 2014
From: aph at redhat.com (Andrew Haley)
Date: Tue, 25 Nov 2014 21:41:46 +0000
Subject: [concurrency-interest] RFR: 8065804: JEP 171:
 Clarifications/corrections for fence intrinsics
In-Reply-To: <CA+kOe08RfnkJ4Dit8ng7HQaSuDer_pA0i4t568uAGMg9dzujhg@mail.gmail.com>
References: <CA+kOe08RfnkJ4Dit8ng7HQaSuDer_pA0i4t568uAGMg9dzujhg@mail.gmail.com>
Message-ID: <5474F79A.7000607@redhat.com>

On 11/24/2014 08:56 PM, Martin Buchholz wrote:
> Hi folks,
> 
> Review carefully - I am trying to learn about fences by explaining them!
> I have borrowed some wording from my reviewers!

+     * Currently hotspot's implementation of a Java language-level volatile
+     * store has the same effect as a storeFence followed by a relaxed store,
+     * although that may be a little stronger than needed.

While this may be true today, I'm hopefully about to commit an
AArch64 OpenJDK port that uses the ARMv8 stlr instruction.  I
don't think that what you've written here is terribly misleading,
but bear in mind that it may be there for some time.

Andrew.


From stephan.diestelhorst at gmail.com  Tue Nov 25 18:24:59 2014
From: stephan.diestelhorst at gmail.com (Stephan Diestelhorst)
Date: Tue, 25 Nov 2014 23:24:59 +0000
Subject: [concurrency-interest] RFR: 8065804: JEP 171:
	Clarifications/corrections for fence intrinsics
In-Reply-To: <CAPUmR1aZNSdk5znSfp4ei1M+ZK1M06uEYujH3Z2_B1wtOqMMhw@mail.gmail.com>
References: <CA+kOe08RfnkJ4Dit8ng7HQaSuDer_pA0i4t568uAGMg9dzujhg@mail.gmail.com>
	<21DC5F06-9597-4CCB-8A25-5CC354A0BCE5@flyingtroika.com>
	<CAPUmR1aZNSdk5znSfp4ei1M+ZK1M06uEYujH3Z2_B1wtOqMMhw@mail.gmail.com>
Message-ID: <2936699.i7HuM0iO2v@d-allen>

Am Dienstag, 25. November 2014, 11:15:36 schrieb Hans Boehm:
> I'm no hardware architect, but fundamentally it seems to me that
> 
> load x
> acquire_fence
> 
> imposes a much more stringent constraint than
> 
> load_acquire x
> 
> Consider the case in which the load from x is an L1 hit, but a preceding
> load (from say y) is a long-latency miss.  If we enforce ordering by just
> waiting for completion of prior operation, the former has to wait for the
> load from y to complete; while the latter doesn't.  I find it hard to
> believe that this doesn't leave an appreciable amount of performance on the
> table, at least for some interesting microarchitectures.

I agree, Hans, that this is a reasonable assumption.  Load_acquire x
does allow roach motel, whereas the acquire fence does not.

>  In addition, for better or worse, fencing requirements on at least
>  Power are actually driven as much by store atomicity issues, as by
>  the ordering issues discussed in the cookbook.  This was not
>  understood in 2005, and unfortunately doesn't seem to be amenable to
>  the kind of straightforward explanation as in Doug's cookbook.

Coming from a strongly ordered architecture to a weakly ordered one
myself, I also needed some mental adjustment about store (multi-copy)
atomicity.  I can imagine others will be unaware of this difference,
too, even in 2014.

Stephan


From davidcholmes at aapt.net.au  Tue Nov 25 18:41:35 2014
From: davidcholmes at aapt.net.au (David Holmes)
Date: Wed, 26 Nov 2014 09:41:35 +1000
Subject: [concurrency-interest] RFR: 8065804: JEP
	171:Clarifications/corrections for fence intrinsics
In-Reply-To: <2936699.i7HuM0iO2v@d-allen>
Message-ID: <NFBBKALFDCPFIDBNKAPCCECLKLAA.davidcholmes@aapt.net.au>

Stephan Diestelhorst writes:
>
> Am Dienstag, 25. November 2014, 11:15:36 schrieb Hans Boehm:
> > I'm no hardware architect, but fundamentally it seems to me that
> >
> > load x
> > acquire_fence
> >
> > imposes a much more stringent constraint than
> >
> > load_acquire x
> >
> > Consider the case in which the load from x is an L1 hit, but a preceding
> > load (from say y) is a long-latency miss.  If we enforce
> ordering by just
> > waiting for completion of prior operation, the former has to
> wait for the
> > load from y to complete; while the latter doesn't.  I find it hard to
> > believe that this doesn't leave an appreciable amount of
> performance on the
> > table, at least for some interesting microarchitectures.
>
> I agree, Hans, that this is a reasonable assumption.  Load_acquire x
> does allow roach motel, whereas the acquire fence does not.
>
> >  In addition, for better or worse, fencing requirements on at least
> >  Power are actually driven as much by store atomicity issues, as by
> >  the ordering issues discussed in the cookbook.  This was not
> >  understood in 2005, and unfortunately doesn't seem to be amenable to
> >  the kind of straightforward explanation as in Doug's cookbook.
>
> Coming from a strongly ordered architecture to a weakly ordered one
> myself, I also needed some mental adjustment about store (multi-copy)
> atomicity.  I can imagine others will be unaware of this difference,
> too, even in 2014.

Sorry I'm missing the connection between fences and multi-copy atomicity.

David

> Stephan
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From stephan.diestelhorst at gmail.com  Tue Nov 25 18:52:42 2014
From: stephan.diestelhorst at gmail.com (Stephan Diestelhorst)
Date: Tue, 25 Nov 2014 23:52:42 +0000
Subject: [concurrency-interest] RFR: 8065804: JEP
	171:Clarifications/corrections for fence intrinsics
In-Reply-To: <NFBBKALFDCPFIDBNKAPCCECLKLAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCCECLKLAA.davidcholmes@aapt.net.au>
Message-ID: <2458774.qkf0xEMZGz@d-allen>

David Holmes wrote:
> Stephan Diestelhorst writes:
> > Am Dienstag, 25. November 2014, 11:15:36 schrieb Hans Boehm:
> > > I'm no hardware architect, but fundamentally it seems to me that
> > >
> > > load x
> > > acquire_fence
> > >
> > > imposes a much more stringent constraint than
> > >
> > > load_acquire x
> > >
> > > Consider the case in which the load from x is an L1 hit, but a
> > > preceding load (from say y) is a long-latency miss.  If we enforce
> > > ordering by just waiting for completion of prior operation, the
> > > former has to wait for the load from y to complete; while the
> > > latter doesn't.  I find it hard to believe that this doesn't leave
> > > an appreciable amount of performance on the table, at least for
> > > some interesting microarchitectures.
> >
> > I agree, Hans, that this is a reasonable assumption.  Load_acquire x
> > does allow roach motel, whereas the acquire fence does not.
> >
> > >  In addition, for better or worse, fencing requirements on at least
> > >  Power are actually driven as much by store atomicity issues, as by
> > >  the ordering issues discussed in the cookbook.  This was not
> > >  understood in 2005, and unfortunately doesn't seem to be amenable to
> > >  the kind of straightforward explanation as in Doug's cookbook.
> >
> > Coming from a strongly ordered architecture to a weakly ordered one
> > myself, I also needed some mental adjustment about store (multi-copy)
> > atomicity.  I can imagine others will be unaware of this difference,
> > too, even in 2014.
> 
> Sorry I'm missing the connection between fences and multi-copy atomicity.

One example is the classic IRIW.  With non-multi copy atomic stores, but
ordered (say through a dependency) loads in the following example:

Memory: foo = bar = 0
_T1_         _T2_         _T3_                              _T4_
st (foo),1   st (bar),1   ld r1, (bar)                      ld r3,(foo)
                          <addr dep / local "fence" here>   <addr dep>
                          ld r2, (foo)                      ld r4, (bar)

You may observe r1 = 1, r2 = 0, r3 = 1, r4 = 0 on non-multi-copy atomic
machines.  On TSO boxes, this is not possible.  That means that the
memory fence that will prevent such a behaviour (DMB on ARM) needs to
carry some additional oomph in ensuring multi-copy atomicity, or rather
prevent you from seeing it (which is the same thing).

Stephan


From davidcholmes at aapt.net.au  Tue Nov 25 19:36:12 2014
From: davidcholmes at aapt.net.au (David Holmes)
Date: Wed, 26 Nov 2014 10:36:12 +1000
Subject: [concurrency-interest] RFR: 8065804:
	JEP171:Clarifications/corrections for fence intrinsics
In-Reply-To: <2458774.qkf0xEMZGz@d-allen>
Message-ID: <NFBBKALFDCPFIDBNKAPCAECMKLAA.davidcholmes@aapt.net.au>

Stephan Diestelhorst writes:
>
> David Holmes wrote:
> > Stephan Diestelhorst writes:
> > > Am Dienstag, 25. November 2014, 11:15:36 schrieb Hans Boehm:
> > > > I'm no hardware architect, but fundamentally it seems to me that
> > > >
> > > > load x
> > > > acquire_fence
> > > >
> > > > imposes a much more stringent constraint than
> > > >
> > > > load_acquire x
> > > >
> > > > Consider the case in which the load from x is an L1 hit, but a
> > > > preceding load (from say y) is a long-latency miss.  If we enforce
> > > > ordering by just waiting for completion of prior operation, the
> > > > former has to wait for the load from y to complete; while the
> > > > latter doesn't.  I find it hard to believe that this doesn't leave
> > > > an appreciable amount of performance on the table, at least for
> > > > some interesting microarchitectures.
> > >
> > > I agree, Hans, that this is a reasonable assumption.  Load_acquire x
> > > does allow roach motel, whereas the acquire fence does not.
> > >
> > > >  In addition, for better or worse, fencing requirements on at least
> > > >  Power are actually driven as much by store atomicity issues, as by
> > > >  the ordering issues discussed in the cookbook.  This was not
> > > >  understood in 2005, and unfortunately doesn't seem to be
> amenable to
> > > >  the kind of straightforward explanation as in Doug's cookbook.
> > >
> > > Coming from a strongly ordered architecture to a weakly ordered one
> > > myself, I also needed some mental adjustment about store (multi-copy)
> > > atomicity.  I can imagine others will be unaware of this difference,
> > > too, even in 2014.
> >
> > Sorry I'm missing the connection between fences and multi-copy
> atomicity.
>
> One example is the classic IRIW.  With non-multi copy atomic stores, but
> ordered (say through a dependency) loads in the following example:
>
> Memory: foo = bar = 0
> _T1_         _T2_         _T3_                              _T4_
> st (foo),1   st (bar),1   ld r1, (bar)                      ld r3,(foo)
>                           <addr dep / local "fence" here>   <addr dep>
>                           ld r2, (foo)                      ld r4, (bar)
>
> You may observe r1 = 1, r2 = 0, r3 = 1, r4 = 0 on non-multi-copy atomic
> machines.  On TSO boxes, this is not possible.  That means that the
> memory fence that will prevent such a behaviour (DMB on ARM) needs to
> carry some additional oomph in ensuring multi-copy atomicity, or rather
> prevent you from seeing it (which is the same thing).

I take it as given that any code for which you may have ordering
constraints, must first have basic atomicity properties for loads and
stores. I would not expect any kind of fence to add multi-copy-atomicity
where there was none.

David

> Stephan
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From boehm at acm.org  Tue Nov 25 21:04:12 2014
From: boehm at acm.org (Hans Boehm)
Date: Tue, 25 Nov 2014 18:04:12 -0800
Subject: [concurrency-interest] RFR: 8065804:
 JEP171:Clarifications/corrections for fence intrinsics
In-Reply-To: <NFBBKALFDCPFIDBNKAPCAECMKLAA.davidcholmes@aapt.net.au>
References: <2458774.qkf0xEMZGz@d-allen>
	<NFBBKALFDCPFIDBNKAPCAECMKLAA.davidcholmes@aapt.net.au>
Message-ID: <CAPUmR1YdWyemKveLA0eHpv_MC=5BjVJtfLkj2RTZAhz_NFivog@mail.gmail.com>

To be concrete here, on Power, loads can normally be ordered by an address
dependency or light-weight fence (lwsync).  However, neither is enough to
prevent the questionable outcome for IRIW, since it doesn't ensure that the
stores in T1 and T2 will be made visible to other threads in a consistent
order.  That outcome can be prevented by using heavyweight fences (sync)
instructions between the loads instead.  Peter Sewell's group concluded
that to enforce correct volatile behavior on Power, you essentially need a
a heavyweight fence between every pair of volatile operations on Power.
That cannot be understood based on simple ordering constraints.

As Stephan pointed out, there are similar issues on ARM, but they're less
commonly encountered in a Java implementation.  If you're lucky, you can
get to the right implementation recipe by looking at only reordering, I
think.


On Tue, Nov 25, 2014 at 4:36 PM, David Holmes <davidcholmes at aapt.net.au>
wrote:

> Stephan Diestelhorst writes:
> >
> > David Holmes wrote:
> > > Stephan Diestelhorst writes:
> > > > Am Dienstag, 25. November 2014, 11:15:36 schrieb Hans Boehm:
> > > > > I'm no hardware architect, but fundamentally it seems to me that
> > > > >
> > > > > load x
> > > > > acquire_fence
> > > > >
> > > > > imposes a much more stringent constraint than
> > > > >
> > > > > load_acquire x
> > > > >
> > > > > Consider the case in which the load from x is an L1 hit, but a
> > > > > preceding load (from say y) is a long-latency miss.  If we enforce
> > > > > ordering by just waiting for completion of prior operation, the
> > > > > former has to wait for the load from y to complete; while the
> > > > > latter doesn't.  I find it hard to believe that this doesn't leave
> > > > > an appreciable amount of performance on the table, at least for
> > > > > some interesting microarchitectures.
> > > >
> > > > I agree, Hans, that this is a reasonable assumption.  Load_acquire x
> > > > does allow roach motel, whereas the acquire fence does not.
> > > >
> > > > >  In addition, for better or worse, fencing requirements on at least
> > > > >  Power are actually driven as much by store atomicity issues, as by
> > > > >  the ordering issues discussed in the cookbook.  This was not
> > > > >  understood in 2005, and unfortunately doesn't seem to be
> > amenable to
> > > > >  the kind of straightforward explanation as in Doug's cookbook.
> > > >
> > > > Coming from a strongly ordered architecture to a weakly ordered one
> > > > myself, I also needed some mental adjustment about store (multi-copy)
> > > > atomicity.  I can imagine others will be unaware of this difference,
> > > > too, even in 2014.
> > >
> > > Sorry I'm missing the connection between fences and multi-copy
> > atomicity.
> >
> > One example is the classic IRIW.  With non-multi copy atomic stores, but
> > ordered (say through a dependency) loads in the following example:
> >
> > Memory: foo = bar = 0
> > _T1_         _T2_         _T3_                              _T4_
> > st (foo),1   st (bar),1   ld r1, (bar)                      ld r3,(foo)
> >                           <addr dep / local "fence" here>   <addr dep>
> >                           ld r2, (foo)                      ld r4, (bar)
> >
> > You may observe r1 = 1, r2 = 0, r3 = 1, r4 = 0 on non-multi-copy atomic
> > machines.  On TSO boxes, this is not possible.  That means that the
> > memory fence that will prevent such a behaviour (DMB on ARM) needs to
> > carry some additional oomph in ensuring multi-copy atomicity, or rather
> > prevent you from seeing it (which is the same thing).
>
> I take it as given that any code for which you may have ordering
> constraints, must first have basic atomicity properties for loads and
> stores. I would not expect any kind of fence to add multi-copy-atomicity
> where there was none.
>
> David
>
> > Stephan
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141125/751d5d06/attachment-0001.html>

From davidcholmes at aapt.net.au  Tue Nov 25 21:10:58 2014
From: davidcholmes at aapt.net.au (David Holmes)
Date: Wed, 26 Nov 2014 12:10:58 +1000
Subject: [concurrency-interest] RFR: 8065804:
	JEP171:Clarifications/corrections for fence intrinsics
In-Reply-To: <CAPUmR1YdWyemKveLA0eHpv_MC=5BjVJtfLkj2RTZAhz_NFivog@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCIECMKLAA.davidcholmes@aapt.net.au>

Hi Hans,

Given IRIW is a thorn in everyone's side and has no known useful benefit, and can hopefully be killed off in the future, lets not get bogged down in IRIW. But none of what you say below relates to multi-copy-atomicity.

Cheers,
David
  -----Original Message-----
  From: hjkhboehm at gmail.com [mailto:hjkhboehm at gmail.com]On Behalf Of Hans Boehm
  Sent: Wednesday, 26 November 2014 12:04 PM
  To: dholmes at ieee.org
  Cc: Stephan Diestelhorst; concurrency-interest at cs.oswego.edu; core-libs-dev
  Subject: Re: [concurrency-interest] RFR: 8065804: JEP171:Clarifications/corrections for fence intrinsics


  To be concrete here, on Power, loads can normally be ordered by an address dependency or light-weight fence (lwsync).  However, neither is enough to prevent the questionable outcome for IRIW, since it doesn't ensure that the stores in T1 and T2 will be made visible to other threads in a consistent order.  That outcome can be prevented by using heavyweight fences (sync) instructions between the loads instead.  Peter Sewell's group concluded that to enforce correct volatile behavior on Power, you essentially need a a heavyweight fence between every pair of volatile operations on Power.  That cannot be understood based on simple ordering constraints.


  As Stephan pointed out, there are similar issues on ARM, but they're less commonly encountered in a Java implementation.  If you're lucky, you can get to the right implementation recipe by looking at only reordering, I think.




  On Tue, Nov 25, 2014 at 4:36 PM, David Holmes <davidcholmes at aapt.net.au> wrote:

    Stephan Diestelhorst writes:
    >
    > David Holmes wrote:
    > > Stephan Diestelhorst writes:
    > > > Am Dienstag, 25. November 2014, 11:15:36 schrieb Hans Boehm:
    > > > > I'm no hardware architect, but fundamentally it seems to me that
    > > > >
    > > > > load x
    > > > > acquire_fence
    > > > >
    > > > > imposes a much more stringent constraint than
    > > > >
    > > > > load_acquire x
    > > > >
    > > > > Consider the case in which the load from x is an L1 hit, but a
    > > > > preceding load (from say y) is a long-latency miss.  If we enforce
    > > > > ordering by just waiting for completion of prior operation, the
    > > > > former has to wait for the load from y to complete; while the
    > > > > latter doesn't.  I find it hard to believe that this doesn't leave
    > > > > an appreciable amount of performance on the table, at least for
    > > > > some interesting microarchitectures.
    > > >
    > > > I agree, Hans, that this is a reasonable assumption.  Load_acquire x
    > > > does allow roach motel, whereas the acquire fence does not.
    > > >
    > > > >  In addition, for better or worse, fencing requirements on at least
    > > > >  Power are actually driven as much by store atomicity issues, as by
    > > > >  the ordering issues discussed in the cookbook.  This was not
    > > > >  understood in 2005, and unfortunately doesn't seem to be
    > amenable to
    > > > >  the kind of straightforward explanation as in Doug's cookbook.
    > > >
    > > > Coming from a strongly ordered architecture to a weakly ordered one
    > > > myself, I also needed some mental adjustment about store (multi-copy)
    > > > atomicity.  I can imagine others will be unaware of this difference,
    > > > too, even in 2014.
    > >
    > > Sorry I'm missing the connection between fences and multi-copy
    > atomicity.
    >
    > One example is the classic IRIW.  With non-multi copy atomic stores, but
    > ordered (say through a dependency) loads in the following example:
    >
    > Memory: foo = bar = 0
    > _T1_         _T2_         _T3_                              _T4_
    > st (foo),1   st (bar),1   ld r1, (bar)                      ld r3,(foo)
    >                           <addr dep / local "fence" here>   <addr dep>
    >                           ld r2, (foo)                      ld r4, (bar)
    >
    > You may observe r1 = 1, r2 = 0, r3 = 1, r4 = 0 on non-multi-copy atomic
    > machines.  On TSO boxes, this is not possible.  That means that the
    > memory fence that will prevent such a behaviour (DMB on ARM) needs to
    > carry some additional oomph in ensuring multi-copy atomicity, or rather
    > prevent you from seeing it (which is the same thing).


    I take it as given that any code for which you may have ordering
    constraints, must first have basic atomicity properties for loads and
    stores. I would not expect any kind of fence to add multi-copy-atomicity
    where there was none.

    David


    > Stephan
    >
    > _______________________________________________
    > Concurrency-interest mailing list
    > Concurrency-interest at cs.oswego.edu
    > http://cs.oswego.edu/mailman/listinfo/concurrency-interest

    _______________________________________________
    Concurrency-interest mailing list
    Concurrency-interest at cs.oswego.edu
    http://cs.oswego.edu/mailman/listinfo/concurrency-interest


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141126/deb4aeb7/attachment.html>

From elizarov at devexperts.com  Wed Nov 26 03:49:11 2014
From: elizarov at devexperts.com (Roman Elizarov)
Date: Wed, 26 Nov 2014 08:49:11 +0000
Subject: [concurrency-interest] RFR:
	8065804:	JEP171:Clarifications/corrections for fence intrinsics
In-Reply-To: <NFBBKALFDCPFIDBNKAPCIECMKLAA.davidcholmes@aapt.net.au>
References: <CAPUmR1YdWyemKveLA0eHpv_MC=5BjVJtfLkj2RTZAhz_NFivog@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCIECMKLAA.davidcholmes@aapt.net.au>
Message-ID: <c806b219426c4c8aa9e5bfa79fc9901d@exchmb02.office.devexperts.com>

There is no conceivable way to kill IRIW consistency requirement while retaining ability to prove correctness of large software systems. If IRIW of volatile variables are not consistent, then volatile reads and writes are not linearizable, which breaks linearizabiliy of all higher-level primitives build on top of them and makes formal reasoning about behavior of concurrent systems practically impossible. There are many fields where this is not acceptable.

/Roman

From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of David Holmes
Sent: Wednesday, November 26, 2014 5:11 AM
To: Hans Boehm
Cc: concurrency-interest at cs.oswego.edu; core-libs-dev
Subject: Re: [concurrency-interest] RFR: 8065804: JEP171:Clarifications/corrections for fence intrinsics

Hi Hans,

Given IRIW is a thorn in everyone's side and has no known useful benefit, and can hopefully be killed off in the future, lets not get bogged down in IRIW. But none of what you say below relates to multi-copy-atomicity.

Cheers,
David
-----Original Message-----
From: hjkhboehm at gmail.com<mailto:hjkhboehm at gmail.com> [mailto:hjkhboehm at gmail.com]On Behalf Of Hans Boehm
Sent: Wednesday, 26 November 2014 12:04 PM
To: dholmes at ieee.org<mailto:dholmes at ieee.org>
Cc: Stephan Diestelhorst; concurrency-interest at cs.oswego.edu<mailto:concurrency-interest at cs.oswego.edu>; core-libs-dev
Subject: Re: [concurrency-interest] RFR: 8065804: JEP171:Clarifications/corrections for fence intrinsics
To be concrete here, on Power, loads can normally be ordered by an address dependency or light-weight fence (lwsync).  However, neither is enough to prevent the questionable outcome for IRIW, since it doesn't ensure that the stores in T1 and T2 will be made visible to other threads in a consistent order.  That outcome can be prevented by using heavyweight fences (sync) instructions between the loads instead.  Peter Sewell's group concluded that to enforce correct volatile behavior on Power, you essentially need a a heavyweight fence between every pair of volatile operations on Power.  That cannot be understood based on simple ordering constraints.

As Stephan pointed out, there are similar issues on ARM, but they're less commonly encountered in a Java implementation.  If you're lucky, you can get to the right implementation recipe by looking at only reordering, I think.


On Tue, Nov 25, 2014 at 4:36 PM, David Holmes <davidcholmes at aapt.net.au<mailto:davidcholmes at aapt.net.au>> wrote:
Stephan Diestelhorst writes:
>
> David Holmes wrote:
> > Stephan Diestelhorst writes:
> > > Am Dienstag, 25. November 2014, 11:15:36 schrieb Hans Boehm:
> > > > I'm no hardware architect, but fundamentally it seems to me that
> > > >
> > > > load x
> > > > acquire_fence
> > > >
> > > > imposes a much more stringent constraint than
> > > >
> > > > load_acquire x
> > > >
> > > > Consider the case in which the load from x is an L1 hit, but a
> > > > preceding load (from say y) is a long-latency miss.  If we enforce
> > > > ordering by just waiting for completion of prior operation, the
> > > > former has to wait for the load from y to complete; while the
> > > > latter doesn't.  I find it hard to believe that this doesn't leave
> > > > an appreciable amount of performance on the table, at least for
> > > > some interesting microarchitectures.
> > >
> > > I agree, Hans, that this is a reasonable assumption.  Load_acquire x
> > > does allow roach motel, whereas the acquire fence does not.
> > >
> > > >  In addition, for better or worse, fencing requirements on at least
> > > >  Power are actually driven as much by store atomicity issues, as by
> > > >  the ordering issues discussed in the cookbook.  This was not
> > > >  understood in 2005, and unfortunately doesn't seem to be
> amenable to
> > > >  the kind of straightforward explanation as in Doug's cookbook.
> > >
> > > Coming from a strongly ordered architecture to a weakly ordered one
> > > myself, I also needed some mental adjustment about store (multi-copy)
> > > atomicity.  I can imagine others will be unaware of this difference,
> > > too, even in 2014.
> >
> > Sorry I'm missing the connection between fences and multi-copy
> atomicity.
>
> One example is the classic IRIW.  With non-multi copy atomic stores, but
> ordered (say through a dependency) loads in the following example:
>
> Memory: foo = bar = 0
> _T1_         _T2_         _T3_                              _T4_
> st (foo),1   st (bar),1   ld r1, (bar)                      ld r3,(foo)
>                           <addr dep / local "fence" here>   <addr dep>
>                           ld r2, (foo)                      ld r4, (bar)
>
> You may observe r1 = 1, r2 = 0, r3 = 1, r4 = 0 on non-multi-copy atomic
> machines.  On TSO boxes, this is not possible.  That means that the
> memory fence that will prevent such a behaviour (DMB on ARM) needs to
> carry some additional oomph in ensuring multi-copy atomicity, or rather
> prevent you from seeing it (which is the same thing).
I take it as given that any code for which you may have ordering
constraints, must first have basic atomicity properties for loads and
stores. I would not expect any kind of fence to add multi-copy-atomicity
where there was none.

David

> Stephan
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu<mailto:Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu<mailto:Concurrency-interest at cs.oswego.edu>
http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141126/633c3691/attachment-0001.html>

From davidcholmes at aapt.net.au  Wed Nov 26 03:54:08 2014
From: davidcholmes at aapt.net.au (David Holmes)
Date: Wed, 26 Nov 2014 18:54:08 +1000
Subject: [concurrency-interest] RFR:
	8065804:JEP171:Clarifications/corrections for fence intrinsics
In-Reply-To: <c806b219426c4c8aa9e5bfa79fc9901d@exchmb02.office.devexperts.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCIECNKLAA.davidcholmes@aapt.net.au>

Can you expand on that please. All previous discussion of IRIW I have seen indicated that the property, while a consequence of existing JMM rules, had no practical use.

Thanks,
David
  -----Original Message-----
  From: Roman Elizarov [mailto:elizarov at devexperts.com]
  Sent: Wednesday, 26 November 2014 6:49 PM
  To: dholmes at ieee.org; Hans Boehm
  Cc: concurrency-interest at cs.oswego.edu; core-libs-dev
  Subject: RE: [concurrency-interest] RFR: 8065804:JEP171:Clarifications/corrections for fence intrinsics


  There is no conceivable way to kill IRIW consistency requirement while retaining ability to prove correctness of large software systems. If IRIW of volatile variables are not consistent, then volatile reads and writes are not linearizable, which breaks linearizabiliy of all higher-level primitives build on top of them and makes formal reasoning about behavior of concurrent systems practically impossible. There are many fields where this is not acceptable.

   

  /Roman

   

  From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of David Holmes
  Sent: Wednesday, November 26, 2014 5:11 AM
  To: Hans Boehm
  Cc: concurrency-interest at cs.oswego.edu; core-libs-dev
  Subject: Re: [concurrency-interest] RFR: 8065804: JEP171:Clarifications/corrections for fence intrinsics

   

  Hi Hans,

   

  Given IRIW is a thorn in everyone's side and has no known useful benefit, and can hopefully be killed off in the future, lets not get bogged down in IRIW. But none of what you say below relates to multi-copy-atomicity.

   

  Cheers,

  David

    -----Original Message-----
    From: hjkhboehm at gmail.com [mailto:hjkhboehm at gmail.com]On Behalf Of Hans Boehm
    Sent: Wednesday, 26 November 2014 12:04 PM
    To: dholmes at ieee.org
    Cc: Stephan Diestelhorst; concurrency-interest at cs.oswego.edu; core-libs-dev
    Subject: Re: [concurrency-interest] RFR: 8065804: JEP171:Clarifications/corrections for fence intrinsics

    To be concrete here, on Power, loads can normally be ordered by an address dependency or light-weight fence (lwsync).  However, neither is enough to prevent the questionable outcome for IRIW, since it doesn't ensure that the stores in T1 and T2 will be made visible to other threads in a consistent order.  That outcome can be prevented by using heavyweight fences (sync) instructions between the loads instead.  Peter Sewell's group concluded that to enforce correct volatile behavior on Power, you essentially need a a heavyweight fence between every pair of volatile operations on Power.  That cannot be understood based on simple ordering constraints. 

     

    As Stephan pointed out, there are similar issues on ARM, but they're less commonly encountered in a Java implementation.  If you're lucky, you can get to the right implementation recipe by looking at only reordering, I think.

     

     

    On Tue, Nov 25, 2014 at 4:36 PM, David Holmes <davidcholmes at aapt.net.au> wrote:

      Stephan Diestelhorst writes:
      >
      > David Holmes wrote:
      > > Stephan Diestelhorst writes:
      > > > Am Dienstag, 25. November 2014, 11:15:36 schrieb Hans Boehm:
      > > > > I'm no hardware architect, but fundamentally it seems to me that
      > > > >
      > > > > load x
      > > > > acquire_fence
      > > > >
      > > > > imposes a much more stringent constraint than
      > > > >
      > > > > load_acquire x
      > > > >
      > > > > Consider the case in which the load from x is an L1 hit, but a
      > > > > preceding load (from say y) is a long-latency miss.  If we enforce
      > > > > ordering by just waiting for completion of prior operation, the
      > > > > former has to wait for the load from y to complete; while the
      > > > > latter doesn't.  I find it hard to believe that this doesn't leave
      > > > > an appreciable amount of performance on the table, at least for
      > > > > some interesting microarchitectures.
      > > >
      > > > I agree, Hans, that this is a reasonable assumption.  Load_acquire x
      > > > does allow roach motel, whereas the acquire fence does not.
      > > >
      > > > >  In addition, for better or worse, fencing requirements on at least
      > > > >  Power are actually driven as much by store atomicity issues, as by
      > > > >  the ordering issues discussed in the cookbook.  This was not
      > > > >  understood in 2005, and unfortunately doesn't seem to be
      > amenable to
      > > > >  the kind of straightforward explanation as in Doug's cookbook.
      > > >
      > > > Coming from a strongly ordered architecture to a weakly ordered one
      > > > myself, I also needed some mental adjustment about store (multi-copy)
      > > > atomicity.  I can imagine others will be unaware of this difference,
      > > > too, even in 2014.
      > >
      > > Sorry I'm missing the connection between fences and multi-copy
      > atomicity.
      >
      > One example is the classic IRIW.  With non-multi copy atomic stores, but
      > ordered (say through a dependency) loads in the following example:
      >
      > Memory: foo = bar = 0
      > _T1_         _T2_         _T3_                              _T4_
      > st (foo),1   st (bar),1   ld r1, (bar)                      ld r3,(foo)
      >                           <addr dep / local "fence" here>   <addr dep>
      >                           ld r2, (foo)                      ld r4, (bar)
      >
      > You may observe r1 = 1, r2 = 0, r3 = 1, r4 = 0 on non-multi-copy atomic
      > machines.  On TSO boxes, this is not possible.  That means that the
      > memory fence that will prevent such a behaviour (DMB on ARM) needs to
      > carry some additional oomph in ensuring multi-copy atomicity, or rather
      > prevent you from seeing it (which is the same thing).

      I take it as given that any code for which you may have ordering
      constraints, must first have basic atomicity properties for loads and
      stores. I would not expect any kind of fence to add multi-copy-atomicity
      where there was none.

      David


      > Stephan
      >
      > _______________________________________________
      > Concurrency-interest mailing list
      > Concurrency-interest at cs.oswego.edu
      > http://cs.oswego.edu/mailman/listinfo/concurrency-interest

      _______________________________________________
      Concurrency-interest mailing list
      Concurrency-interest at cs.oswego.edu
      http://cs.oswego.edu/mailman/listinfo/concurrency-interest

     
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141126/69ff65bc/attachment.html>

From elizarov at devexperts.com  Wed Nov 26 05:59:59 2014
From: elizarov at devexperts.com (Roman Elizarov)
Date: Wed, 26 Nov 2014 10:59:59 +0000
Subject: [concurrency-interest] RFR:
 8065804:JEP171:Clarifications/corrections for fence intrinsics
In-Reply-To: <NFBBKALFDCPFIDBNKAPCIECNKLAA.davidcholmes@aapt.net.au>
References: <c806b219426c4c8aa9e5bfa79fc9901d@exchmb02.office.devexperts.com>
	<NFBBKALFDCPFIDBNKAPCIECNKLAA.davidcholmes@aapt.net.au>
Message-ID: <74ff42b06c314fc9998d512725bae69a@exchmb02.office.devexperts.com>

Whether IRIW has any _practical_ uses is definitely subject to debate. However, there is no tractable way for formal reasoning about properties of large concurrent systems, but via linearizability. Linearizability is the only property that is both local and hierarchical. It lets you build more complex linearizable algorithms from simpler ones, having quite succinct and compelling proofs at each step.

In other words, if you want to be able to construct a formal proof that your [large] concurrent system if correct, then you must have IRIW consistency. Do you need a formal proof of correctness? Maybe not. In many applications hand-waving is enough,  but there are many other applications where hand-waving does not count as a proof. It may be possible to construct formal correctness proofs for some very simple algorithms even on a system that does not provide IRIW, but this is beyond the state of the art of formal verification for anything sufficiently complex.

/Roman

From: David Holmes [mailto:davidcholmes at aapt.net.au]
Sent: Wednesday, November 26, 2014 11:54 AM
To: Roman Elizarov; Hans Boehm
Cc: concurrency-interest at cs.oswego.edu; core-libs-dev
Subject: RE: [concurrency-interest] RFR: 8065804:JEP171:Clarifications/corrections for fence intrinsics

Can you expand on that please. All previous discussion of IRIW I have seen indicated that the property, while a consequence of existing JMM rules, had no practical use.

Thanks,
David
-----Original Message-----
From: Roman Elizarov [mailto:elizarov at devexperts.com]
Sent: Wednesday, 26 November 2014 6:49 PM
To: dholmes at ieee.org<mailto:dholmes at ieee.org>; Hans Boehm
Cc: concurrency-interest at cs.oswego.edu<mailto:concurrency-interest at cs.oswego.edu>; core-libs-dev
Subject: RE: [concurrency-interest] RFR: 8065804:JEP171:Clarifications/corrections for fence intrinsics
There is no conceivable way to kill IRIW consistency requirement while retaining ability to prove correctness of large software systems. If IRIW of volatile variables are not consistent, then volatile reads and writes are not linearizable, which breaks linearizabiliy of all higher-level primitives build on top of them and makes formal reasoning about behavior of concurrent systems practically impossible. There are many fields where this is not acceptable.

/Roman

From: concurrency-interest-bounces at cs.oswego.edu<mailto:concurrency-interest-bounces at cs.oswego.edu> [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of David Holmes
Sent: Wednesday, November 26, 2014 5:11 AM
To: Hans Boehm
Cc: concurrency-interest at cs.oswego.edu<mailto:concurrency-interest at cs.oswego.edu>; core-libs-dev
Subject: Re: [concurrency-interest] RFR: 8065804: JEP171:Clarifications/corrections for fence intrinsics

Hi Hans,

Given IRIW is a thorn in everyone's side and has no known useful benefit, and can hopefully be killed off in the future, lets not get bogged down in IRIW. But none of what you say below relates to multi-copy-atomicity.

Cheers,
David
-----Original Message-----
From: hjkhboehm at gmail.com<mailto:hjkhboehm at gmail.com> [mailto:hjkhboehm at gmail.com]On Behalf Of Hans Boehm
Sent: Wednesday, 26 November 2014 12:04 PM
To: dholmes at ieee.org<mailto:dholmes at ieee.org>
Cc: Stephan Diestelhorst; concurrency-interest at cs.oswego.edu<mailto:concurrency-interest at cs.oswego.edu>; core-libs-dev
Subject: Re: [concurrency-interest] RFR: 8065804: JEP171:Clarifications/corrections for fence intrinsics
To be concrete here, on Power, loads can normally be ordered by an address dependency or light-weight fence (lwsync).  However, neither is enough to prevent the questionable outcome for IRIW, since it doesn't ensure that the stores in T1 and T2 will be made visible to other threads in a consistent order.  That outcome can be prevented by using heavyweight fences (sync) instructions between the loads instead.  Peter Sewell's group concluded that to enforce correct volatile behavior on Power, you essentially need a a heavyweight fence between every pair of volatile operations on Power.  That cannot be understood based on simple ordering constraints.

As Stephan pointed out, there are similar issues on ARM, but they're less commonly encountered in a Java implementation.  If you're lucky, you can get to the right implementation recipe by looking at only reordering, I think.


On Tue, Nov 25, 2014 at 4:36 PM, David Holmes <davidcholmes at aapt.net.au<mailto:davidcholmes at aapt.net.au>> wrote:
Stephan Diestelhorst writes:
>
> David Holmes wrote:
> > Stephan Diestelhorst writes:
> > > Am Dienstag, 25. November 2014, 11:15:36 schrieb Hans Boehm:
> > > > I'm no hardware architect, but fundamentally it seems to me that
> > > >
> > > > load x
> > > > acquire_fence
> > > >
> > > > imposes a much more stringent constraint than
> > > >
> > > > load_acquire x
> > > >
> > > > Consider the case in which the load from x is an L1 hit, but a
> > > > preceding load (from say y) is a long-latency miss.  If we enforce
> > > > ordering by just waiting for completion of prior operation, the
> > > > former has to wait for the load from y to complete; while the
> > > > latter doesn't.  I find it hard to believe that this doesn't leave
> > > > an appreciable amount of performance on the table, at least for
> > > > some interesting microarchitectures.
> > >
> > > I agree, Hans, that this is a reasonable assumption.  Load_acquire x
> > > does allow roach motel, whereas the acquire fence does not.
> > >
> > > >  In addition, for better or worse, fencing requirements on at least
> > > >  Power are actually driven as much by store atomicity issues, as by
> > > >  the ordering issues discussed in the cookbook.  This was not
> > > >  understood in 2005, and unfortunately doesn't seem to be
> amenable to
> > > >  the kind of straightforward explanation as in Doug's cookbook.
> > >
> > > Coming from a strongly ordered architecture to a weakly ordered one
> > > myself, I also needed some mental adjustment about store (multi-copy)
> > > atomicity.  I can imagine others will be unaware of this difference,
> > > too, even in 2014.
> >
> > Sorry I'm missing the connection between fences and multi-copy
> atomicity.
>
> One example is the classic IRIW.  With non-multi copy atomic stores, but
> ordered (say through a dependency) loads in the following example:
>
> Memory: foo = bar = 0
> _T1_         _T2_         _T3_                              _T4_
> st (foo),1   st (bar),1   ld r1, (bar)                      ld r3,(foo)
>                           <addr dep / local "fence" here>   <addr dep>
>                           ld r2, (foo)                      ld r4, (bar)
>
> You may observe r1 = 1, r2 = 0, r3 = 1, r4 = 0 on non-multi-copy atomic
> machines.  On TSO boxes, this is not possible.  That means that the
> memory fence that will prevent such a behaviour (DMB on ARM) needs to
> carry some additional oomph in ensuring multi-copy atomicity, or rather
> prevent you from seeing it (which is the same thing).
I take it as given that any code for which you may have ordering
constraints, must first have basic atomicity properties for loads and
stores. I would not expect any kind of fence to add multi-copy-atomicity
where there was none.

David

> Stephan
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu<mailto:Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu<mailto:Concurrency-interest at cs.oswego.edu>
http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141126/41a7f3ae/attachment-0001.html>

From dt at flyingtroika.com  Wed Nov 26 12:24:39 2014
From: dt at flyingtroika.com (DT)
Date: Wed, 26 Nov 2014 09:24:39 -0800
Subject: [concurrency-interest] RFR:
 8065804:JEP171:Clarifications/corrections for fence intrinsics
In-Reply-To: <74ff42b06c314fc9998d512725bae69a@exchmb02.office.devexperts.com>
References: <c806b219426c4c8aa9e5bfa79fc9901d@exchmb02.office.devexperts.com>	<NFBBKALFDCPFIDBNKAPCIECNKLAA.davidcholmes@aapt.net.au>
	<74ff42b06c314fc9998d512725bae69a@exchmb02.office.devexperts.com>
Message-ID: <54760CD7.9070704@flyingtroika.com>

Roman,
Can you point to any specific article providing the concurrency problem 
statement with a further proof using linearizability to reason about 
solution.

Thanks,
DT

On 11/26/2014 2:59 AM, Roman Elizarov wrote:
>
> Whether IRIW has any _/practical/_ uses is definitely subject to 
> debate. However, there is no tractable way for formal reasoning about 
> properties of large concurrent systems, but via linearizability. 
> Linearizability is the only property that is both local and 
> hierarchical. It lets you build more complex linearizable algorithms 
> from simpler ones, having quite succinct and compelling proofs at each 
> step.
>
> In other words, if you want to be able to construct a formal proof 
> that your [large] concurrent system if correct, then you must have 
> IRIW consistency. Do you need a formal proof of correctness? Maybe 
> not. In many applications hand-waving is enough,  but there are many 
> other applications where hand-waving does not count as a proof. It may 
> be possible to construct formal correctness proofs for some very 
> simple algorithms even on a system that does not provide IRIW, but 
> this is beyond the state of the art of formal verification for 
> anything sufficiently complex.
>
> /Roman
>
> *From:*David Holmes [mailto:davidcholmes at aapt.net.au]
> *Sent:* Wednesday, November 26, 2014 11:54 AM
> *To:* Roman Elizarov; Hans Boehm
> *Cc:* concurrency-interest at cs.oswego.edu; core-libs-dev
> *Subject:* RE: [concurrency-interest] RFR: 
> 8065804:JEP171:Clarifications/corrections for fence intrinsics
>
> Can you expand on that please. All previous discussion of IRIW I have 
> seen indicated that the property, while a consequence of existing JMM 
> rules, had no practical use.
>
> Thanks,
>
> David
>
>     -----Original Message-----
>     *From:* Roman Elizarov [mailto:elizarov at devexperts.com]
>     *Sent:* Wednesday, 26 November 2014 6:49 PM
>     *To:* dholmes at ieee.org <mailto:dholmes at ieee.org>; Hans Boehm
>     *Cc:* concurrency-interest at cs.oswego.edu
>     <mailto:concurrency-interest at cs.oswego.edu>; core-libs-dev
>     *Subject:* RE: [concurrency-interest] RFR:
>     8065804:JEP171:Clarifications/corrections for fence intrinsics
>
>     There is no conceivable way to kill IRIW consistency requirement
>     while retaining ability to prove correctness of large software
>     systems. If IRIW of volatile variables are not consistent, then
>     volatile reads and writes are not linearizable, which breaks
>     linearizabiliy of all higher-level primitives build on top of them
>     and makes formal reasoning about behavior of concurrent systems
>     practically impossible. There are many fields where this is not
>     acceptable.
>
>     /Roman
>
>     *From:*concurrency-interest-bounces at cs.oswego.edu
>     <mailto:concurrency-interest-bounces at cs.oswego.edu>
>     [mailto:concurrency-interest-bounces at cs.oswego.edu] *On Behalf Of
>     *David Holmes
>     *Sent:* Wednesday, November 26, 2014 5:11 AM
>     *To:* Hans Boehm
>     *Cc:* concurrency-interest at cs.oswego.edu
>     <mailto:concurrency-interest at cs.oswego.edu>; core-libs-dev
>     *Subject:* Re: [concurrency-interest] RFR: 8065804:
>     JEP171:Clarifications/corrections for fence intrinsics
>
>     Hi Hans,
>
>     Given IRIW is a thorn in everyone's side and has no known useful
>     benefit, and can hopefully be killed off in the future, lets not
>     get bogged down in IRIW. But none of what you say below relates to
>     multi-copy-atomicity.
>
>     Cheers,
>
>     David
>
>         -----Original Message-----
>         *From:* hjkhboehm at gmail.com
>         <mailto:hjkhboehm at gmail.com>[mailto:hjkhboehm at gmail.com]*On
>         Behalf Of *Hans Boehm
>         *Sent:* Wednesday, 26 November 2014 12:04 PM
>         *To:* dholmes at ieee.org <mailto:dholmes at ieee.org>
>         *Cc:* Stephan Diestelhorst; concurrency-interest at cs.oswego.edu
>         <mailto:concurrency-interest at cs.oswego.edu>; core-libs-dev
>         *Subject:* Re: [concurrency-interest] RFR: 8065804:
>         JEP171:Clarifications/corrections for fence intrinsics
>
>         To be concrete here, on Power, loads can normally be ordered
>         by an address dependency or light-weight fence (lwsync). 
>         However, neither is enough to prevent the questionable outcome
>         for IRIW, since it doesn't ensure that the stores in T1 and T2
>         will be made visible to other threads in a consistent order. 
>         That outcome can be prevented by using heavyweight fences
>         (sync) instructions between the loads instead.  Peter Sewell's
>         group concluded that to enforce correct volatile behavior on
>         Power, you essentially need a a heavyweight fence between
>         every pair of volatile operations on Power.  That cannot be
>         understood based on simple ordering constraints.
>
>         As Stephan pointed out, there are similar issues on ARM, but
>         they're less commonly encountered in a Java implementation. 
>         If you're lucky, you can get to the right implementation
>         recipe by looking at only reordering, I think.
>
>         On Tue, Nov 25, 2014 at 4:36 PM, David Holmes
>         <davidcholmes at aapt.net.au <mailto:davidcholmes at aapt.net.au>>
>         wrote:
>
>             Stephan Diestelhorst writes:
>             >
>             > David Holmes wrote:
>             > > Stephan Diestelhorst writes:
>             > > > Am Dienstag, 25. November 2014, 11:15:36 schrieb
>             Hans Boehm:
>             > > > > I'm no hardware architect, but fundamentally it
>             seems to me that
>             > > > >
>             > > > > load x
>             > > > > acquire_fence
>             > > > >
>             > > > > imposes a much more stringent constraint than
>             > > > >
>             > > > > load_acquire x
>             > > > >
>             > > > > Consider the case in which the load from x is an
>             L1 hit, but a
>             > > > > preceding load (from say y) is a long-latency
>             miss.  If we enforce
>             > > > > ordering by just waiting for completion of prior
>             operation, the
>             > > > > former has to wait for the load from y to
>             complete; while the
>             > > > > latter doesn't.  I find it hard to believe that
>             this doesn't leave
>             > > > > an appreciable amount of performance on the table,
>             at least for
>             > > > > some interesting microarchitectures.
>             > > >
>             > > > I agree, Hans, that this is a reasonable
>             assumption.  Load_acquire x
>             > > > does allow roach motel, whereas the acquire fence
>             does not.
>             > > >
>             > > > >  In addition, for better or worse, fencing
>             requirements on at least
>             > > > >  Power are actually driven as much by store
>             atomicity issues, as by
>             > > > >  the ordering issues discussed in the cookbook. 
>             This was not
>             > > > >  understood in 2005, and unfortunately doesn't
>             seem to be
>             > amenable to
>             > > > >  the kind of straightforward explanation as in
>             Doug's cookbook.
>             > > >
>             > > > Coming from a strongly ordered architecture to a
>             weakly ordered one
>             > > > myself, I also needed some mental adjustment about
>             store (multi-copy)
>             > > > atomicity.  I can imagine others will be unaware of
>             this difference,
>             > > > too, even in 2014.
>             > >
>             > > Sorry I'm missing the connection between fences and
>             multi-copy
>             > atomicity.
>             >
>             > One example is the classic IRIW.  With non-multi copy
>             atomic stores, but
>             > ordered (say through a dependency) loads in the
>             following example:
>             >
>             > Memory: foo = bar = 0
>             > _T1_         _T2_         _T3_                 _T4_
>             > st (foo),1   st (bar),1   ld r1, (bar)                
>             ld r3,(foo)
>             >                           <addr dep / local "fence"
>             here>   <addr dep>
>             >                           ld r2, (foo)                
>             ld r4, (bar)
>             >
>             > You may observe r1 = 1, r2 = 0, r3 = 1, r4 = 0 on
>             non-multi-copy atomic
>             > machines.  On TSO boxes, this is not possible.  That
>             means that the
>             > memory fence that will prevent such a behaviour (DMB on
>             ARM) needs to
>             > carry some additional oomph in ensuring multi-copy
>             atomicity, or rather
>             > prevent you from seeing it (which is the same thing).
>
>             I take it as given that any code for which you may have
>             ordering
>             constraints, must first have basic atomicity properties
>             for loads and
>             stores. I would not expect any kind of fence to add
>             multi-copy-atomicity
>             where there was none.
>
>             David
>
>
>             > Stephan
>             >
>             > _______________________________________________
>             > Concurrency-interest mailing list
>             > Concurrency-interest at cs.oswego.edu
>             <mailto:Concurrency-interest at cs.oswego.edu>
>             > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>             _______________________________________________
>             Concurrency-interest mailing list
>             Concurrency-interest at cs.oswego.edu
>             <mailto:Concurrency-interest at cs.oswego.edu>
>             http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141126/77a5b6bc/attachment-0001.html>

From dl at cs.oswego.edu  Wed Nov 26 13:43:10 2014
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed, 26 Nov 2014 13:43:10 -0500
Subject: [concurrency-interest] Mixing async and blocking components
Message-ID: <54761F3E.9020409@cs.oswego.edu>


The increasing adoption of "structured" async components and
frameworks (CompletableFuture, Rx, reactive-streams, some parallel
uses of java.util.Stream) leads to increasing problems when users mix
them with blocking components and frameworks. When async tasks block,
then overall program liveness may require compensation with more
threads within some Executor.  But there is in general no limit to how
many threads are required to maintain liveness, so programs may
encounter OutOfMemoryErrors trying to create too many threads.
In common cases where upper bounds cannot be established, no one
likes to make a choice between possibly either locking up vs
blowing up programs.

This problem can occur no matter what kind of Executor you use for
async. The only perfect solution is avoidance: not mixing async and
blocking components in this way. I expect that over time, async
replacements for some of the most troublesome IO and network-based
components will appear. It seems that there are (non-JDK) versions of
RMI, JAX-RPC, and adaptors for java.nio, in various stages of
development. (It would be great if there were something similar for
JDBC.) All of these efforts fall outside of java.util.concurrent
though.  Similarly for frameworks like Ron Pressler's Quasar that
automate the equivalents of adaptors.  The best we can do is provide
better coping mechanisms when these do not apply, and possibly add
some support to simplify construction of async replacements for
blocking components.

We improved the situation for ForkJoinPool in an update a few months
ago (that ought to appear in a JDK8u release) by throwing more
manageable max-spare bound-check exceptions before resources are
typically actually exhausted. ForkJoinPool is also currently better
equipped to deal with this than other executors by supporting the
ManagedBlocker API to automate triggering of possible compensation.
But further improvements would be helpful.

One option is to both extend the number of j.u.c synchronization
components (ReentrantLock, Semaphore, etc) that implement
ManagedBlocker, and enhance ThreadPoolExecutor to also include
(optional) compensation mechanics.

Another (partially complementary) approach is to extend support for
variants of a classic simple resource control heuristic (I think
originating circa 1970 in AT&T ESS switches): Sometimes slow down
async task producers by arranging for them to help run tasks.  When
applied to network-based task producers, this may in turn cause
packets to be dropped, which is usually the best of all the bad
options. This isn't a cure-all: drops may lead to distributed liveness
failures and/or local stalls due to unsatisfiable task dependencies.
It also doesn't apply directly when helped-out tasks generate
exponentially more tasks. But in many practical cases, it can
vastly reduce the likelihood of failure.

We already predefine the simplest form of this as the CallerRuns
RejectedExecutionHandler in TPE. Other variants can already be
programmed in both FJP and TPE, but we don't make it particularly
easy. For example, in TPE you can implement a handler that first
executes the oldest existing task, and then retries submission. And in
FJ, you can check local queue size to decide upon submission to first
help run a local or non-local task.  One of the reasons we didn't
predefine any of these is that there seemed to be too many
situation-specific plausible variants. But adding more of these may
help lead people into more successful choices.

Opinions about any of these, or other suggestions, would be welcome.


-Doug

From boehm at acm.org  Wed Nov 26 13:48:33 2014
From: boehm at acm.org (Hans Boehm)
Date: Wed, 26 Nov 2014 10:48:33 -0800
Subject: [concurrency-interest] RFR: 8065804:
 JEP171:Clarifications/corrections for fence intrinsics
In-Reply-To: <NFBBKALFDCPFIDBNKAPCIECMKLAA.davidcholmes@aapt.net.au>
References: <CAPUmR1YdWyemKveLA0eHpv_MC=5BjVJtfLkj2RTZAhz_NFivog@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCIECMKLAA.davidcholmes@aapt.net.au>
Message-ID: <CAPUmR1bieXLA+6zofUu0q-OmseauEhyb-MR-Q_BVES_HUGY9EQ@mail.gmail.com>

Definitions here seem to be less clear than I would like.  What I meant by
"store atomicity", which I think is more or less synonymous with
"multi-copy atomicity" is that a store becomes visible to all observers at
the same time, or equivalently stores become visible to all observers in a
consistent order.  In my view, IRIW is the canonical test for that.

I agree with Roman that IRIW requirements for Java volatiles are here to
stay.  Many of us thought about ways to relax the requirement about 8 or 9
years ago.  In my view:

- Sequential consistency for data-race-free code is the only model that we
can possibly explain to the majority of programmers.  (Even stronger models
like some notions of region serializability may also make sense, but
they'll cost you.)  This requires IRIW.  This model is also by far the
easiest to reason about formally.

- The next weaker model that seems to be somewhat explainable, but really
only to experts, is something along the lines of the C++ acquire/release
model.  This doesn't require IRIW.  It's clearly too weak to replace Java
volatile behavior, since it also fails to work for Dekkers-like settings,
which are fairly common.  (Nonexperts perhaps shouldn't write lock-free
Dekkers-like code, but it's hard to explain precisely what they shouldn't
be doing.)

- A large amount of effort to generate models between those two failed to
generate anything viable.  The general experience was that once you no
longer require IRIW, you also end up failing various other, potentially
more important, litmus tests in ways that are really difficult to explain.
And those models generally looked too complex to me to form a viable basis
for real programs

I think many people, even those who would rather not enforce IRIW,
generally agree with this characterization.

Hans

On Tue, Nov 25, 2014 at 6:10 PM, David Holmes <davidcholmes at aapt.net.au>
wrote:

>  Hi Hans,
>
> Given IRIW is a thorn in everyone's side and has no known useful benefit,
> and can hopefully be killed off in the future, lets not get bogged down in
> IRIW. But none of what you say below relates to multi-copy-atomicity.
>
> Cheers,
> David
>
> -----Original Message-----
> *From:* hjkhboehm at gmail.com [mailto:hjkhboehm at gmail.com]*On Behalf Of *Hans
> Boehm
> *Sent:* Wednesday, 26 November 2014 12:04 PM
> *To:* dholmes at ieee.org
> *Cc:* Stephan Diestelhorst; concurrency-interest at cs.oswego.edu;
> core-libs-dev
> *Subject:* Re: [concurrency-interest] RFR: 8065804:
> JEP171:Clarifications/corrections for fence intrinsics
>
> To be concrete here, on Power, loads can normally be ordered by an address
> dependency or light-weight fence (lwsync).  However, neither is enough to
> prevent the questionable outcome for IRIW, since it doesn't ensure that the
> stores in T1 and T2 will be made visible to other threads in a consistent
> order.  That outcome can be prevented by using heavyweight fences (sync)
> instructions between the loads instead.  Peter Sewell's group concluded
> that to enforce correct volatile behavior on Power, you essentially need a
> a heavyweight fence between every pair of volatile operations on Power.
> That cannot be understood based on simple ordering constraints.
>
> As Stephan pointed out, there are similar issues on ARM, but they're less
> commonly encountered in a Java implementation.  If you're lucky, you can
> get to the right implementation recipe by looking at only reordering, I
> think.
>
>
> On Tue, Nov 25, 2014 at 4:36 PM, David Holmes <davidcholmes at aapt.net.au>
> wrote:
>
>>  Stephan Diestelhorst writes:
>> >
>> > David Holmes wrote:
>> > > Stephan Diestelhorst writes:
>> > > > Am Dienstag, 25. November 2014, 11:15:36 schrieb Hans Boehm:
>> > > > > I'm no hardware architect, but fundamentally it seems to me that
>> > > > >
>> > > > > load x
>> > > > > acquire_fence
>> > > > >
>> > > > > imposes a much more stringent constraint than
>> > > > >
>> > > > > load_acquire x
>> > > > >
>> > > > > Consider the case in which the load from x is an L1 hit, but a
>> > > > > preceding load (from say y) is a long-latency miss.  If we enforce
>> > > > > ordering by just waiting for completion of prior operation, the
>> > > > > former has to wait for the load from y to complete; while the
>> > > > > latter doesn't.  I find it hard to believe that this doesn't leave
>> > > > > an appreciable amount of performance on the table, at least for
>> > > > > some interesting microarchitectures.
>> > > >
>> > > > I agree, Hans, that this is a reasonable assumption.  Load_acquire x
>> > > > does allow roach motel, whereas the acquire fence does not.
>> > > >
>> > > > >  In addition, for better or worse, fencing requirements on at
>> least
>> > > > >  Power are actually driven as much by store atomicity issues, as
>> by
>> > > > >  the ordering issues discussed in the cookbook.  This was not
>> > > > >  understood in 2005, and unfortunately doesn't seem to be
>> > amenable to
>> > > > >  the kind of straightforward explanation as in Doug's cookbook.
>> > > >
>> > > > Coming from a strongly ordered architecture to a weakly ordered one
>> > > > myself, I also needed some mental adjustment about store
>> (multi-copy)
>> > > > atomicity.  I can imagine others will be unaware of this difference,
>> > > > too, even in 2014.
>> > >
>> > > Sorry I'm missing the connection between fences and multi-copy
>> > atomicity.
>> >
>> > One example is the classic IRIW.  With non-multi copy atomic stores, but
>> > ordered (say through a dependency) loads in the following example:
>> >
>> > Memory: foo = bar = 0
>> > _T1_         _T2_         _T3_                              _T4_
>> > st (foo),1   st (bar),1   ld r1, (bar)                      ld r3,(foo)
>> >                           <addr dep / local "fence" here>   <addr dep>
>> >                           ld r2, (foo)                      ld r4, (bar)
>> >
>> > You may observe r1 = 1, r2 = 0, r3 = 1, r4 = 0 on non-multi-copy atomic
>> > machines.  On TSO boxes, this is not possible.  That means that the
>> > memory fence that will prevent such a behaviour (DMB on ARM) needs to
>> > carry some additional oomph in ensuring multi-copy atomicity, or rather
>> > prevent you from seeing it (which is the same thing).
>>
>> I take it as given that any code for which you may have ordering
>> constraints, must first have basic atomicity properties for loads and
>> stores. I would not expect any kind of fence to add multi-copy-atomicity
>> where there was none.
>>
>> David
>>
>> > Stephan
>> >
>> > _______________________________________________
>> > Concurrency-interest mailing list
>> > Concurrency-interest at cs.oswego.edu
>> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141126/8261f657/attachment.html>

From heinz at javaspecialists.eu  Wed Nov 26 13:49:38 2014
From: heinz at javaspecialists.eu (Dr Heinz M. Kabutz)
Date: Wed, 26 Nov 2014 20:49:38 +0200
Subject: [concurrency-interest] ManagedBlockers Not Giving Full Parallelism
	in FJPool
Message-ID: <547620C2.7020309@javaspecialists.eu>

Hi,

thanks to some tips by Paul Sandoz at Devoxx, I started looking at 
ManagedBlockers as a way to allow blocking operations to "play nice" 
with parallel streams.  One of the things I wrote yesterday was a 
"ManagedReentrantLock".  It is a drop-in replacement for the normal 
ReentrantLock, but if you happen to either block on a lock() operation 
or suspend a thread with await() on the condition, it will allow the 
underlying ForkJoinPool to construct new threads in order to maintain 
the number of active threads.  (see end of my email for an example of 
the ManagedReentrantLock - soon to be published in The Java Specialists' 
Newsletter).  Of course, one could also make AQS "play nice" with FJP.

Anyway, the code seems to work reasonably well, and I even wrote a class 
ManagedBlockers to inject instances of the ReentrantLock into existing 
constructs like ArrayBlockingQueue, LinkedBlockingQueue and 
PriorityBlockingQueue (Again, see end of this email for the code).

Whilst writing some sample code to demonstrate how vastly superior 
(*cough* *cough*) this was to using a normal ReentrantLock within a 
parallel stream, I came across some strange observations with stream 
behaviour which I cannot explain.  I'd love to hear if any of you can 
point me to the right place.  It seems as if when I have threads that 
have been suspended on a ManagedBlocker, that a maximum of one thread is 
accessible from the FJP for work, in addition to the invoking thread.  
We thus get a speedup of only 2x over the serial version, whereas on my 
1-4-2 laptop I would've expected it to run as fast as if there were no 
suspended threads.

I've written a small example to demonstrate the problem.  My 
ManagedJamupDemo searches for the max value of a large array.  It starts 
by taking a measurement when we have no manage blocked threads in the 
FJP at all in order to get a baseline speed.  It then in a separate 
thread does a parallel stream in which we call a blocking phaser 
method.  Since Phaser "plays nice" with FJP, it ends up with more 
threads in the FJP than parallelism.  Typical output on my 1-4-2 machine 
is something like:

baseline parallel = 196
baseline serial = 835
new parallel = 444
new serial = 831

My 2-4-1 server is a bit slower, but we can observe there too that we 
don't speed up as much as when we don't have "managed blocked" threads 
in the FJP:

baseline parallel = 369
baseline serial = 1345
new parallel = 678
new serial = 1331

(I am aware that this is not a proper benchmark and that JMH is much 
cooler, ... ;-)  Also that we are going to end up bottlenecking on 
memory access.  However, I first observed this interesting phenomenon of 
having only two active threads when using Thread.sleep instead of memory 
access.)

My initial gut feeling is that this is a bug in FJP and that we should 
see "new parallel" have the same speed as "baseline parallel".  I know 
that the maximum amount of parallelization within a parallel stream is 
currently set to 4x the common pool parallelism in the AbstractTask class:

    static final int LEAF_TARGET = 
ForkJoinPool.getCommonPoolParallelism() << 2;

I'd appreciate if some of you could maybe have a look at this and help 
with the gory details of FJP implementation :-)

import java.util.concurrent.*;
import java.util.function.*;
import java.util.stream.*;

public class ManagedJamupDemo {
  public static void main(String... args) throws InterruptedException {
    int[] large_array = new int[1_000_000_000];
    ThreadLocalRandom random = ThreadLocalRandom.current();
    for (int i = 0; i < large_array.length; i++) {
      large_array[i] = random.nextInt();
    }

    for (int i = 0; i < 10; i++) {
      test(large_array);
    }
  }

  private static void test(int[] large_array) throws InterruptedException {
    long time;

    time = max(1, () -> IntStream.of(large_array).parallel());
    System.out.println("baseline parallel = " + time);

    time = max(1, () -> IntStream.of(large_array));
    System.out.println("baseline serial = " + time);

    int processors = Runtime.getRuntime().availableProcessors();
    Phaser phaser = new Phaser(processors * 2 + 1);

    Thread jamThread = makeJamThread(phaser);
    Thread.sleep(100);

    time = max(1, () -> IntStream.of(large_array).parallel());
    System.out.println("new parallel = " + time);

    time = max(1, () -> IntStream.of(large_array));
    System.out.println("new serial = " + time);

    phaser.arriveAndAwaitAdvance();
    jamThread.join();
  }

  private static Thread makeJamThread(Phaser phaser) {
    Thread jamup = new Thread(() -> {
      int par = Runtime.getRuntime().availableProcessors() * 2;
      IntStream.range(0, par).parallel().forEach(
          i -> {
            phaser.arriveAndAwaitAdvance();
          }
      );
    });
    jamup.start();
    return jamup;
  }

  private static long max(int repeats, Supplier<IntStream> supplier) {
    for (int i = 0; i < repeats; i++) {
      supplier.get().max();
    }
    long time = System.currentTimeMillis();
    for (int i = 0; i < repeats; i++) {
      supplier.get().max();
    }
    time = System.currentTimeMillis() - time;
    return time;
  }
}

Regards

Heinz
-- 
Dr Heinz M. Kabutz (PhD CompSci)
Author of "The Java(tm) Specialists' Newsletter"
Sun/Oracle Java Champion since 2005
JavaOne Rock Star Speaker 2012
http://www.javaspecialists.eu
Tel: +30 69 75 595 262
Skype: kabutz



package eu.javaspecialists.concurrent.locks;

import eu.javaspecialists.concurrent.util.*;

import java.util.*;
import java.util.concurrent.*;
import java.util.concurrent.locks.*;

/**
 * The ManagedReentrantLock is a lock implementation that is
 * compatible with the Fork/Join framework, and therefore also
 * with the Java 8 parallel streams.  Instead of just blocking
 * when the lock is held by another thread, and thereby removing
 * one of the active threads from the Fork/Join pool, we instead
 * use a ManagedBlocker to manage it.
 * <p>
 * The ManagedReentrantLock subclasses ReentrantLock, which means
 * we can use it as a drop-in replacement.  See also the
 * ManagedLocks facade.
 *
 * @author Heinz Kabutz
 * @see ManagedLocks
 */
public class ManagedReentrantLock extends ReentrantLock {
  public ManagedReentrantLock() {
  }

  public ManagedReentrantLock(boolean fair) {
    super(fair);
  }

  // we should always try to achieve our goal within the
  // isReleasable() method instead of block().  This avoids
  // trying to compensate the loss of a thread by creating
  // a new one.
  private abstract class AbstractLockAction
      implements ForkJoinPool.ManagedBlocker {
    boolean hasLock = false;

    public final boolean isReleasable() {
      return hasLock || (hasLock = tryLock());
    }
  }

  private class DoLock extends AbstractLockAction {
    public boolean block() {
      if (isReleasable())
        return true;
      if (!hasLock)
        ManagedReentrantLock.super.lock();
      return true;
    }
  }

  private class DoLockInterruptibly extends AbstractLockAction {
    public boolean block() throws InterruptedException {
      if (isReleasable())
        return true;
      if (!hasLock)
        ManagedReentrantLock.super.lockInterruptibly();
      return true;
    }
  }

  private class TryLocker extends AbstractLockAction {
    private final long time;
    private final TimeUnit unit;

    private TryLocker(long time, TimeUnit unit) {
      this.time = time;
      this.unit = unit;
    }

    public boolean block() throws InterruptedException {
      if (isReleasable())
        return true;
      if (!hasLock)
        ManagedReentrantLock.super.tryLock(time, unit);
      return true;
    }
  }


  public void lock() {
    DoLock locker = new DoLock(); // we want to create this
    // before passing it into the lambda, to prevent it from
    // being created again if the thread is interrupted for some
    // reason
    Interruptions.saveForLater(
        () -> ForkJoinPool.managedBlock(locker));
  }

  public void lockInterruptibly() throws InterruptedException {
    ForkJoinPool.managedBlock(new DoLockInterruptibly());
  }

  public boolean tryLock(long time, TimeUnit unit)
      throws InterruptedException {
    ForkJoinPool.managedBlock(new TryLocker(time, unit));
    return super.isHeldByCurrentThread();
  }

  public Condition newCondition() {
    return new ManagedCondition(super.newCondition());
  }

  private class ManagedCondition implements Condition {
    private final Condition condition;

    private ManagedCondition(Condition condition) {
      this.condition = condition;
    }

    private class Await implements ForkJoinPool.ManagedBlocker {
      public boolean block() throws InterruptedException {
        condition.await();
        return true;
      }

      public boolean isReleasable() {
        return false;
      }
    }

    public void await() throws InterruptedException {
      managedBlock(() -> condition.await());
    }

    public void awaitUninterruptibly() {
      Interruptions.saveForLater(
          () -> managedBlock(
              () -> condition.awaitUninterruptibly())
      );
    }

    public long awaitNanos(long nanosTimeout)
        throws InterruptedException {
      long[] result = {nanosTimeout};
      managedBlock(
          () -> result[0] = condition.awaitNanos(nanosTimeout));
      return result[0];
    }

    public boolean await(long time, TimeUnit unit)
        throws InterruptedException {
      boolean[] result = {false};
      managedBlock(
          () -> result[0] = condition.await(time, unit));
      return result[0];
    }

    public boolean awaitUntil(Date deadline)
        throws InterruptedException {
      boolean[] result = {false};
      managedBlock(
          () -> result[0] = condition.awaitUntil(deadline));
      return result[0];
    }

    public void signal() {
      condition.signal();
    }

    public void signalAll() {
      condition.signalAll();
    }
  }

  private static void managedBlock(
      AlwaysBlockingManagedBlocker blocker)
      throws InterruptedException {
    ForkJoinPool.managedBlock(blocker);
  }

  private static interface AlwaysBlockingManagedBlocker
      extends ForkJoinPool.ManagedBlocker {
    default boolean isReleasable() {
      return false;
    }

    default boolean block() throws InterruptedException {
      doBlock();
      return true;
    }

    void doBlock() throws InterruptedException;
  }

  private Condition getRealCondition(Condition condition) {
    if (!(condition instanceof ManagedCondition))
      throw new IllegalArgumentException("not owner");
    return ((ManagedCondition) condition).condition;
  }

  public boolean hasWaiters(Condition condition) {
    return super.hasWaiters(getRealCondition(condition));
  }

  public int getWaitQueueLength(Condition condition) {
    return super.getWaitQueueLength(getRealCondition(condition));
  }

  protected Collection<Thread> getWaitingThreads(Condition condition) {
    return super.getWaitingThreads(getRealCondition(condition));
  }
}


package eu.javaspecialists.concurrent.util;

import eu.javaspecialists.concurrent.locks.*;

import java.lang.reflect.*;
import java.util.concurrent.*;
import java.util.concurrent.locks.*;

public class ManagedBlockers {
  public static <E> ArrayBlockingQueue<E> makeManaged(
      ArrayBlockingQueue<E> queue) {
    Class<?> clazz = ArrayBlockingQueue.class;

    try {
      Field lockField = clazz.getDeclaredField("lock");
      lockField.setAccessible(true);
      ReentrantLock old = (ReentrantLock) lockField.get(queue);
      boolean fair = old.isFair();
      ReentrantLock lock = new ManagedReentrantLock(fair);
      lockField.set(queue, lock);

      replace(queue, clazz, "notEmpty", lock.newCondition());
      replace(queue, clazz, "notFull", lock.newCondition());

      return queue;
    } catch (IllegalAccessException | NoSuchFieldException e) {
      throw new IllegalStateException(e);
    }
  }

  private static void replace(Object owner,
                              Class<?> clazz, String fieldName,
                              Object fieldValue)
      throws NoSuchFieldException, IllegalAccessException {
    Field field = clazz.getDeclaredField(fieldName);
    field.setAccessible(true);
    field.set(owner, fieldValue);
  }

  public static <E> LinkedBlockingQueue<E> makeManaged(
      LinkedBlockingQueue<E> queue) {
    Class<?> clazz = LinkedBlockingQueue.class;

    ReentrantLock takeLock = new ManagedReentrantLock();
    ReentrantLock putLock = new ManagedReentrantLock();

    try {
      replace(queue, clazz, "takeLock", takeLock);
      replace(queue, clazz, "notEmpty", takeLock.newCondition());
      replace(queue, clazz, "putLock", putLock);
      replace(queue, clazz, "notFull", putLock.newCondition());

      return queue;
    } catch (IllegalAccessException | NoSuchFieldException e) {
      throw new IllegalStateException(e);
    }
  }

  public static <E> PriorityBlockingQueue<E> makeManaged(
      PriorityBlockingQueue<E> queue) {
    Class<?> clazz = PriorityBlockingQueue.class;

    ReentrantLock lock = new ManagedReentrantLock();

    try {
      replace(queue, clazz, "lock", lock);
      replace(queue, clazz, "notEmpty", lock.newCondition());

      return queue;
    } catch (IllegalAccessException | NoSuchFieldException e) {
      throw new IllegalStateException(e);
    }
  }
}


package eu.javaspecialists.concurrent.util;

public class Interruptions {
  public static void saveForLater(Interruptible action) {
    boolean interrupted = false;
    while (true) {
      try {
        action.run();
        if (interrupted) Thread.currentThread().interrupt();
        return;
      } catch (InterruptedException e) {
        interrupted = true;
      }
    }
  }

  @FunctionalInterface
  public static interface Interruptible {
    public void run() throws InterruptedException;
  }
}





From elizarov at devexperts.com  Wed Nov 26 14:00:02 2014
From: elizarov at devexperts.com (Roman Elizarov)
Date: Wed, 26 Nov 2014 19:00:02 +0000
Subject: [concurrency-interest] RFR:
 8065804:JEP171:Clarifications/corrections for fence intrinsics
In-Reply-To: <54760CD7.9070704@flyingtroika.com>
References: <c806b219426c4c8aa9e5bfa79fc9901d@exchmb02.office.devexperts.com>
	<NFBBKALFDCPFIDBNKAPCIECNKLAA.davidcholmes@aapt.net.au>
	<74ff42b06c314fc9998d512725bae69a@exchmb02.office.devexperts.com>,
	<54760CD7.9070704@flyingtroika.com>
Message-ID: <1417028404335.73215@devexperts.com>

I'd suggest to start with the original paper by Herlihy who had come up with the concept of Linearizability in 1990:

Linearizability: A Correctness Condition for Concurrent Objects

http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.142.5315


There were lot of reasearch about linearizability since then (there are almost a thouthand citations for this arcticle) expanding and improving proof techniquies and applying it. There were no breakthroughs of the comparable magnitude since then. All "thread-safe" objects that you enconter in the modern word are linearizable. It is a defacto "golden standard" correctness condition for concurrent objects.


This position is well deserved, because having lineariazable objects as your building blocks makes it super-easy to formally reason about correctness of your code. You will rarely encouter concurrent algorithms that provide weaker guarantees (like quescient consistency), because they all too hard to reason about -- they are either not composable or not local. But when all your concurrent objects are linearizable, then you can ditch happens-before, forget that everything is actually parallel and simply reason about your code in terms of interleaving of "atomic" operations that happen in some global order. That is the beauty of linearizability.


But Linearizability is indeed a pretty strong requirement. Linearizability of your shared memory requires that Independent Reads of Independent Writes (IRIW) are consistent. Can you get away with some weaker requirement and still get all the same goodies as linearizability gets you? I have not seen anything promising in this direction. Whoever makes this breakthrough will surely reap the world's recognition and respect.


/Roman


________________________________
??: DT <dt at flyingtroika.com>
??????????: 26 ?????? 2014 ?. 20:24
????: Roman Elizarov; dholmes at ieee.org; Hans Boehm
?????: core-libs-dev; concurrency-interest at cs.oswego.edu
????: Re: [concurrency-interest] RFR: 8065804:JEP171:Clarifications/corrections for fence intrinsics

Roman,
Can you point to any specific article providing the concurrency problem statement with a further proof using linearizability to reason about solution.

Thanks,
DT

On 11/26/2014 2:59 AM, Roman Elizarov wrote:
Whether IRIW has any _practical_ uses is definitely subject to debate. However, there is no tractable way for formal reasoning about properties of large concurrent systems, but via linearizability. Linearizability is the only property that is both local and hierarchical. It lets you build more complex linearizable algorithms from simpler ones, having quite succinct and compelling proofs at each step.

In other words, if you want to be able to construct a formal proof that your [large] concurrent system if correct, then you must have IRIW consistency. Do you need a formal proof of correctness? Maybe not. In many applications hand-waving is enough,  but there are many other applications where hand-waving does not count as a proof. It may be possible to construct formal correctness proofs for some very simple algorithms even on a system that does not provide IRIW, but this is beyond the state of the art of formal verification for anything sufficiently complex.

/Roman

From: David Holmes [mailto:davidcholmes at aapt.net.au]
Sent: Wednesday, November 26, 2014 11:54 AM
To: Roman Elizarov; Hans Boehm
Cc: concurrency-interest at cs.oswego.edu<mailto:concurrency-interest at cs.oswego.edu>; core-libs-dev
Subject: RE: [concurrency-interest] RFR: 8065804:JEP171:Clarifications/corrections for fence intrinsics

Can you expand on that please. All previous discussion of IRIW I have seen indicated that the property, while a consequence of existing JMM rules, had no practical use.

Thanks,
David
-----Original Message-----
From: Roman Elizarov [mailto:elizarov at devexperts.com]
Sent: Wednesday, 26 November 2014 6:49 PM
To: dholmes at ieee.org<mailto:dholmes at ieee.org>; Hans Boehm
Cc: concurrency-interest at cs.oswego.edu<mailto:concurrency-interest at cs.oswego.edu>; core-libs-dev
Subject: RE: [concurrency-interest] RFR: 8065804:JEP171:Clarifications/corrections for fence intrinsics
There is no conceivable way to kill IRIW consistency requirement while retaining ability to prove correctness of large software systems. If IRIW of volatile variables are not consistent, then volatile reads and writes are not linearizable, which breaks linearizabiliy of all higher-level primitives build on top of them and makes formal reasoning about behavior of concurrent systems practically impossible. There are many fields where this is not acceptable.

/Roman

From: concurrency-interest-bounces at cs.oswego.edu<mailto:concurrency-interest-bounces at cs.oswego.edu> [mailto:concurrency-interest-bounces at cs.oswego.edu] On Behalf Of David Holmes
Sent: Wednesday, November 26, 2014 5:11 AM
To: Hans Boehm
Cc: concurrency-interest at cs.oswego.edu<mailto:concurrency-interest at cs.oswego.edu>; core-libs-dev
Subject: Re: [concurrency-interest] RFR: 8065804: JEP171:Clarifications/corrections for fence intrinsics

Hi Hans,

Given IRIW is a thorn in everyone's side and has no known useful benefit, and can hopefully be killed off in the future, lets not get bogged down in IRIW. But none of what you say below relates to multi-copy-atomicity.

Cheers,
David
-----Original Message-----
From: hjkhboehm at gmail.com<mailto:hjkhboehm at gmail.com> [mailto:hjkhboehm at gmail.com]On Behalf Of Hans Boehm
Sent: Wednesday, 26 November 2014 12:04 PM
To: dholmes at ieee.org<mailto:dholmes at ieee.org>
Cc: Stephan Diestelhorst; concurrency-interest at cs.oswego.edu<mailto:concurrency-interest at cs.oswego.edu>; core-libs-dev
Subject: Re: [concurrency-interest] RFR: 8065804: JEP171:Clarifications/corrections for fence intrinsics
To be concrete here, on Power, loads can normally be ordered by an address dependency or light-weight fence (lwsync).  However, neither is enough to prevent the questionable outcome for IRIW, since it doesn't ensure that the stores in T1 and T2 will be made visible to other threads in a consistent order.  That outcome can be prevented by using heavyweight fences (sync) instructions between the loads instead.  Peter Sewell's group concluded that to enforce correct volatile behavior on Power, you essentially need a a heavyweight fence between every pair of volatile operations on Power.  That cannot be understood based on simple ordering constraints.

As Stephan pointed out, there are similar issues on ARM, but they're less commonly encountered in a Java implementation.  If you're lucky, you can get to the right implementation recipe by looking at only reordering, I think.


On Tue, Nov 25, 2014 at 4:36 PM, David Holmes <davidcholmes at aapt.net.au<mailto:davidcholmes at aapt.net.au>> wrote:
Stephan Diestelhorst writes:
>
> David Holmes wrote:
> > Stephan Diestelhorst writes:
> > > Am Dienstag, 25. November 2014, 11:15:36 schrieb Hans Boehm:
> > > > I'm no hardware architect, but fundamentally it seems to me that
> > > >
> > > > load x
> > > > acquire_fence
> > > >
> > > > imposes a much more stringent constraint than
> > > >
> > > > load_acquire x
> > > >
> > > > Consider the case in which the load from x is an L1 hit, but a
> > > > preceding load (from say y) is a long-latency miss.  If we enforce
> > > > ordering by just waiting for completion of prior operation, the
> > > > former has to wait for the load from y to complete; while the
> > > > latter doesn't.  I find it hard to believe that this doesn't leave
> > > > an appreciable amount of performance on the table, at least for
> > > > some interesting microarchitectures.
> > >
> > > I agree, Hans, that this is a reasonable assumption.  Load_acquire x
> > > does allow roach motel, whereas the acquire fence does not.
> > >
> > > >  In addition, for better or worse, fencing requirements on at least
> > > >  Power are actually driven as much by store atomicity issues, as by
> > > >  the ordering issues discussed in the cookbook.  This was not
> > > >  understood in 2005, and unfortunately doesn't seem to be
> amenable to
> > > >  the kind of straightforward explanation as in Doug's cookbook.
> > >
> > > Coming from a strongly ordered architecture to a weakly ordered one
> > > myself, I also needed some mental adjustment about store (multi-copy)
> > > atomicity.  I can imagine others will be unaware of this difference,
> > > too, even in 2014.
> >
> > Sorry I'm missing the connection between fences and multi-copy
> atomicity.
>
> One example is the classic IRIW.  With non-multi copy atomic stores, but
> ordered (say through a dependency) loads in the following example:
>
> Memory: foo = bar = 0
> _T1_         _T2_         _T3_                              _T4_
> st (foo),1   st (bar),1   ld r1, (bar)                      ld r3,(foo)
>                           <addr dep / local "fence" here>   <addr dep>
>                           ld r2, (foo)                      ld r4, (bar)
>
> You may observe r1 = 1, r2 = 0, r3 = 1, r4 = 0 on non-multi-copy atomic
> machines.  On TSO boxes, this is not possible.  That means that the
> memory fence that will prevent such a behaviour (DMB on ARM) needs to
> carry some additional oomph in ensuring multi-copy atomicity, or rather
> prevent you from seeing it (which is the same thing).
I take it as given that any code for which you may have ordering
constraints, must first have basic atomicity properties for loads and
stores. I would not expect any kind of fence to add multi-copy-atomicity
where there was none.

David

> Stephan
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu<mailto:Concurrency-interest at cs.oswego.edu>
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu<mailto:Concurrency-interest at cs.oswego.edu>
http://cs.oswego.edu/mailman/listinfo/concurrency-interest




_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu<mailto:Concurrency-interest at cs.oswego.edu>
http://cs.oswego.edu/mailman/listinfo/concurrency-interest


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141126/9cbe4e8e/attachment-0001.html>

From heinz at javaspecialists.eu  Wed Nov 26 14:06:47 2014
From: heinz at javaspecialists.eu (Dr Heinz M. Kabutz)
Date: Wed, 26 Nov 2014 21:06:47 +0200
Subject: [concurrency-interest] Mixing async and blocking components
In-Reply-To: <54761F3E.9020409@cs.oswego.edu>
References: <54761F3E.9020409@cs.oswego.edu>
Message-ID: <547624C7.50400@javaspecialists.eu>


Doug Lea wrote:
>
> The increasing adoption of "structured" async components and
> frameworks (CompletableFuture, Rx, reactive-streams, some parallel
> uses of java.util.Stream) leads to increasing problems when users mix
> them with blocking components and frameworks. When async tasks block,
> then overall program liveness may require compensation with more
> threads within some Executor.  But there is in general no limit to how
> many threads are required to maintain liveness, so programs may
> encounter OutOfMemoryErrors trying to create too many threads.
> In common cases where upper bounds cannot be established, no one
> likes to make a choice between possibly either locking up vs
> blowing up programs.
Seems we were both writing our emails at the same time :-)

There is a limit within parallel streams, but probably artificial, due 
to the 4x multiplier for the number of leaves based on the common pool 
parallelism.
>
> This problem can occur no matter what kind of Executor you use for
> async. The only perfect solution is avoidance: not mixing async and
> blocking components in this way. I expect that over time, async
> replacements for some of the most troublesome IO and network-based
> components will appear. It seems that there are (non-JDK) versions of
> RMI, JAX-RPC, and adaptors for java.nio, in various stages of
> development. (It would be great if there were something similar for
> JDBC.) All of these efforts fall outside of java.util.concurrent
> though.  Similarly for frameworks like Ron Pressler's Quasar that
> automate the equivalents of adaptors.  The best we can do is provide
> better coping mechanisms when these do not apply, and possibly add
> some support to simplify construction of async replacements for
> blocking components.
>
> We improved the situation for ForkJoinPool in an update a few months
> ago (that ought to appear in a JDK8u release) by throwing more
> manageable max-spare bound-check exceptions before resources are
> typically actually exhausted. ForkJoinPool is also currently better
> equipped to deal with this than other executors by supporting the
> ManagedBlocker API to automate triggering of possible compensation.
> But further improvements would be helpful.
>
> One option is to both extend the number of j.u.c synchronization
> components (ReentrantLock, Semaphore, etc) that implement
> ManagedBlocker, and enhance ThreadPoolExecutor to also include
> (optional) compensation mechanics.
I would agree if we can do it without causing additional objects to be 
created during locking mechanisms.
> Another (partially complementary) approach is to extend support for
> variants of a classic simple resource control heuristic (I think
> originating circa 1970 in AT&T ESS switches): Sometimes slow down
> async task producers by arranging for them to help run tasks.  When
> applied to network-based task producers, this may in turn cause
> packets to be dropped, which is usually the best of all the bad
> options. This isn't a cure-all: drops may lead to distributed liveness
> failures and/or local stalls due to unsatisfiable task dependencies.
> It also doesn't apply directly when helped-out tasks generate
> exponentially more tasks. But in many practical cases, it can
> vastly reduce the likelihood of failure.
>
> We already predefine the simplest form of this as the CallerRuns
> RejectedExecutionHandler in TPE. Other variants can already be
> programmed in both FJP and TPE, but we don't make it particularly
> easy. For example, in TPE you can implement a handler that first
> executes the oldest existing task, and then retries submission. And in
> FJ, you can check local queue size to decide upon submission to first
> help run a local or non-local task.  One of the reasons we didn't
> predefine any of these is that there seemed to be too many
> situation-specific plausible variants. But adding more of these may
> help lead people into more successful choices.
Something which has always puzzled me is why the 
RejectedExecutionHandler is used both for an overloaded TPE and for a 
TPE that has been shut down.  CallerRuns thus could get into a situation 
where the task is never executed in a TPE that has been shut down.  IMHO 
it would be better to do something like this:

    public static class CallerRunsPolicy implements 
RejectedExecutionHandler {
        public void rejectedExecution(Runnable r, ThreadPoolExecutor e) {
            if (!e.isShutdown()) {
                r.run();
            } else {
                throw new RejectedExecutionException("Task " + 
r.toString() +
                                                 " rejected from " +
                                                 e.toString());
        }
    }

But that is probably a discussion for another thread :-)
>
> Opinions about any of these, or other suggestions, would be welcome.
>
>
> -Doug
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>

From dl at cs.oswego.edu  Wed Nov 26 14:13:53 2014
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed, 26 Nov 2014 14:13:53 -0500
Subject: [concurrency-interest] ManagedBlockers Not Giving Full
 Parallelism in FJPool
In-Reply-To: <547620C2.7020309@javaspecialists.eu>
References: <547620C2.7020309@javaspecialists.eu>
Message-ID: <54762671.8070607@cs.oswego.edu>

On 11/26/2014 01:49 PM, Dr Heinz M. Kabutz wrote:
> Hi,
>
> thanks to some tips by Paul Sandoz at Devoxx, I started looking at
> ManagedBlockers as a way to allow blocking operations to "play nice" with
> parallel streams.

(As you probably already surmised, my last post was in part
motivated by one of your initial questions about this!)

> Whilst writing some sample code to demonstrate how vastly superior (*cough*
> *cough*) this was to using a normal ReentrantLock within a parallel stream, I
> came across some strange observations with stream behaviour which I cannot
> explain.  I'd love to hear if any of you can point me to the right place.  It
> seems as if when I have threads that have been suspended on a ManagedBlocker,
> that a maximum of one thread is accessible from the FJP for work, in addition to
> the invoking thread. We thus get a speedup of only 2x over the serial version,
> whereas on my 1-4-2 laptop I would've expected it to run as fast as if there
> were no suspended threads.

FJP can only guarantee liveness (unless hitting resource exhaustion),
not maximal parallelism. It cannot strictly guarantee to generate
a new thread whenever one may be blocked.
So in some cases, you will only get one additional compensation thread.
The main reason we cannot do better is hinted at in my last post:
We have to be conservative about prospects for exponential explosions in
numbers of thread. (Some pre-JDK7 jsr166y versions had a
method controlling this, but this was removed after finding
that no one could use it effectively.)

-Doug



From yu.lin.86 at gmail.com  Wed Nov 26 14:42:32 2014
From: yu.lin.86 at gmail.com (Yu Lin)
Date: Wed, 26 Nov 2014 13:42:32 -0600
Subject: [concurrency-interest] CompletableFuture in Java 8
In-Reply-To: <CAAL-3PaZDBmjQ7i43+VUeW+OGh+6VAcA3rp9mzmetJar2k_aZw@mail.gmail.com>
References: <CAAL-3PanDAubT2RM-DePuUv_LmSX2=OsDcy82Q+ETBaRuvEwEw@mail.gmail.com>
	<CAAWwtm-evqwXxNsoMLsnm5PNU-n+=O1iR_ZcptfaJ60JHg9jbg@mail.gmail.com>
	<CAAL-3PaZDBmjQ7i43+VUeW+OGh+6VAcA3rp9mzmetJar2k_aZw@mail.gmail.com>
Message-ID: <CAAL-3PYJ90uZ_Kmz1-QjY43nRUCAMxxXJmE9KiVKCddCeMnyfA@mail.gmail.com>

I guess David does mean RxJava. There're some projects use RxJava. David,
in which project do you use it? May I take a look if it's open-source?

On Fri, Nov 21, 2014 at 4:29 PM, Yu Lin <yu.lin.86 at gmail.com> wrote:

> OK, thanks for all your detailed analysis. I guess the slow adaption of a
> new version is why I can't find the use of it. I myself is using Java 6?
> Other similar libraries you mentioned seem to be interesting.
>
> On Fri, Nov 21, 2014 at 1:35 PM, D?vid Karnok <akarnokd at gmail.com> wrote:
>
>> Hi. CompletableFuture is great, but as a Ph.D R&D student myself, I see a
>> few shortcomings:- Slow adaptation of Java 8,- the feature lag in a well
>> known mobile platform and- the alternatives available as library for some
>> time.In addition, it may compose better than Future, but IMHO, it doesn't
>> go far enough. Therefore, I use a FOSS library (with increasing popularity)
>> that allows one to compose asynchronous data streams with much less worry
>> on concurrency and continuation than CF. I use it in 50k loc projects, but
>> I read/heard a well known company is switching almost all of its software
>> stack to the reactive/asynchronous programming idiom this library enables.
>> Excellent material is available (including videos and conference talk) on
>> this subject (and the library).
>> 2014.11.20. 2:45 ezt ?rta ("Yu Lin" <yu.lin.86 at gmail.com>):
>>
>>> Hi all,
>>>
>>> I'm a Ph.D. student at University of Illinois. I'm doing research on the
>>> uses of Java concurrent/asynchronous tasks. A new construct introduced in
>>> Java 8, CompletableFuture, seems to be a very nice and easy to use
>>> concurrent task.
>>>
>>> I'm looking for projects that use CompletableFuture. But since it's very
>>> new, I didn't find any real-world projects use it (only found many toy
>>> examples/demos).
>>>
>>> Does anyone know any projects that use it?
>>>
>>> Thanks,
>>> Yu Lin
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141126/4e693a6f/attachment.html>

From heinz at javaspecialists.eu  Wed Nov 26 14:54:33 2014
From: heinz at javaspecialists.eu (Dr Heinz M. Kabutz)
Date: Wed, 26 Nov 2014 21:54:33 +0200
Subject: [concurrency-interest] ManagedBlockers Not Giving Full
 Parallelism in FJPool
In-Reply-To: <54762671.8070607@cs.oswego.edu>
References: <547620C2.7020309@javaspecialists.eu>
	<54762671.8070607@cs.oswego.edu>
Message-ID: <54762FF9.4080607@javaspecialists.eu>


>> Whilst writing some sample code to demonstrate how vastly superior 
>> (*cough*
>> *cough*) this was to using a normal ReentrantLock within a parallel 
>> stream, I
>> came across some strange observations with stream behaviour which I 
>> cannot
>> explain.  I'd love to hear if any of you can point me to the right 
>> place.  It
>> seems as if when I have threads that have been suspended on a 
>> ManagedBlocker,
>> that a maximum of one thread is accessible from the FJP for work, in 
>> addition to
>> the invoking thread. We thus get a speedup of only 2x over the serial 
>> version,
>> whereas on my 1-4-2 laptop I would've expected it to run as fast as 
>> if there
>> were no suspended threads.
>
> FJP can only guarantee liveness (unless hitting resource exhaustion),
> not maximal parallelism. It cannot strictly guarantee to generate
> a new thread whenever one may be blocked.
> So in some cases, you will only get one additional compensation thread.
> The main reason we cannot do better is hinted at in my last post:
> We have to be conservative about prospects for exponential explosions in
> numbers of thread. (Some pre-JDK7 jsr166y versions had a
> method controlling this, but this was removed after finding
> that no one could use it effectively.)
Right, and that it certainly does admirably.  Thanks for your input.

From dl at cs.oswego.edu  Wed Nov 26 15:29:34 2014
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed, 26 Nov 2014 15:29:34 -0500
Subject: [concurrency-interest] CompletableFuture in Java 8
In-Reply-To: <CAAL-3PYJ90uZ_Kmz1-QjY43nRUCAMxxXJmE9KiVKCddCeMnyfA@mail.gmail.com>
References: <CAAL-3PanDAubT2RM-DePuUv_LmSX2=OsDcy82Q+ETBaRuvEwEw@mail.gmail.com>	<CAAWwtm-evqwXxNsoMLsnm5PNU-n+=O1iR_ZcptfaJ60JHg9jbg@mail.gmail.com>	<CAAL-3PaZDBmjQ7i43+VUeW+OGh+6VAcA3rp9mzmetJar2k_aZw@mail.gmail.com>
	<CAAL-3PYJ90uZ_Kmz1-QjY43nRUCAMxxXJmE9KiVKCddCeMnyfA@mail.gmail.com>
Message-ID: <5476382E.3090603@cs.oswego.edu>


The current "structured async" landscape in Java is indeed a little
confusing at the moment. Here's a quick synopsis of three main
approaches:

1. CompletableFuture allows arranging dynamic completions
(sequences, and's, or's, compositions) for individual async events
and actions. It relies on lambdas and other jdk8 features.
Some Guava and other non-jdk components provide some of this
functionality but run on pre-jdk8 JVMs.

2. Parallel java.util.Streams arrange common bulk processing
of all the elements in an existing collection. There are also
forms based on other stream-like constructions, but in jdk8, all
IO-based forms rely on blocking-IO producers. Also, there is not
a standard way to use streams within dynamic completions.
Hopefully these will change.

3. RxJava, reactive-streams (and to some extent other non-JDK
frameworks) allow you to arrange common processing for elements
as they are asynchronously produced. While you can get this
effect by creating factories producing CompletableFutures,
there's not a standard way to do this. Hopefully this will
change.

-Doug



From david.lloyd at redhat.com  Wed Nov 26 17:44:51 2014
From: david.lloyd at redhat.com (David M. Lloyd)
Date: Wed, 26 Nov 2014 16:44:51 -0600
Subject: [concurrency-interest] Mixing async and blocking components
In-Reply-To: <54761F3E.9020409@cs.oswego.edu>
References: <54761F3E.9020409@cs.oswego.edu>
Message-ID: <547657E3.1000202@redhat.com>

The landscape is further muddled by the fact that many executor tasks, 
whether they generally contain blocking components or not, often have 
implicit dependencies on other tasks.  In our complex systems, stalls 
are often not only due to exhaustion of threads but because an implicit 
ordering of tasks wasn't exploited which in most cases would have 
allowed completion even with limited numbers of threads.

Unrelatedly, in terms of actual run-time tasks like you'd have in a 
common real-world situation (e.g. web server talks to remote service, 
which in turn talks to databases and queues and so forth), we're moving 
towards the model of turning off requests at the source in response to 
load situations.  In practice I find that end users are unable to cope 
with having dozens of knobs to adjust thread pool sizes and queue 
strategies to avoid things like task rejection, and running tasks in the 
submitting thread is often rife with hidden (to the unsuspecting user) 
pitfalls (e.g. the nested task is unexpectedly called within an 
enclosing lock, or can never complete because it has an implicit 
dependency on the calling task).  Having a small number of knobs (or 
just one) that controls the inlet really has a much lower barrier to 
understanding.

In other words, almost any case where you can hit a late-stage resource 
limit with typical Java code results in (at best) a traffic jam of 
backed up work (possibly accompanied by rampant memory usage), or (at 
worst) unrecoverable exceptions (OOME, Rejection, file/thread limits) or 
even deadlocks.  So the simplest solution seems to be the best: throttle 
the work at the earliest source whenever possible, which implicitly 
limits all downstream resource consumption (in an ideal world anyway - 
if each work item causes an exponential explosion of sub-work items, 
they simply have to be intelligently managed).  Adding in various 
"smart" task dependency ordering techniques wherever it fits also can 
really help not only manage the flood a bit, but has also been shown on 
a few occasions to improve overall performance for various reasons.

I think that typical Java code and your typical Java coder will probably 
always be too naive to be able to safely exploit caller-runs types of 
compensation mechanisms in a general sense.  But I'm admittedly somewhat 
of a pessimist.

On 11/26/2014 12:43 PM, Doug Lea wrote:
>
> The increasing adoption of "structured" async components and
> frameworks (CompletableFuture, Rx, reactive-streams, some parallel
> uses of java.util.Stream) leads to increasing problems when users mix
> them with blocking components and frameworks. When async tasks block,
> then overall program liveness may require compensation with more
> threads within some Executor.  But there is in general no limit to how
> many threads are required to maintain liveness, so programs may
> encounter OutOfMemoryErrors trying to create too many threads.
> In common cases where upper bounds cannot be established, no one
> likes to make a choice between possibly either locking up vs
> blowing up programs.
>
> This problem can occur no matter what kind of Executor you use for
> async. The only perfect solution is avoidance: not mixing async and
> blocking components in this way. I expect that over time, async
> replacements for some of the most troublesome IO and network-based
> components will appear. It seems that there are (non-JDK) versions of
> RMI, JAX-RPC, and adaptors for java.nio, in various stages of
> development. (It would be great if there were something similar for
> JDBC.) All of these efforts fall outside of java.util.concurrent
> though.  Similarly for frameworks like Ron Pressler's Quasar that
> automate the equivalents of adaptors.  The best we can do is provide
> better coping mechanisms when these do not apply, and possibly add
> some support to simplify construction of async replacements for
> blocking components.
>
> We improved the situation for ForkJoinPool in an update a few months
> ago (that ought to appear in a JDK8u release) by throwing more
> manageable max-spare bound-check exceptions before resources are
> typically actually exhausted. ForkJoinPool is also currently better
> equipped to deal with this than other executors by supporting the
> ManagedBlocker API to automate triggering of possible compensation.
> But further improvements would be helpful.
>
> One option is to both extend the number of j.u.c synchronization
> components (ReentrantLock, Semaphore, etc) that implement
> ManagedBlocker, and enhance ThreadPoolExecutor to also include
> (optional) compensation mechanics.
>
> Another (partially complementary) approach is to extend support for
> variants of a classic simple resource control heuristic (I think
> originating circa 1970 in AT&T ESS switches): Sometimes slow down
> async task producers by arranging for them to help run tasks.  When
> applied to network-based task producers, this may in turn cause
> packets to be dropped, which is usually the best of all the bad
> options. This isn't a cure-all: drops may lead to distributed liveness
> failures and/or local stalls due to unsatisfiable task dependencies.
> It also doesn't apply directly when helped-out tasks generate
> exponentially more tasks. But in many practical cases, it can
> vastly reduce the likelihood of failure.
>
> We already predefine the simplest form of this as the CallerRuns
> RejectedExecutionHandler in TPE. Other variants can already be
> programmed in both FJP and TPE, but we don't make it particularly
> easy. For example, in TPE you can implement a handler that first
> executes the oldest existing task, and then retries submission. And in
> FJ, you can check local queue size to decide upon submission to first
> help run a local or non-local task.  One of the reasons we didn't
> predefine any of these is that there seemed to be too many
> situation-specific plausible variants. But adding more of these may
> help lead people into more successful choices.
>
> Opinions about any of these, or other suggestions, would be welcome.
>
>
> -Doug
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-- 
- DML

From martinrb at google.com  Wed Nov 26 21:15:13 2014
From: martinrb at google.com (Martin Buchholz)
Date: Wed, 26 Nov 2014 18:15:13 -0800
Subject: [concurrency-interest] RFR: 8065804: JEP 171:
	Clarifications/corrections for fence intrinsics
In-Reply-To: <54767976.70303@oracle.com>
References: <CA+kOe08RfnkJ4Dit8ng7HQaSuDer_pA0i4t568uAGMg9dzujhg@mail.gmail.com>
	<54767976.70303@oracle.com>
Message-ID: <CA+kOe08X+CyazoxTto-AtxV9=Qea9zwaNZwBK+=9NDQHpN=NgQ@mail.gmail.com>

On Wed, Nov 26, 2014 at 5:08 PM, David Holmes <david.holmes at oracle.com> wrote:
> Please explain why you have changed the defined semantics for storeFence.
> You have completely reversed the direction of the barrier.

Yes.  I believe the current spec of storeFence was a copy-paste typo,
and it seems others feel likewise.

From martinrb at google.com  Wed Nov 26 21:21:40 2014
From: martinrb at google.com (Martin Buchholz)
Date: Wed, 26 Nov 2014 18:21:40 -0800
Subject: [concurrency-interest] RFR: 8065804: JEP 171:
 Clarifications/corrections for fence intrinsics
In-Reply-To: <B84AA20B-5714-4FF0-AB17-E918395C9DDB@oracle.com>
References: <CA+kOe08RfnkJ4Dit8ng7HQaSuDer_pA0i4t568uAGMg9dzujhg@mail.gmail.com>
	<5473A344.8090302@oracle.com>
	<CA+kOe0-KiKKNM-LyYvjZiXOGMF7c4FsHifcDq-ZZ65Bb1J=LLA@mail.gmail.com>
	<B84AA20B-5714-4FF0-AB17-E918395C9DDB@oracle.com>
Message-ID: <CA+kOe097JE=XH=SgD-c+JdLh1hELZhuRdT-Je=XWeTEHxc_OWw@mail.gmail.com>

On Tue, Nov 25, 2014 at 6:04 AM, Paul Sandoz <paul.sandoz at oracle.com> wrote:
> Hi Martin,
>
> Thanks for looking into this.
>
> 1141      * Currently hotspot's implementation of a Java language-level volatile
> 1142      * store has the same effect as a storeFence followed by a relaxed store,
> 1143      * although that may be a little stronger than needed.
>
> IIUC to emulate hotpot's volatile store you will need to say that a fullFence immediately follows the relaxed store.

Right - I've been groking that.

> The bit that always confuses me about release and acquire is ordering is restricted to one direction, as talked about in orderAccess.hpp [1]. So for a release, accesses prior to the release cannot move below it, but accesses succeeding the release can move above it. And that seems to apply to Unsafe.storeFence [2] (acting like a monitor exit). Is that contrary to C++ release fences where ordering is restricted both to prior and succeeding accesses? [3]
>
> So what about the following?
>
>   a = r1; // Cannot move below the fence
>   Unsafe.storeFence();
>   b = r2; // Can move above the fence?

I think the hotspot docs need to be more precise about when they're
talking about movement of stores and when about loads.

> // release.  I.e., subsequent memory accesses may float above the
> // release, but prior ones may not float below it.

As I've said elsewhere, the above makes no sense without restricting
the type of access.


From martinrb at google.com  Wed Nov 26 21:29:06 2014
From: martinrb at google.com (Martin Buchholz)
Date: Wed, 26 Nov 2014 18:29:06 -0800
Subject: [concurrency-interest] RFR: 8065804: JEP 171:
 Clarifications/corrections for fence intrinsics
In-Reply-To: <5474F79A.7000607@redhat.com>
References: <CA+kOe08RfnkJ4Dit8ng7HQaSuDer_pA0i4t568uAGMg9dzujhg@mail.gmail.com>
	<5474F79A.7000607@redhat.com>
Message-ID: <CA+kOe09B=1oXsFh9xp6k=yZK7Eb87oV2mhnHVW3qfrx2hEpSag@mail.gmail.com>

On Tue, Nov 25, 2014 at 1:41 PM, Andrew Haley <aph at redhat.com> wrote:
> On 11/24/2014 08:56 PM, Martin Buchholz wrote:
> +     * Currently hotspot's implementation of a Java language-level volatile
> +     * store has the same effect as a storeFence followed by a relaxed store,
> +     * although that may be a little stronger than needed.
>
> While this may be true today

No - it was very wrong, since it doesn't give you sequential consistency!

, I'm hopefully about to commit an
> AArch64 OpenJDK port that uses the ARMv8 stlr instruction.  I
> don't think that what you've written here is terribly misleading,
> but bear in mind that it may be there for some time.

From davidcholmes at aapt.net.au  Wed Nov 26 21:56:23 2014
From: davidcholmes at aapt.net.au (David Holmes)
Date: Thu, 27 Nov 2014 12:56:23 +1000
Subject: [concurrency-interest] RFR: 8065804: JEP
	171:Clarifications/corrections for fence intrinsics
In-Reply-To: <CA+kOe08X+CyazoxTto-AtxV9=Qea9zwaNZwBK+=9NDQHpN=NgQ@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCAEDDKLAA.davidcholmes@aapt.net.au>

Martin Buchholz writes:
> On Wed, Nov 26, 2014 at 5:08 PM, David Holmes 
> <david.holmes at oracle.com> wrote:
> > Please explain why you have changed the defined semantics for 
> storeFence.
> > You have completely reversed the direction of the barrier.
> 
> Yes.  I believe the current spec of storeFence was a copy-paste typo,
> and it seems others feel likewise.

Can whomever wrote that original spec please confirm that.

Thanks,
David


From davidcholmes at aapt.net.au  Wed Nov 26 22:00:34 2014
From: davidcholmes at aapt.net.au (David Holmes)
Date: Thu, 27 Nov 2014 13:00:34 +1000
Subject: [concurrency-interest] RFR: 8065804: JEP 171:
	Clarifications/corrections for fence intrinsics
In-Reply-To: <CA+kOe097JE=XH=SgD-c+JdLh1hELZhuRdT-Je=XWeTEHxc_OWw@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCIEDDKLAA.davidcholmes@aapt.net.au>

Can I make an observation about acquire() and release() - to me they are
meaningless when considered in isolation. Given their definitions they allow
anything to move into a region bounded by acquire() and release(), then you
can effectively move the whole program into the region and thus the
acquire() and release() do not constrain any reorderings. acquire() and
release() only make sense when their own movement is constrained with
respect to something else - such as lock acquisition/release, or when
combined with specific load/store actions.

David

Martin Buchholz writes:
>
> On Tue, Nov 25, 2014 at 6:04 AM, Paul Sandoz
> <paul.sandoz at oracle.com> wrote:
> > Hi Martin,
> >
> > Thanks for looking into this.
> >
> > 1141      * Currently hotspot's implementation of a Java
> language-level volatile
> > 1142      * store has the same effect as a storeFence followed
> by a relaxed store,
> > 1143      * although that may be a little stronger than needed.
> >
> > IIUC to emulate hotpot's volatile store you will need to say
> that a fullFence immediately follows the relaxed store.
>
> Right - I've been groking that.
>
> > The bit that always confuses me about release and acquire is
> ordering is restricted to one direction, as talked about in
> orderAccess.hpp [1]. So for a release, accesses prior to the
> release cannot move below it, but accesses succeeding the release
> can move above it. And that seems to apply to Unsafe.storeFence
> [2] (acting like a monitor exit). Is that contrary to C++ release
> fences where ordering is restricted both to prior and succeeding
> accesses? [3]
> >
> > So what about the following?
> >
> >   a = r1; // Cannot move below the fence
> >   Unsafe.storeFence();
> >   b = r2; // Can move above the fence?
>
> I think the hotspot docs need to be more precise about when they're
> talking about movement of stores and when about loads.
>
> > // release.  I.e., subsequent memory accesses may float above the
> > // release, but prior ones may not float below it.
>
> As I've said elsewhere, the above makes no sense without restricting
> the type of access.
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From martinrb at google.com  Wed Nov 26 22:06:13 2014
From: martinrb at google.com (Martin Buchholz)
Date: Wed, 26 Nov 2014 19:06:13 -0800
Subject: [concurrency-interest] RFR: 8065804: JEP 171:
 Clarifications/corrections for fence intrinsics
In-Reply-To: <NFBBKALFDCPFIDBNKAPCIEDDKLAA.davidcholmes@aapt.net.au>
References: <CA+kOe097JE=XH=SgD-c+JdLh1hELZhuRdT-Je=XWeTEHxc_OWw@mail.gmail.com>
	<NFBBKALFDCPFIDBNKAPCIEDDKLAA.davidcholmes@aapt.net.au>
Message-ID: <CA+kOe0-Gjmipv7kuWEs-Q5pEMeDZ7mEof=py_3c2Lp+QjX57oQ@mail.gmail.com>

On Wed, Nov 26, 2014 at 7:00 PM, David Holmes <davidcholmes at aapt.net.au> wrote:
> Can I make an observation about acquire() and release() - to me they are
> meaningless when considered in isolation. Given their definitions they allow
> anything to move into a region bounded by acquire() and release(), then you
> can effectively move the whole program into the region and thus the
> acquire() and release() do not constrain any reorderings. acquire() and
> release() only make sense when their own movement is constrained with
> respect to something else - such as lock acquisition/release, or when
> combined with specific load/store actions.

David, it seems you are agreeing with my argument below.  The
definitions in the hotspot sources should be fixed, in the same sort
of way that I'm trying to make the specs for Unsafe loads clearer and
more precise.

> David
>
> Martin Buchholz writes:
>> I think the hotspot docs need to be more precise about when they're
>> talking about movement of stores and when about loads.
>>
>> > // release.  I.e., subsequent memory accesses may float above the
>> > // release, but prior ones may not float below it.
>>
>> As I've said elsewhere, the above makes no sense without restricting
>> the type of access.

From davidcholmes at aapt.net.au  Wed Nov 26 22:27:18 2014
From: davidcholmes at aapt.net.au (David Holmes)
Date: Thu, 27 Nov 2014 13:27:18 +1000
Subject: [concurrency-interest] RFR: 8065804: JEP 171:
	Clarifications/corrections for fence intrinsics
In-Reply-To: <CA+kOe0-Gjmipv7kuWEs-Q5pEMeDZ7mEof=py_3c2Lp+QjX57oQ@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCIEDMKLAA.davidcholmes@aapt.net.au>

Martin writes:
> On Wed, Nov 26, 2014 at 7:00 PM, David Holmes 
> <davidcholmes at aapt.net.au> wrote:
> > Can I make an observation about acquire() and release() - to me they are
> > meaningless when considered in isolation. Given their 
> definitions they allow
> > anything to move into a region bounded by acquire() and 
> release(), then you
> > can effectively move the whole program into the region and thus the
> > acquire() and release() do not constrain any reorderings. acquire() and
> > release() only make sense when their own movement is constrained with
> > respect to something else - such as lock acquisition/release, or when
> > combined with specific load/store actions.
> 
> David, it seems you are agreeing with my argument below.  The
> definitions in the hotspot sources should be fixed, in the same sort
> of way that I'm trying to make the specs for Unsafe loads clearer and
> more precise.

Please see:

https://bugs.openjdk.java.net/browse/JDK-7143664

Though I'm not sure my ramblings there reflect my current thoughts on all this. I really think acquire/release are too confusingly used to be useful - by which I mean that the names do not reflect their actions so you will always have to remember/look-up exactly what *release* and *acquire* mean in that context, and hence talking about "acquire semantics" and "release semantics" becomes meaningless. In contrast the loadload|loadstore etc barriers are completely straight-forward to understand from their names. However it seems they are too strong compared to what recent hardware provides.

Hotspot implementations in orderAccess are confusing - barriers with different semantics have been defined in terms of the other, but the low-level implementations provide a barrier that is stronger than the required semantics, so the high-level APIs are "satisfied" correctly, even if not implemented in a way that makes sense if you reason about what each barrier theoretically allows.

David
 
> > David
> >
> > Martin Buchholz writes:
> >> I think the hotspot docs need to be more precise about when they're
> >> talking about movement of stores and when about loads.
> >>
> >> > // release.  I.e., subsequent memory accesses may float above the
> >> > // release, but prior ones may not float below it.
> >>
> >> As I've said elsewhere, the above makes no sense without restricting
> >> the type of access.
> 



From radhakrishnan.mohan at gmail.com  Thu Nov 27 01:46:07 2014
From: radhakrishnan.mohan at gmail.com (Mohan Radhakrishnan)
Date: Thu, 27 Nov 2014 12:16:07 +0530
Subject: [concurrency-interest] CompletableFuture in Java 8
In-Reply-To: <CAAL-3PYJ90uZ_Kmz1-QjY43nRUCAMxxXJmE9KiVKCddCeMnyfA@mail.gmail.com>
References: <CAAL-3PanDAubT2RM-DePuUv_LmSX2=OsDcy82Q+ETBaRuvEwEw@mail.gmail.com>
	<CAAWwtm-evqwXxNsoMLsnm5PNU-n+=O1iR_ZcptfaJ60JHg9jbg@mail.gmail.com>
	<CAAL-3PaZDBmjQ7i43+VUeW+OGh+6VAcA3rp9mzmetJar2k_aZw@mail.gmail.com>
	<CAAL-3PYJ90uZ_Kmz1-QjY43nRUCAMxxXJmE9KiVKCddCeMnyfA@mail.gmail.com>
Message-ID: <CAOoXFP9hRneBagR4hHggCQoyxsJp8tFQNbdmMvoY5dS+4LZH4g@mail.gmail.com>

Don't you think Netflix should be using it ? http://netflix.github.io/#repo

Thanks,
Mohan

On Thu, Nov 27, 2014 at 1:12 AM, Yu Lin <yu.lin.86 at gmail.com> wrote:

> I guess David does mean RxJava. There're some projects use RxJava. David,
> in which project do you use it? May I take a look if it's open-source?
>
> On Fri, Nov 21, 2014 at 4:29 PM, Yu Lin <yu.lin.86 at gmail.com> wrote:
>
>> OK, thanks for all your detailed analysis. I guess the slow adaption of a
>> new version is why I can't find the use of it. I myself is using Java 6?
>> Other similar libraries you mentioned seem to be interesting.
>>
>> On Fri, Nov 21, 2014 at 1:35 PM, D?vid Karnok <akarnokd at gmail.com> wrote:
>>
>>> Hi. CompletableFuture is great, but as a Ph.D R&D student myself, I see
>>> a few shortcomings:- Slow adaptation of Java 8,- the feature lag in a well
>>> known mobile platform and- the alternatives available as library for some
>>> time.In addition, it may compose better than Future, but IMHO, it doesn't
>>> go far enough. Therefore, I use a FOSS library (with increasing popularity)
>>> that allows one to compose asynchronous data streams with much less worry
>>> on concurrency and continuation than CF. I use it in 50k loc projects, but
>>> I read/heard a well known company is switching almost all of its software
>>> stack to the reactive/asynchronous programming idiom this library enables.
>>> Excellent material is available (including videos and conference talk) on
>>> this subject (and the library).
>>> 2014.11.20. 2:45 ezt ?rta ("Yu Lin" <yu.lin.86 at gmail.com>):
>>>
>>>> Hi all,
>>>>
>>>> I'm a Ph.D. student at University of Illinois. I'm doing research on
>>>> the uses of Java concurrent/asynchronous tasks. A new construct introduced
>>>> in Java 8, CompletableFuture, seems to be a very nice and easy to use
>>>> concurrent task.
>>>>
>>>> I'm looking for projects that use CompletableFuture. But since it's
>>>> very new, I didn't find any real-world projects use it (only found many toy
>>>> examples/demos).
>>>>
>>>> Does anyone know any projects that use it?
>>>>
>>>> Thanks,
>>>> Yu Lin
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141127/36ad5b48/attachment-0001.html>

From yu.lin.86 at gmail.com  Thu Nov 27 02:24:55 2014
From: yu.lin.86 at gmail.com (Yu Lin)
Date: Thu, 27 Nov 2014 01:24:55 -0600
Subject: [concurrency-interest] CompletableFuture in Java 8
In-Reply-To: <CAOoXFP9hRneBagR4hHggCQoyxsJp8tFQNbdmMvoY5dS+4LZH4g@mail.gmail.com>
References: <CAAL-3PanDAubT2RM-DePuUv_LmSX2=OsDcy82Q+ETBaRuvEwEw@mail.gmail.com>
	<CAAWwtm-evqwXxNsoMLsnm5PNU-n+=O1iR_ZcptfaJ60JHg9jbg@mail.gmail.com>
	<CAAL-3PaZDBmjQ7i43+VUeW+OGh+6VAcA3rp9mzmetJar2k_aZw@mail.gmail.com>
	<CAAL-3PYJ90uZ_Kmz1-QjY43nRUCAMxxXJmE9KiVKCddCeMnyfA@mail.gmail.com>
	<CAOoXFP9hRneBagR4hHggCQoyxsJp8tFQNbdmMvoY5dS+4LZH4g@mail.gmail.com>
Message-ID: <CAAL-3Pbd55dg8CpYXhiR3kTENgnBLxmPDZ9agAv4-d1F4LXmeQ@mail.gmail.com>

Yes, I'm looking at RxJava and Guava ListenableFuture. Do you guys know
what's the major differences among RxJava, ListenableFuture and
CompletableFuture. What's the advantage of each?

It seems to me all three support callback listeners and async task
chaining. But ListenableFuture doesn't support lambda while RxJava support
more operations for Observable. How to choose which construct to use when
doing asynchronous programming?

Thanks,
Yu

On Thu, Nov 27, 2014 at 12:46 AM, Mohan Radhakrishnan <
radhakrishnan.mohan at gmail.com> wrote:

> Don't you think Netflix should be using it ?
> http://netflix.github.io/#repo
>
> Thanks,
> Mohan
>
> On Thu, Nov 27, 2014 at 1:12 AM, Yu Lin <yu.lin.86 at gmail.com> wrote:
>
>> I guess David does mean RxJava. There're some projects use RxJava.
>> David, in which project do you use it? May I take a look if it's
>> open-source?
>>
>> On Fri, Nov 21, 2014 at 4:29 PM, Yu Lin <yu.lin.86 at gmail.com> wrote:
>>
>>> OK, thanks for all your detailed analysis. I guess the slow adaption of
>>> a new version is why I can't find the use of it. I myself is using Java 6?
>>> Other similar libraries you mentioned seem to be interesting.
>>>
>>> On Fri, Nov 21, 2014 at 1:35 PM, D?vid Karnok <akarnokd at gmail.com>
>>> wrote:
>>>
>>>> Hi. CompletableFuture is great, but as a Ph.D R&D student myself, I see
>>>> a few shortcomings:- Slow adaptation of Java 8,- the feature lag in a well
>>>> known mobile platform and- the alternatives available as library for some
>>>> time.In addition, it may compose better than Future, but IMHO, it doesn't
>>>> go far enough. Therefore, I use a FOSS library (with increasing popularity)
>>>> that allows one to compose asynchronous data streams with much less worry
>>>> on concurrency and continuation than CF. I use it in 50k loc projects, but
>>>> I read/heard a well known company is switching almost all of its software
>>>> stack to the reactive/asynchronous programming idiom this library enables.
>>>> Excellent material is available (including videos and conference talk) on
>>>> this subject (and the library).
>>>> 2014.11.20. 2:45 ezt ?rta ("Yu Lin" <yu.lin.86 at gmail.com>):
>>>>
>>>>> Hi all,
>>>>>
>>>>> I'm a Ph.D. student at University of Illinois. I'm doing research on
>>>>> the uses of Java concurrent/asynchronous tasks. A new construct introduced
>>>>> in Java 8, CompletableFuture, seems to be a very nice and easy to use
>>>>> concurrent task.
>>>>>
>>>>> I'm looking for projects that use CompletableFuture. But since it's
>>>>> very new, I didn't find any real-world projects use it (only found many toy
>>>>> examples/demos).
>>>>>
>>>>> Does anyone know any projects that use it?
>>>>>
>>>>> Thanks,
>>>>> Yu Lin
>>>>>
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at cs.oswego.edu
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>>
>>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141127/1756fd0a/attachment.html>

From peter.levart at gmail.com  Thu Nov 27 02:48:33 2014
From: peter.levart at gmail.com (Peter Levart)
Date: Thu, 27 Nov 2014 08:48:33 +0100
Subject: [concurrency-interest] RFR: 8065804: JEP 171:
 Clarifications/corrections for fence intrinsics
In-Reply-To: <NFBBKALFDCPFIDBNKAPCIEDDKLAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCIEDDKLAA.davidcholmes@aapt.net.au>
Message-ID: <5476D751.5040407@gmail.com>

On 11/27/2014 04:00 AM, David Holmes wrote:
> Can I make an observation about acquire() and release() - to me they are
> meaningless when considered in isolation. Given their definitions they allow
> anything to move into a region bounded by acquire() and release(), then you
> can effectively move the whole program into the region and thus the
> acquire() and release() do not constrain any reorderings.
>   acquire() and
> release() only make sense when their own movement is constrained with
> respect to something else - such as lock acquisition/release, or when
> combined with specific load/store actions.

...or another acquire/release region?

Regards, Peter

>
> David
>
> Martin Buchholz writes:
>> On Tue, Nov 25, 2014 at 6:04 AM, Paul Sandoz
>> <paul.sandoz at oracle.com> wrote:
>>> Hi Martin,
>>>
>>> Thanks for looking into this.
>>>
>>> 1141      * Currently hotspot's implementation of a Java
>> language-level volatile
>>> 1142      * store has the same effect as a storeFence followed
>> by a relaxed store,
>>> 1143      * although that may be a little stronger than needed.
>>>
>>> IIUC to emulate hotpot's volatile store you will need to say
>> that a fullFence immediately follows the relaxed store.
>>
>> Right - I've been groking that.
>>
>>> The bit that always confuses me about release and acquire is
>> ordering is restricted to one direction, as talked about in
>> orderAccess.hpp [1]. So for a release, accesses prior to the
>> release cannot move below it, but accesses succeeding the release
>> can move above it. And that seems to apply to Unsafe.storeFence
>> [2] (acting like a monitor exit). Is that contrary to C++ release
>> fences where ordering is restricted both to prior and succeeding
>> accesses? [3]
>>> So what about the following?
>>>
>>>    a = r1; // Cannot move below the fence
>>>    Unsafe.storeFence();
>>>    b = r2; // Can move above the fence?
>> I think the hotspot docs need to be more precise about when they're
>> talking about movement of stores and when about loads.
>>
>>> // release.  I.e., subsequent memory accesses may float above the
>>> // release, but prior ones may not float below it.
>> As I've said elsewhere, the above makes no sense without restricting
>> the type of access.
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From radhakrishnan.mohan at gmail.com  Thu Nov 27 03:19:18 2014
From: radhakrishnan.mohan at gmail.com (Mohan Radhakrishnan)
Date: Thu, 27 Nov 2014 13:49:18 +0530
Subject: [concurrency-interest] CompletableFuture in Java 8
In-Reply-To: <CAAL-3Pbd55dg8CpYXhiR3kTENgnBLxmPDZ9agAv4-d1F4LXmeQ@mail.gmail.com>
References: <CAAL-3PanDAubT2RM-DePuUv_LmSX2=OsDcy82Q+ETBaRuvEwEw@mail.gmail.com>
	<CAAWwtm-evqwXxNsoMLsnm5PNU-n+=O1iR_ZcptfaJ60JHg9jbg@mail.gmail.com>
	<CAAL-3PaZDBmjQ7i43+VUeW+OGh+6VAcA3rp9mzmetJar2k_aZw@mail.gmail.com>
	<CAAL-3PYJ90uZ_Kmz1-QjY43nRUCAMxxXJmE9KiVKCddCeMnyfA@mail.gmail.com>
	<CAOoXFP9hRneBagR4hHggCQoyxsJp8tFQNbdmMvoY5dS+4LZH4g@mail.gmail.com>
	<CAAL-3Pbd55dg8CpYXhiR3kTENgnBLxmPDZ9agAv4-d1F4LXmeQ@mail.gmail.com>
Message-ID: <CAOoXFP93oDxN-iAQ8U6VP_EGspKFZBVQ=sPN3t8ZTUaZSwae+Q@mail.gmail.com>

I had a question like that and I was asked to look at
http://www.infoq.com/presentations/java-streams-scala-parallel-collections
It may help.

Thanks,
Mohan

On Thu, Nov 27, 2014 at 12:54 PM, Yu Lin <yu.lin.86 at gmail.com> wrote:

> Yes, I'm looking at RxJava and Guava ListenableFuture. Do you guys know
> what's the major differences among RxJava, ListenableFuture and
> CompletableFuture. What's the advantage of each?
>
> It seems to me all three support callback listeners and async task
> chaining. But ListenableFuture doesn't support lambda while RxJava support
> more operations for Observable. How to choose which construct to use when
> doing asynchronous programming?
>
> Thanks,
> Yu
>
> On Thu, Nov 27, 2014 at 12:46 AM, Mohan Radhakrishnan <
> radhakrishnan.mohan at gmail.com> wrote:
>
>> Don't you think Netflix should be using it ?
>> http://netflix.github.io/#repo
>>
>> Thanks,
>> Mohan
>>
>> On Thu, Nov 27, 2014 at 1:12 AM, Yu Lin <yu.lin.86 at gmail.com> wrote:
>>
>>> I guess David does mean RxJava. There're some projects use RxJava.
>>> David, in which project do you use it? May I take a look if it's
>>> open-source?
>>>
>>> On Fri, Nov 21, 2014 at 4:29 PM, Yu Lin <yu.lin.86 at gmail.com> wrote:
>>>
>>>> OK, thanks for all your detailed analysis. I guess the slow adaption of
>>>> a new version is why I can't find the use of it. I myself is using Java 6?
>>>> Other similar libraries you mentioned seem to be interesting.
>>>>
>>>> On Fri, Nov 21, 2014 at 1:35 PM, D?vid Karnok <akarnokd at gmail.com>
>>>> wrote:
>>>>
>>>>> Hi. CompletableFuture is great, but as a Ph.D R&D student myself, I
>>>>> see a few shortcomings:- Slow adaptation of Java 8,- the feature lag in a
>>>>> well known mobile platform and- the alternatives available as library for
>>>>> some time.In addition, it may compose better than Future, but IMHO, it
>>>>> doesn't go far enough. Therefore, I use a FOSS library (with increasing
>>>>> popularity) that allows one to compose asynchronous data streams with much
>>>>> less worry on concurrency and continuation than CF. I use it in 50k loc
>>>>> projects, but I read/heard a well known company is switching almost all of
>>>>> its software stack to the reactive/asynchronous programming idiom this
>>>>> library enables. Excellent material is available (including videos and
>>>>> conference talk) on this subject (and the library).
>>>>> 2014.11.20. 2:45 ezt ?rta ("Yu Lin" <yu.lin.86 at gmail.com>):
>>>>>
>>>>>> Hi all,
>>>>>>
>>>>>> I'm a Ph.D. student at University of Illinois. I'm doing research on
>>>>>> the uses of Java concurrent/asynchronous tasks. A new construct introduced
>>>>>> in Java 8, CompletableFuture, seems to be a very nice and easy to use
>>>>>> concurrent task.
>>>>>>
>>>>>> I'm looking for projects that use CompletableFuture. But since it's
>>>>>> very new, I didn't find any real-world projects use it (only found many toy
>>>>>> examples/demos).
>>>>>>
>>>>>> Does anyone know any projects that use it?
>>>>>>
>>>>>> Thanks,
>>>>>> Yu Lin
>>>>>>
>>>>>> _______________________________________________
>>>>>> Concurrency-interest mailing list
>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>
>>>>>>
>>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141127/e985e85f/attachment-0001.html>

From joe.bowbeer at gmail.com  Thu Nov 27 03:31:27 2014
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Thu, 27 Nov 2014 00:31:27 -0800
Subject: [concurrency-interest] CompletableFuture in Java 8
In-Reply-To: <CAAL-3Pbd55dg8CpYXhiR3kTENgnBLxmPDZ9agAv4-d1F4LXmeQ@mail.gmail.com>
References: <CAAL-3PanDAubT2RM-DePuUv_LmSX2=OsDcy82Q+ETBaRuvEwEw@mail.gmail.com>
	<CAAWwtm-evqwXxNsoMLsnm5PNU-n+=O1iR_ZcptfaJ60JHg9jbg@mail.gmail.com>
	<CAAL-3PaZDBmjQ7i43+VUeW+OGh+6VAcA3rp9mzmetJar2k_aZw@mail.gmail.com>
	<CAAL-3PYJ90uZ_Kmz1-QjY43nRUCAMxxXJmE9KiVKCddCeMnyfA@mail.gmail.com>
	<CAOoXFP9hRneBagR4hHggCQoyxsJp8tFQNbdmMvoY5dS+4LZH4g@mail.gmail.com>
	<CAAL-3Pbd55dg8CpYXhiR3kTENgnBLxmPDZ9agAv4-d1F4LXmeQ@mail.gmail.com>
Message-ID: <CAHzJPErORCFNLgi1SNW8+KnxR3ZAXAow_DMJTnJdP6w6qOrZqA@mail.gmail.com>

ListenableFuture was implemented before Java8 and so predated
CompletableFuture.  I think they're very similar in terms of features. Both
enable a "promise" style of programming that I think can look similar to
the "reactive" programming enabled by Rx.

However, Rx Observables can emit multiple values, which is different from
futures/promises, which can be completed/satisfied only once.

On Wed, Nov 26, 2014 at 11:24 PM, Yu Lin <yu.lin.86 at gmail.com> wrote:

> Yes, I'm looking at RxJava and Guava ListenableFuture. Do you guys know
> what's the major differences among RxJava, ListenableFuture and
> CompletableFuture. What's the advantage of each?
>
> It seems to me all three support callback listeners and async task
> chaining. But ListenableFuture doesn't support lambda while RxJava support
> more operations for Observable. How to choose which construct to use when
> doing asynchronous programming?
>
> Thanks,
> Yu
>
> On Thu, Nov 27, 2014 at 12:46 AM, Mohan Radhakrishnan <
> radhakrishnan.mohan at gmail.com> wrote:
>
>> Don't you think Netflix should be using it ?
>> http://netflix.github.io/#repo
>>
>> Thanks,
>> Mohan
>>
>> On Thu, Nov 27, 2014 at 1:12 AM, Yu Lin <yu.lin.86 at gmail.com> wrote:
>>
>>> I guess David does mean RxJava. There're some projects use RxJava.
>>> David, in which project do you use it? May I take a look if it's
>>> open-source?
>>>
>>> On Fri, Nov 21, 2014 at 4:29 PM, Yu Lin <yu.lin.86 at gmail.com> wrote:
>>>
>>>> OK, thanks for all your detailed analysis. I guess the slow adaption of
>>>> a new version is why I can't find the use of it. I myself is using Java 6?
>>>> Other similar libraries you mentioned seem to be interesting.
>>>>
>>>> On Fri, Nov 21, 2014 at 1:35 PM, D?vid Karnok <akarnokd at gmail.com>
>>>> wrote:
>>>>
>>>>> Hi. CompletableFuture is great, but as a Ph.D R&D student myself, I
>>>>> see a few shortcomings:- Slow adaptation of Java 8,- the feature lag in a
>>>>> well known mobile platform and- the alternatives available as library for
>>>>> some time.In addition, it may compose better than Future, but IMHO, it
>>>>> doesn't go far enough. Therefore, I use a FOSS library (with increasing
>>>>> popularity) that allows one to compose asynchronous data streams with much
>>>>> less worry on concurrency and continuation than CF. I use it in 50k loc
>>>>> projects, but I read/heard a well known company is switching almost all of
>>>>> its software stack to the reactive/asynchronous programming idiom this
>>>>> library enables. Excellent material is available (including videos and
>>>>> conference talk) on this subject (and the library).
>>>>> 2014.11.20. 2:45 ezt ?rta ("Yu Lin" <yu.lin.86 at gmail.com>):
>>>>>
>>>>>> Hi all,
>>>>>>
>>>>>> I'm a Ph.D. student at University of Illinois. I'm doing research on
>>>>>> the uses of Java concurrent/asynchronous tasks. A new construct introduced
>>>>>> in Java 8, CompletableFuture, seems to be a very nice and easy to use
>>>>>> concurrent task.
>>>>>>
>>>>>> I'm looking for projects that use CompletableFuture. But since it's
>>>>>> very new, I didn't find any real-world projects use it (only found many toy
>>>>>> examples/demos).
>>>>>>
>>>>>> Does anyone know any projects that use it?
>>>>>>
>>>>>> Thanks,
>>>>>> Yu Lin
>>>>>>
>>>>>> _______________________________________________
>>>>>> Concurrency-interest mailing list
>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>
>>>>>>
>>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141127/1d762dec/attachment.html>

From radhakrishnan.mohan at gmail.com  Thu Nov 27 06:37:38 2014
From: radhakrishnan.mohan at gmail.com (Mohan Radhakrishnan)
Date: Thu, 27 Nov 2014 17:07:38 +0530
Subject: [concurrency-interest] CompletableFuture in Java 8
In-Reply-To: <CAHzJPErORCFNLgi1SNW8+KnxR3ZAXAow_DMJTnJdP6w6qOrZqA@mail.gmail.com>
References: <CAAL-3PanDAubT2RM-DePuUv_LmSX2=OsDcy82Q+ETBaRuvEwEw@mail.gmail.com>
	<CAAWwtm-evqwXxNsoMLsnm5PNU-n+=O1iR_ZcptfaJ60JHg9jbg@mail.gmail.com>
	<CAAL-3PaZDBmjQ7i43+VUeW+OGh+6VAcA3rp9mzmetJar2k_aZw@mail.gmail.com>
	<CAAL-3PYJ90uZ_Kmz1-QjY43nRUCAMxxXJmE9KiVKCddCeMnyfA@mail.gmail.com>
	<CAOoXFP9hRneBagR4hHggCQoyxsJp8tFQNbdmMvoY5dS+4LZH4g@mail.gmail.com>
	<CAAL-3Pbd55dg8CpYXhiR3kTENgnBLxmPDZ9agAv4-d1F4LXmeQ@mail.gmail.com>
	<CAHzJPErORCFNLgi1SNW8+KnxR3ZAXAow_DMJTnJdP6w6qOrZqA@mail.gmail.com>
Message-ID: <CAOoXFP_B8ESrHGmQnKwcpJy2wP-HRdHKeBoXnogDS20_ukvXGg@mail.gmail.com>

The method used to emit multiple values from a RxJava Observable is
something  like this. It wasn't very difficult even for a layman like me.

    public Observable<Object> getMetaSpaceCapacity() {



        Observable<Long> init = null;

        Observable<Long> committed = null;

        Observable<Long> max = null;

        Observable<Long> used = null;



            Future<Optional<MemoryPoolMXBean>> memoryPool =

                es.submit(  new Callable<Optional<MemoryPoolMXBean>>(){

                @Override

                public Optional<MemoryPoolMXBean> call()

                {

                return msp.getMemoryPool();//Don't think repeated calls are
needed

                }

                } );



              Optional<MemoryPoolMXBean> mpmxbOpt = null;

              try {

                 mpmxbOpt = memoryPool.get();

              } catch (InterruptedException | ExecutionException e) {

                l.debug(getStackTrace(e));

             }finally{

               es.shutdown();

            }

              if(mpmxbOpt.isPresent()){

               MemoryPoolMXBean mpmx = mpmxbOpt.get();

               MemoryUsage mu = mpmx.getUsage();

               init = Observable.from(mu.getInit());

               committed = Observable.from(mu.getCommitted());

               used = Observable.from(mu.getUsed());

               max = Observable.from(mu.getMax());

           }


    return Observable.merge(Observable.zip(init, committed, used, max,(i,c,u
,m) -> {

        List<Long> results = Arrays.asList(i,c,u,m);

        return Observable.create((Subscriber<? super String> subscriber) ->
{

            for(Long lv : results){

             l.debug(String.valueOf(lv.longValue()));

            subscriber.onNext(String.valueOf(lv.longValue()));

            }

            subscriber.onCompleted();

        });

    }));


    }


Mohan

On Thu, Nov 27, 2014 at 2:01 PM, Joe Bowbeer <joe.bowbeer at gmail.com> wrote:

> ListenableFuture was implemented before Java8 and so predated
> CompletableFuture.  I think they're very similar in terms of features. Both
> enable a "promise" style of programming that I think can look similar to
> the "reactive" programming enabled by Rx.
>
> However, Rx Observables can emit multiple values, which is different from
> futures/promises, which can be completed/satisfied only once.
>
> On Wed, Nov 26, 2014 at 11:24 PM, Yu Lin <yu.lin.86 at gmail.com> wrote:
>
>> Yes, I'm looking at RxJava and Guava ListenableFuture. Do you guys know
>> what's the major differences among RxJava, ListenableFuture and
>> CompletableFuture. What's the advantage of each?
>>
>> It seems to me all three support callback listeners and async task
>> chaining. But ListenableFuture doesn't support lambda while RxJava support
>> more operations for Observable. How to choose which construct to use when
>> doing asynchronous programming?
>>
>> Thanks,
>> Yu
>>
>> On Thu, Nov 27, 2014 at 12:46 AM, Mohan Radhakrishnan <
>> radhakrishnan.mohan at gmail.com> wrote:
>>
>>> Don't you think Netflix should be using it ?
>>> http://netflix.github.io/#repo
>>>
>>> Thanks,
>>> Mohan
>>>
>>> On Thu, Nov 27, 2014 at 1:12 AM, Yu Lin <yu.lin.86 at gmail.com> wrote:
>>>
>>>> I guess David does mean RxJava. There're some projects use RxJava.
>>>> David, in which project do you use it? May I take a look if it's
>>>> open-source?
>>>>
>>>> On Fri, Nov 21, 2014 at 4:29 PM, Yu Lin <yu.lin.86 at gmail.com> wrote:
>>>>
>>>>> OK, thanks for all your detailed analysis. I guess the slow adaption
>>>>> of a new version is why I can't find the use of it. I myself is using Java
>>>>> 6? Other similar libraries you mentioned seem to be interesting.
>>>>>
>>>>> On Fri, Nov 21, 2014 at 1:35 PM, D?vid Karnok <akarnokd at gmail.com>
>>>>> wrote:
>>>>>
>>>>>> Hi. CompletableFuture is great, but as a Ph.D R&D student myself, I
>>>>>> see a few shortcomings:- Slow adaptation of Java 8,- the feature lag in a
>>>>>> well known mobile platform and- the alternatives available as library for
>>>>>> some time.In addition, it may compose better than Future, but IMHO, it
>>>>>> doesn't go far enough. Therefore, I use a FOSS library (with increasing
>>>>>> popularity) that allows one to compose asynchronous data streams with much
>>>>>> less worry on concurrency and continuation than CF. I use it in 50k loc
>>>>>> projects, but I read/heard a well known company is switching almost all of
>>>>>> its software stack to the reactive/asynchronous programming idiom this
>>>>>> library enables. Excellent material is available (including videos and
>>>>>> conference talk) on this subject (and the library).
>>>>>> 2014.11.20. 2:45 ezt ?rta ("Yu Lin" <yu.lin.86 at gmail.com>):
>>>>>>
>>>>>>> Hi all,
>>>>>>>
>>>>>>> I'm a Ph.D. student at University of Illinois. I'm doing research on
>>>>>>> the uses of Java concurrent/asynchronous tasks. A new construct introduced
>>>>>>> in Java 8, CompletableFuture, seems to be a very nice and easy to use
>>>>>>> concurrent task.
>>>>>>>
>>>>>>> I'm looking for projects that use CompletableFuture. But since it's
>>>>>>> very new, I didn't find any real-world projects use it (only found many toy
>>>>>>> examples/demos).
>>>>>>>
>>>>>>> Does anyone know any projects that use it?
>>>>>>>
>>>>>>> Thanks,
>>>>>>> Yu Lin
>>>>>>>
>>>>>>> _______________________________________________
>>>>>>> Concurrency-interest mailing list
>>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>
>>>>>>>
>>>>>
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141127/fba50a2e/attachment-0001.html>

From paul.sandoz at oracle.com  Thu Nov 27 06:54:47 2014
From: paul.sandoz at oracle.com (Paul Sandoz)
Date: Thu, 27 Nov 2014 12:54:47 +0100
Subject: [concurrency-interest] Mixing async and blocking components
In-Reply-To: <547624C7.50400@javaspecialists.eu>
References: <54761F3E.9020409@cs.oswego.edu>
	<547624C7.50400@javaspecialists.eu>
Message-ID: <41B12DD6-4D55-4EC7-8D36-39DBE6B25B45@oracle.com>

On Nov 26, 2014, at 8:06 PM, Dr Heinz M. Kabutz <heinz at javaspecialists.eu> wrote:
> 
> There is a limit within parallel streams, but probably artificial, due to the 4x multiplier for the number of leaves based on the common pool parallelism.

This is used to control the number of splits and in turn the number of tasks produced to balance out the work while not creating too many tasks such that the benefits of parallelism would be lost.

If the size before any splitting, root_size, is known (or it's estimate is known) then splitting will stop when a sub-spliterator reports a size is less than the root_size / (4 * P). For something like an ArrayList this results in a balanced tree of tasks with P * 4 leaf tasks. For something like a LinkedList a right-heavy tree [*] will be produced with potentially many more tasks (since parallelism is "extracted" by peeling of prefixes of elements into arrays of increasing size).

As you have observed that splitting heuristic is hardcoded (as is the arithmetic progression for peeling prefixes of elements) and we may need policies or strategies to better control such behaviour, by for example, passing in a policy instance to the stream.parallel() call. Also there might be a policy indicating that I/O and blocking operations occur in a stream source and/or in operations.

The challenge is defining such polices so they are easy to understand, don't unduly bias to particular system configurations, and have a small fixed cost.

Paul.

[*] If you want to visual such trees download code i used at my recent JavaOne and Devoxx talks:

  http://cr.openjdk.java.net/~psandoz/stream-javaone-14.zip

(If you have graphviz and chrome installed on a Mac it should "just" work.)
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 841 bytes
Desc: Message signed with OpenPGP using GPGMail
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141127/e20d2467/attachment.bin>

From dl at cs.oswego.edu  Thu Nov 27 07:24:21 2014
From: dl at cs.oswego.edu (Doug Lea)
Date: Thu, 27 Nov 2014 07:24:21 -0500
Subject: [concurrency-interest] RFR: 8065804: JEP
 171:Clarifications/corrections for fence intrinsics
In-Reply-To: <NFBBKALFDCPFIDBNKAPCAEDDKLAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCAEDDKLAA.davidcholmes@aapt.net.au>
Message-ID: <547717F5.4020907@cs.oswego.edu>

On 11/26/2014 09:56 PM, David Holmes wrote:
> Martin Buchholz writes:
>> On Wed, Nov 26, 2014 at 5:08 PM, David Holmes
>> <david.holmes at oracle.com> wrote:
>>> Please explain why you have changed the defined semantics for
>> storeFence.
>>> You have completely reversed the direction of the barrier.
>>
>> Yes.  I believe the current spec of storeFence was a copy-paste typo,
>> and it seems others feel likewise.
>
> Can whomever wrote that original spec please confirm that.
>

The translations of loadFence == [LoadLoad|LoadStore]
and storeFence == [StoreStore|LoadStore] into prose got mangled
at some point. (Probably by me; sorry if so!)

-Doug



From viktor.klang at gmail.com  Thu Nov 27 08:59:19 2014
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Thu, 27 Nov 2014 14:59:19 +0100
Subject: [concurrency-interest] ManagedBlockers Not Giving Full
 Parallelism in FJPool
In-Reply-To: <547620C2.7020309@javaspecialists.eu>
References: <547620C2.7020309@javaspecialists.eu>
Message-ID: <CANPzfU8tCpgWdUGK171QSESvMVww4U68Jj1KV8PGqHe60YcaCA@mail.gmail.com>

Hi Heinz,

when we worked on `scala.concurrent` we added `BlockContext` as a general
mechanism to deal with demarcation of, and dealing with, blocking
operations.

BlockContext:
https://github.com/scala/scala/blob/2.11.x/src/library/scala/concurrent/BlockContext.scala

Usage:

    import scala.concurrent.blocking

    val line = blocking { System.in.readLine() } // Also serves as
excellent documentation where in your code you are being naughty

For FJP it uses FJP.ManagedBlocker:
https://github.com/scala/scala/blob/2.11.x/src/library/scala/concurrent/impl/ExecutionContextImpl.scala#L42

Here's my current work on enabling support in TPE:
https://github.com/akka/akka/blob/wip-managed-blocking-for-tpe-%E2%88%9A/akka-actor/src/main/scala/akka/dispatch/ThreadPoolBuilder.scala#L59

The idea I have used is to be able to specify how many threads must -not-
be blocked at a given point in time, and we can as such, instead of
executing the blocking code block, throw a `RejectedExecutionException`.

This means that non-blocking (and of course un-demarcated blocking)
operations are not penalized, and the blocking is still managed (as in
preventing stalls).


On Wed, Nov 26, 2014 at 7:49 PM, Dr Heinz M. Kabutz <
heinz at javaspecialists.eu> wrote:

> Hi,
>
> thanks to some tips by Paul Sandoz at Devoxx, I started looking at
> ManagedBlockers as a way to allow blocking operations to "play nice" with
> parallel streams.  One of the things I wrote yesterday was a
> "ManagedReentrantLock".  It is a drop-in replacement for the normal
> ReentrantLock, but if you happen to either block on a lock() operation or
> suspend a thread with await() on the condition, it will allow the
> underlying ForkJoinPool to construct new threads in order to maintain the
> number of active threads.  (see end of my email for an example of the
> ManagedReentrantLock - soon to be published in The Java Specialists'
> Newsletter).  Of course, one could also make AQS "play nice" with FJP.
>
> Anyway, the code seems to work reasonably well, and I even wrote a class
> ManagedBlockers to inject instances of the ReentrantLock into existing
> constructs like ArrayBlockingQueue, LinkedBlockingQueue and
> PriorityBlockingQueue (Again, see end of this email for the code).
>
> Whilst writing some sample code to demonstrate how vastly superior
> (*cough* *cough*) this was to using a normal ReentrantLock within a
> parallel stream, I came across some strange observations with stream
> behaviour which I cannot explain.  I'd love to hear if any of you can point
> me to the right place.  It seems as if when I have threads that have been
> suspended on a ManagedBlocker, that a maximum of one thread is accessible
> from the FJP for work, in addition to the invoking thread.  We thus get a
> speedup of only 2x over the serial version, whereas on my 1-4-2 laptop I
> would've expected it to run as fast as if there were no suspended threads.
>
> I've written a small example to demonstrate the problem.  My
> ManagedJamupDemo searches for the max value of a large array.  It starts by
> taking a measurement when we have no manage blocked threads in the FJP at
> all in order to get a baseline speed.  It then in a separate thread does a
> parallel stream in which we call a blocking phaser method.  Since Phaser
> "plays nice" with FJP, it ends up with more threads in the FJP than
> parallelism.  Typical output on my 1-4-2 machine is something like:
>
> baseline parallel = 196
> baseline serial = 835
> new parallel = 444
> new serial = 831
>
> My 2-4-1 server is a bit slower, but we can observe there too that we
> don't speed up as much as when we don't have "managed blocked" threads in
> the FJP:
>
> baseline parallel = 369
> baseline serial = 1345
> new parallel = 678
> new serial = 1331
>
> (I am aware that this is not a proper benchmark and that JMH is much
> cooler, ... ;-)  Also that we are going to end up bottlenecking on memory
> access.  However, I first observed this interesting phenomenon of having
> only two active threads when using Thread.sleep instead of memory access.)
>
> My initial gut feeling is that this is a bug in FJP and that we should see
> "new parallel" have the same speed as "baseline parallel".  I know that the
> maximum amount of parallelization within a parallel stream is currently set
> to 4x the common pool parallelism in the AbstractTask class:
>
>    static final int LEAF_TARGET = ForkJoinPool.getCommonPoolParallelism()
> << 2;
>
> I'd appreciate if some of you could maybe have a look at this and help
> with the gory details of FJP implementation :-)
>
> import java.util.concurrent.*;
> import java.util.function.*;
> import java.util.stream.*;
>
> public class ManagedJamupDemo {
>  public static void main(String... args) throws InterruptedException {
>    int[] large_array = new int[1_000_000_000];
>    ThreadLocalRandom random = ThreadLocalRandom.current();
>    for (int i = 0; i < large_array.length; i++) {
>      large_array[i] = random.nextInt();
>    }
>
>    for (int i = 0; i < 10; i++) {
>      test(large_array);
>    }
>  }
>
>  private static void test(int[] large_array) throws InterruptedException {
>    long time;
>
>    time = max(1, () -> IntStream.of(large_array).parallel());
>    System.out.println("baseline parallel = " + time);
>
>    time = max(1, () -> IntStream.of(large_array));
>    System.out.println("baseline serial = " + time);
>
>    int processors = Runtime.getRuntime().availableProcessors();
>    Phaser phaser = new Phaser(processors * 2 + 1);
>
>    Thread jamThread = makeJamThread(phaser);
>    Thread.sleep(100);
>
>    time = max(1, () -> IntStream.of(large_array).parallel());
>    System.out.println("new parallel = " + time);
>
>    time = max(1, () -> IntStream.of(large_array));
>    System.out.println("new serial = " + time);
>
>    phaser.arriveAndAwaitAdvance();
>    jamThread.join();
>  }
>
>  private static Thread makeJamThread(Phaser phaser) {
>    Thread jamup = new Thread(() -> {
>      int par = Runtime.getRuntime().availableProcessors() * 2;
>      IntStream.range(0, par).parallel().forEach(
>          i -> {
>            phaser.arriveAndAwaitAdvance();
>          }
>      );
>    });
>    jamup.start();
>    return jamup;
>  }
>
>  private static long max(int repeats, Supplier<IntStream> supplier) {
>    for (int i = 0; i < repeats; i++) {
>      supplier.get().max();
>    }
>    long time = System.currentTimeMillis();
>    for (int i = 0; i < repeats; i++) {
>      supplier.get().max();
>    }
>    time = System.currentTimeMillis() - time;
>    return time;
>  }
> }
>
> Regards
>
> Heinz
> --
> Dr Heinz M. Kabutz (PhD CompSci)
> Author of "The Java(tm) Specialists' Newsletter"
> Sun/Oracle Java Champion since 2005
> JavaOne Rock Star Speaker 2012
> http://www.javaspecialists.eu
> Tel: +30 69 75 595 262
> Skype: kabutz
>
>
>
> package eu.javaspecialists.concurrent.locks;
>
> import eu.javaspecialists.concurrent.util.*;
>
> import java.util.*;
> import java.util.concurrent.*;
> import java.util.concurrent.locks.*;
>
> /**
> * The ManagedReentrantLock is a lock implementation that is
> * compatible with the Fork/Join framework, and therefore also
> * with the Java 8 parallel streams.  Instead of just blocking
> * when the lock is held by another thread, and thereby removing
> * one of the active threads from the Fork/Join pool, we instead
> * use a ManagedBlocker to manage it.
> * <p>
> * The ManagedReentrantLock subclasses ReentrantLock, which means
> * we can use it as a drop-in replacement.  See also the
> * ManagedLocks facade.
> *
> * @author Heinz Kabutz
> * @see ManagedLocks
> */
> public class ManagedReentrantLock extends ReentrantLock {
>  public ManagedReentrantLock() {
>  }
>
>  public ManagedReentrantLock(boolean fair) {
>    super(fair);
>  }
>
>  // we should always try to achieve our goal within the
>  // isReleasable() method instead of block().  This avoids
>  // trying to compensate the loss of a thread by creating
>  // a new one.
>  private abstract class AbstractLockAction
>      implements ForkJoinPool.ManagedBlocker {
>    boolean hasLock = false;
>
>    public final boolean isReleasable() {
>      return hasLock || (hasLock = tryLock());
>    }
>  }
>
>  private class DoLock extends AbstractLockAction {
>    public boolean block() {
>      if (isReleasable())
>        return true;
>      if (!hasLock)
>        ManagedReentrantLock.super.lock();
>      return true;
>    }
>  }
>
>  private class DoLockInterruptibly extends AbstractLockAction {
>    public boolean block() throws InterruptedException {
>      if (isReleasable())
>        return true;
>      if (!hasLock)
>        ManagedReentrantLock.super.lockInterruptibly();
>      return true;
>    }
>  }
>
>  private class TryLocker extends AbstractLockAction {
>    private final long time;
>    private final TimeUnit unit;
>
>    private TryLocker(long time, TimeUnit unit) {
>      this.time = time;
>      this.unit = unit;
>    }
>
>    public boolean block() throws InterruptedException {
>      if (isReleasable())
>        return true;
>      if (!hasLock)
>        ManagedReentrantLock.super.tryLock(time, unit);
>      return true;
>    }
>  }
>
>
>  public void lock() {
>    DoLock locker = new DoLock(); // we want to create this
>    // before passing it into the lambda, to prevent it from
>    // being created again if the thread is interrupted for some
>    // reason
>    Interruptions.saveForLater(
>        () -> ForkJoinPool.managedBlock(locker));
>  }
>
>  public void lockInterruptibly() throws InterruptedException {
>    ForkJoinPool.managedBlock(new DoLockInterruptibly());
>  }
>
>  public boolean tryLock(long time, TimeUnit unit)
>      throws InterruptedException {
>    ForkJoinPool.managedBlock(new TryLocker(time, unit));
>    return super.isHeldByCurrentThread();
>  }
>
>  public Condition newCondition() {
>    return new ManagedCondition(super.newCondition());
>  }
>
>  private class ManagedCondition implements Condition {
>    private final Condition condition;
>
>    private ManagedCondition(Condition condition) {
>      this.condition = condition;
>    }
>
>    private class Await implements ForkJoinPool.ManagedBlocker {
>      public boolean block() throws InterruptedException {
>        condition.await();
>        return true;
>      }
>
>      public boolean isReleasable() {
>        return false;
>      }
>    }
>
>    public void await() throws InterruptedException {
>      managedBlock(() -> condition.await());
>    }
>
>    public void awaitUninterruptibly() {
>      Interruptions.saveForLater(
>          () -> managedBlock(
>              () -> condition.awaitUninterruptibly())
>      );
>    }
>
>    public long awaitNanos(long nanosTimeout)
>        throws InterruptedException {
>      long[] result = {nanosTimeout};
>      managedBlock(
>          () -> result[0] = condition.awaitNanos(nanosTimeout));
>      return result[0];
>    }
>
>    public boolean await(long time, TimeUnit unit)
>        throws InterruptedException {
>      boolean[] result = {false};
>      managedBlock(
>          () -> result[0] = condition.await(time, unit));
>      return result[0];
>    }
>
>    public boolean awaitUntil(Date deadline)
>        throws InterruptedException {
>      boolean[] result = {false};
>      managedBlock(
>          () -> result[0] = condition.awaitUntil(deadline));
>      return result[0];
>    }
>
>    public void signal() {
>      condition.signal();
>    }
>
>    public void signalAll() {
>      condition.signalAll();
>    }
>  }
>
>  private static void managedBlock(
>      AlwaysBlockingManagedBlocker blocker)
>      throws InterruptedException {
>    ForkJoinPool.managedBlock(blocker);
>  }
>
>  private static interface AlwaysBlockingManagedBlocker
>      extends ForkJoinPool.ManagedBlocker {
>    default boolean isReleasable() {
>      return false;
>    }
>
>    default boolean block() throws InterruptedException {
>      doBlock();
>      return true;
>    }
>
>    void doBlock() throws InterruptedException;
>  }
>
>  private Condition getRealCondition(Condition condition) {
>    if (!(condition instanceof ManagedCondition))
>      throw new IllegalArgumentException("not owner");
>    return ((ManagedCondition) condition).condition;
>  }
>
>  public boolean hasWaiters(Condition condition) {
>    return super.hasWaiters(getRealCondition(condition));
>  }
>
>  public int getWaitQueueLength(Condition condition) {
>    return super.getWaitQueueLength(getRealCondition(condition));
>  }
>
>  protected Collection<Thread> getWaitingThreads(Condition condition) {
>    return super.getWaitingThreads(getRealCondition(condition));
>  }
> }
>
>
> package eu.javaspecialists.concurrent.util;
>
> import eu.javaspecialists.concurrent.locks.*;
>
> import java.lang.reflect.*;
> import java.util.concurrent.*;
> import java.util.concurrent.locks.*;
>
> public class ManagedBlockers {
>  public static <E> ArrayBlockingQueue<E> makeManaged(
>      ArrayBlockingQueue<E> queue) {
>    Class<?> clazz = ArrayBlockingQueue.class;
>
>    try {
>      Field lockField = clazz.getDeclaredField("lock");
>      lockField.setAccessible(true);
>      ReentrantLock old = (ReentrantLock) lockField.get(queue);
>      boolean fair = old.isFair();
>      ReentrantLock lock = new ManagedReentrantLock(fair);
>      lockField.set(queue, lock);
>
>      replace(queue, clazz, "notEmpty", lock.newCondition());
>      replace(queue, clazz, "notFull", lock.newCondition());
>
>      return queue;
>    } catch (IllegalAccessException | NoSuchFieldException e) {
>      throw new IllegalStateException(e);
>    }
>  }
>
>  private static void replace(Object owner,
>                              Class<?> clazz, String fieldName,
>                              Object fieldValue)
>      throws NoSuchFieldException, IllegalAccessException {
>    Field field = clazz.getDeclaredField(fieldName);
>    field.setAccessible(true);
>    field.set(owner, fieldValue);
>  }
>
>  public static <E> LinkedBlockingQueue<E> makeManaged(
>      LinkedBlockingQueue<E> queue) {
>    Class<?> clazz = LinkedBlockingQueue.class;
>
>    ReentrantLock takeLock = new ManagedReentrantLock();
>    ReentrantLock putLock = new ManagedReentrantLock();
>
>    try {
>      replace(queue, clazz, "takeLock", takeLock);
>      replace(queue, clazz, "notEmpty", takeLock.newCondition());
>      replace(queue, clazz, "putLock", putLock);
>      replace(queue, clazz, "notFull", putLock.newCondition());
>
>      return queue;
>    } catch (IllegalAccessException | NoSuchFieldException e) {
>      throw new IllegalStateException(e);
>    }
>  }
>
>  public static <E> PriorityBlockingQueue<E> makeManaged(
>      PriorityBlockingQueue<E> queue) {
>    Class<?> clazz = PriorityBlockingQueue.class;
>
>    ReentrantLock lock = new ManagedReentrantLock();
>
>    try {
>      replace(queue, clazz, "lock", lock);
>      replace(queue, clazz, "notEmpty", lock.newCondition());
>
>      return queue;
>    } catch (IllegalAccessException | NoSuchFieldException e) {
>      throw new IllegalStateException(e);
>    }
>  }
> }
>
>
> package eu.javaspecialists.concurrent.util;
>
> public class Interruptions {
>  public static void saveForLater(Interruptible action) {
>    boolean interrupted = false;
>    while (true) {
>      try {
>        action.run();
>        if (interrupted) Thread.currentThread().interrupt();
>        return;
>      } catch (InterruptedException e) {
>        interrupted = true;
>      }
>    }
>  }
>
>  @FunctionalInterface
>  public static interface Interruptible {
>    public void run() throws InterruptedException;
>  }
> }
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



-- 
Cheers,
?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141127/664ff718/attachment-0001.html>

From viktor.klang at gmail.com  Thu Nov 27 09:04:40 2014
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Thu, 27 Nov 2014 15:04:40 +0100
Subject: [concurrency-interest] Mixing async and blocking components
In-Reply-To: <547624C7.50400@javaspecialists.eu>
References: <54761F3E.9020409@cs.oswego.edu>
	<547624C7.50400@javaspecialists.eu>
Message-ID: <CANPzfU_s22S_wWFeOL4VWkVHVrwvAqsix3faiJHdd+VM5PRjSg@mail.gmail.com>

On Wed, Nov 26, 2014 at 8:06 PM, Dr Heinz M. Kabutz <
heinz at javaspecialists.eu> wrote:

>
> Doug Lea wrote:
>
>>
>> The increasing adoption of "structured" async components and
>> frameworks (CompletableFuture, Rx, reactive-streams, some parallel
>> uses of java.util.Stream) leads to increasing problems when users mix
>> them with blocking components and frameworks. When async tasks block,
>> then overall program liveness may require compensation with more
>> threads within some Executor.  But there is in general no limit to how
>> many threads are required to maintain liveness, so programs may
>> encounter OutOfMemoryErrors trying to create too many threads.
>> In common cases where upper bounds cannot be established, no one
>> likes to make a choice between possibly either locking up vs
>> blowing up programs.
>>
> Seems we were both writing our emails at the same time :-)
>
> There is a limit within parallel streams, but probably artificial, due to
> the 4x multiplier for the number of leaves based on the common pool
> parallelism.
>
>>
>> This problem can occur no matter what kind of Executor you use for
>> async. The only perfect solution is avoidance: not mixing async and
>> blocking components in this way. I expect that over time, async
>> replacements for some of the most troublesome IO and network-based
>> components will appear. It seems that there are (non-JDK) versions of
>> RMI, JAX-RPC, and adaptors for java.nio, in various stages of
>> development. (It would be great if there were something similar for
>> JDBC.) All of these efforts fall outside of java.util.concurrent
>> though.  Similarly for frameworks like Ron Pressler's Quasar that
>> automate the equivalents of adaptors.  The best we can do is provide
>> better coping mechanisms when these do not apply, and possibly add
>> some support to simplify construction of async replacements for
>> blocking components.
>>
>> We improved the situation for ForkJoinPool in an update a few months
>> ago (that ought to appear in a JDK8u release) by throwing more
>> manageable max-spare bound-check exceptions before resources are
>> typically actually exhausted. ForkJoinPool is also currently better
>> equipped to deal with this than other executors by supporting the
>> ManagedBlocker API to automate triggering of possible compensation.
>> But further improvements would be helpful.
>>
>> One option is to both extend the number of j.u.c synchronization
>> components (ReentrantLock, Semaphore, etc) that implement
>> ManagedBlocker, and enhance ThreadPoolExecutor to also include
>> (optional) compensation mechanics.
>>
> I would agree if we can do it without causing additional objects to be
> created during locking mechanisms.
>
>> Another (partially complementary) approach is to extend support for
>> variants of a classic simple resource control heuristic (I think
>> originating circa 1970 in AT&T ESS switches): Sometimes slow down
>> async task producers by arranging for them to help run tasks.  When
>> applied to network-based task producers, this may in turn cause
>> packets to be dropped, which is usually the best of all the bad
>> options. This isn't a cure-all: drops may lead to distributed liveness
>> failures and/or local stalls due to unsatisfiable task dependencies.
>> It also doesn't apply directly when helped-out tasks generate
>> exponentially more tasks. But in many practical cases, it can
>> vastly reduce the likelihood of failure.
>>
>> We already predefine the simplest form of this as the CallerRuns
>> RejectedExecutionHandler in TPE. Other variants can already be
>> programmed in both FJP and TPE, but we don't make it particularly
>> easy. For example, in TPE you can implement a handler that first
>> executes the oldest existing task, and then retries submission. And in
>> FJ, you can check local queue size to decide upon submission to first
>> help run a local or non-local task.  One of the reasons we didn't
>> predefine any of these is that there seemed to be too many
>> situation-specific plausible variants. But adding more of these may
>> help lead people into more successful choices.
>>
> Something which has always puzzled me is why the RejectedExecutionHandler
> is used both for an overloaded TPE and for a TPE that has been shut down.
> CallerRuns thus could get into a situation where the task is never executed
> in a TPE that has been shut down.  IMHO it would be better to do something
> like this:
>
>    public static class CallerRunsPolicy implements
> RejectedExecutionHandler {
>        public void rejectedExecution(Runnable r, ThreadPoolExecutor e) {
>            if (!e.isShutdown()) {
>                r.run();
>            } else {
>                throw new RejectedExecutionException("Task " +
> r.toString() +
>                                                 " rejected from " +
>                                                 e.toString());
>        }
>    }
>
> But that is probably a discussion for another thread :-)


Hehe, been there, done that! :)


class SaneRejectedExecutionHandler extends RejectedExecutionHandler {
  def rejectedExecution(runnable: Runnable, threadPoolExecutor:
ThreadPoolExecutor): Unit = {
    if (threadPoolExecutor.isShutdown) throw new
RejectedExecutionException("Shutdown")
    else runnable.run()
  }
}

Added this to `scala.concurrent` back in 2011:
https://github.com/akka/akka/commit/727c7de58df502ee8073ca10856196d0a338c5f1#diff-410abf8aac6a89a01b05db2cbcf1765cR221


>
>
>> Opinions about any of these, or other suggestions, would be welcome.
>>
>>
>> -Doug
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>  _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>



-- 
Cheers,
?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141127/2acb6b6d/attachment.html>

From jh at squareup.com  Thu Nov 27 09:54:33 2014
From: jh at squareup.com (Josh Humphries)
Date: Thu, 27 Nov 2014 09:54:33 -0500
Subject: [concurrency-interest] CompletableFuture in Java 8
In-Reply-To: <CAAL-3Pbd55dg8CpYXhiR3kTENgnBLxmPDZ9agAv4-d1F4LXmeQ@mail.gmail.com>
References: <CAAL-3PanDAubT2RM-DePuUv_LmSX2=OsDcy82Q+ETBaRuvEwEw@mail.gmail.com>
	<CAAWwtm-evqwXxNsoMLsnm5PNU-n+=O1iR_ZcptfaJ60JHg9jbg@mail.gmail.com>
	<CAAL-3PaZDBmjQ7i43+VUeW+OGh+6VAcA3rp9mzmetJar2k_aZw@mail.gmail.com>
	<CAAL-3PYJ90uZ_Kmz1-QjY43nRUCAMxxXJmE9KiVKCddCeMnyfA@mail.gmail.com>
	<CAOoXFP9hRneBagR4hHggCQoyxsJp8tFQNbdmMvoY5dS+4LZH4g@mail.gmail.com>
	<CAAL-3Pbd55dg8CpYXhiR3kTENgnBLxmPDZ9agAv4-d1F4LXmeQ@mail.gmail.com>
Message-ID: <CAHJZN-tDZFVWDW18tNksVOZQHWJdbYttQW+DNwZpmy9kiv3h0w@mail.gmail.com>

Why do you say ListenableFuture doesn't support lambdas? In Java8, any
single-abstract-method interface can be the target of a lambda expressions,
so Guava's Function is just as suited as the Function interface added in
Java8.

Perhaps you're only looking at the ListenableFuture interface -- which just
has addListener(Runnable,Executor). Take a look at Futures, which is full
of static helper methods. They include additional methods for working with
ListenableFutures and callbacks, including a way to "transform" a
ListenableFuture with a Function. I'm pretty sure this was done so the
ListenableFuture interface itself could be narrow without requiring
implementations provide the full breadth (since it's pre-Java8, there is no
luxury of default and static methods on the interface).

At Square, we use ListenableFutures heavily in framework code (to support
pre-Java8 apps). But we also have a lot of stuff moving to Java8. So I
wrote adapters from ListenableFuture -> CompletionStage and vice versa. All
of the building blocks needed are present in ListenableFuture. With these
adapters, CompletableFutures (more importantly, the CompletionStage API)
can still be used even though framework APIs expect and return
ListenableFutures.

But we actually have very little use of CompletableFutures. A lot of
projects prefer serial code (with blocking operations) because the code is
generally more readable (thus easier to maintain) and the performance is
adequate. Code that is async and parallel is often encapsulated behind a
library, which generally uses Java7 (and ListenableFutures) to support the
various projects that haven't yet made the jump to Java8.



----
*Josh Humphries*
Manager, Shared Systems  |  Platform Engineering
Atlanta, GA  |  678-400-4867
*Square* (www.squareup.com)

On Thu, Nov 27, 2014 at 2:24 AM, Yu Lin <yu.lin.86 at gmail.com> wrote:

> Yes, I'm looking at RxJava and Guava ListenableFuture. Do you guys know
> what's the major differences among RxJava, ListenableFuture and
> CompletableFuture. What's the advantage of each?
>
> It seems to me all three support callback listeners and async task
> chaining. But ListenableFuture doesn't support lambda while RxJava support
> more operations for Observable. How to choose which construct to use when
> doing asynchronous programming?
>
> Thanks,
> Yu
>
> On Thu, Nov 27, 2014 at 12:46 AM, Mohan Radhakrishnan <
> radhakrishnan.mohan at gmail.com> wrote:
>
>> Don't you think Netflix should be using it ?
>> http://netflix.github.io/#repo
>>
>> Thanks,
>> Mohan
>>
>> On Thu, Nov 27, 2014 at 1:12 AM, Yu Lin <yu.lin.86 at gmail.com> wrote:
>>
>>> I guess David does mean RxJava. There're some projects use RxJava.
>>> David, in which project do you use it? May I take a look if it's
>>> open-source?
>>>
>>> On Fri, Nov 21, 2014 at 4:29 PM, Yu Lin <yu.lin.86 at gmail.com> wrote:
>>>
>>>> OK, thanks for all your detailed analysis. I guess the slow adaption of
>>>> a new version is why I can't find the use of it. I myself is using Java 6?
>>>> Other similar libraries you mentioned seem to be interesting.
>>>>
>>>> On Fri, Nov 21, 2014 at 1:35 PM, D?vid Karnok <akarnokd at gmail.com>
>>>> wrote:
>>>>
>>>>> Hi. CompletableFuture is great, but as a Ph.D R&D student myself, I
>>>>> see a few shortcomings:- Slow adaptation of Java 8,- the feature lag in a
>>>>> well known mobile platform and- the alternatives available as library for
>>>>> some time.In addition, it may compose better than Future, but IMHO, it
>>>>> doesn't go far enough. Therefore, I use a FOSS library (with increasing
>>>>> popularity) that allows one to compose asynchronous data streams with much
>>>>> less worry on concurrency and continuation than CF. I use it in 50k loc
>>>>> projects, but I read/heard a well known company is switching almost all of
>>>>> its software stack to the reactive/asynchronous programming idiom this
>>>>> library enables. Excellent material is available (including videos and
>>>>> conference talk) on this subject (and the library).
>>>>> 2014.11.20. 2:45 ezt ?rta ("Yu Lin" <yu.lin.86 at gmail.com>):
>>>>>
>>>>>> Hi all,
>>>>>>
>>>>>> I'm a Ph.D. student at University of Illinois. I'm doing research on
>>>>>> the uses of Java concurrent/asynchronous tasks. A new construct introduced
>>>>>> in Java 8, CompletableFuture, seems to be a very nice and easy to use
>>>>>> concurrent task.
>>>>>>
>>>>>> I'm looking for projects that use CompletableFuture. But since it's
>>>>>> very new, I didn't find any real-world projects use it (only found many toy
>>>>>> examples/demos).
>>>>>>
>>>>>> Does anyone know any projects that use it?
>>>>>>
>>>>>> Thanks,
>>>>>> Yu Lin
>>>>>>
>>>>>> _______________________________________________
>>>>>> Concurrency-interest mailing list
>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>
>>>>>>
>>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141127/171fa7fa/attachment-0001.html>

From yu.lin.86 at gmail.com  Thu Nov 27 18:43:32 2014
From: yu.lin.86 at gmail.com (Yu Lin)
Date: Thu, 27 Nov 2014 17:43:32 -0600
Subject: [concurrency-interest] CompletableFuture in Java 8
In-Reply-To: <CAHJZN-tDZFVWDW18tNksVOZQHWJdbYttQW+DNwZpmy9kiv3h0w@mail.gmail.com>
References: <CAAL-3PanDAubT2RM-DePuUv_LmSX2=OsDcy82Q+ETBaRuvEwEw@mail.gmail.com>
	<CAAWwtm-evqwXxNsoMLsnm5PNU-n+=O1iR_ZcptfaJ60JHg9jbg@mail.gmail.com>
	<CAAL-3PaZDBmjQ7i43+VUeW+OGh+6VAcA3rp9mzmetJar2k_aZw@mail.gmail.com>
	<CAAL-3PYJ90uZ_Kmz1-QjY43nRUCAMxxXJmE9KiVKCddCeMnyfA@mail.gmail.com>
	<CAOoXFP9hRneBagR4hHggCQoyxsJp8tFQNbdmMvoY5dS+4LZH4g@mail.gmail.com>
	<CAAL-3Pbd55dg8CpYXhiR3kTENgnBLxmPDZ9agAv4-d1F4LXmeQ@mail.gmail.com>
	<CAHJZN-tDZFVWDW18tNksVOZQHWJdbYttQW+DNwZpmy9kiv3h0w@mail.gmail.com>
Message-ID: <CAAL-3PZg7MCiUbNEOY0-=vAskfDh5w7OrmAv4JEw_1vp6YdmKw@mail.gmail.com>

If you use ListenableFuture in Java8, it does support lambda (they're
independent). What I mean is ListenableFuture is pre-Java8, so it's
probably not designed to take advantage of lambda. To me, async task
chaining by Guava "Futures.transform" is harder to read than
CompletableFuture (with or without lambda).

Josh, when you plan to jump to Java8, why do you use adapters from
ListenableFuture
-> CompletionStage, why not change all ListenableFutures to CompletableFutures
(is it much harder than using adapters)? Adapter is just a wrapper and
makes the code more verbose.

Also, how do you compare ListenableFuture and CompletableFuture? I'm still
trying to know the differences/advantages between them.

On Thu, Nov 27, 2014 at 8:54 AM, Josh Humphries <jh at squareup.com> wrote:

> Why do you say ListenableFuture doesn't support lambdas? In Java8, any
> single-abstract-method interface can be the target of a lambda expressions,
> so Guava's Function is just as suited as the Function interface added in
> Java8.
>
> Perhaps you're only looking at the ListenableFuture interface -- which
> just has addListener(Runnable,Executor). Take a look at Futures, which is
> full of static helper methods. They include additional methods for working
> with ListenableFutures and callbacks, including a way to "transform" a
> ListenableFuture with a Function. I'm pretty sure this was done so the
> ListenableFuture interface itself could be narrow without requiring
> implementations provide the full breadth (since it's pre-Java8, there is no
> luxury of default and static methods on the interface).
>
> At Square, we use ListenableFutures heavily in framework code (to support
> pre-Java8 apps). But we also have a lot of stuff moving to Java8. So I
> wrote adapters from ListenableFuture -> CompletionStage and vice versa. All
> of the building blocks needed are present in ListenableFuture. With these
> adapters, CompletableFutures (more importantly, the CompletionStage API)
> can still be used even though framework APIs expect and return
> ListenableFutures.
>
> But we actually have very little use of CompletableFutures. A lot of
> projects prefer serial code (with blocking operations) because the code is
> generally more readable (thus easier to maintain) and the performance is
> adequate. Code that is async and parallel is often encapsulated behind a
> library, which generally uses Java7 (and ListenableFutures) to support the
> various projects that haven't yet made the jump to Java8.
>
>
>
> ----
> *Josh Humphries*
> Manager, Shared Systems  |  Platform Engineering
> Atlanta, GA  |  678-400-4867
> *Square* (www.squareup.com)
>
> On Thu, Nov 27, 2014 at 2:24 AM, Yu Lin <yu.lin.86 at gmail.com> wrote:
>
>> Yes, I'm looking at RxJava and Guava ListenableFuture. Do you guys know
>> what's the major differences among RxJava, ListenableFuture and
>> CompletableFuture. What's the advantage of each?
>>
>> It seems to me all three support callback listeners and async task
>> chaining. But ListenableFuture doesn't support lambda while RxJava support
>> more operations for Observable. How to choose which construct to use when
>> doing asynchronous programming?
>>
>> Thanks,
>> Yu
>>
>> On Thu, Nov 27, 2014 at 12:46 AM, Mohan Radhakrishnan <
>> radhakrishnan.mohan at gmail.com> wrote:
>>
>>> Don't you think Netflix should be using it ?
>>> http://netflix.github.io/#repo
>>>
>>> Thanks,
>>> Mohan
>>>
>>> On Thu, Nov 27, 2014 at 1:12 AM, Yu Lin <yu.lin.86 at gmail.com> wrote:
>>>
>>>> I guess David does mean RxJava. There're some projects use RxJava.
>>>> David, in which project do you use it? May I take a look if it's
>>>> open-source?
>>>>
>>>> On Fri, Nov 21, 2014 at 4:29 PM, Yu Lin <yu.lin.86 at gmail.com> wrote:
>>>>
>>>>> OK, thanks for all your detailed analysis. I guess the slow adaption
>>>>> of a new version is why I can't find the use of it. I myself is using Java
>>>>> 6? Other similar libraries you mentioned seem to be interesting.
>>>>>
>>>>> On Fri, Nov 21, 2014 at 1:35 PM, D?vid Karnok <akarnokd at gmail.com>
>>>>> wrote:
>>>>>
>>>>>> Hi. CompletableFuture is great, but as a Ph.D R&D student myself, I
>>>>>> see a few shortcomings:- Slow adaptation of Java 8,- the feature lag in a
>>>>>> well known mobile platform and- the alternatives available as library for
>>>>>> some time.In addition, it may compose better than Future, but IMHO, it
>>>>>> doesn't go far enough. Therefore, I use a FOSS library (with increasing
>>>>>> popularity) that allows one to compose asynchronous data streams with much
>>>>>> less worry on concurrency and continuation than CF. I use it in 50k loc
>>>>>> projects, but I read/heard a well known company is switching almost all of
>>>>>> its software stack to the reactive/asynchronous programming idiom this
>>>>>> library enables. Excellent material is available (including videos and
>>>>>> conference talk) on this subject (and the library).
>>>>>> 2014.11.20. 2:45 ezt ?rta ("Yu Lin" <yu.lin.86 at gmail.com>):
>>>>>>
>>>>>>> Hi all,
>>>>>>>
>>>>>>> I'm a Ph.D. student at University of Illinois. I'm doing research on
>>>>>>> the uses of Java concurrent/asynchronous tasks. A new construct introduced
>>>>>>> in Java 8, CompletableFuture, seems to be a very nice and easy to use
>>>>>>> concurrent task.
>>>>>>>
>>>>>>> I'm looking for projects that use CompletableFuture. But since it's
>>>>>>> very new, I didn't find any real-world projects use it (only found many toy
>>>>>>> examples/demos).
>>>>>>>
>>>>>>> Does anyone know any projects that use it?
>>>>>>>
>>>>>>> Thanks,
>>>>>>> Yu Lin
>>>>>>>
>>>>>>> _______________________________________________
>>>>>>> Concurrency-interest mailing list
>>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>
>>>>>>>
>>>>>
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141127/379b13ef/attachment.html>

From jh at squareup.com  Thu Nov 27 22:45:19 2014
From: jh at squareup.com (Josh Humphries)
Date: Thu, 27 Nov 2014 22:45:19 -0500
Subject: [concurrency-interest] CompletableFuture in Java 8
In-Reply-To: <CAAL-3PZg7MCiUbNEOY0-=vAskfDh5w7OrmAv4JEw_1vp6YdmKw@mail.gmail.com>
References: <CAAL-3PanDAubT2RM-DePuUv_LmSX2=OsDcy82Q+ETBaRuvEwEw@mail.gmail.com>
	<CAAWwtm-evqwXxNsoMLsnm5PNU-n+=O1iR_ZcptfaJ60JHg9jbg@mail.gmail.com>
	<CAAL-3PaZDBmjQ7i43+VUeW+OGh+6VAcA3rp9mzmetJar2k_aZw@mail.gmail.com>
	<CAAL-3PYJ90uZ_Kmz1-QjY43nRUCAMxxXJmE9KiVKCddCeMnyfA@mail.gmail.com>
	<CAOoXFP9hRneBagR4hHggCQoyxsJp8tFQNbdmMvoY5dS+4LZH4g@mail.gmail.com>
	<CAAL-3Pbd55dg8CpYXhiR3kTENgnBLxmPDZ9agAv4-d1F4LXmeQ@mail.gmail.com>
	<CAHJZN-tDZFVWDW18tNksVOZQHWJdbYttQW+DNwZpmy9kiv3h0w@mail.gmail.com>
	<CAAL-3PZg7MCiUbNEOY0-=vAskfDh5w7OrmAv4JEw_1vp6YdmKw@mail.gmail.com>
Message-ID: <CAHJZN-uviNyTfK-6LALZsBjbhZFbvOm+o4t1KwNUsfP7EiwV6w@mail.gmail.com>

Hi, Yu,
We use ListenableFuture in framework code: libraries that are used by all
projects, including those that haven't yet upgraded to Java8. So needing to
support Java7 is certainly one reason we continue to use ListenableFuture
instead of CompletableFuture. And letting apps running on Java8 use the
CompletionStage API but still inter-op with these libraries is the main
reason we built the adapters.

Yes, the API for CompletionStage is much better since method
chaining, using a monadic idiom, is much easier to read rather than having
to nest invocations of static methods (e.g. Futures). But there are some
drawbacks in my opinion.

Below are some of my nits about the Java8 APIs:

I disagree with how the CompletionStage interface is so tightly coupled to
the CompletableFuture implementation. Why does CompletionStage have a
toCompletableFuture() method? It feels like the interface should instead
have just extended Future.

I dislike that the only CompletionStage implementation in the JRE,
CompletableFuture, is like Guava's SettableFuture -- where the result is
set imperatively. This has its uses for sure. But an implementation whose
result comes from executing a unit of logic, like FutureTask
(CompletionStageTask?), is a glaring omission.

On that note, an ExecutorService that returns CompletionStages (or lack
thereof) is another obvious gap in the APIs in Java8 -- something like
Guava's ListeningExecutorService. CompletableFuture's runAsync and
supplyAsync methods do not suffice for a few reasons:

   1. They don't allow interruption of the task when the future is
   cancelled.
   2. They can't throw checked exceptions. A callAsync method (taking a
   Callable) would have been nice to avoid the try/catch boiler-plate in the
   Supplier -- especially since CompletableFuture.completeExceptionally
   readily supports checked exceptions.
   3. They don't support scheduling the task for deferred execution (like
   in a ScheduledExecutorService).

Finally, I'm not fond of the presence of the obtrude* methods in
CompletableFuture's public API. I feel like they should be protected (or
maybe just not exist). Mutating a future's value, after it has completed
and possibly already been inspected, seems like a potential source of
hard-to-track-down bugs.


----
*Josh Humphries*
Manager, Shared Systems  |  Platform Engineering
Atlanta, GA  |  678-400-4867
*Square* (www.squareup.com)

On Thu, Nov 27, 2014 at 6:43 PM, Yu Lin <yu.lin.86 at gmail.com> wrote:

> If you use ListenableFuture in Java8, it does support lambda (they're
> independent). What I mean is ListenableFuture is pre-Java8, so it's
> probably not designed to take advantage of lambda. To me, async task
> chaining by Guava "Futures.transform" is harder to read than
> CompletableFuture (with or without lambda).
>
> Josh, when you plan to jump to Java8, why do you use adapters from ListenableFuture
> -> CompletionStage, why not change all ListenableFutures to CompletableFutures
> (is it much harder than using adapters)? Adapter is just a wrapper and
> makes the code more verbose.
>
> Also, how do you compare ListenableFuture and CompletableFuture? I'm
> still trying to know the differences/advantages between them.
>
> On Thu, Nov 27, 2014 at 8:54 AM, Josh Humphries <jh at squareup.com> wrote:
>
>> Why do you say ListenableFuture doesn't support lambdas? In Java8, any
>> single-abstract-method interface can be the target of a lambda expressions,
>> so Guava's Function is just as suited as the Function interface added in
>> Java8.
>>
>> Perhaps you're only looking at the ListenableFuture interface -- which
>> just has addListener(Runnable,Executor). Take a look at Futures, which is
>> full of static helper methods. They include additional methods for working
>> with ListenableFutures and callbacks, including a way to "transform" a
>> ListenableFuture with a Function. I'm pretty sure this was done so the
>> ListenableFuture interface itself could be narrow without requiring
>> implementations provide the full breadth (since it's pre-Java8, there is no
>> luxury of default and static methods on the interface).
>>
>> At Square, we use ListenableFutures heavily in framework code (to support
>> pre-Java8 apps). But we also have a lot of stuff moving to Java8. So I
>> wrote adapters from ListenableFuture -> CompletionStage and vice versa. All
>> of the building blocks needed are present in ListenableFuture. With these
>> adapters, CompletableFutures (more importantly, the CompletionStage API)
>> can still be used even though framework APIs expect and return
>> ListenableFutures.
>>
>> But we actually have very little use of CompletableFutures. A lot of
>> projects prefer serial code (with blocking operations) because the code is
>> generally more readable (thus easier to maintain) and the performance is
>> adequate. Code that is async and parallel is often encapsulated behind a
>> library, which generally uses Java7 (and ListenableFutures) to support the
>> various projects that haven't yet made the jump to Java8.
>>
>>
>>
>> ----
>> *Josh Humphries*
>> Manager, Shared Systems  |  Platform Engineering
>> Atlanta, GA  |  678-400-4867
>> *Square* (www.squareup.com)
>>
>> On Thu, Nov 27, 2014 at 2:24 AM, Yu Lin <yu.lin.86 at gmail.com> wrote:
>>
>>> Yes, I'm looking at RxJava and Guava ListenableFuture. Do you guys know
>>> what's the major differences among RxJava, ListenableFuture and
>>> CompletableFuture. What's the advantage of each?
>>>
>>> It seems to me all three support callback listeners and async task
>>> chaining. But ListenableFuture doesn't support lambda while RxJava support
>>> more operations for Observable. How to choose which construct to use when
>>> doing asynchronous programming?
>>>
>>> Thanks,
>>> Yu
>>>
>>> On Thu, Nov 27, 2014 at 12:46 AM, Mohan Radhakrishnan <
>>> radhakrishnan.mohan at gmail.com> wrote:
>>>
>>>> Don't you think Netflix should be using it ?
>>>> http://netflix.github.io/#repo
>>>>
>>>> Thanks,
>>>> Mohan
>>>>
>>>> On Thu, Nov 27, 2014 at 1:12 AM, Yu Lin <yu.lin.86 at gmail.com> wrote:
>>>>
>>>>> I guess David does mean RxJava. There're some projects use RxJava.
>>>>> David, in which project do you use it? May I take a look if it's
>>>>> open-source?
>>>>>
>>>>> On Fri, Nov 21, 2014 at 4:29 PM, Yu Lin <yu.lin.86 at gmail.com> wrote:
>>>>>
>>>>>> OK, thanks for all your detailed analysis. I guess the slow adaption
>>>>>> of a new version is why I can't find the use of it. I myself is using Java
>>>>>> 6? Other similar libraries you mentioned seem to be interesting.
>>>>>>
>>>>>> On Fri, Nov 21, 2014 at 1:35 PM, D?vid Karnok <akarnokd at gmail.com>
>>>>>> wrote:
>>>>>>
>>>>>>> Hi. CompletableFuture is great, but as a Ph.D R&D student myself, I
>>>>>>> see a few shortcomings:- Slow adaptation of Java 8,- the feature lag in a
>>>>>>> well known mobile platform and- the alternatives available as library for
>>>>>>> some time.In addition, it may compose better than Future, but IMHO, it
>>>>>>> doesn't go far enough. Therefore, I use a FOSS library (with increasing
>>>>>>> popularity) that allows one to compose asynchronous data streams with much
>>>>>>> less worry on concurrency and continuation than CF. I use it in 50k loc
>>>>>>> projects, but I read/heard a well known company is switching almost all of
>>>>>>> its software stack to the reactive/asynchronous programming idiom this
>>>>>>> library enables. Excellent material is available (including videos and
>>>>>>> conference talk) on this subject (and the library).
>>>>>>> 2014.11.20. 2:45 ezt ?rta ("Yu Lin" <yu.lin.86 at gmail.com>):
>>>>>>>
>>>>>>>> Hi all,
>>>>>>>>
>>>>>>>> I'm a Ph.D. student at University of Illinois. I'm doing research
>>>>>>>> on the uses of Java concurrent/asynchronous tasks. A new construct
>>>>>>>> introduced in Java 8, CompletableFuture, seems to be a very nice and easy
>>>>>>>> to use concurrent task.
>>>>>>>>
>>>>>>>> I'm looking for projects that use CompletableFuture. But since it's
>>>>>>>> very new, I didn't find any real-world projects use it (only found many toy
>>>>>>>> examples/demos).
>>>>>>>>
>>>>>>>> Does anyone know any projects that use it?
>>>>>>>>
>>>>>>>> Thanks,
>>>>>>>> Yu Lin
>>>>>>>>
>>>>>>>> _______________________________________________
>>>>>>>> Concurrency-interest mailing list
>>>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>>
>>>>>>>>
>>>>>>
>>>>>
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at cs.oswego.edu
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>>
>>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141127/8cdef1a2/attachment-0001.html>

From Sebastian.Millies at softwareag.com  Fri Nov 28 05:08:31 2014
From: Sebastian.Millies at softwareag.com (Millies, Sebastian)
Date: Fri, 28 Nov 2014 10:08:31 +0000
Subject: [concurrency-interest] CompletableFuture in Java 8
Message-ID: <32F15738E8E5524DA4F01A0FA4A8E490FD888672@HQMBX5.eur.ad.sag>

comments inline.



> -----Original Message-----

> From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-

> bounces at cs.oswego.edu] On Behalf Of Josh Humphries

> Sent: Friday, November 28, 2014 4:45 AM

> To: Yu Lin

> Cc: concurrency-interest at cs.oswego.edu

> Subject: Re: [concurrency-interest] CompletableFuture in Java 8

>

[snip]

>

> Yes, the API for CompletionStage is much better since method chaining, using a

> monadic idiom, is much easier to read rather than having to nest invocations of static

> methods (e.g. Futures). But there are some drawbacks in my opinion.

>

> Below are some of my nits about the Java8 APIs:

>

> I disagree with how the CompletionStage interface is so tightly coupled to the

> CompletableFuture implementation. Why does CompletionStage have a

> toCompletableFuture() method?



Yes, I agree that?s a drawback. To draw a parallel: It is as if the Stream interface

had a toList() method, coupling it to Collections, which it fortunately doesn?t have.



>It feels like the interface should instead have just

> extended Future.

>

> I dislike that the only CompletionStage implementation in the JRE,

> CompletableFuture, is like Guava's SettableFuture -- where the result is set

> imperatively. This has its uses for sure. But an implementation whose result comes

> from executing a unit of logic, like FutureTask (CompletionStageTask?), is a glaring

> omission.



Don?t the factory methods (supplyAsync etc.) support just this? The ?unit of logic? is the

Supplier that computes the result, and it will run in a separate task.



> On that note, an ExecutorService that returns CompletionStages (or lack thereof) is

> another obvious gap in the APIs in Java8 -- something like Guava's

> ListeningExecutorService. CompletableFuture's runAsync and supplyAsync methods do

> not suffice for a few reasons:

> 1. They don't allow interruption of the task when the future is cancelled.



This might have to do with the fact that one cannot interrupt tasks, only threads. And I

also guess it might be difficult to deal with interruption when processing has moved on

to dependent stages (while the original thread might be re-used for some other

computation?) Mind you, I?m only guessing here, being a complete layman.



> 2. They can't throw checked exceptions. A callAsync method (taking a Callable)

> would have been nice to avoid the try/catch boiler-plate in the Supplier -- especially

> since CompletableFuture.completeExceptionally readily supports checked exceptions.



There was a discussion about error handling in this group towards the end of August.

Peter Levart showed how it is possible to rather elegantly handle checked exceptions

because they automatically get wrapped in CompletionException. In general, with

Java 8 the use of checked exceptions is more and more discouraged.



> 3. They don't support scheduling the task for deferred execution (like in a

> ScheduledExecutorService).



That is true. Also in August, there was a discussion of how to implement your own

utilities for this. (None too difficult. )Doug Lea has said that it is definitely on the radar.



> Finally, I'm not fond of the presence of the obtrude* methods in CompletableFuture's

> public API. I feel like they should be protected (or maybe just not exist). Mutating a

> future's value, after it has completed and possibly already been inspected, seems like

> a potential source of hard-to-track-down bugs.



Many people seem to be unhappy with obtrude, but the possible problem that you mention
is clearly described in the Java docs, which also say ?this method is designed for use only
in error recovery actions?.

> ----

> Josh Humphries

> Manager, Shared Systems  |  Platform Engineering Atlanta, GA  |  678-400-4867

> Square (www.squareup.com)

Software AG ? Sitz/Registered office: Uhlandstra?e 12, 64297 Darmstadt, Germany ? Registergericht/Commercial register: Darmstadt HRB 1562 - Vorstand/Management Board: Karl-Heinz Streibich (Vorsitzender/Chairman), Eric Duffaut, Dr. Wolfram Jost, Arnd Zinnhardt; - Aufsichtsratsvorsitzender/Chairman of the Supervisory Board: Dr. Andreas Bereczky - http://www.softwareag.com

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141128/226b01b4/attachment.html>
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: ATT00001.txt
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141128/226b01b4/attachment.txt>

From viktor.klang at gmail.com  Fri Nov 28 06:30:48 2014
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Fri, 28 Nov 2014 12:30:48 +0100
Subject: [concurrency-interest] CompletableFuture in Java 8
In-Reply-To: <CAHJZN-uviNyTfK-6LALZsBjbhZFbvOm+o4t1KwNUsfP7EiwV6w@mail.gmail.com>
References: <CAAL-3PanDAubT2RM-DePuUv_LmSX2=OsDcy82Q+ETBaRuvEwEw@mail.gmail.com>
	<CAAWwtm-evqwXxNsoMLsnm5PNU-n+=O1iR_ZcptfaJ60JHg9jbg@mail.gmail.com>
	<CAAL-3PaZDBmjQ7i43+VUeW+OGh+6VAcA3rp9mzmetJar2k_aZw@mail.gmail.com>
	<CAAL-3PYJ90uZ_Kmz1-QjY43nRUCAMxxXJmE9KiVKCddCeMnyfA@mail.gmail.com>
	<CAOoXFP9hRneBagR4hHggCQoyxsJp8tFQNbdmMvoY5dS+4LZH4g@mail.gmail.com>
	<CAAL-3Pbd55dg8CpYXhiR3kTENgnBLxmPDZ9agAv4-d1F4LXmeQ@mail.gmail.com>
	<CAHJZN-tDZFVWDW18tNksVOZQHWJdbYttQW+DNwZpmy9kiv3h0w@mail.gmail.com>
	<CAAL-3PZg7MCiUbNEOY0-=vAskfDh5w7OrmAv4JEw_1vp6YdmKw@mail.gmail.com>
	<CAHJZN-uviNyTfK-6LALZsBjbhZFbvOm+o4t1KwNUsfP7EiwV6w@mail.gmail.com>
Message-ID: <CANPzfU9fnMvdkbnKWb35hg5RXFnDgH3dQTGegeSSPAXVpYSPug@mail.gmail.com>

On Fri, Nov 28, 2014 at 4:45 AM, Josh Humphries <jh at squareup.com> wrote:

> Hi, Yu,
> We use ListenableFuture in framework code: libraries that are used by all
> projects, including those that haven't yet upgraded to Java8. So needing to
> support Java7 is certainly one reason we continue to use ListenableFuture
> instead of CompletableFuture. And letting apps running on Java8 use the
> CompletionStage API but still inter-op with these libraries is the main
> reason we built the adapters.
>
> Yes, the API for CompletionStage is much better since method
> chaining, using a monadic idiom, is much easier to read rather than having
> to nest invocations of static methods (e.g. Futures). But there are some
> drawbacks in my opinion.
>
> Below are some of my nits about the Java8 APIs:
>
> I disagree with how the CompletionStage interface is so tightly coupled to
> the CompletableFuture implementation. Why does CompletionStage have a
> toCompletableFuture() method? It feels like the interface should instead
> have just extended Future.
>

I agree that `toCompletableFuture` is less than ideal, but having
CompletionStage extend Future would be way worse, there is not a single
method on j.u.c.Future that I'd ever want to call, or ever want any of my
users to call.


>
> I dislike that the only CompletionStage implementation in the JRE,
> CompletableFuture, is like Guava's SettableFuture -- where the result is
> set imperatively. This has its uses for sure. But an implementation whose
> result comes from executing a unit of logic, like FutureTask
> (CompletionStageTask?), is a glaring omission.
>

This is a one-liner tho: unitCompletableFuture.theApplyAsync( unit ->
createResult, executor)


>
> On that note, an ExecutorService that returns CompletionStages (or lack
> thereof) is another obvious gap in the APIs in Java8 -- something like
> Guava's ListeningExecutorService. CompletableFuture's runAsync and
> supplyAsync methods do not suffice for a few reasons:
>
>    1. They don't allow interruption of the task when the future is
>    cancelled.
>    2. They can't throw checked exceptions. A callAsync method (taking a
>    Callable) would have been nice to avoid the try/catch boiler-plate in the
>    Supplier -- especially since CompletableFuture.completeExceptionally
>    readily supports checked exceptions.
>    3. They don't support scheduling the task for deferred execution (like
>    in a ScheduledExecutorService).
>
> Finally, I'm not fond of the presence of the obtrude* methods in
> CompletableFuture's public API. I feel like they should be protected (or
> maybe just not exist). Mutating a future's value, after it has completed
> and possibly already been inspected, seems like a potential source of
> hard-to-track-down bugs.
>

TBH I think ExecutorService needs retirement, it mixes too many concerns
already. It would be extremely simple to introduce a new branch in the
Executor hierarchy and build on top of that instead.


>
>
> ----
> *Josh Humphries*
> Manager, Shared Systems  |  Platform Engineering
> Atlanta, GA  |  678-400-4867
> *Square* (www.squareup.com)
>
> On Thu, Nov 27, 2014 at 6:43 PM, Yu Lin <yu.lin.86 at gmail.com> wrote:
>
>> If you use ListenableFuture in Java8, it does support lambda (they're
>> independent). What I mean is ListenableFuture is pre-Java8, so it's
>> probably not designed to take advantage of lambda. To me, async task
>> chaining by Guava "Futures.transform" is harder to read than
>> CompletableFuture (with or without lambda).
>>
>> Josh, when you plan to jump to Java8, why do you use adapters from ListenableFuture
>> -> CompletionStage, why not change all ListenableFutures to CompletableFutures
>> (is it much harder than using adapters)? Adapter is just a wrapper and
>> makes the code more verbose.
>>
>> Also, how do you compare ListenableFuture and CompletableFuture? I'm
>> still trying to know the differences/advantages between them.
>>
>> On Thu, Nov 27, 2014 at 8:54 AM, Josh Humphries <jh at squareup.com> wrote:
>>
>>> Why do you say ListenableFuture doesn't support lambdas? In Java8, any
>>> single-abstract-method interface can be the target of a lambda expressions,
>>> so Guava's Function is just as suited as the Function interface added in
>>> Java8.
>>>
>>> Perhaps you're only looking at the ListenableFuture interface -- which
>>> just has addListener(Runnable,Executor). Take a look at Futures, which is
>>> full of static helper methods. They include additional methods for working
>>> with ListenableFutures and callbacks, including a way to "transform" a
>>> ListenableFuture with a Function. I'm pretty sure this was done so the
>>> ListenableFuture interface itself could be narrow without requiring
>>> implementations provide the full breadth (since it's pre-Java8, there is no
>>> luxury of default and static methods on the interface).
>>>
>>> At Square, we use ListenableFutures heavily in framework code (to
>>> support pre-Java8 apps). But we also have a lot of stuff moving to Java8.
>>> So I wrote adapters from ListenableFuture -> CompletionStage and vice
>>> versa. All of the building blocks needed are present in ListenableFuture.
>>> With these adapters, CompletableFutures (more importantly, the
>>> CompletionStage API) can still be used even though framework APIs expect
>>> and return ListenableFutures.
>>>
>>> But we actually have very little use of CompletableFutures. A lot of
>>> projects prefer serial code (with blocking operations) because the code is
>>> generally more readable (thus easier to maintain) and the performance is
>>> adequate. Code that is async and parallel is often encapsulated behind a
>>> library, which generally uses Java7 (and ListenableFutures) to support the
>>> various projects that haven't yet made the jump to Java8.
>>>
>>>
>>>
>>> ----
>>> *Josh Humphries*
>>> Manager, Shared Systems  |  Platform Engineering
>>> Atlanta, GA  |  678-400-4867
>>> *Square* (www.squareup.com)
>>>
>>> On Thu, Nov 27, 2014 at 2:24 AM, Yu Lin <yu.lin.86 at gmail.com> wrote:
>>>
>>>> Yes, I'm looking at RxJava and Guava ListenableFuture. Do you guys know
>>>> what's the major differences among RxJava, ListenableFuture and
>>>> CompletableFuture. What's the advantage of each?
>>>>
>>>> It seems to me all three support callback listeners and async task
>>>> chaining. But ListenableFuture doesn't support lambda while RxJava support
>>>> more operations for Observable. How to choose which construct to use when
>>>> doing asynchronous programming?
>>>>
>>>> Thanks,
>>>> Yu
>>>>
>>>> On Thu, Nov 27, 2014 at 12:46 AM, Mohan Radhakrishnan <
>>>> radhakrishnan.mohan at gmail.com> wrote:
>>>>
>>>>> Don't you think Netflix should be using it ?
>>>>> http://netflix.github.io/#repo
>>>>>
>>>>> Thanks,
>>>>> Mohan
>>>>>
>>>>> On Thu, Nov 27, 2014 at 1:12 AM, Yu Lin <yu.lin.86 at gmail.com> wrote:
>>>>>
>>>>>> I guess David does mean RxJava. There're some projects use RxJava.
>>>>>> David, in which project do you use it? May I take a look if it's
>>>>>> open-source?
>>>>>>
>>>>>> On Fri, Nov 21, 2014 at 4:29 PM, Yu Lin <yu.lin.86 at gmail.com> wrote:
>>>>>>
>>>>>>> OK, thanks for all your detailed analysis. I guess the slow adaption
>>>>>>> of a new version is why I can't find the use of it. I myself is using Java
>>>>>>> 6? Other similar libraries you mentioned seem to be interesting.
>>>>>>>
>>>>>>> On Fri, Nov 21, 2014 at 1:35 PM, D?vid Karnok <akarnokd at gmail.com>
>>>>>>> wrote:
>>>>>>>
>>>>>>>> Hi. CompletableFuture is great, but as a Ph.D R&D student myself, I
>>>>>>>> see a few shortcomings:- Slow adaptation of Java 8,- the feature lag in a
>>>>>>>> well known mobile platform and- the alternatives available as library for
>>>>>>>> some time.In addition, it may compose better than Future, but IMHO, it
>>>>>>>> doesn't go far enough. Therefore, I use a FOSS library (with increasing
>>>>>>>> popularity) that allows one to compose asynchronous data streams with much
>>>>>>>> less worry on concurrency and continuation than CF. I use it in 50k loc
>>>>>>>> projects, but I read/heard a well known company is switching almost all of
>>>>>>>> its software stack to the reactive/asynchronous programming idiom this
>>>>>>>> library enables. Excellent material is available (including videos and
>>>>>>>> conference talk) on this subject (and the library).
>>>>>>>> 2014.11.20. 2:45 ezt ?rta ("Yu Lin" <yu.lin.86 at gmail.com>):
>>>>>>>>
>>>>>>>>> Hi all,
>>>>>>>>>
>>>>>>>>> I'm a Ph.D. student at University of Illinois. I'm doing research
>>>>>>>>> on the uses of Java concurrent/asynchronous tasks. A new construct
>>>>>>>>> introduced in Java 8, CompletableFuture, seems to be a very nice and easy
>>>>>>>>> to use concurrent task.
>>>>>>>>>
>>>>>>>>> I'm looking for projects that use CompletableFuture. But since
>>>>>>>>> it's very new, I didn't find any real-world projects use it (only found
>>>>>>>>> many toy examples/demos).
>>>>>>>>>
>>>>>>>>> Does anyone know any projects that use it?
>>>>>>>>>
>>>>>>>>> Thanks,
>>>>>>>>> Yu Lin
>>>>>>>>>
>>>>>>>>> _______________________________________________
>>>>>>>>> Concurrency-interest mailing list
>>>>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>>>
>>>>>>>>>
>>>>>>>
>>>>>>
>>>>>> _______________________________________________
>>>>>> Concurrency-interest mailing list
>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>
>>>>>>
>>>>>
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


-- 
Cheers,
?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141128/0953d28a/attachment-0001.html>

From jh at squareup.com  Fri Nov 28 09:07:05 2014
From: jh at squareup.com (Josh Humphries)
Date: Fri, 28 Nov 2014 09:07:05 -0500
Subject: [concurrency-interest] CompletableFuture in Java 8
In-Reply-To: <32F15738E8E5524DA4F01A0FA4A8E490FD888672@HQMBX5.eur.ad.sag>
References: <32F15738E8E5524DA4F01A0FA4A8E490FD888672@HQMBX5.eur.ad.sag>
Message-ID: <CAHJZN-uNu=Y2FoN0fbUf_w_0sL+eDb_O7HSa2tbsF3N6JChPKg@mail.gmail.com>

>
> > I dislike that the only CompletionStage implementation in the JRE,
>
> > CompletableFuture, is like Guava's SettableFuture -- where the result is
> set
>
> > imperatively. This has its uses for sure. But an implementation whose
> result comes
>
> > from executing a unit of logic, like FutureTask (CompletionStageTask?),
> is a glaring
>
> > omission.
>
>
>
> Don?t the factory methods (supplyAsync etc.) support just this? The ?unit
> of logic? is the
>
> Supplier that computes the result, and it will run in a separate task.
>

They do. But my objection was the API. The result of these factory methods,
the CompletableFuture, still has complete* methods in its public API,
allowing the result to be prematurely set elsewhere.

This mixes the two styles of futures (task-based and settable) in the same
API, which I don't think is appropriate.

They were separated insofar as the CompletionStage interface doesn't have
the complete* (or obtrude*) methods. What I'd have liked to have seen is
another implementation of that interface that does *not* expose such
methods.


> > On that note, an ExecutorService that returns CompletionStages (or lack
> thereof) is
>
> > another obvious gap in the APIs in Java8 -- something like Guava's
>
> > ListeningExecutorService. CompletableFuture's runAsync and supplyAsync
> methods do
>
> > not suffice for a few reasons:
>
> > 1. They don't allow interruption of the task when the future is
> cancelled.
>
>
>
> This might have to do with the fact that one cannot interrupt tasks, only
> threads. And I
>
> also guess it might be difficult to deal with interruption when processing
> has moved on
>
> to dependent stages (while the original thread might be re-used for some
> other
>
> computation?) Mind you, I?m only guessing here, being a complete layman.
>

See Future#cancel(boolean). It allows interrupting a task by interrupting
the thread the task is running on (iff the task is still running). The
parameter is unused in CompletableFuture, but works in FutureTask.


> > 2. They can't throw checked exceptions. A callAsync method (taking a
> Callable)
>
> > would have been nice to avoid the try/catch boiler-plate in the Supplier
> -- especially
>
> > since CompletableFuture.completeExceptionally readily supports checked
> exceptions.
>
>
>
> There was a discussion about error handling in this group towards the end
> of August.
>
> Peter Levart showed how it is possible to rather elegantly handle checked
> exceptions
>
> because they automatically get wrapped in CompletionException. In general,
> with
>
> Java 8 the use of checked exceptions is more and more discouraged.
>

That's great for *getting* the value out of a CompletableFuture. I'm
talking about producing the value using, for example, the supplyAsync
method. Any Throwable is clearly supported as the cause of failure for a
task (and just wrapped in CompletionException when you try to query the
result). But all methods for producing the value, aside from
completeExceptionally, require you to use try/catch and wrap the cause in
an unchecked exception.


> > 3. They don't support scheduling the task for deferred execution (like
> in a
>
> > ScheduledExecutorService).
>
>
>
> That is true. Also in August, there was a discussion of how to implement
> your own
>
> utilities for this. (None too difficult. )Doug Lea has said that it is
> definitely on the radar.
>

>
> > Finally, I'm not fond of the presence of the obtrude* methods in
> CompletableFuture's
>
> > public API. I feel like they should be protected (or maybe just not
> exist). Mutating a
>
> > future's value, after it has completed and possibly already been
> inspected, seems like
>
> > a potential source of hard-to-track-down bugs.
>
>
>
> Many people seem to be unhappy with obtrude, but the possible problem that
> you mention
>
> is clearly described in the Java docs, which also say ?this method is
> designed for use only
>
> in error recovery actions?.
>


Sure. Although I think CompletionStage#exceptionally is strictly better for
this sort of use case.

 > ----
>
> > Josh Humphries
>
> > Manager, Shared Systems  |  Platform Engineering Atlanta, GA  |
> 678-400-4867
>
> > Square (www.squareup.com)
>
>    Software AG ? Sitz/Registered office: Uhlandstra?e 12, 64297
> Darmstadt, Germany ? Registergericht/Commercial register: Darmstadt HRB
> 1562 - Vorstand/Management Board: Karl-Heinz Streibich
> (Vorsitzender/Chairman), Eric Duffaut, Dr. Wolfram Jost, Arnd Zinnhardt; -
> Aufsichtsratsvorsitzender/Chairman of the Supervisory Board: Dr. Andreas
> Bereczky - *http://www.softwareag.com* <http://www.softwareag.com>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141128/4c23c739/attachment.html>

From jh at squareup.com  Fri Nov 28 09:29:06 2014
From: jh at squareup.com (Josh Humphries)
Date: Fri, 28 Nov 2014 09:29:06 -0500
Subject: [concurrency-interest] CompletableFuture in Java 8
In-Reply-To: <CANPzfU9fnMvdkbnKWb35hg5RXFnDgH3dQTGegeSSPAXVpYSPug@mail.gmail.com>
References: <CAAL-3PanDAubT2RM-DePuUv_LmSX2=OsDcy82Q+ETBaRuvEwEw@mail.gmail.com>
	<CAAWwtm-evqwXxNsoMLsnm5PNU-n+=O1iR_ZcptfaJ60JHg9jbg@mail.gmail.com>
	<CAAL-3PaZDBmjQ7i43+VUeW+OGh+6VAcA3rp9mzmetJar2k_aZw@mail.gmail.com>
	<CAAL-3PYJ90uZ_Kmz1-QjY43nRUCAMxxXJmE9KiVKCddCeMnyfA@mail.gmail.com>
	<CAOoXFP9hRneBagR4hHggCQoyxsJp8tFQNbdmMvoY5dS+4LZH4g@mail.gmail.com>
	<CAAL-3Pbd55dg8CpYXhiR3kTENgnBLxmPDZ9agAv4-d1F4LXmeQ@mail.gmail.com>
	<CAHJZN-tDZFVWDW18tNksVOZQHWJdbYttQW+DNwZpmy9kiv3h0w@mail.gmail.com>
	<CAAL-3PZg7MCiUbNEOY0-=vAskfDh5w7OrmAv4JEw_1vp6YdmKw@mail.gmail.com>
	<CAHJZN-uviNyTfK-6LALZsBjbhZFbvOm+o4t1KwNUsfP7EiwV6w@mail.gmail.com>
	<CANPzfU9fnMvdkbnKWb35hg5RXFnDgH3dQTGegeSSPAXVpYSPug@mail.gmail.com>
Message-ID: <CAHJZN-vCAsHnw+GJ=oGp1LH_OOAYXTVxgrYihGtu9pJhObeJUg@mail.gmail.com>

>
> Below are some of my nits about the Java8 APIs:
>>
>> I disagree with how the CompletionStage interface is so tightly coupled
>> to the CompletableFuture implementation. Why does CompletionStage have a
>> toCompletableFuture() method? It feels like the interface should instead
>> have just extended Future.
>>
>
> I agree that `toCompletableFuture` is less than ideal, but having
> CompletionStage extend Future would be way worse, there is not a single
> method on j.u.c.Future that I'd ever want to call, or ever want any of my
> users to call.
>

CompletionStage provides no way to actually get the result of computation,
to block until it's done (or even query if it's done), attempt to cancel
the task, etc. In my mind, those are the only reasons you'd ever call
CompletionStage.toCompletableFuture, and I think having that behavior in
the CompletionStage interface would have been better.

I think a big benefit of ListenableFuture (and now CompletableFuture) is
that you can use them in both blocking and fully asynchronous idioms. So
being able to do things like get the result (or block for it) are valuable
to me.



> I dislike that the only CompletionStage implementation in the JRE,
>> CompletableFuture, is like Guava's SettableFuture -- where the result is
>> set imperatively. This has its uses for sure. But an implementation whose
>> result comes from executing a unit of logic, like FutureTask
>> (CompletionStageTask?), is a glaring omission.
>>
>
> This is a one-liner tho: unitCompletableFuture.theApplyAsync( unit ->
> createResult, executor)
>


Sorry if it wasn't clear. But my nit is about API purity, not
functionality. Sure, the functionality is there (and
CompletableFuture.supplyAsync is even more concise that your example). But
the result is an object (CompletableFuture) that conflates the two styles
of use for futures -- 1) completion being implicitly tied to the execution
of a task and 2) imperative completion.



> On that note, an ExecutorService that returns CompletionStages (or lack
>> thereof) is another obvious gap in the APIs in Java8 -- something like
>> Guava's ListeningExecutorService. CompletableFuture's runAsync and
>> supplyAsync methods do not suffice for a few reasons:
>>
>>    1. They don't allow interruption of the task when the future is
>>    cancelled.
>>    2. They can't throw checked exceptions. A callAsync method (taking a
>>    Callable) would have been nice to avoid the try/catch boiler-plate in the
>>    Supplier -- especially since CompletableFuture.completeExceptionally
>>    readily supports checked exceptions.
>>    3. They don't support scheduling the task for deferred execution
>>    (like in a ScheduledExecutorService).
>>
>> Finally, I'm not fond of the presence of the obtrude* methods in
>> CompletableFuture's public API. I feel like they should be protected (or
>> maybe just not exist). Mutating a future's value, after it has completed
>> and possibly already been inspected, seems like a potential source of
>> hard-to-track-down bugs.
>>
>
> TBH I think ExecutorService needs retirement, it mixes too many concerns
> already. It would be extremely simple to introduce a new branch in the
> Executor hierarchy and build on top of that instead.
>
>
>>
>>
>> ----
>> *Josh Humphries*
>> Manager, Shared Systems  |  Platform Engineering
>> Atlanta, GA  |  678-400-4867
>> *Square* (www.squareup.com)
>>
>> On Thu, Nov 27, 2014 at 6:43 PM, Yu Lin <yu.lin.86 at gmail.com> wrote:
>>
>>> If you use ListenableFuture in Java8, it does support lambda (they're
>>> independent). What I mean is ListenableFuture is pre-Java8, so it's
>>> probably not designed to take advantage of lambda. To me, async task
>>> chaining by Guava "Futures.transform" is harder to read than
>>> CompletableFuture (with or without lambda).
>>>
>>> Josh, when you plan to jump to Java8, why do you use adapters from ListenableFuture
>>> -> CompletionStage, why not change all ListenableFutures to CompletableFutures
>>> (is it much harder than using adapters)? Adapter is just a wrapper and
>>> makes the code more verbose.
>>>
>>> Also, how do you compare ListenableFuture and CompletableFuture? I'm
>>> still trying to know the differences/advantages between them.
>>>
>>> On Thu, Nov 27, 2014 at 8:54 AM, Josh Humphries <jh at squareup.com> wrote:
>>>
>>>> Why do you say ListenableFuture doesn't support lambdas? In Java8, any
>>>> single-abstract-method interface can be the target of a lambda expressions,
>>>> so Guava's Function is just as suited as the Function interface added in
>>>> Java8.
>>>>
>>>> Perhaps you're only looking at the ListenableFuture interface -- which
>>>> just has addListener(Runnable,Executor). Take a look at Futures, which is
>>>> full of static helper methods. They include additional methods for working
>>>> with ListenableFutures and callbacks, including a way to "transform" a
>>>> ListenableFuture with a Function. I'm pretty sure this was done so the
>>>> ListenableFuture interface itself could be narrow without requiring
>>>> implementations provide the full breadth (since it's pre-Java8, there is no
>>>> luxury of default and static methods on the interface).
>>>>
>>>> At Square, we use ListenableFutures heavily in framework code (to
>>>> support pre-Java8 apps). But we also have a lot of stuff moving to Java8.
>>>> So I wrote adapters from ListenableFuture -> CompletionStage and vice
>>>> versa. All of the building blocks needed are present in ListenableFuture.
>>>> With these adapters, CompletableFutures (more importantly, the
>>>> CompletionStage API) can still be used even though framework APIs expect
>>>> and return ListenableFutures.
>>>>
>>>> But we actually have very little use of CompletableFutures. A lot of
>>>> projects prefer serial code (with blocking operations) because the code is
>>>> generally more readable (thus easier to maintain) and the performance is
>>>> adequate. Code that is async and parallel is often encapsulated behind a
>>>> library, which generally uses Java7 (and ListenableFutures) to support the
>>>> various projects that haven't yet made the jump to Java8.
>>>>
>>>>
>>>>
>>>> ----
>>>> *Josh Humphries*
>>>> Manager, Shared Systems  |  Platform Engineering
>>>> Atlanta, GA  |  678-400-4867
>>>> *Square* (www.squareup.com)
>>>>
>>>> On Thu, Nov 27, 2014 at 2:24 AM, Yu Lin <yu.lin.86 at gmail.com> wrote:
>>>>
>>>>> Yes, I'm looking at RxJava and Guava ListenableFuture. Do you guys
>>>>> know what's the major differences among RxJava, ListenableFuture and
>>>>> CompletableFuture. What's the advantage of each?
>>>>>
>>>>> It seems to me all three support callback listeners and async task
>>>>> chaining. But ListenableFuture doesn't support lambda while RxJava support
>>>>> more operations for Observable. How to choose which construct to use when
>>>>> doing asynchronous programming?
>>>>>
>>>>> Thanks,
>>>>> Yu
>>>>>
>>>>> On Thu, Nov 27, 2014 at 12:46 AM, Mohan Radhakrishnan <
>>>>> radhakrishnan.mohan at gmail.com> wrote:
>>>>>
>>>>>> Don't you think Netflix should be using it ?
>>>>>> http://netflix.github.io/#repo
>>>>>>
>>>>>> Thanks,
>>>>>> Mohan
>>>>>>
>>>>>> On Thu, Nov 27, 2014 at 1:12 AM, Yu Lin <yu.lin.86 at gmail.com> wrote:
>>>>>>
>>>>>>> I guess David does mean RxJava. There're some projects use RxJava.
>>>>>>> David, in which project do you use it? May I take a look if it's
>>>>>>> open-source?
>>>>>>>
>>>>>>> On Fri, Nov 21, 2014 at 4:29 PM, Yu Lin <yu.lin.86 at gmail.com> wrote:
>>>>>>>
>>>>>>>> OK, thanks for all your detailed analysis. I guess the slow
>>>>>>>> adaption of a new version is why I can't find the use of it. I myself is
>>>>>>>> using Java 6? Other similar libraries you mentioned seem to be interesting.
>>>>>>>>
>>>>>>>> On Fri, Nov 21, 2014 at 1:35 PM, D?vid Karnok <akarnokd at gmail.com>
>>>>>>>> wrote:
>>>>>>>>
>>>>>>>>> Hi. CompletableFuture is great, but as a Ph.D R&D student myself,
>>>>>>>>> I see a few shortcomings:- Slow adaptation of Java 8,- the feature lag in a
>>>>>>>>> well known mobile platform and- the alternatives available as library for
>>>>>>>>> some time.In addition, it may compose better than Future, but IMHO, it
>>>>>>>>> doesn't go far enough. Therefore, I use a FOSS library (with increasing
>>>>>>>>> popularity) that allows one to compose asynchronous data streams with much
>>>>>>>>> less worry on concurrency and continuation than CF. I use it in 50k loc
>>>>>>>>> projects, but I read/heard a well known company is switching almost all of
>>>>>>>>> its software stack to the reactive/asynchronous programming idiom this
>>>>>>>>> library enables. Excellent material is available (including videos and
>>>>>>>>> conference talk) on this subject (and the library).
>>>>>>>>> 2014.11.20. 2:45 ezt ?rta ("Yu Lin" <yu.lin.86 at gmail.com>):
>>>>>>>>>
>>>>>>>>>> Hi all,
>>>>>>>>>>
>>>>>>>>>> I'm a Ph.D. student at University of Illinois. I'm doing research
>>>>>>>>>> on the uses of Java concurrent/asynchronous tasks. A new construct
>>>>>>>>>> introduced in Java 8, CompletableFuture, seems to be a very nice and easy
>>>>>>>>>> to use concurrent task.
>>>>>>>>>>
>>>>>>>>>> I'm looking for projects that use CompletableFuture. But since
>>>>>>>>>> it's very new, I didn't find any real-world projects use it (only found
>>>>>>>>>> many toy examples/demos).
>>>>>>>>>>
>>>>>>>>>> Does anyone know any projects that use it?
>>>>>>>>>>
>>>>>>>>>> Thanks,
>>>>>>>>>> Yu Lin
>>>>>>>>>>
>>>>>>>>>> _______________________________________________
>>>>>>>>>> Concurrency-interest mailing list
>>>>>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>
>>>>>>>
>>>>>>> _______________________________________________
>>>>>>> Concurrency-interest mailing list
>>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>
>>>>>>>
>>>>>>
>>>>>
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at cs.oswego.edu
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>>
>>>>
>>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
>
> --
> Cheers,
> ?
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141128/3e9c2a95/attachment-0001.html>

From dl at cs.oswego.edu  Fri Nov 28 10:22:00 2014
From: dl at cs.oswego.edu (Doug Lea)
Date: Fri, 28 Nov 2014 10:22:00 -0500 (EST)
Subject: [concurrency-interest] A new (?) concurrency primitive:
 WriterReaderPhaser
In-Reply-To: <0C2AA13D-E34D-4474-9E68-90F5AD5C1F3B@azulsystems.com>
References: <730432BA-BCC5-4BC4-8F2B-1DD9D1518536@azulsystems.com>
	<5473166A.7030904@gmail.com>
	<4D0323E3-8E8E-4867-B6DD-117BB48879A7@azulsystems.com>
	<5474AC98.5020106@gmail.com>
	<CAAApjO1mBZiD+AANfHVxXTGf_VyCPmPf+5c9RWksS0cGR070kA@mail.gmail.com>
	<0C2AA13D-E34D-4474-9E68-90F5AD5C1F3B@azulsystems.com>
Message-ID: <59059.66.92.46.124.1417188120.squirrel@altair.cs.oswego.edu>


Hijacking this thread to post a few meta-notes:

WriterReaderPhaser and its LeftRight analogs are based on phased
even-odd techniques (used for example in parallel relaxation), along
with some RCU-ish phase rules. I can imagine a couple of usages
besides HdrHistogram.  But we are increasingly cautious about adding
niche synchronization utilities used by only a small number of
developers to JDK. Already there are a few of these (for example
Exchanger) that arguably make more sense in some other (possibly
non-JDK) utility package.  Other candidates include specialized
bounded buffers that we have resisted adding to JDK even though there
are clearly advanced-developer audiences for them.

As concurrency and parallelism become more widespread, audiences
become more bifurcated: Those relying on simpler, more uniform APIs
with strong guarantees vs those willing to accept more usage
preconditions, and/or weaker composability and consistency guarantees,
almost always for the sake of better time/space efficiency.  Stay
tuned for some ideas on how to better support both audiences.

Short of this, one question is whether it is possible to further
generalize WriterReaderPhaser, LeftRight and/or variants to become a
useful API for a wider audience. I'll post some notes on this
hopefully soon. As seen for example in the current discussions about
CompletableFuture, it is always challenging to balance reactions among
those arguing that a general-purpose component does too little, or too
much, or is too different than what people are used to.


-Doug



From viktor.klang at gmail.com  Fri Nov 28 11:57:36 2014
From: viktor.klang at gmail.com (=?UTF-8?B?4oiaaWt0b3Ig0qBsYW5n?=)
Date: Fri, 28 Nov 2014 17:57:36 +0100
Subject: [concurrency-interest] CompletableFuture in Java 8
In-Reply-To: <CAHJZN-vCAsHnw+GJ=oGp1LH_OOAYXTVxgrYihGtu9pJhObeJUg@mail.gmail.com>
References: <CAAL-3PanDAubT2RM-DePuUv_LmSX2=OsDcy82Q+ETBaRuvEwEw@mail.gmail.com>
	<CAAWwtm-evqwXxNsoMLsnm5PNU-n+=O1iR_ZcptfaJ60JHg9jbg@mail.gmail.com>
	<CAAL-3PaZDBmjQ7i43+VUeW+OGh+6VAcA3rp9mzmetJar2k_aZw@mail.gmail.com>
	<CAAL-3PYJ90uZ_Kmz1-QjY43nRUCAMxxXJmE9KiVKCddCeMnyfA@mail.gmail.com>
	<CAOoXFP9hRneBagR4hHggCQoyxsJp8tFQNbdmMvoY5dS+4LZH4g@mail.gmail.com>
	<CAAL-3Pbd55dg8CpYXhiR3kTENgnBLxmPDZ9agAv4-d1F4LXmeQ@mail.gmail.com>
	<CAHJZN-tDZFVWDW18tNksVOZQHWJdbYttQW+DNwZpmy9kiv3h0w@mail.gmail.com>
	<CAAL-3PZg7MCiUbNEOY0-=vAskfDh5w7OrmAv4JEw_1vp6YdmKw@mail.gmail.com>
	<CAHJZN-uviNyTfK-6LALZsBjbhZFbvOm+o4t1KwNUsfP7EiwV6w@mail.gmail.com>
	<CANPzfU9fnMvdkbnKWb35hg5RXFnDgH3dQTGegeSSPAXVpYSPug@mail.gmail.com>
	<CAHJZN-vCAsHnw+GJ=oGp1LH_OOAYXTVxgrYihGtu9pJhObeJUg@mail.gmail.com>
Message-ID: <CANPzfU9RBqX3zF40F7ZV4oPQGMVtBjo7dqQsZbt3Bw4P=qUyVQ@mail.gmail.com>

Thanks for your reply, Josh!
Answers inline

On Fri, Nov 28, 2014 at 3:29 PM, Josh Humphries <jh at squareup.com> wrote:

> Below are some of my nits about the Java8 APIs:
>>>
>>> I disagree with how the CompletionStage interface is so tightly coupled
>>> to the CompletableFuture implementation. Why does CompletionStage have a
>>> toCompletableFuture() method? It feels like the interface should instead
>>> have just extended Future.
>>>
>>
>> I agree that `toCompletableFuture` is less than ideal, but having
>> CompletionStage extend Future would be way worse, there is not a single
>> method on j.u.c.Future that I'd ever want to call, or ever want any of my
>> users to call.
>>
>
> CompletionStage provides no way to actually get the result of computation,
>

Most of the methods on it gives you the result. But I assume you mean block
to get the result.


> to block until it's done (or even query if it's done),
>

I argue, based on experience, that blocking shouldn't be done in 99% of the
cases (blocking is the leading cause to why most software does not scale
beyond 4 threads), and if you really must do so, and know that it is OK to
block the current thread of execution, you can very easily:

val cf = new CompletableFuture<R>()
stage.whenComplete((r, e) -> if (e != null) cf.completeExceptionally(e)
else cf.complete(r))
cf.get()

or, in the current API: stage.toCompletableFuture().get()


> attempt to cancel the task,
>

This is also something that should be avoided, in principle because it is
inherently racy, and also because if -all- consumers are allowed to cancel,
then you cannot safely share it among several consumers anymore because any
one of them may cancel it for everyone else, so this leads us into
defensive copying, which I think we all agree is an antipattern.


> etc. In my mind, those are the only reasons you'd ever call
> CompletionStage.toCompletableFuture, and I think having that behavior in
> the CompletionStage interface would have been better.
>

I hope my arguments against it changes that conclusion :)


>
> I think a big benefit of ListenableFuture (and now CompletableFuture) is
> that you can use them in both blocking and fully asynchronous idioms. So
> being able to do things like get the result (or block for it) are valuable
> to me.
>

In my experience, offing both blocking and non-blocking is the worst
option, because the ones that don't want to play nice will do blocking
everywhere, and then you're back to "won't scale beyond 4 cores". (I don't
know how much time I have wasted trying to convert blocking code to
non-blocking. And very, very rarely have I ever done the opposite.)


>
>
>
>> I dislike that the only CompletionStage implementation in the JRE,
>>> CompletableFuture, is like Guava's SettableFuture -- where the result is
>>> set imperatively. This has its uses for sure. But an implementation whose
>>> result comes from executing a unit of logic, like FutureTask
>>> (CompletionStageTask?), is a glaring omission.
>>>
>>
>> This is a one-liner tho: unitCompletableFuture.theApplyAsync( unit ->
>> createResult, executor)
>>
>
>
> Sorry if it wasn't clear. But my nit is about API purity, not
> functionality. Sure, the functionality is there (and
> CompletableFuture.supplyAsync is even more concise that your example). But
> the result is an object (CompletableFuture) that conflates the two styles
> of use for futures -- 1) completion being implicitly tied to the execution
> of a task and 2) imperative completion.
>

I couldn't help but notice that you're objecting against having 2 styles
here, but argue that CompletionStage should support both blocking and
non-blocking operations. Is this intentional?

In my experience, the more behavior an interface exposes (exhibit A:
java.util.Collection), the harder, and more costly, it will be for
implementors to implement it efficiently. I think CompletionStage offers a
pragmatic interface for multiple Future-style APIs to conform to and
implement that does not tie interface to implementation (like
CompletableFuture would do).

Now, my preference would be to have removed the 3 versions of every method
(x, xAsync, xAsync + Executor) and instead only have `x` + Executor and
then you can supply a CallingThreadExecutor, ForkJoinPool.commonPool() or
<custom> and you get exactly the same behavior without the interface
pollution.

Regarding CompletableFuture, I would probably personally not use it because
of `cancel`, `get`, `obtrude`, `complete` etc, and thanks to
CompletionStage I can avoid all that :)

In scala.concurrent, we have Promise, which is the writer-handle
(write-once), and Future, which is the read-handle (read-many), we don't
support cancellation because it is not read-only. The only officially
supported way of blocking on the result of a Future is to go through the
auxiliary "Await.result(future, timeout)"-method (which makes it simple to
outlaw if one wants to avoid blocking). Noteworthy is that
"Await.result(?)" uses `scala.concurrent.blocking` demarcation so for
ForkJoinPools it can play nice with ManagedBlocker transparent to the user
code.

So in the end, we encourage async transformation, clear separation between
readers and writer and if blocking is -mandated- it can play nice with
managed blocking. It has turned out very well, I'd say!

If I have one regret, it is that I introduced "Await" at all. Blocking is
just bad.


>
>
>
>> On that note, an ExecutorService that returns CompletionStages (or lack
>>> thereof) is another obvious gap in the APIs in Java8 -- something like
>>> Guava's ListeningExecutorService. CompletableFuture's runAsync and
>>> supplyAsync methods do not suffice for a few reasons:
>>>
>>>    1. They don't allow interruption of the task when the future is
>>>    cancelled.
>>>    2. They can't throw checked exceptions. A callAsync method (taking a
>>>    Callable) would have been nice to avoid the try/catch boiler-plate in the
>>>    Supplier -- especially since CompletableFuture.completeExceptionally
>>>    readily supports checked exceptions.
>>>    3. They don't support scheduling the task for deferred execution
>>>    (like in a ScheduledExecutorService).
>>>
>>> Finally, I'm not fond of the presence of the obtrude* methods in
>>> CompletableFuture's public API. I feel like they should be protected (or
>>> maybe just not exist). Mutating a future's value, after it has completed
>>> and possibly already been inspected, seems like a potential source of
>>> hard-to-track-down bugs.
>>>
>>
>> TBH I think ExecutorService needs retirement, it mixes too many concerns
>> already. It would be extremely simple to introduce a new branch in the
>> Executor hierarchy and build on top of that instead.
>>
>>
>>>
>>>
>>> ----
>>> *Josh Humphries*
>>> Manager, Shared Systems  |  Platform Engineering
>>> Atlanta, GA  |  678-400-4867
>>> *Square* (www.squareup.com)
>>>
>>> On Thu, Nov 27, 2014 at 6:43 PM, Yu Lin <yu.lin.86 at gmail.com> wrote:
>>>
>>>> If you use ListenableFuture in Java8, it does support lambda (they're
>>>> independent). What I mean is ListenableFuture is pre-Java8, so it's
>>>> probably not designed to take advantage of lambda. To me, async task
>>>> chaining by Guava "Futures.transform" is harder to read than
>>>> CompletableFuture (with or without lambda).
>>>>
>>>> Josh, when you plan to jump to Java8, why do you use adapters from ListenableFuture
>>>> -> CompletionStage, why not change all ListenableFutures to CompletableFutures
>>>> (is it much harder than using adapters)? Adapter is just a wrapper and
>>>> makes the code more verbose.
>>>>
>>>> Also, how do you compare ListenableFuture and CompletableFuture? I'm
>>>> still trying to know the differences/advantages between them.
>>>>
>>>> On Thu, Nov 27, 2014 at 8:54 AM, Josh Humphries <jh at squareup.com>
>>>> wrote:
>>>>
>>>>> Why do you say ListenableFuture doesn't support lambdas? In Java8, any
>>>>> single-abstract-method interface can be the target of a lambda expressions,
>>>>> so Guava's Function is just as suited as the Function interface added in
>>>>> Java8.
>>>>>
>>>>> Perhaps you're only looking at the ListenableFuture interface -- which
>>>>> just has addListener(Runnable,Executor). Take a look at Futures, which is
>>>>> full of static helper methods. They include additional methods for working
>>>>> with ListenableFutures and callbacks, including a way to "transform" a
>>>>> ListenableFuture with a Function. I'm pretty sure this was done so the
>>>>> ListenableFuture interface itself could be narrow without requiring
>>>>> implementations provide the full breadth (since it's pre-Java8, there is no
>>>>> luxury of default and static methods on the interface).
>>>>>
>>>>> At Square, we use ListenableFutures heavily in framework code (to
>>>>> support pre-Java8 apps). But we also have a lot of stuff moving to Java8.
>>>>> So I wrote adapters from ListenableFuture -> CompletionStage and vice
>>>>> versa. All of the building blocks needed are present in ListenableFuture.
>>>>> With these adapters, CompletableFutures (more importantly, the
>>>>> CompletionStage API) can still be used even though framework APIs expect
>>>>> and return ListenableFutures.
>>>>>
>>>>> But we actually have very little use of CompletableFutures. A lot of
>>>>> projects prefer serial code (with blocking operations) because the code is
>>>>> generally more readable (thus easier to maintain) and the performance is
>>>>> adequate. Code that is async and parallel is often encapsulated behind a
>>>>> library, which generally uses Java7 (and ListenableFutures) to support the
>>>>> various projects that haven't yet made the jump to Java8.
>>>>>
>>>>>
>>>>>
>>>>> ----
>>>>> *Josh Humphries*
>>>>> Manager, Shared Systems  |  Platform Engineering
>>>>> Atlanta, GA  |  678-400-4867
>>>>> *Square* (www.squareup.com)
>>>>>
>>>>> On Thu, Nov 27, 2014 at 2:24 AM, Yu Lin <yu.lin.86 at gmail.com> wrote:
>>>>>
>>>>>> Yes, I'm looking at RxJava and Guava ListenableFuture. Do you guys
>>>>>> know what's the major differences among RxJava, ListenableFuture and
>>>>>> CompletableFuture. What's the advantage of each?
>>>>>>
>>>>>> It seems to me all three support callback listeners and async task
>>>>>> chaining. But ListenableFuture doesn't support lambda while RxJava support
>>>>>> more operations for Observable. How to choose which construct to use when
>>>>>> doing asynchronous programming?
>>>>>>
>>>>>> Thanks,
>>>>>> Yu
>>>>>>
>>>>>> On Thu, Nov 27, 2014 at 12:46 AM, Mohan Radhakrishnan <
>>>>>> radhakrishnan.mohan at gmail.com> wrote:
>>>>>>
>>>>>>> Don't you think Netflix should be using it ?
>>>>>>> http://netflix.github.io/#repo
>>>>>>>
>>>>>>> Thanks,
>>>>>>> Mohan
>>>>>>>
>>>>>>> On Thu, Nov 27, 2014 at 1:12 AM, Yu Lin <yu.lin.86 at gmail.com> wrote:
>>>>>>>
>>>>>>>> I guess David does mean RxJava. There're some projects use RxJava.
>>>>>>>> David, in which project do you use it? May I take a look if it's
>>>>>>>> open-source?
>>>>>>>>
>>>>>>>> On Fri, Nov 21, 2014 at 4:29 PM, Yu Lin <yu.lin.86 at gmail.com>
>>>>>>>> wrote:
>>>>>>>>
>>>>>>>>> OK, thanks for all your detailed analysis. I guess the slow
>>>>>>>>> adaption of a new version is why I can't find the use of it. I myself is
>>>>>>>>> using Java 6? Other similar libraries you mentioned seem to be interesting.
>>>>>>>>>
>>>>>>>>> On Fri, Nov 21, 2014 at 1:35 PM, D?vid Karnok <akarnokd at gmail.com>
>>>>>>>>> wrote:
>>>>>>>>>
>>>>>>>>>> Hi. CompletableFuture is great, but as a Ph.D R&D student myself,
>>>>>>>>>> I see a few shortcomings:- Slow adaptation of Java 8,- the feature lag in a
>>>>>>>>>> well known mobile platform and- the alternatives available as library for
>>>>>>>>>> some time.In addition, it may compose better than Future, but IMHO, it
>>>>>>>>>> doesn't go far enough. Therefore, I use a FOSS library (with increasing
>>>>>>>>>> popularity) that allows one to compose asynchronous data streams with much
>>>>>>>>>> less worry on concurrency and continuation than CF. I use it in 50k loc
>>>>>>>>>> projects, but I read/heard a well known company is switching almost all of
>>>>>>>>>> its software stack to the reactive/asynchronous programming idiom this
>>>>>>>>>> library enables. Excellent material is available (including videos and
>>>>>>>>>> conference talk) on this subject (and the library).
>>>>>>>>>> 2014.11.20. 2:45 ezt ?rta ("Yu Lin" <yu.lin.86 at gmail.com>):
>>>>>>>>>>
>>>>>>>>>>> Hi all,
>>>>>>>>>>>
>>>>>>>>>>> I'm a Ph.D. student at University of Illinois. I'm doing
>>>>>>>>>>> research on the uses of Java concurrent/asynchronous tasks. A new construct
>>>>>>>>>>> introduced in Java 8, CompletableFuture, seems to be a very nice and easy
>>>>>>>>>>> to use concurrent task.
>>>>>>>>>>>
>>>>>>>>>>> I'm looking for projects that use CompletableFuture. But since
>>>>>>>>>>> it's very new, I didn't find any real-world projects use it (only found
>>>>>>>>>>> many toy examples/demos).
>>>>>>>>>>>
>>>>>>>>>>> Does anyone know any projects that use it?
>>>>>>>>>>>
>>>>>>>>>>> Thanks,
>>>>>>>>>>> Yu Lin
>>>>>>>>>>>
>>>>>>>>>>> _______________________________________________
>>>>>>>>>>> Concurrency-interest mailing list
>>>>>>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>>>>>
>>>>>>>>>>>
>>>>>>>>>
>>>>>>>>
>>>>>>>> _______________________________________________
>>>>>>>> Concurrency-interest mailing list
>>>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>>
>>>>>>>>
>>>>>>>
>>>>>>
>>>>>> _______________________________________________
>>>>>> Concurrency-interest mailing list
>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>
>>>>>>
>>>>>
>>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>
>>
>> --
>> Cheers,
>> ?
>>
>
>


-- 
Cheers,
?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141128/46a16add/attachment-0001.html>

From jh at squareup.com  Fri Nov 28 13:50:08 2014
From: jh at squareup.com (Josh Humphries)
Date: Fri, 28 Nov 2014 13:50:08 -0500
Subject: [concurrency-interest] CompletableFuture in Java 8
In-Reply-To: <CANPzfU9RBqX3zF40F7ZV4oPQGMVtBjo7dqQsZbt3Bw4P=qUyVQ@mail.gmail.com>
References: <CAAL-3PanDAubT2RM-DePuUv_LmSX2=OsDcy82Q+ETBaRuvEwEw@mail.gmail.com>
	<CAAWwtm-evqwXxNsoMLsnm5PNU-n+=O1iR_ZcptfaJ60JHg9jbg@mail.gmail.com>
	<CAAL-3PaZDBmjQ7i43+VUeW+OGh+6VAcA3rp9mzmetJar2k_aZw@mail.gmail.com>
	<CAAL-3PYJ90uZ_Kmz1-QjY43nRUCAMxxXJmE9KiVKCddCeMnyfA@mail.gmail.com>
	<CAOoXFP9hRneBagR4hHggCQoyxsJp8tFQNbdmMvoY5dS+4LZH4g@mail.gmail.com>
	<CAAL-3Pbd55dg8CpYXhiR3kTENgnBLxmPDZ9agAv4-d1F4LXmeQ@mail.gmail.com>
	<CAHJZN-tDZFVWDW18tNksVOZQHWJdbYttQW+DNwZpmy9kiv3h0w@mail.gmail.com>
	<CAAL-3PZg7MCiUbNEOY0-=vAskfDh5w7OrmAv4JEw_1vp6YdmKw@mail.gmail.com>
	<CAHJZN-uviNyTfK-6LALZsBjbhZFbvOm+o4t1KwNUsfP7EiwV6w@mail.gmail.com>
	<CANPzfU9fnMvdkbnKWb35hg5RXFnDgH3dQTGegeSSPAXVpYSPug@mail.gmail.com>
	<CAHJZN-vCAsHnw+GJ=oGp1LH_OOAYXTVxgrYihGtu9pJhObeJUg@mail.gmail.com>
	<CANPzfU9RBqX3zF40F7ZV4oPQGMVtBjo7dqQsZbt3Bw4P=qUyVQ@mail.gmail.com>
Message-ID: <CAHJZN-vV-hm2ivu0ge33y-5UXViWXYpuPwE8C=g2V0CXqAi5aw@mail.gmail.com>

Thanks for your input, Viktor. It looks like our use cases are very
different, hence our differences of opinion.

We have plenty of services that use blocking I/O, JDBC, etc. We have no
problem scaling over 4 cores (perhaps I don't understand what you mean by
that expression, but we can easily saturate much greater than 4 cores/host
across multiple hosts). Because so many tools we use involve blocking, we
use normal thread pools (ThreadPoolExecutorService), not ForkJoinPools.

Since synchronous code is often easier to write and maintain and has not
posed an issue for scalability for us, the blocking option is fine for our
purposes. But we tend to use asynchronous idioms under-the-hood in
frameworks/libraries, to maximize parallelism and reduce latency.

So, as a client of an asynchronous task, being able to do both blocking and
non-blocking consumption is valuable as it means we don't need different
APIs (or verbose wrappers/adapters from async->blocking) for typical
usages, but it also still allows "power users" to easily do things
asynchronously.

However, as the implementor of an asynchronous task, I don't like the fact
that CompletableFuture gives the consumer a chance to actually produce the
value (via the complete* and obtrude*). CompletableFuture is an
implementation detail of the *producer* -- it's one approach to fulfilling
the result. So having that in the public API that *clients* use (e.g.
CompletionStage) is leaky.

So to me, splitting imperative completion and task-based implicit
completion into different interfaces is a different concern than splitting
blocking and non-blocking forms of consumption.

BTW, I agree 100% about CompletionStage's API being way too broad. Keeping
the API broad would probably be okay if they had default implementations,
so that interface implementors need only worry about a handful of methods
to actually implement instead of umpteen.




----
*Josh Humphries*
Manager, Shared Systems  |  Platform Engineering
Atlanta, GA  |  678-400-4867
*Square* (www.squareup.com)

On Fri, Nov 28, 2014 at 11:57 AM, ?iktor ?lang <viktor.klang at gmail.com>
wrote:

> Thanks for your reply, Josh!
> Answers inline
>
> On Fri, Nov 28, 2014 at 3:29 PM, Josh Humphries <jh at squareup.com> wrote:
>
>> Below are some of my nits about the Java8 APIs:
>>>>
>>>> I disagree with how the CompletionStage interface is so tightly coupled
>>>> to the CompletableFuture implementation. Why does CompletionStage have a
>>>> toCompletableFuture() method? It feels like the interface should instead
>>>> have just extended Future.
>>>>
>>>
>>> I agree that `toCompletableFuture` is less than ideal, but having
>>> CompletionStage extend Future would be way worse, there is not a single
>>> method on j.u.c.Future that I'd ever want to call, or ever want any of my
>>> users to call.
>>>
>>
>> CompletionStage provides no way to actually get the result of computation,
>>
>
> Most of the methods on it gives you the result. But I assume you mean
> block to get the result.
>
>
>> to block until it's done (or even query if it's done),
>>
>
> I argue, based on experience, that blocking shouldn't be done in 99% of
> the cases (blocking is the leading cause to why most software does not
> scale beyond 4 threads), and if you really must do so, and know that it is
> OK to block the current thread of execution, you can very easily:
>
> val cf = new CompletableFuture<R>()
> stage.whenComplete((r, e) -> if (e != null) cf.completeExceptionally(e)
> else cf.complete(r))
> cf.get()
>
> or, in the current API: stage.toCompletableFuture().get()
>
>
>> attempt to cancel the task,
>>
>
> This is also something that should be avoided, in principle because it is
> inherently racy, and also because if -all- consumers are allowed to cancel,
> then you cannot safely share it among several consumers anymore because any
> one of them may cancel it for everyone else, so this leads us into
> defensive copying, which I think we all agree is an antipattern.
>
>
>> etc. In my mind, those are the only reasons you'd ever call
>> CompletionStage.toCompletableFuture, and I think having that behavior in
>> the CompletionStage interface would have been better.
>>
>
> I hope my arguments against it changes that conclusion :)
>
>
>>
>> I think a big benefit of ListenableFuture (and now CompletableFuture) is
>> that you can use them in both blocking and fully asynchronous idioms. So
>> being able to do things like get the result (or block for it) are valuable
>> to me.
>>
>
> In my experience, offing both blocking and non-blocking is the worst
> option, because the ones that don't want to play nice will do blocking
> everywhere, and then you're back to "won't scale beyond 4 cores". (I don't
> know how much time I have wasted trying to convert blocking code to
> non-blocking. And very, very rarely have I ever done the opposite.)
>
>
>>
>>
>>
>>> I dislike that the only CompletionStage implementation in the JRE,
>>>> CompletableFuture, is like Guava's SettableFuture -- where the result is
>>>> set imperatively. This has its uses for sure. But an implementation whose
>>>> result comes from executing a unit of logic, like FutureTask
>>>> (CompletionStageTask?), is a glaring omission.
>>>>
>>>
>>> This is a one-liner tho: unitCompletableFuture.theApplyAsync( unit ->
>>> createResult, executor)
>>>
>>
>>
>> Sorry if it wasn't clear. But my nit is about API purity, not
>> functionality. Sure, the functionality is there (and
>> CompletableFuture.supplyAsync is even more concise that your example). But
>> the result is an object (CompletableFuture) that conflates the two styles
>> of use for futures -- 1) completion being implicitly tied to the execution
>> of a task and 2) imperative completion.
>>
>
> I couldn't help but notice that you're objecting against having 2 styles
> here, but argue that CompletionStage should support both blocking and
> non-blocking operations. Is this intentional?
>
> In my experience, the more behavior an interface exposes (exhibit A:
> java.util.Collection), the harder, and more costly, it will be for
> implementors to implement it efficiently. I think CompletionStage offers a
> pragmatic interface for multiple Future-style APIs to conform to and
> implement that does not tie interface to implementation (like
> CompletableFuture would do).
>
> Now, my preference would be to have removed the 3 versions of every method
> (x, xAsync, xAsync + Executor) and instead only have `x` + Executor and
> then you can supply a CallingThreadExecutor, ForkJoinPool.commonPool() or
> <custom> and you get exactly the same behavior without the interface
> pollution.
>
> Regarding CompletableFuture, I would probably personally not use it
> because of `cancel`, `get`, `obtrude`, `complete` etc, and thanks to
> CompletionStage I can avoid all that :)
>
> In scala.concurrent, we have Promise, which is the writer-handle
> (write-once), and Future, which is the read-handle (read-many), we don't
> support cancellation because it is not read-only. The only officially
> supported way of blocking on the result of a Future is to go through the
> auxiliary "Await.result(future, timeout)"-method (which makes it simple to
> outlaw if one wants to avoid blocking). Noteworthy is that
> "Await.result(?)" uses `scala.concurrent.blocking` demarcation so for
> ForkJoinPools it can play nice with ManagedBlocker transparent to the user
> code.
>
> So in the end, we encourage async transformation, clear separation between
> readers and writer and if blocking is -mandated- it can play nice with
> managed blocking. It has turned out very well, I'd say!
>
> If I have one regret, it is that I introduced "Await" at all. Blocking is
> just bad.
>
>
>>
>>
>>
>>> On that note, an ExecutorService that returns CompletionStages (or lack
>>>> thereof) is another obvious gap in the APIs in Java8 -- something like
>>>> Guava's ListeningExecutorService. CompletableFuture's runAsync and
>>>> supplyAsync methods do not suffice for a few reasons:
>>>>
>>>>    1. They don't allow interruption of the task when the future is
>>>>    cancelled.
>>>>    2. They can't throw checked exceptions. A callAsync method (taking
>>>>    a Callable) would have been nice to avoid the try/catch boiler-plate in the
>>>>    Supplier -- especially since CompletableFuture.completeExceptionally
>>>>    readily supports checked exceptions.
>>>>    3. They don't support scheduling the task for deferred execution
>>>>    (like in a ScheduledExecutorService).
>>>>
>>>> Finally, I'm not fond of the presence of the obtrude* methods in
>>>> CompletableFuture's public API. I feel like they should be protected (or
>>>> maybe just not exist). Mutating a future's value, after it has completed
>>>> and possibly already been inspected, seems like a potential source of
>>>> hard-to-track-down bugs.
>>>>
>>>
>>> TBH I think ExecutorService needs retirement, it mixes too many concerns
>>> already. It would be extremely simple to introduce a new branch in the
>>> Executor hierarchy and build on top of that instead.
>>>
>>>
>>>>
>>>>
>>>> ----
>>>> *Josh Humphries*
>>>> Manager, Shared Systems  |  Platform Engineering
>>>> Atlanta, GA  |  678-400-4867
>>>> *Square* (www.squareup.com)
>>>>
>>>> On Thu, Nov 27, 2014 at 6:43 PM, Yu Lin <yu.lin.86 at gmail.com> wrote:
>>>>
>>>>> If you use ListenableFuture in Java8, it does support lambda (they're
>>>>> independent). What I mean is ListenableFuture is pre-Java8, so it's
>>>>> probably not designed to take advantage of lambda. To me, async task
>>>>> chaining by Guava "Futures.transform" is harder to read than
>>>>> CompletableFuture (with or without lambda).
>>>>>
>>>>> Josh, when you plan to jump to Java8, why do you use adapters from ListenableFuture
>>>>> -> CompletionStage, why not change all ListenableFutures to CompletableFutures
>>>>> (is it much harder than using adapters)? Adapter is just a wrapper
>>>>> and makes the code more verbose.
>>>>>
>>>>> Also, how do you compare ListenableFuture and CompletableFuture? I'm
>>>>> still trying to know the differences/advantages between them.
>>>>>
>>>>> On Thu, Nov 27, 2014 at 8:54 AM, Josh Humphries <jh at squareup.com>
>>>>> wrote:
>>>>>
>>>>>> Why do you say ListenableFuture doesn't support lambdas? In Java8,
>>>>>> any single-abstract-method interface can be the target of a lambda
>>>>>> expressions, so Guava's Function is just as suited as the Function
>>>>>> interface added in Java8.
>>>>>>
>>>>>> Perhaps you're only looking at the ListenableFuture interface --
>>>>>> which just has addListener(Runnable,Executor). Take a look at Futures,
>>>>>> which is full of static helper methods. They include additional methods for
>>>>>> working with ListenableFutures and callbacks, including a way to
>>>>>> "transform" a ListenableFuture with a Function. I'm pretty sure this was
>>>>>> done so the ListenableFuture interface itself could be narrow without
>>>>>> requiring implementations provide the full breadth (since it's pre-Java8,
>>>>>> there is no luxury of default and static methods on the interface).
>>>>>>
>>>>>> At Square, we use ListenableFutures heavily in framework code (to
>>>>>> support pre-Java8 apps). But we also have a lot of stuff moving to Java8.
>>>>>> So I wrote adapters from ListenableFuture -> CompletionStage and vice
>>>>>> versa. All of the building blocks needed are present in ListenableFuture.
>>>>>> With these adapters, CompletableFutures (more importantly, the
>>>>>> CompletionStage API) can still be used even though framework APIs expect
>>>>>> and return ListenableFutures.
>>>>>>
>>>>>> But we actually have very little use of CompletableFutures. A lot of
>>>>>> projects prefer serial code (with blocking operations) because the code is
>>>>>> generally more readable (thus easier to maintain) and the performance is
>>>>>> adequate. Code that is async and parallel is often encapsulated behind a
>>>>>> library, which generally uses Java7 (and ListenableFutures) to support the
>>>>>> various projects that haven't yet made the jump to Java8.
>>>>>>
>>>>>>
>>>>>>
>>>>>> ----
>>>>>> *Josh Humphries*
>>>>>> Manager, Shared Systems  |  Platform Engineering
>>>>>> Atlanta, GA  |  678-400-4867
>>>>>> *Square* (www.squareup.com)
>>>>>>
>>>>>> On Thu, Nov 27, 2014 at 2:24 AM, Yu Lin <yu.lin.86 at gmail.com> wrote:
>>>>>>
>>>>>>> Yes, I'm looking at RxJava and Guava ListenableFuture. Do you guys
>>>>>>> know what's the major differences among RxJava, ListenableFuture and
>>>>>>> CompletableFuture. What's the advantage of each?
>>>>>>>
>>>>>>> It seems to me all three support callback listeners and async task
>>>>>>> chaining. But ListenableFuture doesn't support lambda while RxJava support
>>>>>>> more operations for Observable. How to choose which construct to use when
>>>>>>> doing asynchronous programming?
>>>>>>>
>>>>>>> Thanks,
>>>>>>> Yu
>>>>>>>
>>>>>>> On Thu, Nov 27, 2014 at 12:46 AM, Mohan Radhakrishnan <
>>>>>>> radhakrishnan.mohan at gmail.com> wrote:
>>>>>>>
>>>>>>>> Don't you think Netflix should be using it ?
>>>>>>>> http://netflix.github.io/#repo
>>>>>>>>
>>>>>>>> Thanks,
>>>>>>>> Mohan
>>>>>>>>
>>>>>>>> On Thu, Nov 27, 2014 at 1:12 AM, Yu Lin <yu.lin.86 at gmail.com>
>>>>>>>> wrote:
>>>>>>>>
>>>>>>>>> I guess David does mean RxJava. There're some projects use RxJava.
>>>>>>>>> David, in which project do you use it? May I take a look if it's
>>>>>>>>> open-source?
>>>>>>>>>
>>>>>>>>> On Fri, Nov 21, 2014 at 4:29 PM, Yu Lin <yu.lin.86 at gmail.com>
>>>>>>>>> wrote:
>>>>>>>>>
>>>>>>>>>> OK, thanks for all your detailed analysis. I guess the slow
>>>>>>>>>> adaption of a new version is why I can't find the use of it. I myself is
>>>>>>>>>> using Java 6? Other similar libraries you mentioned seem to be interesting.
>>>>>>>>>>
>>>>>>>>>> On Fri, Nov 21, 2014 at 1:35 PM, D?vid Karnok <akarnokd at gmail.com
>>>>>>>>>> > wrote:
>>>>>>>>>>
>>>>>>>>>>> Hi. CompletableFuture is great, but as a Ph.D R&D student
>>>>>>>>>>> myself, I see a few shortcomings:- Slow adaptation of Java 8,- the feature
>>>>>>>>>>> lag in a well known mobile platform and- the alternatives available as
>>>>>>>>>>> library for some time.In addition, it may compose better than Future, but
>>>>>>>>>>> IMHO, it doesn't go far enough. Therefore, I use a FOSS library (with
>>>>>>>>>>> increasing popularity) that allows one to compose asynchronous data streams
>>>>>>>>>>> with much less worry on concurrency and continuation than CF. I use it in
>>>>>>>>>>> 50k loc projects, but I read/heard a well known company is switching almost
>>>>>>>>>>> all of its software stack to the reactive/asynchronous programming idiom
>>>>>>>>>>> this library enables. Excellent material is available (including videos and
>>>>>>>>>>> conference talk) on this subject (and the library).
>>>>>>>>>>> 2014.11.20. 2:45 ezt ?rta ("Yu Lin" <yu.lin.86 at gmail.com>):
>>>>>>>>>>>
>>>>>>>>>>>> Hi all,
>>>>>>>>>>>>
>>>>>>>>>>>> I'm a Ph.D. student at University of Illinois. I'm doing
>>>>>>>>>>>> research on the uses of Java concurrent/asynchronous tasks. A new construct
>>>>>>>>>>>> introduced in Java 8, CompletableFuture, seems to be a very nice and easy
>>>>>>>>>>>> to use concurrent task.
>>>>>>>>>>>>
>>>>>>>>>>>> I'm looking for projects that use CompletableFuture. But since
>>>>>>>>>>>> it's very new, I didn't find any real-world projects use it (only found
>>>>>>>>>>>> many toy examples/demos).
>>>>>>>>>>>>
>>>>>>>>>>>> Does anyone know any projects that use it?
>>>>>>>>>>>>
>>>>>>>>>>>> Thanks,
>>>>>>>>>>>> Yu Lin
>>>>>>>>>>>>
>>>>>>>>>>>> _______________________________________________
>>>>>>>>>>>> Concurrency-interest mailing list
>>>>>>>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>>>>>>
>>>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>
>>>>>>>>> _______________________________________________
>>>>>>>>> Concurrency-interest mailing list
>>>>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>>>
>>>>>>>>>
>>>>>>>>
>>>>>>>
>>>>>>> _______________________________________________
>>>>>>> Concurrency-interest mailing list
>>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>>
>>>>>>>
>>>>>>
>>>>>
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>
>>>
>>> --
>>> Cheers,
>>> ?
>>>
>>
>>
>
>
> --
> Cheers,
> ?
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141128/7e0b2156/attachment-0001.html>

From philfrei at aol.com  Fri Nov 28 21:02:22 2014
From: philfrei at aol.com (philfrei at aol.com)
Date: Fri, 28 Nov 2014 21:02:22 -0500
Subject: [concurrency-interest] ConcurrentSkipListSet fail
Message-ID: <8D1D98FA5A20946-1570-2730F@webmail-va140.sysops.aol.com>

I have an audio engine with event scheduling provided by a ConcurrentSkipListSet. Various apps I have written that worked on Java 7 are now failing badly on Java 8. Today, I loaded Java 8 in order to debug the issue and I discovered that, for example, if there are 6 AudioEvent adds, while the audio engine is simultaneously checking the queue for events to execute on the audio playback thread, maybe 2 or 4 or all will successfully load into the ConcurrentSkipListSet. It is rather intermittent. When the adds do not complete, there is a hang of that thread. The audio thread itself will keep playing.


In researching, I discovered that there is a known issue listed:
http://bugs.java.com/bugdatabase/view_bug.do?bug_id=8060435
It seems like it might be related.


I don't seem to be able to add my bug to the bug database. I'm stopped by a screen asking about my "contract" with Oracle (I am just an independent developer).


Since this mailing list involves concurrency issues, I thought I'd just check in to see if anyone has more information, especially if there is any hope of this getting fixed soon. Also, I thought I'd offer to try and create an example of the code that fails, though it will be hard to limit it to just a couple classes.


Here are some printlns that illustrate. Six AudioEvents were loaded as an array to be scheduled, and on the fifth, the thread stops producing messages.



playPadWithFixedFlange started
  EventScheduler.schedule, events.length:6
    EventScheduler.schedule i:0
    EventScheduler.schedule events[i]:events.AudioEvent at 647e05
    EventScheduler.schedule i:1
    EventScheduler.schedule events[i]:events.AudioEvent at 1b84c92
    EventScheduler.schedule i:2
    EventScheduler.schedule events[i]:events.AudioEvent at 1c7c054
    EventScheduler.schedule i:3
    EventScheduler.schedule events[i]:events.AudioEvent at 12204a1
    EventScheduler.schedule i:4
    EventScheduler.schedule events[i]:events.AudioEvent at a298b7




At this point, the audio thread continues to play. I can't recall if any or four or five of the scheduled audio events played, but the sixth never gets scheduled. I think in this instance, none of these events are heard.


The method where the fail occurs:



public void schedule(AudioEvent[] events, long startFrame)
{
    System.out.println("  EventScheduler.schedule, events.length:"
		+ events.length);


    for (int i = 0, n = events.length; i < n; i++)
    {
        System.out.println("    EventScheduler.schedule i:" + i);
        System.out.println("    EventScheduler.schedule events[i]:"
	            + events[i].toString());


        schedule.add(new AudioEvent(events[i].getAudioCommand(), 
                    events[i].getFrame() + startFrame));
    }

}


"schedule" is initialized as follows:


schedule = new ConcurrentSkipListSet<AudioEvent>();



If a workaround has to be programmed, any suggestions? It is very important that there be a minimum of blocking to the audio thread, which is why I chose the concurrent collection.


My apologies if this is an inappropriate place to present this.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141128/ff548dd2/attachment.html>

From davidcholmes at aapt.net.au  Fri Nov 28 21:44:06 2014
From: davidcholmes at aapt.net.au (David Holmes)
Date: Sat, 29 Nov 2014 12:44:06 +1000
Subject: [concurrency-interest] ConcurrentSkipListSet fail
In-Reply-To: <8D1D98FA5A20946-1570-2730F@webmail-va140.sysops.aol.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCMEELKLAA.davidcholmes@aapt.net.au>

Use the openjdk bug site:

https://bugs.openjdk.java.net/browse/JDK-8060435

but you still need an OpenJDK user name to add comments.

There is a workaround for the above: -Djava.util.secureRandomSeed=true

David

 -----Original Message-----
From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of
philfrei at aol.com
Sent: Saturday, 29 November 2014 12:02 PM
To: concurrency-interest at cs.oswego.edu
Subject: [concurrency-interest] ConcurrentSkipListSet fail


  I have an audio engine with event scheduling provided by a
ConcurrentSkipListSet. Various apps I have written that worked on Java 7 are
now failing badly on Java 8. Today, I loaded Java 8 in order to debug the
issue and I discovered that, for example, if there are 6 AudioEvent adds,
while the audio engine is simultaneously checking the queue for events to
execute on the audio playback thread, maybe 2 or 4 or all will successfully
load into the ConcurrentSkipListSet. It is rather intermittent. When the
adds do not complete, there is a hang of that thread. The audio thread
itself will keep playing.


  In researching, I discovered that there is a known issue listed:
  http://bugs.java.com/bugdatabase/view_bug.do?bug_id=8060435
  It seems like it might be related.


  I don't seem to be able to add my bug to the bug database. I'm stopped by
a screen asking about my "contract" with Oracle (I am just an independent
developer).


  Since this mailing list involves concurrency issues, I thought I'd just
check in to see if anyone has more information, especially if there is any
hope of this getting fixed soon. Also, I thought I'd offer to try and create
an example of the code that fails, though it will be hard to limit it to
just a couple classes.


  Here are some printlns that illustrate. Six AudioEvents were loaded as an
array to be scheduled, and on the fifth, the thread stops producing
messages.


  playPadWithFixedFlange started
    EventScheduler.schedule, events.length:6
      EventScheduler.schedule i:0
      EventScheduler.schedule events[i]:events.AudioEvent at 647e05
      EventScheduler.schedule i:1
      EventScheduler.schedule events[i]:events.AudioEvent at 1b84c92
      EventScheduler.schedule i:2
      EventScheduler.schedule events[i]:events.AudioEvent at 1c7c054
      EventScheduler.schedule i:3
      EventScheduler.schedule events[i]:events.AudioEvent at 12204a1
      EventScheduler.schedule i:4
      EventScheduler.schedule events[i]:events.AudioEvent at a298b7




  At this point, the audio thread continues to play. I can't recall if any
or four or five of the scheduled audio events played, but the sixth never
gets scheduled. I think in this instance, none of these events are heard.


  The method where the fail occurs:


  public void schedule(AudioEvent[] events, long startFrame)
  {
      System.out.println("  EventScheduler.schedule, events.length:"
  + events.length);


      for (int i = 0, n = events.length; i < n; i++)
      {
          System.out.println("    EventScheduler.schedule i:" + i);
          System.out.println("    EventScheduler.schedule events[i]:"
              + events[i].toString());


          schedule.add(new AudioEvent(events[i].getAudioCommand(),
                      events[i].getFrame() + startFrame));
      }
  }


  "schedule" is initialized as follows:


  schedule = new ConcurrentSkipListSet<AudioEvent>();


  If a workaround has to be programmed, any suggestions? It is very
important that there be a minimum of blocking to the audio thread, which is
why I chose the concurrent collection.


  My apologies if this is an inappropriate place to present this.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141129/661f4cde/attachment.html>

From cowwoc at bbs.darktech.org  Fri Nov 28 22:42:31 2014
From: cowwoc at bbs.darktech.org (cowwoc)
Date: Fri, 28 Nov 2014 20:42:31 -0700 (MST)
Subject: [concurrency-interest] Please clarify Javadoc of
	CompletableFuture.thenCompose()
Message-ID: <1417232551381-11504.post@n7.nabble.com>

I don't know about the rest of you, but no matter how many times I read the
Javadoc for CompletableFuture.thenCompose() I don't end up with the same
interpretation as mentioned at
http://www.nurkiewicz.com/2013/05/java-8-definitive-guide-to.html

I find http://www.nurkiewicz.com/2013/05/java-8-definitive-guide-to.html
much easier to understand.

Is it possible to change the Javadoc to something along the lines of
"Returns a CompletionStage that completes with the same value as the
CompletionStage returned by the function"?

Honestly, I think all of these methods could use a more layman explanation
and perhaps some example code. Is it possible to have a technical writer
review this?

Gili



--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/Please-clarify-Javadoc-of-CompletableFuture-thenCompose-tp11504.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From boehm at acm.org  Fri Nov 28 23:56:04 2014
From: boehm at acm.org (Hans Boehm)
Date: Fri, 28 Nov 2014 20:56:04 -0800
Subject: [concurrency-interest] RFR: 8065804: JEP 171:
 Clarifications/corrections for fence intrinsics
In-Reply-To: <5476D751.5040407@gmail.com>
References: <NFBBKALFDCPFIDBNKAPCIEDDKLAA.davidcholmes@aapt.net.au>
	<5476D751.5040407@gmail.com>
Message-ID: <CAPUmR1Y92kM8ZXrppWrC2+hC4PpG_MBRgzTnxFm5YLA8vmXbOA@mail.gmail.com>

I basically agree with David's observation.  However the C++

atomic_thread_fence(memory_order_acquire)

actually has somewhat different semantics from load(memory_order_acquire).
It basically ensures that prior atomic loads L are not reordered with later
(i.e. following the fence in program order) loads and stores, making it
something like a LoadLoad|LoadStore fence.  Thus the fence orders two sets
of operations where the acquire load orders a single operation with respect
to a set.  This makes the fence versions of memory_order_acquire and
memory_order_release meaningful, but somewhat different from the non-fence
versions.  The terminology is probably not great, but that seems to be the
most common usage now.

On Wed, Nov 26, 2014 at 11:48 PM, Peter Levart <peter.levart at gmail.com>
wrote:

> On 11/27/2014 04:00 AM, David Holmes wrote:
>
>> Can I make an observation about acquire() and release() - to me they are
>> meaningless when considered in isolation. Given their definitions they
>> allow
>> anything to move into a region bounded by acquire() and release(), then
>> you
>> can effectively move the whole program into the region and thus the
>> acquire() and release() do not constrain any reorderings.
>>   acquire() and
>> release() only make sense when their own movement is constrained with
>> respect to something else - such as lock acquisition/release, or when
>> combined with specific load/store actions.
>>
>
> ...or another acquire/release region?
>
> Regards, Peter
>
>
>
>> David
>>
>> Martin Buchholz writes:
>>
>>> On Tue, Nov 25, 2014 at 6:04 AM, Paul Sandoz
>>> <paul.sandoz at oracle.com> wrote:
>>>
>>>> Hi Martin,
>>>>
>>>> Thanks for looking into this.
>>>>
>>>> 1141      * Currently hotspot's implementation of a Java
>>>>
>>> language-level volatile
>>>
>>>> 1142      * store has the same effect as a storeFence followed
>>>>
>>> by a relaxed store,
>>>
>>>> 1143      * although that may be a little stronger than needed.
>>>>
>>>> IIUC to emulate hotpot's volatile store you will need to say
>>>>
>>> that a fullFence immediately follows the relaxed store.
>>>
>>> Right - I've been groking that.
>>>
>>>  The bit that always confuses me about release and acquire is
>>>>
>>> ordering is restricted to one direction, as talked about in
>>> orderAccess.hpp [1]. So for a release, accesses prior to the
>>> release cannot move below it, but accesses succeeding the release
>>> can move above it. And that seems to apply to Unsafe.storeFence
>>> [2] (acting like a monitor exit). Is that contrary to C++ release
>>> fences where ordering is restricted both to prior and succeeding
>>> accesses? [3]
>>>
>>>> So what about the following?
>>>>
>>>>    a = r1; // Cannot move below the fence
>>>>    Unsafe.storeFence();
>>>>    b = r2; // Can move above the fence?
>>>>
>>> I think the hotspot docs need to be more precise about when they're
>>> talking about movement of stores and when about loads.
>>>
>>>  // release.  I.e., subsequent memory accesses may float above the
>>>> // release, but prior ones may not float below it.
>>>>
>>> As I've said elsewhere, the above makes no sense without restricting
>>> the type of access.
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141128/ed23e4cd/attachment.html>

From TEREKHOV at de.ibm.com  Sat Nov 29 09:16:00 2014
From: TEREKHOV at de.ibm.com (Alexander Terekhov)
Date: Sat, 29 Nov 2014 15:16:00 +0100
Subject: [concurrency-interest] RFR: 8065804: JEP 171:
 Clarifications/corrections for fence intrinsics
In-Reply-To: <CAPUmR1Y92kM8ZXrppWrC2+hC4PpG_MBRgzTnxFm5YLA8vmXbOA@mail.gmail.com>
Message-ID: <OF668E15B6.16E8E54A-ONC1257D9F.004D0BCB-C1257D9F.004E6053@de.ibm.com>

> atomic_thread_fence(memory_order_acquire)

int r1 = x; // ordinary load

int r2 = y.load(memory_order_relaxed); // atomic relaxed load

if (z.load(memory_order_relaxed) == SOMETHING_I_AM_WAITING_FOR) {

  atomic_thread_fence(memory_order_acquire);

  .
  .
  .

}

IIUC r1 and r2 (and etc.) loads prior to z can be reordered with
respect to the fence... and hence the term 'acquire' fits quite well.

regards,
alexander.


Hans Boehm <boehm at acm.org>@cs.oswego.edu on 29.11.2014 05:56:04

Sent by:	concurrency-interest-bounces at cs.oswego.edu


To:	Peter Levart <peter.levart at gmail.com>
cc:	Vladimir Kozlov <vladimir.kozlov at oracle.com>, concurrency-interest
       <concurrency-interest at cs.oswego.edu>, Martin Buchholz
       <martinrb at google.com>, core-libs-dev
       <core-libs-dev at openjdk.java.net>, dholmes at ieee.org
Subject:	Re: [concurrency-interest] RFR: 8065804: JEP 171:
       Clarifications/corrections for fence intrinsics


I basically agree with David's observation.? However the C++

atomic_thread_fence(memory_order_acquire)

actually has somewhat different semantics from load(memory_order_acquire).
It basically ensures that prior atomic loads L are not reordered with later
(i.e. following the fence in program order) loads and stores, making it
something like a LoadLoad|LoadStore fence.? Thus the fence orders two sets
of operations where the acquire load orders a single operation with respect
to a set.? This makes the fence versions of memory_order_acquire and
memory_order_release meaningful, but somewhat different from the non-fence
versions.? The terminology is probably not great, but that seems to be the
most common usage now.

On Wed, Nov 26, 2014 at 11:48 PM, Peter Levart <peter.levart at gmail.com>
wrote:
      On 11/27/2014 04:00 AM, David Holmes wrote:
            Can I make an observation about acquire() and release() - to me
            they are
            meaningless when considered in isolation. Given their
            definitions they allow
            anything to move into a region bounded by acquire() and release
            (), then you
            can effectively move the whole program into the region and thus
            the
            acquire() and release() do not constrain any reorderings.
            ? acquire() and
            release() only make sense when their own movement is
            constrained with
            respect to something else - such as lock acquisition/release,
            or when
            combined with specific load/store actions.

      ...or another acquire/release region?

      Regards, Peter



            David

            Martin Buchholz writes:
                  On Tue, Nov 25, 2014 at 6:04 AM, Paul Sandoz
                  <paul.sandoz at oracle.com> wrote:
                        Hi Martin,

                        Thanks for looking into this.

                        1141? ? ? * Currently hotspot's implementation of a
                        Java
                  language-level volatile
                        1142? ? ? * store has the same effect as a
                        storeFence followed
                  by a relaxed store,
                        1143? ? ? * although that may be a little stronger
                        than needed.

                        IIUC to emulate hotpot's volatile store you will
                        need to say
                  that a fullFence immediately follows the relaxed store.

                  Right - I've been groking that.

                        The bit that always confuses me about release and
                        acquire is
                  ordering is restricted to one direction, as talked about
                  in
                  orderAccess.hpp [1]. So for a release, accesses prior to
                  the
                  release cannot move below it, but accesses succeeding the
                  release
                  can move above it. And that seems to apply to
                  Unsafe.storeFence
                  [2] (acting like a monitor exit). Is that contrary to C++
                  release
                  fences where ordering is restricted both to prior and
                  succeeding
                  accesses? [3]
                        So what about the following?

                        ? ?a = r1; // Cannot move below the fence
                        ? ?Unsafe.storeFence();
                        ? ?b = r2; // Can move above the fence?
                  I think the hotspot docs need to be more precise about
                  when they're
                  talking about movement of stores and when about loads.

                        // release.? I.e., subsequent memory accesses may
                        float above the
                        // release, but prior ones may not float below it.
                  As I've said elsewhere, the above makes no sense without
                  restricting
                  the type of access.

                  _______________________________________________
                  Concurrency-interest mailing list
                  Concurrency-interest at cs.oswego.edu
                  http://cs.oswego.edu/mailman/listinfo/concurrency-interest

      _______________________________________________
      Concurrency-interest mailing list
      Concurrency-interest at cs.oswego.edu
      http://cs.oswego.edu/mailman/listinfo/concurrency-interest
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest




From TEREKHOV at de.ibm.com  Sat Nov 29 10:12:38 2014
From: TEREKHOV at de.ibm.com (Alexander Terekhov)
Date: Sat, 29 Nov 2014 16:12:38 +0100
Subject: [concurrency-interest] RFR: 8065804: JEP 171:
 Clarifications/corrections for fence intrinsics
In-Reply-To: <CAPUmR1Y92kM8ZXrppWrC2+hC4PpG_MBRgzTnxFm5YLA8vmXbOA@mail.gmail.com>
Message-ID: <OF3934E623.3E21FC36-ONC1257D9F.0053175E-C1257D9F.00538FF5@de.ibm.com>

> memory_order_release meaningful, but somewhat different from the
non-fence

it would be nice to have release fence with an artificial dependency
to define a set of actually release stores and not constraining other
subsequent stores (and the order of release stores with respect to
each other), e.g.:

// set multiple flags each indicating 'release' without imposing
// ordering on 'release' stores respect to each other and not
// constraining other subsequent stores

.
.
.

if (atomic_thread_fence(memory_order_release)) {

  flag1.store(READY, memory_order_relaxed);
  flag2.store(READY, memory_order_relaxed);

}

regards,
alexander.

Hans Boehm <boehm at acm.org>@cs.oswego.edu on 29.11.2014 05:56:04

Sent by:	concurrency-interest-bounces at cs.oswego.edu


To:	Peter Levart <peter.levart at gmail.com>
cc:	Vladimir Kozlov <vladimir.kozlov at oracle.com>, concurrency-interest
       <concurrency-interest at cs.oswego.edu>, Martin Buchholz
       <martinrb at google.com>, core-libs-dev
       <core-libs-dev at openjdk.java.net>, dholmes at ieee.org
Subject:	Re: [concurrency-interest] RFR: 8065804: JEP 171:
       Clarifications/corrections for fence intrinsics


I basically agree with David's observation.? However the C++

atomic_thread_fence(memory_order_acquire)

actually has somewhat different semantics from load(memory_order_acquire).
It basically ensures that prior atomic loads L are not reordered with later
(i.e. following the fence in program order) loads and stores, making it
something like a LoadLoad|LoadStore fence.? Thus the fence orders two sets
of operations where the acquire load orders a single operation with respect
to a set.? This makes the fence versions of memory_order_acquire and
memory_order_release meaningful, but somewhat different from the non-fence
versions.? The terminology is probably not great, but that seems to be the
most common usage now.

On Wed, Nov 26, 2014 at 11:48 PM, Peter Levart <peter.levart at gmail.com>
wrote:
      On 11/27/2014 04:00 AM, David Holmes wrote:
            Can I make an observation about acquire() and release() - to me
            they are
            meaningless when considered in isolation. Given their
            definitions they allow
            anything to move into a region bounded by acquire() and release
            (), then you
            can effectively move the whole program into the region and thus
            the
            acquire() and release() do not constrain any reorderings.
            ? acquire() and
            release() only make sense when their own movement is
            constrained with
            respect to something else - such as lock acquisition/release,
            or when
            combined with specific load/store actions.

      ...or another acquire/release region?

      Regards, Peter



            David

            Martin Buchholz writes:
                  On Tue, Nov 25, 2014 at 6:04 AM, Paul Sandoz
                  <paul.sandoz at oracle.com> wrote:
                        Hi Martin,

                        Thanks for looking into this.

                        1141? ? ? * Currently hotspot's implementation of a
                        Java
                  language-level volatile
                        1142? ? ? * store has the same effect as a
                        storeFence followed
                  by a relaxed store,
                        1143? ? ? * although that may be a little stronger
                        than needed.

                        IIUC to emulate hotpot's volatile store you will
                        need to say
                  that a fullFence immediately follows the relaxed store.

                  Right - I've been groking that.

                        The bit that always confuses me about release and
                        acquire is
                  ordering is restricted to one direction, as talked about
                  in
                  orderAccess.hpp [1]. So for a release, accesses prior to
                  the
                  release cannot move below it, but accesses succeeding the
                  release
                  can move above it. And that seems to apply to
                  Unsafe.storeFence
                  [2] (acting like a monitor exit). Is that contrary to C++
                  release
                  fences where ordering is restricted both to prior and
                  succeeding
                  accesses? [3]
                        So what about the following?

                        ? ?a = r1; // Cannot move below the fence
                        ? ?Unsafe.storeFence();
                        ? ?b = r2; // Can move above the fence?
                  I think the hotspot docs need to be more precise about
                  when they're
                  talking about movement of stores and when about loads.

                        // release.? I.e., subsequent memory accesses may
                        float above the
                        // release, but prior ones may not float below it.
                  As I've said elsewhere, the above makes no sense without
                  restricting
                  the type of access.

                  _______________________________________________
                  Concurrency-interest mailing list
                  Concurrency-interest at cs.oswego.edu
                  http://cs.oswego.edu/mailman/listinfo/concurrency-interest

      _______________________________________________
      Concurrency-interest mailing list
      Concurrency-interest at cs.oswego.edu
      http://cs.oswego.edu/mailman/listinfo/concurrency-interest
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest




From peter.levart at gmail.com  Sat Nov 29 12:47:32 2014
From: peter.levart at gmail.com (Peter Levart)
Date: Sat, 29 Nov 2014 18:47:32 +0100
Subject: [concurrency-interest] ConcurrentSkipListSet fail
In-Reply-To: <8D1D98FA5A20946-1570-2730F@webmail-va140.sysops.aol.com>
References: <8D1D98FA5A20946-1570-2730F@webmail-va140.sysops.aol.com>
Message-ID: <547A06B4.4010106@gmail.com>

Hi Phil,

Could you try to once more to re-produce the situation in your program 
where the thread adding to SkipListSet blocks and then dump the threads 
of the program?

It's easy. You just send a QUIT signal to the java process running the 
program and thread dump will be printed on the standard output. On Linux 
you can send the QUIT signal either by command:

     killl -QUIT <process-id>

or by pressing Ctrl-\ in the terminal from which the java process is 
started.

On Windows, I think that pressing Ctrl-Break in the terminal that is 
attached to the java process triggers thread dump, but I haven't tried this.

You can use "jstack" command too.

Please post the thread stack traces here, so we can see where the 
SkipListSet.add blocks and whether it is related to the bug you mention...

Regards, Peter

On 11/29/2014 03:02 AM, philfrei at aol.com wrote:
> I have an audio engine with event scheduling provided by a 
> ConcurrentSkipListSet. Various apps I have written that worked on Java 
> 7 are now failing badly on Java 8. Today, I loaded Java 8 in order to 
> debug the issue and I discovered that, for example, if there are 6 
> AudioEvent adds, while the audio engine is simultaneously checking the 
> queue for events to execute on the audio playback thread, maybe 2 or 4 
> or all will successfully load into the ConcurrentSkipListSet. It is 
> rather intermittent. When the adds do not complete, there is a hang of 
> that thread. The audio thread itself will keep playing.
>
> In researching, I discovered that there is a known issue listed:
> http://bugs.java.com/bugdatabase/view_bug.do?bug_id=8060435
> It seems like it might be related.
>
> I don't seem to be able to add my bug to the bug database. I'm stopped 
> by a screen asking about my "contract" with Oracle (I am just an 
> independent developer).
>
> Since this mailing list involves concurrency issues, I thought I'd 
> just check in to see if anyone has more information, especially if 
> there is any hope of this getting fixed soon. Also, I thought I'd 
> offer to try and create an example of the code that fails, though it 
> will be hard to limit it to just a couple classes.
>
> Here are some printlns that illustrate. Six AudioEvents were loaded as 
> an array to be scheduled, and on the fifth, the thread stops producing 
> messages.
>
> playPadWithFixedFlange started
>   EventScheduler.schedule, events.length:6
>     EventScheduler.schedule i:0
>     EventScheduler.schedule events[i]:events.AudioEvent at 647e05
>     EventScheduler.schedule i:1
>     EventScheduler.schedule events[i]:events.AudioEvent at 1b84c92
>     EventScheduler.schedule i:2
>     EventScheduler.schedule events[i]:events.AudioEvent at 1c7c054
>     EventScheduler.schedule i:3
>     EventScheduler.schedule events[i]:events.AudioEvent at 12204a1
>     EventScheduler.schedule i:4
>     EventScheduler.schedule events[i]:events.AudioEvent at a298b7
>
>
> At this point, the audio thread continues to play. I can't recall if 
> any or four or five of the scheduled audio events played, but the 
> sixth never gets scheduled. I think in this instance, none of these 
> events are heard.
>
> The method where the fail occurs:
>
> public void schedule(AudioEvent[] events, long startFrame)
> {
>     System.out.println("  EventScheduler.schedule, events.length:"
> + events.length);
>
>     for (int i = 0, n = events.length; i < n; i++)
>     {
>         System.out.println("  EventScheduler.schedule i:" + i);
>         System.out.println("  EventScheduler.schedule events[i]:"
>             + events[i].toString());
>
> schedule.add(new AudioEvent(events[i].getAudioCommand(),
>                     events[i].getFrame() + startFrame));
> }
> }
>
> "schedule" is initialized as follows:
>
> schedule = new ConcurrentSkipListSet<AudioEvent>();
>
> If a workaround has to be programmed, any suggestions? It is very 
> important that there be a minimum of blocking to the audio thread, 
> which is why I chose the concurrent collection.
>
> My apologies if this is an inappropriate place to present this.
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141129/7b74c384/attachment.html>

From philfrei at aol.com  Sun Nov 30 04:08:23 2014
From: philfrei at aol.com (philfrei at aol.com)
Date: Sun, 30 Nov 2014 04:08:23 -0500
Subject: [concurrency-interest] ConcurrentSkipListSet fail
Message-ID: <8D1DA9453DD2D69-FF8-236EF@webmail-vm012.sysops.aol.com>

@David Holmes


I tried to use the workaround you provided: -Djava.util.secureRandomSeed=true
but the problem still occurs intermittently.


@Peter Levart
I was able to obtain the thread dump you requested. I thought I'd also post the code which elicited the fail. It is at the following URL:
  http://adonax.com/ConcurrentSkipListSetProblem/TestFlanger.jar


I apologize for not taking time to pare the code down further.


The point of failure is intermittent. Following is an example of console output when when the program runs to completion (happens occasionally):



playPadWithFixedFlange started
  EventScheduler.schedule, events.length:6
    EventScheduler.schedule i:0
    EventScheduler.schedule events[i]:events.AudioEvent at 647e05
    EventScheduler.schedule i:1
    EventScheduler.schedule events[i]:events.AudioEvent at 1b84c92
    EventScheduler.schedule i:2
    EventScheduler.schedule events[i]:events.AudioEvent at 1c7c054
    EventScheduler.schedule i:3
    EventScheduler.schedule events[i]:events.AudioEvent at 12204a1
    EventScheduler.schedule i:4
    EventScheduler.schedule events[i]:events.AudioEvent at a298b7
    EventScheduler.schedule i:5
    EventScheduler.schedule events[i]:events.AudioEvent at 14991ad
  PFAudioMixer.scheduleASAP, nextEventFrame:12288
playing for 10 seconds
PFAudioMixer frame:12289 , nextEventFrame = 23313
PFAudioMixer frame:23314 , nextEventFrame = 34338
PFAudioMixer frame:34339 , nextEventFrame = 45363
PFAudioMixer frame:45364 , nextEventFrame = 56388
PFAudioMixer frame:56389 , nextEventFrame = 67413
PFAudioMixer frame:67414 , nextEventFrame = 9223372036854775807
about to schedule releases
  EventScheduler.schedule, events.length:6
    EventScheduler.schedule i:0
    EventScheduler.schedule events[i]:events.AudioEvent at d93b30
    EventScheduler.schedule i:1
    EventScheduler.schedule events[i]:events.AudioEvent at 16d3586
    EventScheduler.schedule i:2
    EventScheduler.schedule events[i]:events.AudioEvent at 154617c
    EventScheduler.schedule i:3
    EventScheduler.schedule events[i]:events.AudioEvent at a14482
    EventScheduler.schedule i:4
    EventScheduler.schedule events[i]:events.AudioEvent at 140e19d
    EventScheduler.schedule i:5
    EventScheduler.schedule events[i]:events.AudioEvent at 17327b6
  PFAudioMixer.scheduleASAP, nextEventFrame:452608
fadout
PFAudioMixer frame:452609 , nextEventFrame = 452608
PFAudioMixer frame:452609 , nextEventFrame = 452608
PFAudioMixer frame:452609 , nextEventFrame = 452608
PFAudioMixer frame:452609 , nextEventFrame = 452608
PFAudioMixer frame:452609 , nextEventFrame = 452608
PFAudioMixer frame:452609 , nextEventFrame = 9223372036854775807
playPadWithFixedFlange done
PFAudioMixer.FrameMixerPlayer.run exiting



Program synopsis: After initialization of objects and of directives, a set of 6 notes are played polyphonically with a slightly staggered entrance. A flanger is launched and runs for 10 seconds, then a release of all the notes is scheduled and 5 seconds is allowed for the notes to all fade to silence before the audio mixer is turned off and the program exits.


In the following "fail" the tones are never get heard. This indicates to me that the inner audio thread (PFAudioMixer.FrameMixerPlayer.run) probably never locates scheduled items in the ConcurrentSkipListSet with a matching frameNumber, despite two being added to the collection (the third add may not have completed).


Following is the requested thread dump:



playPadWithFixedFlange started
  EventScheduler.schedule, events.length:6
    EventScheduler.schedule i:0
    EventScheduler.schedule events[i]:events.AudioEvent at 192e0f4
    EventScheduler.schedule i:1
    EventScheduler.schedule events[i]:events.AudioEvent at 1f4acd0
    EventScheduler.schedule i:2
    EventScheduler.schedule events[i]:events.AudioEvent at bedef2
2014-11-30 00:40:19
Full thread dump Java HotSpot(TM) Client VM (25.25-b02 mixed mode):


"Java Sound Event Dispatcher" #9 daemon prio=5 os_prio=0 tid=0x1331c400 nid=0x5ac in Object.wait() [0x1397f000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x02d4c5e0> (a com.sun.media.sound.EventDispatcher)
	at com.sun.media.sound.EventDispatcher.dispatchEvents(Unknown Source)
	- locked <0x02d4c5e0> (a com.sun.media.sound.EventDispatcher)
	at com.sun.media.sound.EventDispatcher.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)


"Thread-0" #8 prio=5 os_prio=0 tid=0x132e1400 nid=0xe8c in Object.wait() [0x1361f000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x02d4b900> (a java.lang.Object)
	at com.sun.media.sound.DirectAudioDevice$DirectDL.write(Unknown Source)
	- locked <0x02d4b900> (a java.lang.Object)
	at core.PFAudioMixer$FrameMixerPlayer.run(PFAudioMixer.java:280)
	at java.lang.Thread.run(Unknown Source)


"Service Thread" #7 daemon prio=9 os_prio=0 tid=0x1327ec00 nid=0xad8 runnable [0x00000000]
   java.lang.Thread.State: RUNNABLE


"C1 CompilerThread0" #6 daemon prio=9 os_prio=2 tid=0x13254c00 nid=0x70c waiting on condition [0x00000000]
   java.lang.Thread.State: RUNNABLE


"Attach Listener" #5 daemon prio=5 os_prio=2 tid=0x13253400 nid=0xf70 runnable [0x00000000]
   java.lang.Thread.State: RUNNABLE


"Signal Dispatcher" #4 daemon prio=9 os_prio=2 tid=0x13251c00 nid=0xaac waiting on condition [0x00000000]
   java.lang.Thread.State: RUNNABLE


"Finalizer" #3 daemon prio=8 os_prio=1 tid=0x02bfd000 nid=0xc3c in Object.wait() [0x1323f000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x02c056d8> (a java.lang.ref.ReferenceQueue$Lock)
	at java.lang.ref.ReferenceQueue.remove(Unknown Source)
	- locked <0x02c056d8> (a java.lang.ref.ReferenceQueue$Lock)
	at java.lang.ref.ReferenceQueue.remove(Unknown Source)
	at java.lang.ref.Finalizer$FinalizerThread.run(Unknown Source)


"Reference Handler" #2 daemon prio=10 os_prio=2 tid=0x02bf8000 nid=0x694 in Object.wait() [0x131ef000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x02c05200> (a java.lang.ref.Reference$Lock)
	at java.lang.Object.wait(Unknown Source)
	at java.lang.ref.Reference$ReferenceHandler.run(Unknown Source)
	- locked <0x02c05200> (a java.lang.ref.Reference$Lock)


"main" #1 prio=5 os_prio=0 tid=0x003c9800 nid=0x7d8 runnable [0x00a7f000]
   java.lang.Thread.State: RUNNABLE
	at java.util.concurrent.ConcurrentSkipListMap.cpr(Unknown Source)
	at java.util.concurrent.ConcurrentSkipListMap.findPredecessor(Unknown Source)
	at java.util.concurrent.ConcurrentSkipListMap.doPut(Unknown Source)
	at java.util.concurrent.ConcurrentSkipListMap.putIfAbsent(Unknown Source)
	at java.util.concurrent.ConcurrentSkipListSet.add(Unknown Source)
	at events.EventScheduler.schedule(EventScheduler.java:53)
	at core.PFAudioMixer.scheduleASAP(PFAudioMixer.java:150)
	at test.TestFlanger.main(TestFlanger.java:78)


"VM Thread" os_prio=2 tid=0x02bf4c00 nid=0xf68 runnable 


"VM Periodic Task Thread" os_prio=2 tid=0x13280c00 nid=0x40c waiting on condition 


JNI global references: 22


Heap
 def new generation   total 4928K, used 1616K [0x02c00000, 0x03150000, 0x08150000)
  eden space 4416K,  36% used [0x02c00000, 0x02d94280, 0x03050000)
  from space 512K,   0% used [0x03050000, 0x03050000, 0x030d0000)
  to   space 512K,   0% used [0x030d0000, 0x030d0000, 0x03150000)
 tenured generation   total 10944K, used 0K [0x08150000, 0x08c00000, 0x12c00000)
   the space 10944K,   0% used [0x08150000, 0x08150000, 0x08150200, 0x08c00000)
 Metaspace       used 2133K, capacity 2312K, committed 2368K, reserved 4480K
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141130/a6e4e293/attachment-0001.html>

From davidcholmes at aapt.net.au  Sun Nov 30 04:58:17 2014
From: davidcholmes at aapt.net.au (David Holmes)
Date: Sun, 30 Nov 2014 19:58:17 +1000
Subject: [concurrency-interest] ConcurrentSkipListSet fail
In-Reply-To: <8D1DA9453DD2D69-FF8-236EF@webmail-vm012.sysops.aol.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCAEFBKLAA.davidcholmes@aapt.net.au>

This looks more like an issue with DirectAudio to me. Why does the
DirectAudioDevice$DirectDL.write block?

David
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of
philfrei at aol.com
  Sent: Sunday, 30 November 2014 7:08 PM
  To: concurrency-interest at cs.oswego.edu
  Subject: Re: [concurrency-interest] ConcurrentSkipListSet fail


  @David Holmes


  I tried to use the workaround you
provided: -Djava.util.secureRandomSeed=true
  but the problem still occurs intermittently.


  @Peter Levart
  I was able to obtain the thread dump you requested. I thought I'd also
post the code which elicited the fail. It is at the following URL:
    http://adonax.com/ConcurrentSkipListSetProblem/TestFlanger.jar


  I apologize for not taking time to pare the code down further.


  The point of failure is intermittent. Following is an example of console
output when when the program runs to completion (happens occasionally):


  playPadWithFixedFlange started
    EventScheduler.schedule, events.length:6
      EventScheduler.schedule i:0
      EventScheduler.schedule events[i]:events.AudioEvent at 647e05
      EventScheduler.schedule i:1
      EventScheduler.schedule events[i]:events.AudioEvent at 1b84c92
      EventScheduler.schedule i:2
      EventScheduler.schedule events[i]:events.AudioEvent at 1c7c054
      EventScheduler.schedule i:3
      EventScheduler.schedule events[i]:events.AudioEvent at 12204a1
      EventScheduler.schedule i:4
      EventScheduler.schedule events[i]:events.AudioEvent at a298b7
      EventScheduler.schedule i:5
      EventScheduler.schedule events[i]:events.AudioEvent at 14991ad
    PFAudioMixer.scheduleASAP, nextEventFrame:12288
  playing for 10 seconds
  PFAudioMixer frame:12289 , nextEventFrame = 23313
  PFAudioMixer frame:23314 , nextEventFrame = 34338
  PFAudioMixer frame:34339 , nextEventFrame = 45363
  PFAudioMixer frame:45364 , nextEventFrame = 56388
  PFAudioMixer frame:56389 , nextEventFrame = 67413
  PFAudioMixer frame:67414 , nextEventFrame = 9223372036854775807
  about to schedule releases
    EventScheduler.schedule, events.length:6
      EventScheduler.schedule i:0
      EventScheduler.schedule events[i]:events.AudioEvent at d93b30
      EventScheduler.schedule i:1
      EventScheduler.schedule events[i]:events.AudioEvent at 16d3586
      EventScheduler.schedule i:2
      EventScheduler.schedule events[i]:events.AudioEvent at 154617c
      EventScheduler.schedule i:3
      EventScheduler.schedule events[i]:events.AudioEvent at a14482
      EventScheduler.schedule i:4
      EventScheduler.schedule events[i]:events.AudioEvent at 140e19d
      EventScheduler.schedule i:5
      EventScheduler.schedule events[i]:events.AudioEvent at 17327b6
    PFAudioMixer.scheduleASAP, nextEventFrame:452608
  fadout
  PFAudioMixer frame:452609 , nextEventFrame = 452608
  PFAudioMixer frame:452609 , nextEventFrame = 452608
  PFAudioMixer frame:452609 , nextEventFrame = 452608
  PFAudioMixer frame:452609 , nextEventFrame = 452608
  PFAudioMixer frame:452609 , nextEventFrame = 452608
  PFAudioMixer frame:452609 , nextEventFrame = 9223372036854775807
  playPadWithFixedFlange done
  PFAudioMixer.FrameMixerPlayer.run exiting


  Program synopsis: After initialization of objects and of directives, a set
of 6 notes are played polyphonically with a slightly staggered entrance. A
flanger is launched and runs for 10 seconds, then a release of all the notes
is scheduled and 5 seconds is allowed for the notes to all fade to silence
before the audio mixer is turned off and the program exits.


  In the following "fail" the tones are never get heard. This indicates to
me that the inner audio thread (PFAudioMixer.FrameMixerPlayer.run) probably
never locates scheduled items in the ConcurrentSkipListSet with a matching
frameNumber, despite two being added to the collection (the third add may
not have completed).


  Following is the requested thread dump:


  playPadWithFixedFlange started
    EventScheduler.schedule, events.length:6
      EventScheduler.schedule i:0
      EventScheduler.schedule events[i]:events.AudioEvent at 192e0f4
      EventScheduler.schedule i:1
      EventScheduler.schedule events[i]:events.AudioEvent at 1f4acd0
      EventScheduler.schedule i:2
      EventScheduler.schedule events[i]:events.AudioEvent at bedef2
  2014-11-30 00:40:19
  Full thread dump Java HotSpot(TM) Client VM (25.25-b02 mixed mode):


  "Java Sound Event Dispatcher" #9 daemon prio=5 os_prio=0 tid=0x1331c400
nid=0x5ac in Object.wait() [0x1397f000]
     java.lang.Thread.State: TIMED_WAITING (on object monitor)
  at java.lang.Object.wait(Native Method)
  - waiting on <0x02d4c5e0> (a com.sun.media.sound.EventDispatcher)
  at com.sun.media.sound.EventDispatcher.dispatchEvents(Unknown Source)
  - locked <0x02d4c5e0> (a com.sun.media.sound.EventDispatcher)
  at com.sun.media.sound.EventDispatcher.run(Unknown Source)
  at java.lang.Thread.run(Unknown Source)


  "Thread-0" #8 prio=5 os_prio=0 tid=0x132e1400 nid=0xe8c in Object.wait()
[0x1361f000]
     java.lang.Thread.State: TIMED_WAITING (on object monitor)
  at java.lang.Object.wait(Native Method)
  - waiting on <0x02d4b900> (a java.lang.Object)
  at com.sun.media.sound.DirectAudioDevice$DirectDL.write(Unknown Source)
  - locked <0x02d4b900> (a java.lang.Object)
  at core.PFAudioMixer$FrameMixerPlayer.run(PFAudioMixer.java:280)
  at java.lang.Thread.run(Unknown Source)


  "Service Thread" #7 daemon prio=9 os_prio=0 tid=0x1327ec00 nid=0xad8
runnable [0x00000000]
     java.lang.Thread.State: RUNNABLE


  "C1 CompilerThread0" #6 daemon prio=9 os_prio=2 tid=0x13254c00 nid=0x70c
waiting on condition [0x00000000]
     java.lang.Thread.State: RUNNABLE


  "Attach Listener" #5 daemon prio=5 os_prio=2 tid=0x13253400 nid=0xf70
runnable [0x00000000]
     java.lang.Thread.State: RUNNABLE


  "Signal Dispatcher" #4 daemon prio=9 os_prio=2 tid=0x13251c00 nid=0xaac
waiting on condition [0x00000000]
     java.lang.Thread.State: RUNNABLE


  "Finalizer" #3 daemon prio=8 os_prio=1 tid=0x02bfd000 nid=0xc3c in
Object.wait() [0x1323f000]
     java.lang.Thread.State: WAITING (on object monitor)
  at java.lang.Object.wait(Native Method)
  - waiting on <0x02c056d8> (a java.lang.ref.ReferenceQueue$Lock)
  at java.lang.ref.ReferenceQueue.remove(Unknown Source)
  - locked <0x02c056d8> (a java.lang.ref.ReferenceQueue$Lock)
  at java.lang.ref.ReferenceQueue.remove(Unknown Source)
  at java.lang.ref.Finalizer$FinalizerThread.run(Unknown Source)


  "Reference Handler" #2 daemon prio=10 os_prio=2 tid=0x02bf8000 nid=0x694
in Object.wait() [0x131ef000]
     java.lang.Thread.State: WAITING (on object monitor)
  at java.lang.Object.wait(Native Method)
  - waiting on <0x02c05200> (a java.lang.ref.Reference$Lock)
  at java.lang.Object.wait(Unknown Source)
  at java.lang.ref.Reference$ReferenceHandler.run(Unknown Source)
  - locked <0x02c05200> (a java.lang.ref.Reference$Lock)


  "main" #1 prio=5 os_prio=0 tid=0x003c9800 nid=0x7d8 runnable [0x00a7f000]
     java.lang.Thread.State: RUNNABLE
  at java.util.concurrent.ConcurrentSkipListMap.cpr(Unknown Source)
  at java.util.concurrent.ConcurrentSkipListMap.findPredecessor(Unknown
Source)
  at java.util.concurrent.ConcurrentSkipListMap.doPut(Unknown Source)
  at java.util.concurrent.ConcurrentSkipListMap.putIfAbsent(Unknown Source)
  at java.util.concurrent.ConcurrentSkipListSet.add(Unknown Source)
  at events.EventScheduler.schedule(EventScheduler.java:53)
  at core.PFAudioMixer.scheduleASAP(PFAudioMixer.java:150)
  at test.TestFlanger.main(TestFlanger.java:78)


  "VM Thread" os_prio=2 tid=0x02bf4c00 nid=0xf68 runnable


  "VM Periodic Task Thread" os_prio=2 tid=0x13280c00 nid=0x40c waiting on
condition


  JNI global references: 22


  Heap
   def new generation   total 4928K, used 1616K [0x02c00000, 0x03150000,
0x08150000)
    eden space 4416K,  36% used [0x02c00000, 0x02d94280, 0x03050000)
    from space 512K,   0% used [0x03050000, 0x03050000, 0x030d0000)
    to   space 512K,   0% used [0x030d0000, 0x030d0000, 0x03150000)
   tenured generation   total 10944K, used 0K [0x08150000, 0x08c00000,
0x12c00000)
     the space 10944K,   0% used [0x08150000, 0x08150000, 0x08150200,
0x08c00000)
   Metaspace       used 2133K, capacity 2312K, committed 2368K, reserved
4480K
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141130/0388522a/attachment-0001.html>

From peter.levart at gmail.com  Sun Nov 30 06:24:13 2014
From: peter.levart at gmail.com (Peter Levart)
Date: Sun, 30 Nov 2014 12:24:13 +0100
Subject: [concurrency-interest] ConcurrentSkipListSet fail
In-Reply-To: <8D1DA9453DD2D69-FF8-236EF@webmail-vm012.sysops.aol.com>
References: <8D1DA9453DD2D69-FF8-236EF@webmail-vm012.sysops.aol.com>
Message-ID: <547AFE5D.9090207@gmail.com>

Hi Phil,

If the below stack-trace is really the thread that blocks (it might be 
spinning):

On 11/30/2014 10:08 AM, philfrei at aol.com wrote:
> "main" #1 prio=5 os_prio=0 tid=0x003c9800 nid=0x7d8 runnable [0x00a7f000]
>  java.lang.Thread.State: RUNNABLE
> at java.util.concurrent.ConcurrentSkipListMap.cpr(Unknown Source)
> at java.util.concurrent.ConcurrentSkipListMap.findPredecessor(Unknown 
> Source)
> at java.util.concurrent.ConcurrentSkipListMap.doPut(Unknown Source)
> at java.util.concurrent.ConcurrentSkipListMap.putIfAbsent(Unknown Source)
> at java.util.concurrent.ConcurrentSkipListSet.add(Unknown Source)
> at events.EventScheduler.schedule(EventScheduler.java:53)
> at core.PFAudioMixer.scheduleASAP(PFAudioMixer.java:150)
> at test.TestFlanger.main(TestFlanger.java:78)

... then I would suspect one of two things:

- ConcurrentSkipListMap has a bug
- The Comparable.compareTo implementation of elements being added is not 
"by the spec".

Looking at the source of AudioEvent you posted (the element being added 
to ConcurrentSkipListSet):

public class AudioEvent implements Comparable<AudioEvent>
{
     private final long frame;

     public long getFrame() { return frame; }

     private final AudioCommand audioCommand;

     public AudioCommand getAudioCommand()
     {
         return audioCommand;
     }

     public AudioEvent(AudioCommand audioCommand, long frame)
     {
         this.audioCommand = audioCommand;
         this.frame = frame;
     }

     @Override
     public int compareTo(AudioEvent other)
     {
         if (frame < other.getFrame()) return -1;
         if (frame >= other.getFrame()) return 1;

         if (this.hashCode() < other.hashCode()) return -1;
         if (this.hashCode() > other.hashCode()) return 1;

         System.out.println("compareTo for AudioEvent tied. ");
         return 0;
     }
}


...I can observe that compareTo is quite strange:

- comparing an instance of AudioEvent with itself returns 1.
- code below comparing this.frame and other.frame values is not reachable.
- and most importantly, it violates the compareTo contract.

The javadoc for Comparable.compareTo says among other things:

"The implementor must ensure sgn(x.compareTo(y)) == -sgn(y.compareTo(x)) 
for all x and y."

Let's take the following example:

AudioEvent x = new AudioEvent(null, 0L);
AudioEvent y = new AudioEvent(null, 0L);

x.compareTo(y) returns 1
y.compareTo(x) returns 1

Hm....


Regards, Peter




-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141130/8a0e2067/attachment.html>

From peter.levart at gmail.com  Sun Nov 30 06:33:34 2014
From: peter.levart at gmail.com (Peter Levart)
Date: Sun, 30 Nov 2014 12:33:34 +0100
Subject: [concurrency-interest] ConcurrentSkipListSet fail
In-Reply-To: <547AFE5D.9090207@gmail.com>
References: <8D1DA9453DD2D69-FF8-236EF@webmail-vm012.sysops.aol.com>
	<547AFE5D.9090207@gmail.com>
Message-ID: <547B008E.1020905@gmail.com>

Phil, try the following implementation:

public class AudioEvent implements Comparable<AudioEvent> {
     private static final AtomicLong seq = new AtomicLong();

     private final long frame;
     private final AudioCommand audioCommand;
     private final long id;

     public AudioEvent(AudioCommand audioCommand, long frame) {
         this.audioCommand = audioCommand;
         this.frame = frame;
         this.id = seq.getAndIncrement();
     }

     @Override
     public int compareTo(AudioEvent other) {
         if (frame < other.frame) return -1;
         if (frame > other.frame) return 1;

         if (id < other.id) return -1;
         if (id > other.id) return 1;

         return 0;
     }
}


Regards, Peter

On 11/30/2014 12:24 PM, Peter Levart wrote:
> Hi Phil,
>
> If the below stack-trace is really the thread that blocks (it might be 
> spinning):
>
> On 11/30/2014 10:08 AM, philfrei at aol.com wrote:
>> "main" #1 prio=5 os_prio=0 tid=0x003c9800 nid=0x7d8 runnable [0x00a7f000]
>>  java.lang.Thread.State: RUNNABLE
>> at java.util.concurrent.ConcurrentSkipListMap.cpr(Unknown Source)
>> at java.util.concurrent.ConcurrentSkipListMap.findPredecessor(Unknown 
>> Source)
>> at java.util.concurrent.ConcurrentSkipListMap.doPut(Unknown Source)
>> at java.util.concurrent.ConcurrentSkipListMap.putIfAbsent(Unknown Source)
>> at java.util.concurrent.ConcurrentSkipListSet.add(Unknown Source)
>> at events.EventScheduler.schedule(EventScheduler.java:53)
>> at core.PFAudioMixer.scheduleASAP(PFAudioMixer.java:150)
>> at test.TestFlanger.main(TestFlanger.java:78)
>
> ... then I would suspect one of two things:
>
> - ConcurrentSkipListMap has a bug
> - The Comparable.compareTo implementation of elements being added is 
> not "by the spec".
>
> Looking at the source of AudioEvent you posted (the element being 
> added to ConcurrentSkipListSet):
>
> public class AudioEvent implements Comparable<AudioEvent>
> {
>     private final long frame;
>
>     public long getFrame() { return frame; }
>
>     private final AudioCommand audioCommand;
>
>     public AudioCommand getAudioCommand()
>     {
>         return audioCommand;
>     }
>
>     public AudioEvent(AudioCommand audioCommand, long frame)
>     {
>         this.audioCommand = audioCommand;
>         this.frame = frame;
>     }
>
>     @Override
>     public int compareTo(AudioEvent other)
>     {
>         if (frame < other.getFrame()) return -1;
>         if (frame >= other.getFrame()) return 1;
>
>         if (this.hashCode() < other.hashCode()) return -1;
>         if (this.hashCode() > other.hashCode()) return 1;
>
>         System.out.println("compareTo for AudioEvent tied. ");
>         return 0;
>     }
> }
>
>
> ...I can observe that compareTo is quite strange:
>
> - comparing an instance of AudioEvent with itself returns 1.
> - code below comparing this.frame and other.frame values is not reachable.
> - and most importantly, it violates the compareTo contract.
>
> The javadoc for Comparable.compareTo says among other things:
>
> "The implementor must ensure sgn(x.compareTo(y)) == 
> -sgn(y.compareTo(x)) for all x and y."
>
> Let's take the following example:
>
> AudioEvent x = new AudioEvent(null, 0L);
> AudioEvent y = new AudioEvent(null, 0L);
>
> x.compareTo(y) returns 1
> y.compareTo(x) returns 1
>
> Hm....
>
>
> Regards, Peter
>
>
>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141130/fbefc59d/attachment.html>

From dl at cs.oswego.edu  Sun Nov 30 08:32:16 2014
From: dl at cs.oswego.edu (Doug Lea)
Date: Sun, 30 Nov 2014 08:32:16 -0500
Subject: [concurrency-interest] RFR: 8065804: JEP 171:
 Clarifications/corrections for fence intrinsics
In-Reply-To: <OF3934E623.3E21FC36-ONC1257D9F.0053175E-C1257D9F.00538FF5@de.ibm.com>
References: <OF3934E623.3E21FC36-ONC1257D9F.0053175E-C1257D9F.00538FF5@de.ibm.com>
Message-ID: <547B1C60.8090101@cs.oswego.edu>

On 11/29/2014 10:12 AM, Alexander Terekhov wrote:
>> memory_order_release meaningful, but somewhat different from the
> non-fence
>
> it would be nice to have release fence with an artificial dependency
> to define a set of actually release stores and not constraining other
> subsequent stores (and the order of release stores with respect to
> each other), e.g.:

This is the form (at Java level) we agreed to the last time we
discussed adding such support (about 5 years ago). It is still a
possibility for JDK9+ even if initial JVM implementations are
in terms of coarser grained fences. See
http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/atomic/Fences.html
(that was never shipped.)

-Doug


>
> // set multiple flags each indicating 'release' without imposing
> // ordering on 'release' stores respect to each other and not
> // constraining other subsequent stores
>
> .
> .
> .
>
> if (atomic_thread_fence(memory_order_release)) {
>
>    flag1.store(READY, memory_order_relaxed);
>    flag2.store(READY, memory_order_relaxed);
>
> }
>
> regards,
> alexander.
>
> Hans Boehm <boehm at acm.org>@cs.oswego.edu on 29.11.2014 05:56:04
>
> Sent by:	concurrency-interest-bounces at cs.oswego.edu
>
>
> To:	Peter Levart <peter.levart at gmail.com>
> cc:	Vladimir Kozlov <vladimir.kozlov at oracle.com>, concurrency-interest
>         <concurrency-interest at cs.oswego.edu>, Martin Buchholz
>         <martinrb at google.com>, core-libs-dev
>         <core-libs-dev at openjdk.java.net>, dholmes at ieee.org
> Subject:	Re: [concurrency-interest] RFR: 8065804: JEP 171:
>         Clarifications/corrections for fence intrinsics
>
>
> I basically agree with David's observation.  However the C++
>
> atomic_thread_fence(memory_order_acquire)
>
> actually has somewhat different semantics from load(memory_order_acquire).
> It basically ensures that prior atomic loads L are not reordered with later
> (i.e. following the fence in program order) loads and stores, making it
> something like a LoadLoad|LoadStore fence.  Thus the fence orders two sets
> of operations where the acquire load orders a single operation with respect
> to a set.  This makes the fence versions of memory_order_acquire and
> memory_order_release meaningful, but somewhat different from the non-fence
> versions.  The terminology is probably not great, but that seems to be the
> most common usage now.
>
> On Wed, Nov 26, 2014 at 11:48 PM, Peter Levart <peter.levart at gmail.com>
> wrote:
>        On 11/27/2014 04:00 AM, David Holmes wrote:
>              Can I make an observation about acquire() and release() - to me
>              they are
>              meaningless when considered in isolation. Given their
>              definitions they allow
>              anything to move into a region bounded by acquire() and release
>              (), then you
>              can effectively move the whole program into the region and thus
>              the
>              acquire() and release() do not constrain any reorderings.
>                acquire() and
>              release() only make sense when their own movement is
>              constrained with
>              respect to something else - such as lock acquisition/release,
>              or when
>              combined with specific load/store actions.
>
>        ...or another acquire/release region?
>
>        Regards, Peter
>
>
>
>              David
>
>              Martin Buchholz writes:
>                    On Tue, Nov 25, 2014 at 6:04 AM, Paul Sandoz
>                    <paul.sandoz at oracle.com> wrote:
>                          Hi Martin,
>
>                          Thanks for looking into this.
>
>                          1141      * Currently hotspot's implementation of a
>                          Java
>                    language-level volatile
>                          1142      * store has the same effect as a
>                          storeFence followed
>                    by a relaxed store,
>                          1143      * although that may be a little stronger
>                          than needed.
>
>                          IIUC to emulate hotpot's volatile store you will
>                          need to say
>                    that a fullFence immediately follows the relaxed store.
>
>                    Right - I've been groking that.
>
>                          The bit that always confuses me about release and
>                          acquire is
>                    ordering is restricted to one direction, as talked about
>                    in
>                    orderAccess.hpp [1]. So for a release, accesses prior to
>                    the
>                    release cannot move below it, but accesses succeeding the
>                    release
>                    can move above it. And that seems to apply to
>                    Unsafe.storeFence
>                    [2] (acting like a monitor exit). Is that contrary to C++
>                    release
>                    fences where ordering is restricted both to prior and
>                    succeeding
>                    accesses? [3]
>                          So what about the following?
>
>                             a = r1; // Cannot move below the fence
>                             Unsafe.storeFence();
>                             b = r2; // Can move above the fence?
>                    I think the hotspot docs need to be more precise about
>                    when they're
>                    talking about movement of stores and when about loads.
>
>                          // release.  I.e., subsequent memory accesses may
>                          float above the
>                          // release, but prior ones may not float below it.
>                    As I've said elsewhere, the above makes no sense without
>                    restricting
>                    the type of access.
>
>                    _______________________________________________
>                    Concurrency-interest mailing list
>                    Concurrency-interest at cs.oswego.edu
>                    http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>        _______________________________________________
>        Concurrency-interest mailing list
>        Concurrency-interest at cs.oswego.edu
>        http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From dl at cs.oswego.edu  Sun Nov 30 08:43:08 2014
From: dl at cs.oswego.edu (Doug Lea)
Date: Sun, 30 Nov 2014 08:43:08 -0500
Subject: [concurrency-interest] Please clarify Javadoc of
	CompletableFuture.thenCompose()
In-Reply-To: <1417232551381-11504.post@n7.nabble.com>
References: <1417232551381-11504.post@n7.nabble.com>
Message-ID: <547B1EEC.3060306@cs.oswego.edu>

On 11/28/2014 10:42 PM, cowwoc wrote:
> I don't know about the rest of you, but no matter how many times I read the
> Javadoc for CompletableFuture.thenCompose() I don't end up with the same
> interpretation as mentioned at
> http://www.nurkiewicz.com/2013/05/java-8-definitive-guide-to.html
>
> I find http://www.nurkiewicz.com/2013/05/java-8-definitive-guide-to.html
> much easier to understand.
>
> Is it possible to change the Javadoc to something along the lines of
> "Returns a CompletionStage that completes with the same value as the
> CompletionStage returned by the function"?

No, because it is not always true (because of exception handling).
It is fine for people to think only about non-exceptional
cases when trying to understand basic idea, but that wouldn't make for
good specs.

>
> Honestly, I think all of these methods could use a more layman explanation
> and perhaps some example code.

We often face the tension in j.u.c between making specs exactly
right and making them approachable. Usually when this happens
we add more class-level docs and examples. We didn't in
CompletableFuture/CompletionStage. We ought to, or at least point
to an introductory overview. Suggestions welcome.

-Doug





From tim at peierls.net  Sun Nov 30 09:30:07 2014
From: tim at peierls.net (Tim Peierls)
Date: Sun, 30 Nov 2014 09:30:07 -0500
Subject: [concurrency-interest] Please clarify Javadoc of
	CompletableFuture.thenCompose()
In-Reply-To: <547B1EEC.3060306@cs.oswego.edu>
References: <1417232551381-11504.post@n7.nabble.com>
	<547B1EEC.3060306@cs.oswego.edu>
Message-ID: <CA+F8eeTsA6zz9-qTwxh+HdgX=TKGp8vvaNt-jzHm3G2FEShPfg@mail.gmail.com>

But the CompletableFuture.thenCompose doc seems both confusing and wrong:

Description copied from interface: CompletionStage
>
> <http://docs.oracle.com/javase/8/docs/api/java/util/concurrent/CompletionStage.html#thenCompose-java.util.function.Function->Returns
> a new CompletionStage that, when this stage completes normally, is executed
> with this stage as the argument to the supplied function. See the
> CompletionStage
> <http://docs.oracle.com/javase/8/docs/api/java/util/concurrent/CompletionStage.html> documentation
> for rules covering exceptional completion.

Confusing, because of the inherited CompletionStage doc: The type of "this"
and the return type promised by the doc are both referred to as "stage"
when in fact both are specifically CompletableFuture. That's imprecise, not
wrong, but it makes the reader stumble right off.

Wrong, because the supplied function is (ignoring bounds) T ->
CompletionStage<U>. "This stage" has type CompletableFuture<T>, so it can't
itself be an argument to the supplied function. (Similarly with
CompletionStage.thenCompose.)

The confused reader looks for parallels with thenApply and finds this
situation:

CompletableFuture<T>.thenApply(T -> U)   // "this stage's result"
CompletableFuture<T>.thenCompose(T -> CompletionStage<U>)  // "this stage"
???

The former makes sense, the latter doesn't.

--tim

On Sun, Nov 30, 2014 at 8:43 AM, Doug Lea <dl at cs.oswego.edu> wrote:

> On 11/28/2014 10:42 PM, cowwoc wrote:
>
>> I don't know about the rest of you, but no matter how many times I read
>> the
>> Javadoc for CompletableFuture.thenCompose() I don't end up with the same
>> interpretation as mentioned at
>> http://www.nurkiewicz.com/2013/05/java-8-definitive-guide-to.html
>>
>> I find http://www.nurkiewicz.com/2013/05/java-8-definitive-guide-to.html
>> much easier to understand.
>>
>> Is it possible to change the Javadoc to something along the lines of
>> "Returns a CompletionStage that completes with the same value as the
>> CompletionStage returned by the function"?
>>
>
> No, because it is not always true (because of exception handling).
> It is fine for people to think only about non-exceptional
> cases when trying to understand basic idea, but that wouldn't make for
> good specs.
>
>
>> Honestly, I think all of these methods could use a more layman explanation
>> and perhaps some example code.
>>
>
> We often face the tension in j.u.c between making specs exactly
> right and making them approachable. Usually when this happens
> we add more class-level docs and examples. We didn't in
> CompletableFuture/CompletionStage. We ought to, or at least point
> to an introductory overview. Suggestions welcome.
>
> -Doug
>
>
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141130/4660571d/attachment.html>

From dl at cs.oswego.edu  Sun Nov 30 10:42:21 2014
From: dl at cs.oswego.edu (Doug Lea)
Date: Sun, 30 Nov 2014 10:42:21 -0500
Subject: [concurrency-interest] CompletableFuture in Java 8
In-Reply-To: <CAHJZN-uviNyTfK-6LALZsBjbhZFbvOm+o4t1KwNUsfP7EiwV6w@mail.gmail.com>
References: <CAAL-3PanDAubT2RM-DePuUv_LmSX2=OsDcy82Q+ETBaRuvEwEw@mail.gmail.com>	<CAAWwtm-evqwXxNsoMLsnm5PNU-n+=O1iR_ZcptfaJ60JHg9jbg@mail.gmail.com>	<CAAL-3PaZDBmjQ7i43+VUeW+OGh+6VAcA3rp9mzmetJar2k_aZw@mail.gmail.com>	<CAAL-3PYJ90uZ_Kmz1-QjY43nRUCAMxxXJmE9KiVKCddCeMnyfA@mail.gmail.com>	<CAOoXFP9hRneBagR4hHggCQoyxsJp8tFQNbdmMvoY5dS+4LZH4g@mail.gmail.com>	<CAAL-3Pbd55dg8CpYXhiR3kTENgnBLxmPDZ9agAv4-d1F4LXmeQ@mail.gmail.com>	<CAHJZN-tDZFVWDW18tNksVOZQHWJdbYttQW+DNwZpmy9kiv3h0w@mail.gmail.com>	<CAAL-3PZg7MCiUbNEOY0-=vAskfDh5w7OrmAv4JEw_1vp6YdmKw@mail.gmail.com>
	<CAHJZN-uviNyTfK-6LALZsBjbhZFbvOm+o4t1KwNUsfP7EiwV6w@mail.gmail.com>
Message-ID: <547B3ADD.4070700@cs.oswego.edu>

On 11/27/2014 10:45 PM, Josh Humphries wrote:

A few notes not covered in other responses...

>
> Below are some of my nits about the Java8 APIs:
>
> I disagree with how the CompletionStage interface is so tightly coupled to
> the CompletableFuture implementation. Why does CompletionStage have a
> toCompletableFuture() method? It feels like the interface should instead have
>  just extended Future.

Viktor recapped the good arguments for not extending Future. But
we still needed a standard means for interoperable implementations to
extract values. Method toCompletableFuture() is the simplest way to get
this effect by anyCompletableFuture developer (usually a one-liner),
and is marked as optional in case they don't care about interoperability.

>
> I dislike that the only CompletionStage implementation in the JRE,
> CompletableFuture, is like Guava's SettableFuture -- where the result is set
>  imperatively. This has its uses for sure. But an implementation whose result
>  comes from executing a unit of logic, like FutureTask
> (CompletionStageTask?), is a glaring omission.

The goal was to provide a robust, efficient base implementation supporting
all reasonable usage policies. We expected more cases of delegation
for frameworks with restricted policies (similarly for obtrude*).
This hasn't happened much yet.


> 1. They don't allow interruption of the task when the future is cancelled.

> 3. They don't support scheduling the task for deferred execution (like in a
> ScheduledExecutorService).

In most cases, this is the same issue: You'd like to
somehow remove/suppress a cancelled timeout or timer-generated action
upon cancellation, mainly for the sake of resource-control.
Most other cases reduce to the mixed sync/async usage issues
mentioned in other posts, that we cannot do much about in j.u.c itself.

In retrospect, maybe we should have added a few static methods and
a default internal static ScheduledExecutor to support these usages,
since everyone seems to need them. One reason we didn't is that
these further invite defining methods producing streams of periodic
CFs, which leads to further APIs bridging CFs with streams/observables,
which we triaged out of jdk8 (see my other posts on this.)
Still, something along these lines (perhaps just a class with
static CF utility methods) should be added.

On 11/28/2014 11:57 AM, ?iktor ?lang wrote:

> Now, my preference would be to have removed the 3 versions of every method
> (x, xAsync, xAsync + Executor) and instead only have `x` + Executor and then
> you can supply a CallingThreadExecutor, ForkJoinPool.commonPool() or <custom>
> and you get exactly the same behavior without the interface pollution.

If the API were expressed in a language with implicits, this might have
been more tempting. (Well, except that everyone would then instead
be complaining about implicits :-) Also, it there were a Unit type,
the API would shrink by almost a factor of three
(Runnable as Unit->Unit etc).

But the variants are regular enough that users don't seem to
complain about any of them in particular, only of the resulting
too-bigness feeling.
On the other hand, it does look overly imposing to developers of
layered/delegated classes. Adding default implementations using
lambda-based function adaptors might be a good idea, but it is
probably too late for that in CompletionStage itself. We could
add class AbstractCompletionStage though.


-Doug







From dl at cs.oswego.edu  Sun Nov 30 11:03:13 2014
From: dl at cs.oswego.edu (Doug Lea)
Date: Sun, 30 Nov 2014 11:03:13 -0500
Subject: [concurrency-interest] Please clarify Javadoc of
	CompletableFuture.thenCompose()
In-Reply-To: <CA+F8eeTsA6zz9-qTwxh+HdgX=TKGp8vvaNt-jzHm3G2FEShPfg@mail.gmail.com>
References: <1417232551381-11504.post@n7.nabble.com>	<547B1EEC.3060306@cs.oswego.edu>
	<CA+F8eeTsA6zz9-qTwxh+HdgX=TKGp8vvaNt-jzHm3G2FEShPfg@mail.gmail.com>
Message-ID: <547B3FC1.5060801@cs.oswego.edu>

On 11/30/2014 09:30 AM, Tim Peierls wrote:
>
> Confusing, because of the inherited CompletionStage doc: The type of "this" and
> the return type promised by the doc are both referred to as "stage" when in fact
> both are specifically CompletableFuture. That's imprecise, not wrong, but it
> makes the reader stumble right off.

OK, but we do this uniformly in java.util/j.u.c. For example, see the many
mentions of "this collection" in Collection implementations.
Is this case is different enough to warrant copy/paste/edit?

>
> Wrong, because the supplied function is (ignoring bounds) T ->
> CompletionStage<U>. "This stage" has type CompletableFuture<T>, so it can't
> itself be an argument to the supplied function.

Thanks for catching this! Somehow "this stage's result" got truncated to
"this stage". Will fix.

-Doug



From tim at peierls.net  Sun Nov 30 11:59:01 2014
From: tim at peierls.net (Tim Peierls)
Date: Sun, 30 Nov 2014 11:59:01 -0500
Subject: [concurrency-interest] Please clarify Javadoc of
	CompletableFuture.thenCompose()
In-Reply-To: <547B3FC1.5060801@cs.oswego.edu>
References: <1417232551381-11504.post@n7.nabble.com>
	<547B1EEC.3060306@cs.oswego.edu>
	<CA+F8eeTsA6zz9-qTwxh+HdgX=TKGp8vvaNt-jzHm3G2FEShPfg@mail.gmail.com>
	<547B3FC1.5060801@cs.oswego.edu>
Message-ID: <CA+F8eeQHddfkfgsnNrfFpDCma7nPs3ad6_EtAzwK0hNtmbkzbg@mail.gmail.com>

On Sun, Nov 30, 2014 at 11:03 AM, Doug Lea <dl at cs.oswego.edu> wrote:

> On 11/30/2014 09:30 AM, Tim Peierls wrote:
>
>>
>> Confusing, because of the inherited CompletionStage doc: The type of
>> "this" and
>> the return type promised by the doc are both referred to as "stage" when
>> in fact
>> both are specifically CompletableFuture. That's imprecise, not wrong, but
>> it
>> makes the reader stumble right off.
>>
>
> OK, but we do this uniformly in java.util/j.u.c.

For example, see the many
> mentions of "this collection" in Collection implementations.
>


Well... Two seconds of looking and I found an inconsistency: type parameter
E for Set is "type of elements maintained by this set" and for Queue is
"type of elements held in this collection" (compare E in List, SortedSet,
Deque, ...). Further brief manual scan picked up no other instances of
"this collection" in the docs for commonly used Collection subtypes in j.u.

And just because we do it, doesn't mean it's a good thing. In most cases we
are getting away with it, but read on for my reasoning about when it
becomes inappropriate.




> Is this case is different enough to warrant copy/paste/edit?


What tips it over, for me, is the fact that the inheritance is not
interface Foo <- class xxxFoo but CompletionStage <- CompletableFuture,
i.e., two type names with some similarity but no identical components.
References to "this set" in HashSet doc, for example, are instantly clear,
but references to "this stage" in CompletableFuture doc require an extra
few steps: "oh, right, a CompletableFuture is a CompletionStage, which is a
'stage'".

Similar reasoning for covariant return types: I can easily forgive "returns
a new set" (or Set) when return type is HashSet, but it takes me a second
to think through "returns a new CompletionStage" when return type is
CompletableFuture.

You could argue that since these are specs, not tutorials, the reader of
CompletableFuture's doc ought to be able to see "stage" and read
CompletableFuture without effort, but I think that's asking a lot.

--tim
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141130/6b5a5d17/attachment.html>

From philfrei at aol.com  Sun Nov 30 12:37:50 2014
From: philfrei at aol.com (philfrei at aol.com)
Date: Sun, 30 Nov 2014 12:37:50 -0500
Subject: [concurrency-interest] ConcurrentSkipListSet fail
In-Reply-To: <547B008E.1020905@gmail.com>
References: <8D1DA9453DD2D69-FF8-236EF@webmail-vm012.sysops.aol.com>
	<547AFE5D.9090207@gmail.com> <547B008E.1020905@gmail.com>
Message-ID: <8D1DADB7F7BCA46-1A60-36971@webmail-vm060.sysops.aol.com>

How embarrassing! I fixed the Compare function, replacing the ">=" with ">" being the essential ingredient, and everything runs fine.



To everyone on this list, my apologies for the false alarm and not doing a better job of testing before panicking about a possible regression. 


To Peter in particular, thank you very much for your help and patience!


Best wishes, Phil



-----Original Message-----
From: Peter Levart <peter.levart at gmail.com>
To: philfrei <philfrei at aol.com>; concurrency-interest <concurrency-interest at cs.oswego.edu>
Sent: Sun, Nov 30, 2014 3:33 am
Subject: Re: [concurrency-interest] ConcurrentSkipListSet fail


          
    Phil, try the following implementation:
    
    public class AudioEvent implements Comparable<AudioEvent> {
        private static final AtomicLong seq = new AtomicLong();
    
        private final long frame;
        private final AudioCommand audioCommand;
        private final long id;
        
        public AudioEvent(AudioCommand audioCommand, long frame) {
            this.audioCommand = audioCommand;
            this.frame = frame;
            this.id = seq.getAndIncrement();
        }
    
        @Override
        public int compareTo(AudioEvent other) {
            if (frame < other.frame) return -1;
            if (frame > other.frame) return 1;
    
            if (id < other.id) return -1;
            if (id > other.id) return 1;
    
            return 0;
        }
    }
    
    
    Regards, Peter
    
    
On 11/30/2014 12:24 PM, Peter Levart      wrote:
    
    
            Hi Phil,
      
      If the below stack-trace is really the thread that blocks (it      might be spinning):
      
      
On 11/30/2014 10:08 AM, philfrei at aol.com wrote:
      
      
          
"main" #1 prio=5              os_prio=0 tid=0x003c9800 nid=0x7d8 runnable [0x00a7f000]
          
                java.lang.Thread.State: RUNNABLE
          
 at              java.util.concurrent.ConcurrentSkipListMap.cpr(Unknown              Source)
          
 at              java.util.concurrent.ConcurrentSkipListMap.findPredecessor(Unknown              Source)
          
 at              java.util.concurrent.ConcurrentSkipListMap.doPut(Unknown              Source)
          
 at              java.util.concurrent.ConcurrentSkipListMap.putIfAbsent(Unknown              Source)
          
 at              java.util.concurrent.ConcurrentSkipListSet.add(Unknown              Source)
          
 at              events.EventScheduler.schedule(EventScheduler.java:53)
          
 at              core.PFAudioMixer.scheduleASAP(PFAudioMixer.java:150)
          
 at              test.TestFlanger.main(TestFlanger.java:78)
        
      
      ... then I would suspect one of two things:
      
      - ConcurrentSkipListMap has a bug
      - The Comparable.compareTo implementation of elements being added      is not "by the spec".
      
      Looking at the source of AudioEvent you posted (the element being      added to ConcurrentSkipListSet):
      
      public class AudioEvent implements Comparable<AudioEvent>
      {
          private final long frame;
          
          public long getFrame() { return frame; }
          
          private final AudioCommand audioCommand;
          
          public AudioCommand getAudioCommand()
          {
              return audioCommand;
          }
          
          public AudioEvent(AudioCommand audioCommand, long frame)
          {
              this.audioCommand = audioCommand;
              this.frame = frame;
          }
          
          @Override
          public int compareTo(AudioEvent other)
          {
              if (frame < other.getFrame()) return -1;
              if (frame >= other.getFrame()) return 1;
              
              if (this.hashCode() < other.hashCode()) return -1;
              if (this.hashCode() > other.hashCode()) return 1;
                  
              System.out.println("compareTo for AudioEvent tied. ");
              return 0;
          }
      }
      
      
      ...I can observe that compareTo is quite strange:
      
      - comparing an instance of AudioEvent with itself returns 1.
      - code below comparing this.frame and other.frame values is not      reachable.
      - and most importantly, it violates the compareTo contract.
      
      The javadoc for Comparable.compareTo says among other things:
      
      "The implementor must ensure sgn(x.compareTo(y)) ==      -sgn(y.compareTo(x)) for all x and y."
      
      Let's take the following example:
      
      AudioEvent x = new AudioEvent(null, 0L);
      AudioEvent y = new AudioEvent(null, 0L);
      
      x.compareTo(y) returns 1
      y.compareTo(x) returns 1
      
      Hm....
      
      
      Regards, Peter
      
      
      
      
      
    
  


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141130/2447c6b4/attachment.html>

From dt at flyingtroika.com  Sun Nov 30 15:32:51 2014
From: dt at flyingtroika.com (DT)
Date: Sun, 30 Nov 2014 12:32:51 -0800
Subject: [concurrency-interest] RFR: 8065804: JEP 171:
	Clarifications/corrections for fence intrinsics
In-Reply-To: <547B1C60.8090101@cs.oswego.edu>
References: <OF3934E623.3E21FC36-ONC1257D9F.0053175E-C1257D9F.00538FF5@de.ibm.com>
	<547B1C60.8090101@cs.oswego.edu>
Message-ID: <D1DCC0E3-827D-4181-8303-D77A1F9563B2@flyingtroika.com>

We are discussing semantics of fences,  release/ acquire in the context of two different processes are trying to change/ update a bit of information. I got such impression. Can be wrong. Can we say that the same semantics is going two work regardless of the number of concurrent processes trying to gain access to a part of memory and should not change freedom from mutual blocking that is any process that would like to enter the critical section must be able to do so within a finite time. For instance for hard real time system timing must be deterministic. So we have to deal with equitability property.



> On Nov 30, 2014, at 5:32 AM, Doug Lea <dl at cs.oswego.edu> wrote:
> 
> On 11/29/2014 10:12 AM, Alexander Terekhov wrote:
>>> memory_order_release meaningful, but somewhat different from the
>> non-fence
>> 
>> it would be nice to have release fence with an artificial dependency
>> to define a set of actually release stores and not constraining other
>> subsequent stores (and the order of release stores with respect to
>> each other), e.g.:
> 
> This is the form (at Java level) we agreed to the last time we
> discussed adding such support (about 5 years ago). It is still a
> possibility for JDK9+ even if initial JVM implementations are
> in terms of coarser grained fences. See
> http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/atomic/Fences.html
> (that was never shipped.)
> 
> -Doug
> 
> 
>> 
>> // set multiple flags each indicating 'release' without imposing
>> // ordering on 'release' stores respect to each other and not
>> // constraining other subsequent stores
>> 
>> .
>> .
>> .
>> 
>> if (atomic_thread_fence(memory_order_release)) {
>> 
>>   flag1.store(READY, memory_order_relaxed);
>>   flag2.store(READY, memory_order_relaxed);
>> 
>> }
>> 
>> regards,
>> alexander.
>> 
>> Hans Boehm <boehm at acm.org>@cs.oswego.edu on 29.11.2014 05:56:04
>> 
>> Sent by:    concurrency-interest-bounces at cs.oswego.edu
>> 
>> 
>> To:    Peter Levart <peter.levart at gmail.com>
>> cc:    Vladimir Kozlov <vladimir.kozlov at oracle.com>, concurrency-interest
>>        <concurrency-interest at cs.oswego.edu>, Martin Buchholz
>>        <martinrb at google.com>, core-libs-dev
>>        <core-libs-dev at openjdk.java.net>, dholmes at ieee.org
>> Subject:    Re: [concurrency-interest] RFR: 8065804: JEP 171:
>>        Clarifications/corrections for fence intrinsics
>> 
>> 
>> I basically agree with David's observation.  However the C++
>> 
>> atomic_thread_fence(memory_order_acquire)
>> 
>> actually has somewhat different semantics from load(memory_order_acquire).
>> It basically ensures that prior atomic loads L are not reordered with later
>> (i.e. following the fence in program order) loads and stores, making it
>> something like a LoadLoad|LoadStore fence.  Thus the fence orders two sets
>> of operations where the acquire load orders a single operation with respect
>> to a set.  This makes the fence versions of memory_order_acquire and
>> memory_order_release meaningful, but somewhat different from the non-fence
>> versions.  The terminology is probably not great, but that seems to be the
>> most common usage now.
>> 
>> On Wed, Nov 26, 2014 at 11:48 PM, Peter Levart <peter.levart at gmail.com>
>> wrote:
>>       On 11/27/2014 04:00 AM, David Holmes wrote:
>>             Can I make an observation about acquire() and release() - to me
>>             they are
>>             meaningless when considered in isolation. Given their
>>             definitions they allow
>>             anything to move into a region bounded by acquire() and release
>>             (), then you
>>             can effectively move the whole program into the region and thus
>>             the
>>             acquire() and release() do not constrain any reorderings.
>>               acquire() and
>>             release() only make sense when their own movement is
>>             constrained with
>>             respect to something else - such as lock acquisition/release,
>>             or when
>>             combined with specific load/store actions.
>> 
>>       ...or another acquire/release region?
>> 
>>       Regards, Peter
>> 
>> 
>> 
>>             David
>> 
>>             Martin Buchholz writes:
>>                   On Tue, Nov 25, 2014 at 6:04 AM, Paul Sandoz
>>                   <paul.sandoz at oracle.com> wrote:
>>                         Hi Martin,
>> 
>>                         Thanks for looking into this.
>> 
>>                         1141      * Currently hotspot's implementation of a
>>                         Java
>>                   language-level volatile
>>                         1142      * store has the same effect as a
>>                         storeFence followed
>>                   by a relaxed store,
>>                         1143      * although that may be a little stronger
>>                         than needed.
>> 
>>                         IIUC to emulate hotpot's volatile store you will
>>                         need to say
>>                   that a fullFence immediately follows the relaxed store.
>> 
>>                   Right - I've been groking that.
>> 
>>                         The bit that always confuses me about release and
>>                         acquire is
>>                   ordering is restricted to one direction, as talked about
>>                   in
>>                   orderAccess.hpp [1]. So for a release, accesses prior to
>>                   the
>>                   release cannot move below it, but accesses succeeding the
>>                   release
>>                   can move above it. And that seems to apply to
>>                   Unsafe.storeFence
>>                   [2] (acting like a monitor exit). Is that contrary to C++
>>                   release
>>                   fences where ordering is restricted both to prior and
>>                   succeeding
>>                   accesses? [3]
>>                         So what about the following?
>> 
>>                            a = r1; // Cannot move below the fence
>>                            Unsafe.storeFence();
>>                            b = r2; // Can move above the fence?
>>                   I think the hotspot docs need to be more precise about
>>                   when they're
>>                   talking about movement of stores and when about loads.
>> 
>>                         // release.  I.e., subsequent memory accesses may
>>                         float above the
>>                         // release, but prior ones may not float below it.
>>                   As I've said elsewhere, the above makes no sense without
>>                   restricting
>>                   the type of access.
>> 
>>                   _______________________________________________
>>                   Concurrency-interest mailing list
>>                   Concurrency-interest at cs.oswego.edu
>>                   http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> 
>>       _______________________________________________
>>       Concurrency-interest mailing list
>>       Concurrency-interest at cs.oswego.edu
>>       http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>> 
>> 
>> 
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From cowwoc at bbs.darktech.org  Sun Nov 30 19:59:25 2014
From: cowwoc at bbs.darktech.org (cowwoc)
Date: Sun, 30 Nov 2014 17:59:25 -0700 (MST)
Subject: [concurrency-interest] Please clarify Javadoc of
	CompletableFuture.thenCompose()
In-Reply-To: <CA+F8eeQHddfkfgsnNrfFpDCma7nPs3ad6_EtAzwK0hNtmbkzbg@mail.gmail.com>
References: <1417232551381-11504.post@n7.nabble.com>
	<547B1EEC.3060306@cs.oswego.edu>
	<CA+F8eeTsA6zz9-qTwxh+HdgX=TKGp8vvaNt-jzHm3G2FEShPfg@mail.gmail.com>
	<547B3FC1.5060801@cs.oswego.edu>
	<CA+F8eeQHddfkfgsnNrfFpDCma7nPs3ad6_EtAzwK0hNtmbkzbg@mail.gmail.com>
Message-ID: <547BBF1D.1020908@bbs.darktech.org>

+1

The CompletableFuture documentation is extremely difficult to comprehend 
(much more so than other j.u.c classes). Keep in mind that I am coming 
at this with full comprehension of Javascript Promises (having used them 
daily for over 6 months). I can't imagine people with no prior exposure 
to Promises managing to get this working.

On a related note, I find Generics compiler error messages to be 
extremely cryptic as well (much more so than any other kind of compiler 
error). Where can I bring up this issue for improvement? The error 
messages seem like they were written for consumption by other computers, 
certainly not by human beings. This has become much more of a problem 
with the introduction of lambdas and CompletionFuture because I might 
myself having to deal with incorrect cast errors much more frequently. I 
think we can convey the same messages in a much more human-friendly manner.

Thanks,
Gili

On 30/11/2014 12:01 PM, tpeierls [via JSR166 Concurrency] wrote:
> On Sun, Nov 30, 2014 at 11:03 AM, Doug Lea <[hidden email] 
> </user/SendEmail.jtp?type=node&node=11518&i=0>> wrote:
>
>     On 11/30/2014 09:30 AM, Tim Peierls wrote:
>
>
>         Confusing, because of the inherited CompletionStage doc: The
>         type of "this" and
>         the return type promised by the doc are both referred to as
>         "stage" when in fact
>         both are specifically CompletableFuture. That's imprecise, not
>         wrong, but it
>         makes the reader stumble right off.
>
>
>     OK, but we do this uniformly in java.util/j.u.c. 
>
>     For example, see the many
>     mentions of "this collection" in Collection implementations.
>
>
>
> Well... Two seconds of looking and I found an inconsistency: type 
> parameter E for Set is "type of elements maintained by this set" and 
> for Queue is "type of elements held in this collection" (compare E in 
> List, SortedSet, Deque, ...). Further brief manual scan picked up no 
> other instances of "this collection" in the docs for commonly used 
> Collection subtypes in j.u.
>
> And just because we do it, doesn't mean it's a good thing. In most 
> cases we are getting away with it, but read on for my reasoning about 
> when it becomes inappropriate.
>
>
>     Is this case is different enough to warrant copy/paste/edit?
>
>
> What tips it over, for me, is the fact that the inheritance is not 
> interface Foo <- class xxxFoo but CompletionStage <- 
> CompletableFuture, i.e., two type names with some similarity but no 
> identical components. References to "this set" in HashSet doc, for 
> example, are instantly clear, but references to "this stage" in 
> CompletableFuture doc require an extra few steps: "oh, right, a 
> CompletableFuture is a CompletionStage, which is a 'stage'".
>
> Similar reasoning for covariant return types: I can easily forgive 
> "returns a new set" (or Set) when return type is HashSet, but it takes 
> me a second to think through "returns a new CompletionStage" when 
> return type is CompletableFuture.
>
> You could argue that since these are specs, not tutorials, the reader 
> of CompletableFuture's doc ought to be able to see "stage" and read 
> CompletableFuture without effort, but I think that's asking a lot.
>
> --tim
>
>
> _______________________________________________
> Concurrency-interest mailing list
> [hidden email] </user/SendEmail.jtp?type=node&node=11518&i=1>
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
> ------------------------------------------------------------------------
> If you reply to this email, your message will be added to the 
> discussion below:
> http://jsr166-concurrency.10961.n7.nabble.com/Please-clarify-Javadoc-of-CompletableFuture-thenCompose-tp11504p11518.html 
>
> To unsubscribe from Please clarify Javadoc of 
> CompletableFuture.thenCompose(), click here 
> <http://jsr166-concurrency.10961.n7.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=11504&code=Y293d29jQGJicy5kYXJrdGVjaC5vcmd8MTE1MDR8MTU3NDMyMTI0Nw==>.
> NAML 
> <http://jsr166-concurrency.10961.n7.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml> 
>





--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/Please-clarify-Javadoc-of-CompletableFuture-thenCompose-tp11504p11521.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20141130/1d5ff112/attachment.html>

