From smaldini at pivotal.io  Mon Mar  2 09:03:29 2015
From: smaldini at pivotal.io (Stephane Maldini)
Date: Mon, 2 Mar 2015 14:03:29 +0000
Subject: [concurrency-interest] Potential projects/code that should use
	Rx
In-Reply-To: <CAAL-3PYXy=0ojUzmGbtY+MsqjT5dUYGqpea=Z6nYbN+T5kPqCA@mail.gmail.com>
References: <CAAL-3PYXy=0ojUzmGbtY+MsqjT5dUYGqpea=Z6nYbN+T5kPqCA@mail.gmail.com>
Message-ID: <CAFm0joHqZppTND4ZeQvFScXGqHpuGv6-C1oTVO7rz7C4NAwN9w@mail.gmail.com>

I guess you might prefer working on multiple values when you want to do
MicroBatching or Analytics, that's our use case for Rx.

On Thu, Feb 5, 2015 at 3:48 AM, Yu Lin <yu.lin.86 at gmail.com> wrote:

> Hello,
>
> I saw that Rx will be included in Java9:
>
> http://cs.oswego.edu/pipermail/concurrency-interest/2015-January/013641.html
>
> I'm starting to learn when and how to use Rx. Some tutorials say that the
> advantage of Rx (compared with CompletableFuture) is that it can emit
> multiple values asynchronously  instead of just one value, plus the
> declarative operators. But the tutorials only show toy examples.
>
> I'm still wondering when should we definitely use Rx. Is there any real
> code (not toy examples) shows the advantage of Rx? And do you guys know any
> projects could potentially (or should) use Rx but they haven't done so yet?
>
> Thanks,
> Yu
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


-- 
Stephane Maldini | Solutions Architect, CSO EMEA | London | Pivotal
W: pivotal.io | T: @smaldini <https://twitter.com/smaldini>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150302/fb601a8a/attachment.html>

From thurston at nomagicsoftware.com  Tue Mar  3 12:26:50 2015
From: thurston at nomagicsoftware.com (thurstonn)
Date: Tue, 3 Mar 2015 10:26:50 -0700 (MST)
Subject: [concurrency-interest] jdk9 Candidate classes Flow and
	SubmissionPublisher
In-Reply-To: <54B7F7FD.6000507@cs.oswego.edu>
References: <54B7F7FD.6000507@cs.oswego.edu>
Message-ID: <1425403610993-12384.post@n7.nabble.com>

Is there somewhere where we can have access to the sources?



--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/jdk9-Candidate-classes-Flow-and-SubmissionPublisher-tp11967p12384.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From dl at cs.oswego.edu  Tue Mar  3 19:44:46 2015
From: dl at cs.oswego.edu (Doug Lea)
Date: Tue, 03 Mar 2015 19:44:46 -0500
Subject: [concurrency-interest] jdk9 Candidate classes Flow and
	SubmissionPublisher
In-Reply-To: <1425403610993-12384.post@n7.nabble.com>
References: <54B7F7FD.6000507@cs.oswego.edu>
	<1425403610993-12384.post@n7.nabble.com>
Message-ID: <54F6557E.4090200@cs.oswego.edu>

On 03/03/2015 12:26 PM, thurstonn wrote:
> Is there somewhere where we can have access to the sources?

In the usual places:

http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/

For other instructions and links, see:
http://gee.cs.oswego.edu/dl/concurrency-interest/index.html


-Doug


From martinrb at google.com  Wed Mar  4 22:24:45 2015
From: martinrb at google.com (Martin Buchholz)
Date: Wed, 4 Mar 2015 19:24:45 -0800
Subject: [concurrency-interest] Future.isCompletedNormally
	Future.isCompletedAbnormally
Message-ID: <CA+kOe08Gz+Mdje7W1GwCXz7VF1oTxQbV=uC0Y2oFD3-L0+We0w@mail.gmail.com>

ForkJoinTask is a Future implementation with existing methods
isCompletedNormally isCompletedAbnormally.

We could make these methods on Future itself with (inefficient) default
implementations and efficient implementations on FutureTask and
CompletableFuture.

Worth doing?

Here's a v0.1:

Index: CompletableFuture.java
===================================================================
RCS file:
/export/home/jsr166/jsr166/jsr166/src/main/java/util/concurrent/CompletableFuture.java,v
retrieving revision 1.158
diff -u -r1.158 CompletableFuture.java
--- CompletableFuture.java    17 Jan 2015 20:05:02 -0000    1.158
+++ CompletableFuture.java    5 Mar 2015 03:18:19 -0000
@@ -2314,6 +2314,32 @@
     }

     /**
+     * Returns {@code true} if this CompletableFuture exceptionally and
was not cancelled.
+     *
+     * @return {@code true} if this CompletableFuture completed
exceptionally and was not cancelled
+     */
+    public final boolean isCompletedAbnormally() {
+        Object r;
+        return ((r = result) instanceof AltResult)
+            && r != NIL
+            && !(((AltResult)r).ex instanceof CancellationException);
+    }
+
+    /**
+     * Returns {@code true} if this task completed without throwing an
+     * exception and was not cancelled.
+     *
+     * @return {@code true} if this task completed without throwing an
+     * exception and was not cancelled
+     */
+    public final boolean isCompletedNormally() {
+        Object r;
+        return (r = result) != null
+            && (r == NIL
+                || !(r instanceof AltResult));
+    }
+
+    /**
      * Forcibly sets or resets the value subsequently returned by
      * method {@link #get()} and related methods, whether or not
      * already completed. This method is designed for use only in
Index: Future.java
===================================================================
RCS file:
/export/home/jsr166/jsr166/jsr166/src/main/java/util/concurrent/Future.java,v
retrieving revision 1.40
diff -u -r1.40 Future.java
--- Future.java    17 Feb 2015 18:55:39 -0000    1.40
+++ Future.java    5 Mar 2015 03:18:19 -0000
@@ -110,6 +110,66 @@
     boolean isDone();

     /**
+     * Returns {@code true} if this task completed by throwing an
exception.
+     *
+     * @return {@code true} if this task completed by throwing an exception
+     */
+    default boolean isCompletedAbnormally() {
+        if (!isDone() || isCancelled())
+            return false;
+        // Support Future implementations that throw
+        // InterruptedException even when result is available.
+        boolean interrupted = false;
+        try {
+            retry: for (;;) {
+                try {
+                    get();
+                } catch (ExecutionException e) {
+                    return true;
+                } catch (InterruptedException e) {
+                    interrupted = true;
+                    continue retry;
+                }
+                return false;
+            }
+        } finally {
+            if (interrupted)
+                Thread.currentThread().interrupt();
+        }
+    }
+
+    /**
+     * Returns {@code true} if this task completed without throwing an
+     * exception and was not cancelled.
+     *
+     * @return {@code true} if this task completed without throwing an
+     * exception and was not cancelled
+     */
+    default boolean isCompletedNormally() {
+        if (!isDone() || isCancelled())
+            return false;
+        // Support Future implementations that throw
+        // InterruptedException even when result is available.
+        boolean interrupted = false;
+        try {
+            retry: for (;;) {
+                try {
+                    get();
+                    return true;
+                } catch (ExecutionException e) {
+                } catch (InterruptedException e) {
+                    interrupted = true;
+                    continue retry;
+                }
+                return false;
+            }
+        } finally {
+            if (interrupted)
+                Thread.currentThread().interrupt();
+        }
+    }
+
+    /**
      * Waits if necessary for the computation to complete, and then
      * retrieves its result.
      *
Index: FutureTask.java
===================================================================
RCS file:
/export/home/jsr166/jsr166/jsr166/src/main/java/util/concurrent/FutureTask.java,v
retrieving revision 1.111
diff -u -r1.111 FutureTask.java
--- FutureTask.java    5 Mar 2015 00:32:52 -0000    1.111
+++ FutureTask.java    5 Mar 2015 03:18:19 -0000
@@ -133,6 +133,32 @@
         return state != NEW;
     }

+    /**
+     * Returns {@code true} if this task completed by throwing an
exception.
+     *
+     * @return {@code true} if this task completed by throwing an exception
+     */
+    public final boolean isCompletedAbnormally() {
+        int s;
+        while ((s = state) == COMPLETING)
+            Thread.yield();
+        return s == EXCEPTIONAL;
+    }
+
+    /**
+     * Returns {@code true} if this task completed without throwing an
+     * exception and was not cancelled.
+     *
+     * @return {@code true} if this task completed without throwing an
+     * exception and was not cancelled
+     */
+    public final boolean isCompletedNormally() {
+        int s;
+        while ((s = state) == COMPLETING)
+            Thread.yield();
+        return s == NORMAL;
+    }
+
     public boolean cancel(boolean mayInterruptIfRunning) {
         if (!(state == NEW &&
               U.compareAndSwapInt(this, STATE, NEW,
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150304/895f9c0f/attachment.html>

From viktor.klang at gmail.com  Wed Mar  4 23:40:24 2015
From: viktor.klang at gmail.com (Viktor Klang)
Date: Thu, 5 Mar 2015 05:40:24 +0100
Subject: [concurrency-interest] Future.isCompletedNormally
	Future.isCompletedAbnormally
In-Reply-To: <CA+kOe08Gz+Mdje7W1GwCXz7VF1oTxQbV=uC0Y2oFD3-L0+We0w@mail.gmail.com>
References: <CA+kOe08Gz+Mdje7W1GwCXz7VF1oTxQbV=uC0Y2oFD3-L0+We0w@mail.gmail.com>
Message-ID: <CANPzfU_MfeJcx_e=ntyZkERaaf418j2u=Ve9UC7YFh2dSbNytg@mail.gmail.com>

Making the default methods blocking would be horrible IMO.

-- 
Cheers,
?
On 5 Mar 2015 10:48, "Martin Buchholz" <martinrb at google.com> wrote:

> ForkJoinTask is a Future implementation with existing methods
> isCompletedNormally isCompletedAbnormally.
>
> We could make these methods on Future itself with (inefficient) default
> implementations and efficient implementations on FutureTask and
> CompletableFuture.
>
> Worth doing?
>
> Here's a v0.1:
>
> Index: CompletableFuture.java
> ===================================================================
> RCS file:
> /export/home/jsr166/jsr166/jsr166/src/main/java/util/concurrent/CompletableFuture.java,v
> retrieving revision 1.158
> diff -u -r1.158 CompletableFuture.java
> --- CompletableFuture.java    17 Jan 2015 20:05:02 -0000    1.158
> +++ CompletableFuture.java    5 Mar 2015 03:18:19 -0000
> @@ -2314,6 +2314,32 @@
>      }
>
>      /**
> +     * Returns {@code true} if this CompletableFuture exceptionally and
> was not cancelled.
> +     *
> +     * @return {@code true} if this CompletableFuture completed
> exceptionally and was not cancelled
> +     */
> +    public final boolean isCompletedAbnormally() {
> +        Object r;
> +        return ((r = result) instanceof AltResult)
> +            && r != NIL
> +            && !(((AltResult)r).ex instanceof CancellationException);
> +    }
> +
> +    /**
> +     * Returns {@code true} if this task completed without throwing an
> +     * exception and was not cancelled.
> +     *
> +     * @return {@code true} if this task completed without throwing an
> +     * exception and was not cancelled
> +     */
> +    public final boolean isCompletedNormally() {
> +        Object r;
> +        return (r = result) != null
> +            && (r == NIL
> +                || !(r instanceof AltResult));
> +    }
> +
> +    /**
>       * Forcibly sets or resets the value subsequently returned by
>       * method {@link #get()} and related methods, whether or not
>       * already completed. This method is designed for use only in
> Index: Future.java
> ===================================================================
> RCS file:
> /export/home/jsr166/jsr166/jsr166/src/main/java/util/concurrent/Future.java,v
> retrieving revision 1.40
> diff -u -r1.40 Future.java
> --- Future.java    17 Feb 2015 18:55:39 -0000    1.40
> +++ Future.java    5 Mar 2015 03:18:19 -0000
> @@ -110,6 +110,66 @@
>      boolean isDone();
>
>      /**
> +     * Returns {@code true} if this task completed by throwing an
> exception.
> +     *
> +     * @return {@code true} if this task completed by throwing an
> exception
> +     */
> +    default boolean isCompletedAbnormally() {
> +        if (!isDone() || isCancelled())
> +            return false;
> +        // Support Future implementations that throw
> +        // InterruptedException even when result is available.
> +        boolean interrupted = false;
> +        try {
> +            retry: for (;;) {
> +                try {
> +                    get();
> +                } catch (ExecutionException e) {
> +                    return true;
> +                } catch (InterruptedException e) {
> +                    interrupted = true;
> +                    continue retry;
> +                }
> +                return false;
> +            }
> +        } finally {
> +            if (interrupted)
> +                Thread.currentThread().interrupt();
> +        }
> +    }
> +
> +    /**
> +     * Returns {@code true} if this task completed without throwing an
> +     * exception and was not cancelled.
> +     *
> +     * @return {@code true} if this task completed without throwing an
> +     * exception and was not cancelled
> +     */
> +    default boolean isCompletedNormally() {
> +        if (!isDone() || isCancelled())
> +            return false;
> +        // Support Future implementations that throw
> +        // InterruptedException even when result is available.
> +        boolean interrupted = false;
> +        try {
> +            retry: for (;;) {
> +                try {
> +                    get();
> +                    return true;
> +                } catch (ExecutionException e) {
> +                } catch (InterruptedException e) {
> +                    interrupted = true;
> +                    continue retry;
> +                }
> +                return false;
> +            }
> +        } finally {
> +            if (interrupted)
> +                Thread.currentThread().interrupt();
> +        }
> +    }
> +
> +    /**
>       * Waits if necessary for the computation to complete, and then
>       * retrieves its result.
>       *
> Index: FutureTask.java
> ===================================================================
> RCS file:
> /export/home/jsr166/jsr166/jsr166/src/main/java/util/concurrent/FutureTask.java,v
> retrieving revision 1.111
> diff -u -r1.111 FutureTask.java
> --- FutureTask.java    5 Mar 2015 00:32:52 -0000    1.111
> +++ FutureTask.java    5 Mar 2015 03:18:19 -0000
> @@ -133,6 +133,32 @@
>          return state != NEW;
>      }
>
> +    /**
> +     * Returns {@code true} if this task completed by throwing an
> exception.
> +     *
> +     * @return {@code true} if this task completed by throwing an
> exception
> +     */
> +    public final boolean isCompletedAbnormally() {
> +        int s;
> +        while ((s = state) == COMPLETING)
> +            Thread.yield();
> +        return s == EXCEPTIONAL;
> +    }
> +
> +    /**
> +     * Returns {@code true} if this task completed without throwing an
> +     * exception and was not cancelled.
> +     *
> +     * @return {@code true} if this task completed without throwing an
> +     * exception and was not cancelled
> +     */
> +    public final boolean isCompletedNormally() {
> +        int s;
> +        while ((s = state) == COMPLETING)
> +            Thread.yield();
> +        return s == NORMAL;
> +    }
> +
>      public boolean cancel(boolean mayInterruptIfRunning) {
>          if (!(state == NEW &&
>                U.compareAndSwapInt(this, STATE, NEW,
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150305/e321cb57/attachment-0001.html>

From viktor.klang at gmail.com  Wed Mar  4 23:43:51 2015
From: viktor.klang at gmail.com (Viktor Klang)
Date: Thu, 5 Mar 2015 05:43:51 +0100
Subject: [concurrency-interest] Future.isCompletedNormally
	Future.isCompletedAbnormally
In-Reply-To: <CANPzfU_MfeJcx_e=ntyZkERaaf418j2u=Ve9UC7YFh2dSbNytg@mail.gmail.com>
References: <CA+kOe08Gz+Mdje7W1GwCXz7VF1oTxQbV=uC0Y2oFD3-L0+We0w@mail.gmail.com>
	<CANPzfU_MfeJcx_e=ntyZkERaaf418j2u=Ve9UC7YFh2dSbNytg@mail.gmail.com>
Message-ID: <CANPzfU9=ZqmUa2G_xJ5Q4Oy0CxV17MX-1qSN5Q0PrAmM4AWcGQ@mail.gmail.com>

Never mind, saw the !IsDone()

-- 
Cheers,
?
On 5 Mar 2015 11:40, "Viktor Klang" <viktor.klang at gmail.com> wrote:

> Making the default methods blocking would be horrible IMO.
>
> --
> Cheers,
> ?
> On 5 Mar 2015 10:48, "Martin Buchholz" <martinrb at google.com> wrote:
>
>> ForkJoinTask is a Future implementation with existing methods
>> isCompletedNormally isCompletedAbnormally.
>>
>> We could make these methods on Future itself with (inefficient) default
>> implementations and efficient implementations on FutureTask and
>> CompletableFuture.
>>
>> Worth doing?
>>
>> Here's a v0.1:
>>
>> Index: CompletableFuture.java
>> ===================================================================
>> RCS file:
>> /export/home/jsr166/jsr166/jsr166/src/main/java/util/concurrent/CompletableFuture.java,v
>> retrieving revision 1.158
>> diff -u -r1.158 CompletableFuture.java
>> --- CompletableFuture.java    17 Jan 2015 20:05:02 -0000    1.158
>> +++ CompletableFuture.java    5 Mar 2015 03:18:19 -0000
>> @@ -2314,6 +2314,32 @@
>>      }
>>
>>      /**
>> +     * Returns {@code true} if this CompletableFuture exceptionally and
>> was not cancelled.
>> +     *
>> +     * @return {@code true} if this CompletableFuture completed
>> exceptionally and was not cancelled
>> +     */
>> +    public final boolean isCompletedAbnormally() {
>> +        Object r;
>> +        return ((r = result) instanceof AltResult)
>> +            && r != NIL
>> +            && !(((AltResult)r).ex instanceof CancellationException);
>> +    }
>> +
>> +    /**
>> +     * Returns {@code true} if this task completed without throwing an
>> +     * exception and was not cancelled.
>> +     *
>> +     * @return {@code true} if this task completed without throwing an
>> +     * exception and was not cancelled
>> +     */
>> +    public final boolean isCompletedNormally() {
>> +        Object r;
>> +        return (r = result) != null
>> +            && (r == NIL
>> +                || !(r instanceof AltResult));
>> +    }
>> +
>> +    /**
>>       * Forcibly sets or resets the value subsequently returned by
>>       * method {@link #get()} and related methods, whether or not
>>       * already completed. This method is designed for use only in
>> Index: Future.java
>> ===================================================================
>> RCS file:
>> /export/home/jsr166/jsr166/jsr166/src/main/java/util/concurrent/Future.java,v
>> retrieving revision 1.40
>> diff -u -r1.40 Future.java
>> --- Future.java    17 Feb 2015 18:55:39 -0000    1.40
>> +++ Future.java    5 Mar 2015 03:18:19 -0000
>> @@ -110,6 +110,66 @@
>>      boolean isDone();
>>
>>      /**
>> +     * Returns {@code true} if this task completed by throwing an
>> exception.
>> +     *
>> +     * @return {@code true} if this task completed by throwing an
>> exception
>> +     */
>> +    default boolean isCompletedAbnormally() {
>> +        if (!isDone() || isCancelled())
>> +            return false;
>> +        // Support Future implementations that throw
>> +        // InterruptedException even when result is available.
>> +        boolean interrupted = false;
>> +        try {
>> +            retry: for (;;) {
>> +                try {
>> +                    get();
>> +                } catch (ExecutionException e) {
>> +                    return true;
>> +                } catch (InterruptedException e) {
>> +                    interrupted = true;
>> +                    continue retry;
>> +                }
>> +                return false;
>> +            }
>> +        } finally {
>> +            if (interrupted)
>> +                Thread.currentThread().interrupt();
>> +        }
>> +    }
>> +
>> +    /**
>> +     * Returns {@code true} if this task completed without throwing an
>> +     * exception and was not cancelled.
>> +     *
>> +     * @return {@code true} if this task completed without throwing an
>> +     * exception and was not cancelled
>> +     */
>> +    default boolean isCompletedNormally() {
>> +        if (!isDone() || isCancelled())
>> +            return false;
>> +        // Support Future implementations that throw
>> +        // InterruptedException even when result is available.
>> +        boolean interrupted = false;
>> +        try {
>> +            retry: for (;;) {
>> +                try {
>> +                    get();
>> +                    return true;
>> +                } catch (ExecutionException e) {
>> +                } catch (InterruptedException e) {
>> +                    interrupted = true;
>> +                    continue retry;
>> +                }
>> +                return false;
>> +            }
>> +        } finally {
>> +            if (interrupted)
>> +                Thread.currentThread().interrupt();
>> +        }
>> +    }
>> +
>> +    /**
>>       * Waits if necessary for the computation to complete, and then
>>       * retrieves its result.
>>       *
>> Index: FutureTask.java
>> ===================================================================
>> RCS file:
>> /export/home/jsr166/jsr166/jsr166/src/main/java/util/concurrent/FutureTask.java,v
>> retrieving revision 1.111
>> diff -u -r1.111 FutureTask.java
>> --- FutureTask.java    5 Mar 2015 00:32:52 -0000    1.111
>> +++ FutureTask.java    5 Mar 2015 03:18:19 -0000
>> @@ -133,6 +133,32 @@
>>          return state != NEW;
>>      }
>>
>> +    /**
>> +     * Returns {@code true} if this task completed by throwing an
>> exception.
>> +     *
>> +     * @return {@code true} if this task completed by throwing an
>> exception
>> +     */
>> +    public final boolean isCompletedAbnormally() {
>> +        int s;
>> +        while ((s = state) == COMPLETING)
>> +            Thread.yield();
>> +        return s == EXCEPTIONAL;
>> +    }
>> +
>> +    /**
>> +     * Returns {@code true} if this task completed without throwing an
>> +     * exception and was not cancelled.
>> +     *
>> +     * @return {@code true} if this task completed without throwing an
>> +     * exception and was not cancelled
>> +     */
>> +    public final boolean isCompletedNormally() {
>> +        int s;
>> +        while ((s = state) == COMPLETING)
>> +            Thread.yield();
>> +        return s == NORMAL;
>> +    }
>> +
>>      public boolean cancel(boolean mayInterruptIfRunning) {
>>          if (!(state == NEW &&
>>                U.compareAndSwapInt(this, STATE, NEW,
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150305/4e66ff23/attachment.html>

From joe.bowbeer at gmail.com  Wed Mar  4 23:56:48 2015
From: joe.bowbeer at gmail.com (joe.bowbeer at gmail.com)
Date: Wed, 04 Mar 2015 20:56:48 -0800 (PST)
Subject: [concurrency-interest] Future.isCompletedNormally
 Future.isCompletedAbnormally
In-Reply-To: <CANPzfU_MfeJcx_e=ntyZkERaaf418j2u=Ve9UC7YFh2dSbNytg@mail.gmail.com>
References: <CANPzfU_MfeJcx_e=ntyZkERaaf418j2u=Ve9UC7YFh2dSbNytg@mail.gmail.com>
Message-ID: <1425531408728.f31a6788@Nodemailer>

I have a similar complaint as Viktor. These default implementations seem too heavyweight.



?
Sent from Mailbox

On Wed, Mar 4, 2015 at 8:49 PM, Viktor Klang <viktor.klang at gmail.com>
wrote:

> Making the default methods blocking would be horrible IMO.
> -- 
> Cheers,
> ?
> On 5 Mar 2015 10:48, "Martin Buchholz" <martinrb at google.com> wrote:
>> ForkJoinTask is a Future implementation with existing methods
>> isCompletedNormally isCompletedAbnormally.
>>
>> We could make these methods on Future itself with (inefficient) default
>> implementations and efficient implementations on FutureTask and
>> CompletableFuture.
>>
>> Worth doing?
>>
>> Here's a v0.1:
>>
>> Index: CompletableFuture.java
>> ===================================================================
>> RCS file:
>> /export/home/jsr166/jsr166/jsr166/src/main/java/util/concurrent/CompletableFuture.java,v
>> retrieving revision 1.158
>> diff -u -r1.158 CompletableFuture.java
>> --- CompletableFuture.java    17 Jan 2015 20:05:02 -0000    1.158
>> +++ CompletableFuture.java    5 Mar 2015 03:18:19 -0000
>> @@ -2314,6 +2314,32 @@
>>      }
>>
>>      /**
>> +     * Returns {@code true} if this CompletableFuture exceptionally and
>> was not cancelled.
>> +     *
>> +     * @return {@code true} if this CompletableFuture completed
>> exceptionally and was not cancelled
>> +     */
>> +    public final boolean isCompletedAbnormally() {
>> +        Object r;
>> +        return ((r = result) instanceof AltResult)
>> +            && r != NIL
>> +            && !(((AltResult)r).ex instanceof CancellationException);
>> +    }
>> +
>> +    /**
>> +     * Returns {@code true} if this task completed without throwing an
>> +     * exception and was not cancelled.
>> +     *
>> +     * @return {@code true} if this task completed without throwing an
>> +     * exception and was not cancelled
>> +     */
>> +    public final boolean isCompletedNormally() {
>> +        Object r;
>> +        return (r = result) != null
>> +            && (r == NIL
>> +                || !(r instanceof AltResult));
>> +    }
>> +
>> +    /**
>>       * Forcibly sets or resets the value subsequently returned by
>>       * method {@link #get()} and related methods, whether or not
>>       * already completed. This method is designed for use only in
>> Index: Future.java
>> ===================================================================
>> RCS file:
>> /export/home/jsr166/jsr166/jsr166/src/main/java/util/concurrent/Future.java,v
>> retrieving revision 1.40
>> diff -u -r1.40 Future.java
>> --- Future.java    17 Feb 2015 18:55:39 -0000    1.40
>> +++ Future.java    5 Mar 2015 03:18:19 -0000
>> @@ -110,6 +110,66 @@
>>      boolean isDone();
>>
>>      /**
>> +     * Returns {@code true} if this task completed by throwing an
>> exception.
>> +     *
>> +     * @return {@code true} if this task completed by throwing an
>> exception
>> +     */
>> +    default boolean isCompletedAbnormally() {
>> +        if (!isDone() || isCancelled())
>> +            return false;
>> +        // Support Future implementations that throw
>> +        // InterruptedException even when result is available.
>> +        boolean interrupted = false;
>> +        try {
>> +            retry: for (;;) {
>> +                try {
>> +                    get();
>> +                } catch (ExecutionException e) {
>> +                    return true;
>> +                } catch (InterruptedException e) {
>> +                    interrupted = true;
>> +                    continue retry;
>> +                }
>> +                return false;
>> +            }
>> +        } finally {
>> +            if (interrupted)
>> +                Thread.currentThread().interrupt();
>> +        }
>> +    }
>> +
>> +    /**
>> +     * Returns {@code true} if this task completed without throwing an
>> +     * exception and was not cancelled.
>> +     *
>> +     * @return {@code true} if this task completed without throwing an
>> +     * exception and was not cancelled
>> +     */
>> +    default boolean isCompletedNormally() {
>> +        if (!isDone() || isCancelled())
>> +            return false;
>> +        // Support Future implementations that throw
>> +        // InterruptedException even when result is available.
>> +        boolean interrupted = false;
>> +        try {
>> +            retry: for (;;) {
>> +                try {
>> +                    get();
>> +                    return true;
>> +                } catch (ExecutionException e) {
>> +                } catch (InterruptedException e) {
>> +                    interrupted = true;
>> +                    continue retry;
>> +                }
>> +                return false;
>> +            }
>> +        } finally {
>> +            if (interrupted)
>> +                Thread.currentThread().interrupt();
>> +        }
>> +    }
>> +
>> +    /**
>>       * Waits if necessary for the computation to complete, and then
>>       * retrieves its result.
>>       *
>> Index: FutureTask.java
>> ===================================================================
>> RCS file:
>> /export/home/jsr166/jsr166/jsr166/src/main/java/util/concurrent/FutureTask.java,v
>> retrieving revision 1.111
>> diff -u -r1.111 FutureTask.java
>> --- FutureTask.java    5 Mar 2015 00:32:52 -0000    1.111
>> +++ FutureTask.java    5 Mar 2015 03:18:19 -0000
>> @@ -133,6 +133,32 @@
>>          return state != NEW;
>>      }
>>
>> +    /**
>> +     * Returns {@code true} if this task completed by throwing an
>> exception.
>> +     *
>> +     * @return {@code true} if this task completed by throwing an
>> exception
>> +     */
>> +    public final boolean isCompletedAbnormally() {
>> +        int s;
>> +        while ((s = state) == COMPLETING)
>> +            Thread.yield();
>> +        return s == EXCEPTIONAL;
>> +    }
>> +
>> +    /**
>> +     * Returns {@code true} if this task completed without throwing an
>> +     * exception and was not cancelled.
>> +     *
>> +     * @return {@code true} if this task completed without throwing an
>> +     * exception and was not cancelled
>> +     */
>> +    public final boolean isCompletedNormally() {
>> +        int s;
>> +        while ((s = state) == COMPLETING)
>> +            Thread.yield();
>> +        return s == NORMAL;
>> +    }
>> +
>>      public boolean cancel(boolean mayInterruptIfRunning) {
>>          if (!(state == NEW &&
>>                U.compareAndSwapInt(this, STATE, NEW,
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150304/1c5736f2/attachment-0001.html>

From davidcholmes at aapt.net.au  Thu Mar  5 00:11:26 2015
From: davidcholmes at aapt.net.au (David Holmes)
Date: Thu, 5 Mar 2015 15:11:26 +1000
Subject: [concurrency-interest]
	Future.isCompletedNormallyFuture.isCompletedAbnormally
In-Reply-To: <CANPzfU_MfeJcx_e=ntyZkERaaf418j2u=Ve9UC7YFh2dSbNytg@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCGEBJKOAA.davidcholmes@aapt.net.au>

Victor,

They aren't blocking - you will only get to the get() if isDone() is true, so all you are accessing is the result to see if it is a real result or an exception occurred.

David
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Viktor Klang
  Sent: Thursday, 5 March 2015 2:40 PM
  To: Martin Buchholz
  Cc: concurrency-interest
  Subject: Re: [concurrency-interest] Future.isCompletedNormallyFuture.isCompletedAbnormally


  Making the default methods blocking would be horrible IMO.

  -- 
  Cheers,
  ?

  On 5 Mar 2015 10:48, "Martin Buchholz" <martinrb at google.com> wrote:

    ForkJoinTask is a Future implementation with existing methods isCompletedNormally isCompletedAbnormally.


    We could make these methods on Future itself with (inefficient) default implementations and efficient implementations on FutureTask and CompletableFuture.  


    Worth doing?


    Here's a v0.1:

    Index: CompletableFuture.java
    ===================================================================
    RCS file: /export/home/jsr166/jsr166/jsr166/src/main/java/util/concurrent/CompletableFuture.java,v
    retrieving revision 1.158
    diff -u -r1.158 CompletableFuture.java
    --- CompletableFuture.java    17 Jan 2015 20:05:02 -0000    1.158
    +++ CompletableFuture.java    5 Mar 2015 03:18:19 -0000
    @@ -2314,6 +2314,32 @@
         }
     
         /**
    +     * Returns {@code true} if this CompletableFuture exceptionally and was not cancelled.
    +     *
    +     * @return {@code true} if this CompletableFuture completed exceptionally and was not cancelled
    +     */
    +    public final boolean isCompletedAbnormally() {
    +        Object r;
    +        return ((r = result) instanceof AltResult)
    +            && r != NIL
    +            && !(((AltResult)r).ex instanceof CancellationException);
    +    }
    +
    +    /**
    +     * Returns {@code true} if this task completed without throwing an
    +     * exception and was not cancelled.
    +     *
    +     * @return {@code true} if this task completed without throwing an
    +     * exception and was not cancelled
    +     */
    +    public final boolean isCompletedNormally() {
    +        Object r;
    +        return (r = result) != null
    +            && (r == NIL
    +                || !(r instanceof AltResult));
    +    }
    +
    +    /**
          * Forcibly sets or resets the value subsequently returned by
          * method {@link #get()} and related methods, whether or not
          * already completed. This method is designed for use only in
    Index: Future.java
    ===================================================================
    RCS file: /export/home/jsr166/jsr166/jsr166/src/main/java/util/concurrent/Future.java,v
    retrieving revision 1.40
    diff -u -r1.40 Future.java
    --- Future.java    17 Feb 2015 18:55:39 -0000    1.40
    +++ Future.java    5 Mar 2015 03:18:19 -0000
    @@ -110,6 +110,66 @@
         boolean isDone();
     
         /**
    +     * Returns {@code true} if this task completed by throwing an exception.
    +     *
    +     * @return {@code true} if this task completed by throwing an exception
    +     */
    +    default boolean isCompletedAbnormally() {
    +        if (!isDone() || isCancelled())
    +            return false;
    +        // Support Future implementations that throw
    +        // InterruptedException even when result is available.
    +        boolean interrupted = false;
    +        try {
    +            retry: for (;;) {
    +                try {
    +                    get();
    +                } catch (ExecutionException e) {
    +                    return true;
    +                } catch (InterruptedException e) {
    +                    interrupted = true;
    +                    continue retry;
    +                }
    +                return false;
    +            }
    +        } finally {
    +            if (interrupted)
    +                Thread.currentThread().interrupt();
    +        }
    +    }
    +
    +    /**
    +     * Returns {@code true} if this task completed without throwing an
    +     * exception and was not cancelled.
    +     *
    +     * @return {@code true} if this task completed without throwing an
    +     * exception and was not cancelled
    +     */
    +    default boolean isCompletedNormally() {
    +        if (!isDone() || isCancelled())
    +            return false;
    +        // Support Future implementations that throw
    +        // InterruptedException even when result is available.
    +        boolean interrupted = false;
    +        try {
    +            retry: for (;;) {
    +                try {
    +                    get();
    +                    return true;
    +                } catch (ExecutionException e) {
    +                } catch (InterruptedException e) {
    +                    interrupted = true;
    +                    continue retry;
    +                }
    +                return false;
    +            }
    +        } finally {
    +            if (interrupted)
    +                Thread.currentThread().interrupt();
    +        }
    +    }
    +
    +    /**
          * Waits if necessary for the computation to complete, and then
          * retrieves its result.
          *
    Index: FutureTask.java
    ===================================================================
    RCS file: /export/home/jsr166/jsr166/jsr166/src/main/java/util/concurrent/FutureTask.java,v
    retrieving revision 1.111
    diff -u -r1.111 FutureTask.java
    --- FutureTask.java    5 Mar 2015 00:32:52 -0000    1.111
    +++ FutureTask.java    5 Mar 2015 03:18:19 -0000
    @@ -133,6 +133,32 @@
             return state != NEW;
         }
     
    +    /**
    +     * Returns {@code true} if this task completed by throwing an exception.
    +     *
    +     * @return {@code true} if this task completed by throwing an exception
    +     */
    +    public final boolean isCompletedAbnormally() {
    +        int s;
    +        while ((s = state) == COMPLETING)
    +            Thread.yield();
    +        return s == EXCEPTIONAL;
    +    }
    +
    +    /**
    +     * Returns {@code true} if this task completed without throwing an
    +     * exception and was not cancelled.
    +     *
    +     * @return {@code true} if this task completed without throwing an
    +     * exception and was not cancelled
    +     */
    +    public final boolean isCompletedNormally() {
    +        int s;
    +        while ((s = state) == COMPLETING)
    +            Thread.yield();
    +        return s == NORMAL;
    +    }
    +
         public boolean cancel(boolean mayInterruptIfRunning) {
             if (!(state == NEW &&
                   U.compareAndSwapInt(this, STATE, NEW,



    _______________________________________________
    Concurrency-interest mailing list
    Concurrency-interest at cs.oswego.edu
    http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150305/d81fac60/attachment.html>

From davidcholmes at aapt.net.au  Thu Mar  5 00:12:24 2015
From: davidcholmes at aapt.net.au (David Holmes)
Date: Thu, 5 Mar 2015 15:12:24 +1000
Subject: [concurrency-interest]
	Future.isCompletedNormallyFuture.isCompletedAbnormally
In-Reply-To: <CANPzfU9=ZqmUa2G_xJ5Q4Oy0CxV17MX-1qSN5Q0PrAmM4AWcGQ@mail.gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCKEBJKOAA.davidcholmes@aapt.net.au>

Never mind, just saw your follow up :(

David
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Viktor Klang
  Sent: Thursday, 5 March 2015 2:44 PM
  To: Martin Buchholz
  Cc: concurrency-interest
  Subject: Re: [concurrency-interest] Future.isCompletedNormallyFuture.isCompletedAbnormally


  Never mind, saw the !IsDone()

  -- 
  Cheers,
  ?

  On 5 Mar 2015 11:40, "Viktor Klang" <viktor.klang at gmail.com> wrote:

    Making the default methods blocking would be horrible IMO.

    -- 
    Cheers,
    ?

    On 5 Mar 2015 10:48, "Martin Buchholz" <martinrb at google.com> wrote:

      ForkJoinTask is a Future implementation with existing methods isCompletedNormally isCompletedAbnormally.


      We could make these methods on Future itself with (inefficient) default implementations and efficient implementations on FutureTask and CompletableFuture.  


      Worth doing?


      Here's a v0.1:

      Index: CompletableFuture.java
      ===================================================================
      RCS file: /export/home/jsr166/jsr166/jsr166/src/main/java/util/concurrent/CompletableFuture.java,v
      retrieving revision 1.158
      diff -u -r1.158 CompletableFuture.java
      --- CompletableFuture.java    17 Jan 2015 20:05:02 -0000    1.158
      +++ CompletableFuture.java    5 Mar 2015 03:18:19 -0000
      @@ -2314,6 +2314,32 @@
           }
       
           /**
      +     * Returns {@code true} if this CompletableFuture exceptionally and was not cancelled.
      +     *
      +     * @return {@code true} if this CompletableFuture completed exceptionally and was not cancelled
      +     */
      +    public final boolean isCompletedAbnormally() {
      +        Object r;
      +        return ((r = result) instanceof AltResult)
      +            && r != NIL
      +            && !(((AltResult)r).ex instanceof CancellationException);
      +    }
      +
      +    /**
      +     * Returns {@code true} if this task completed without throwing an
      +     * exception and was not cancelled.
      +     *
      +     * @return {@code true} if this task completed without throwing an
      +     * exception and was not cancelled
      +     */
      +    public final boolean isCompletedNormally() {
      +        Object r;
      +        return (r = result) != null
      +            && (r == NIL
      +                || !(r instanceof AltResult));
      +    }
      +
      +    /**
            * Forcibly sets or resets the value subsequently returned by
            * method {@link #get()} and related methods, whether or not
            * already completed. This method is designed for use only in
      Index: Future.java
      ===================================================================
      RCS file: /export/home/jsr166/jsr166/jsr166/src/main/java/util/concurrent/Future.java,v
      retrieving revision 1.40
      diff -u -r1.40 Future.java
      --- Future.java    17 Feb 2015 18:55:39 -0000    1.40
      +++ Future.java    5 Mar 2015 03:18:19 -0000
      @@ -110,6 +110,66 @@
           boolean isDone();
       
           /**
      +     * Returns {@code true} if this task completed by throwing an exception.
      +     *
      +     * @return {@code true} if this task completed by throwing an exception
      +     */
      +    default boolean isCompletedAbnormally() {
      +        if (!isDone() || isCancelled())
      +            return false;
      +        // Support Future implementations that throw
      +        // InterruptedException even when result is available.
      +        boolean interrupted = false;
      +        try {
      +            retry: for (;;) {
      +                try {
      +                    get();
      +                } catch (ExecutionException e) {
      +                    return true;
      +                } catch (InterruptedException e) {
      +                    interrupted = true;
      +                    continue retry;
      +                }
      +                return false;
      +            }
      +        } finally {
      +            if (interrupted)
      +                Thread.currentThread().interrupt();
      +        }
      +    }
      +
      +    /**
      +     * Returns {@code true} if this task completed without throwing an
      +     * exception and was not cancelled.
      +     *
      +     * @return {@code true} if this task completed without throwing an
      +     * exception and was not cancelled
      +     */
      +    default boolean isCompletedNormally() {
      +        if (!isDone() || isCancelled())
      +            return false;
      +        // Support Future implementations that throw
      +        // InterruptedException even when result is available.
      +        boolean interrupted = false;
      +        try {
      +            retry: for (;;) {
      +                try {
      +                    get();
      +                    return true;
      +                } catch (ExecutionException e) {
      +                } catch (InterruptedException e) {
      +                    interrupted = true;
      +                    continue retry;
      +                }
      +                return false;
      +            }
      +        } finally {
      +            if (interrupted)
      +                Thread.currentThread().interrupt();
      +        }
      +    }
      +
      +    /**
            * Waits if necessary for the computation to complete, and then
            * retrieves its result.
            *
      Index: FutureTask.java
      ===================================================================
      RCS file: /export/home/jsr166/jsr166/jsr166/src/main/java/util/concurrent/FutureTask.java,v
      retrieving revision 1.111
      diff -u -r1.111 FutureTask.java
      --- FutureTask.java    5 Mar 2015 00:32:52 -0000    1.111
      +++ FutureTask.java    5 Mar 2015 03:18:19 -0000
      @@ -133,6 +133,32 @@
               return state != NEW;
           }
       
      +    /**
      +     * Returns {@code true} if this task completed by throwing an exception.
      +     *
      +     * @return {@code true} if this task completed by throwing an exception
      +     */
      +    public final boolean isCompletedAbnormally() {
      +        int s;
      +        while ((s = state) == COMPLETING)
      +            Thread.yield();
      +        return s == EXCEPTIONAL;
      +    }
      +
      +    /**
      +     * Returns {@code true} if this task completed without throwing an
      +     * exception and was not cancelled.
      +     *
      +     * @return {@code true} if this task completed without throwing an
      +     * exception and was not cancelled
      +     */
      +    public final boolean isCompletedNormally() {
      +        int s;
      +        while ((s = state) == COMPLETING)
      +            Thread.yield();
      +        return s == NORMAL;
      +    }
      +
           public boolean cancel(boolean mayInterruptIfRunning) {
               if (!(state == NEW &&
                     U.compareAndSwapInt(this, STATE, NEW,



      _______________________________________________
      Concurrency-interest mailing list
      Concurrency-interest at cs.oswego.edu
      http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150305/cac9bd47/attachment-0001.html>

From vitalyd at gmail.com  Thu Mar  5 00:23:31 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Thu, 5 Mar 2015 00:23:31 -0500
Subject: [concurrency-interest] Future.isCompletedNormally
	Future.isCompletedAbnormally
In-Reply-To: <CA+kOe08Gz+Mdje7W1GwCXz7VF1oTxQbV=uC0Y2oFD3-L0+We0w@mail.gmail.com>
References: <CA+kOe08Gz+Mdje7W1GwCXz7VF1oTxQbV=uC0Y2oFD3-L0+We0w@mail.gmail.com>
Message-ID: <CAHjP37HXDrxzrjqdOd0CooZnMJ_NqVY24D0hDW-sP4nGpMOm8Q@mail.gmail.com>

What's the motivation for doing that? Why moving this up in the hierarchy
is desired? These methods seem like utilities one would write on top of
their own concrete implementations.

Also, what's the purpose of the labeled loop? Moreover, what if get () on
that impl always throws interrupted exception - this loops forever? I guess
I don't like this because it makes assumptions about how the implementation
behaves in get ().

sent from my phone
On Mar 4, 2015 10:48 PM, "Martin Buchholz" <martinrb at google.com> wrote:

> ForkJoinTask is a Future implementation with existing methods
> isCompletedNormally isCompletedAbnormally.
>
> We could make these methods on Future itself with (inefficient) default
> implementations and efficient implementations on FutureTask and
> CompletableFuture.
>
> Worth doing?
>
> Here's a v0.1:
>
> Index: CompletableFuture.java
> ===================================================================
> RCS file:
> /export/home/jsr166/jsr166/jsr166/src/main/java/util/concurrent/CompletableFuture.java,v
> retrieving revision 1.158
> diff -u -r1.158 CompletableFuture.java
> --- CompletableFuture.java    17 Jan 2015 20:05:02 -0000    1.158
> +++ CompletableFuture.java    5 Mar 2015 03:18:19 -0000
> @@ -2314,6 +2314,32 @@
>      }
>
>      /**
> +     * Returns {@code true} if this CompletableFuture exceptionally and
> was not cancelled.
> +     *
> +     * @return {@code true} if this CompletableFuture completed
> exceptionally and was not cancelled
> +     */
> +    public final boolean isCompletedAbnormally() {
> +        Object r;
> +        return ((r = result) instanceof AltResult)
> +            && r != NIL
> +            && !(((AltResult)r).ex instanceof CancellationException);
> +    }
> +
> +    /**
> +     * Returns {@code true} if this task completed without throwing an
> +     * exception and was not cancelled.
> +     *
> +     * @return {@code true} if this task completed without throwing an
> +     * exception and was not cancelled
> +     */
> +    public final boolean isCompletedNormally() {
> +        Object r;
> +        return (r = result) != null
> +            && (r == NIL
> +                || !(r instanceof AltResult));
> +    }
> +
> +    /**
>       * Forcibly sets or resets the value subsequently returned by
>       * method {@link #get()} and related methods, whether or not
>       * already completed. This method is designed for use only in
> Index: Future.java
> ===================================================================
> RCS file:
> /export/home/jsr166/jsr166/jsr166/src/main/java/util/concurrent/Future.java,v
> retrieving revision 1.40
> diff -u -r1.40 Future.java
> --- Future.java    17 Feb 2015 18:55:39 -0000    1.40
> +++ Future.java    5 Mar 2015 03:18:19 -0000
> @@ -110,6 +110,66 @@
>      boolean isDone();
>
>      /**
> +     * Returns {@code true} if this task completed by throwing an
> exception.
> +     *
> +     * @return {@code true} if this task completed by throwing an
> exception
> +     */
> +    default boolean isCompletedAbnormally() {
> +        if (!isDone() || isCancelled())
> +            return false;
> +        // Support Future implementations that throw
> +        // InterruptedException even when result is available.
> +        boolean interrupted = false;
> +        try {
> +            retry: for (;;) {
> +                try {
> +                    get();
> +                } catch (ExecutionException e) {
> +                    return true;
> +                } catch (InterruptedException e) {
> +                    interrupted = true;
> +                    continue retry;
> +                }
> +                return false;
> +            }
> +        } finally {
> +            if (interrupted)
> +                Thread.currentThread().interrupt();
> +        }
> +    }
> +
> +    /**
> +     * Returns {@code true} if this task completed without throwing an
> +     * exception and was not cancelled.
> +     *
> +     * @return {@code true} if this task completed without throwing an
> +     * exception and was not cancelled
> +     */
> +    default boolean isCompletedNormally() {
> +        if (!isDone() || isCancelled())
> +            return false;
> +        // Support Future implementations that throw
> +        // InterruptedException even when result is available.
> +        boolean interrupted = false;
> +        try {
> +            retry: for (;;) {
> +                try {
> +                    get();
> +                    return true;
> +                } catch (ExecutionException e) {
> +                } catch (InterruptedException e) {
> +                    interrupted = true;
> +                    continue retry;
> +                }
> +                return false;
> +            }
> +        } finally {
> +            if (interrupted)
> +                Thread.currentThread().interrupt();
> +        }
> +    }
> +
> +    /**
>       * Waits if necessary for the computation to complete, and then
>       * retrieves its result.
>       *
> Index: FutureTask.java
> ===================================================================
> RCS file:
> /export/home/jsr166/jsr166/jsr166/src/main/java/util/concurrent/FutureTask.java,v
> retrieving revision 1.111
> diff -u -r1.111 FutureTask.java
> --- FutureTask.java    5 Mar 2015 00:32:52 -0000    1.111
> +++ FutureTask.java    5 Mar 2015 03:18:19 -0000
> @@ -133,6 +133,32 @@
>          return state != NEW;
>      }
>
> +    /**
> +     * Returns {@code true} if this task completed by throwing an
> exception.
> +     *
> +     * @return {@code true} if this task completed by throwing an
> exception
> +     */
> +    public final boolean isCompletedAbnormally() {
> +        int s;
> +        while ((s = state) == COMPLETING)
> +            Thread.yield();
> +        return s == EXCEPTIONAL;
> +    }
> +
> +    /**
> +     * Returns {@code true} if this task completed without throwing an
> +     * exception and was not cancelled.
> +     *
> +     * @return {@code true} if this task completed without throwing an
> +     * exception and was not cancelled
> +     */
> +    public final boolean isCompletedNormally() {
> +        int s;
> +        while ((s = state) == COMPLETING)
> +            Thread.yield();
> +        return s == NORMAL;
> +    }
> +
>      public boolean cancel(boolean mayInterruptIfRunning) {
>          if (!(state == NEW &&
>                U.compareAndSwapInt(this, STATE, NEW,
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150305/ef258fe0/attachment.html>

From martinrb at google.com  Thu Mar  5 00:25:53 2015
From: martinrb at google.com (Martin Buchholz)
Date: Wed, 4 Mar 2015 21:25:53 -0800
Subject: [concurrency-interest] Future.isCompletedNormally
	Future.isCompletedAbnormally
In-Reply-To: <1425531408728.f31a6788@Nodemailer>
References: <CANPzfU_MfeJcx_e=ntyZkERaaf418j2u=Ve9UC7YFh2dSbNytg@mail.gmail.com>
	<1425531408728.f31a6788@Nodemailer>
Message-ID: <CA+kOe08+b1=nwK17-t98m5w3jZ55Gwdbf9LYRABL7xn07OsN4Q@mail.gmail.com>

On Wed, Mar 4, 2015 at 8:56 PM, <joe.bowbeer at gmail.com> wrote:

> I have a similar complaint as Viktor. These default implementations seem
> too heavyweight.
>

Most concrete implementations will sprout efficient implementations. And
the cost of the default implementations is "only" an exception throw/catch,
and that only when the future failed, which is supposed to be rare.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150304/4e9bb687/attachment.html>

From martinrb at google.com  Thu Mar  5 00:33:06 2015
From: martinrb at google.com (Martin Buchholz)
Date: Wed, 4 Mar 2015 21:33:06 -0800
Subject: [concurrency-interest] Future.isCompletedNormally
	Future.isCompletedAbnormally
In-Reply-To: <CAHjP37HXDrxzrjqdOd0CooZnMJ_NqVY24D0hDW-sP4nGpMOm8Q@mail.gmail.com>
References: <CA+kOe08Gz+Mdje7W1GwCXz7VF1oTxQbV=uC0Y2oFD3-L0+We0w@mail.gmail.com>
	<CAHjP37HXDrxzrjqdOd0CooZnMJ_NqVY24D0hDW-sP4nGpMOm8Q@mail.gmail.com>
Message-ID: <CA+kOe09=h=qig6CyrgabyTnwasKVe3Np4RbQK9sZ3zEro7iefg@mail.gmail.com>

On Wed, Mar 4, 2015 at 9:23 PM, Vitaly Davidovich <vitalyd at gmail.com> wrote:

> What's the motivation for doing that? Why moving this up in the hierarchy
> is desired? These methods seem like utilities one would write on top of
> their own concrete implementations.
>

The motivation is "if these methods make sense for ForkJoinTask, then they
make sense for all Futures".

> Also, what's the purpose of the labeled loop? Moreover, what if get () on
> that impl always throws interrupted exception - this loops forever? I guess
> I don't like this because it makes assumptions about how the implementation
> behaves in get ().
>

Let's first decide whether we want these methods before fixing esoteric
bugs (throwing InterruptedException without clearing the interrupt status,
really?)
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150304/98f08f2c/attachment-0001.html>

From vitalyd at gmail.com  Thu Mar  5 00:41:26 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Thu, 5 Mar 2015 00:41:26 -0500
Subject: [concurrency-interest] Future.isCompletedNormally
	Future.isCompletedAbnormally
In-Reply-To: <CA+kOe09=h=qig6CyrgabyTnwasKVe3Np4RbQK9sZ3zEro7iefg@mail.gmail.com>
References: <CA+kOe08Gz+Mdje7W1GwCXz7VF1oTxQbV=uC0Y2oFD3-L0+We0w@mail.gmail.com>
	<CAHjP37HXDrxzrjqdOd0CooZnMJ_NqVY24D0hDW-sP4nGpMOm8Q@mail.gmail.com>
	<CA+kOe09=h=qig6CyrgabyTnwasKVe3Np4RbQK9sZ3zEro7iefg@mail.gmail.com>
Message-ID: <CAHjP37ER22zzeNd40vj3frojK+8_vriX=ijoYL+KvrbdCRU7ng@mail.gmail.com>

Personally,  I'm not seeing the "makes sense for FJT thus also for higher
level Future" reasoning, but maybe that's just me.

It's not about resetting the interrupt status (which happens as well), but
even simpler case where someone decided to throw interrupted exception on
each get ().  I guess what I don't like about this is control flow (with
possible infinite looping) based on calling into the abyss (i.e. some
random Future impl).

Moreover, as mentioned, these seem like util/helper methods and not a core
concept to the interface - I'm not sure slapping them on via default
methods is worth the clutter.

sent from my phone
On Mar 5, 2015 12:33 AM, "Martin Buchholz" <martinrb at google.com> wrote:

>
>
> On Wed, Mar 4, 2015 at 9:23 PM, Vitaly Davidovich <vitalyd at gmail.com>
> wrote:
>
>> What's the motivation for doing that? Why moving this up in the hierarchy
>> is desired? These methods seem like utilities one would write on top of
>> their own concrete implementations.
>>
>
> The motivation is "if these methods make sense for ForkJoinTask, then they
> make sense for all Futures".
>
>> Also, what's the purpose of the labeled loop? Moreover, what if get () on
>> that impl always throws interrupted exception - this loops forever? I guess
>> I don't like this because it makes assumptions about how the implementation
>> behaves in get ().
>>
>
> Let's first decide whether we want these methods before fixing esoteric
> bugs (throwing InterruptedException without clearing the interrupt status,
> really?)
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150305/b27caaaa/attachment.html>

From joe.bowbeer at gmail.com  Thu Mar  5 01:08:07 2015
From: joe.bowbeer at gmail.com (joe.bowbeer at gmail.com)
Date: Wed, 04 Mar 2015 22:08:07 -0800 (PST)
Subject: [concurrency-interest] Future.isCompletedNormally
 Future.isCompletedAbnormally
In-Reply-To: <CA+kOe09=h=qig6CyrgabyTnwasKVe3Np4RbQK9sZ3zEro7iefg@mail.gmail.com>
References: <CA+kOe09=h=qig6CyrgabyTnwasKVe3Np4RbQK9sZ3zEro7iefg@mail.gmail.com>
Message-ID: <1425535687472.e2510674@Nodemailer>

To clarify my position: These methods would be fine if the default implementations were not so complicated (aka heavyweight, and prone to failure when applied to custom Future implementations).

However, I doubt that a better implementation is possible without introducing a sibling of get() to support this kind of use.





?
Sent from Mailbox

On Wed, Mar 4, 2015 at 9:50 PM, Martin Buchholz <martinrb at google.com>
wrote:

> On Wed, Mar 4, 2015 at 9:23 PM, Vitaly Davidovich <vitalyd at gmail.com> wrote:
>> What's the motivation for doing that? Why moving this up in the hierarchy
>> is desired? These methods seem like utilities one would write on top of
>> their own concrete implementations.
>>
> The motivation is "if these methods make sense for ForkJoinTask, then they
> make sense for all Futures".
>> Also, what's the purpose of the labeled loop? Moreover, what if get () on
>> that impl always throws interrupted exception - this loops forever? I guess
>> I don't like this because it makes assumptions about how the implementation
>> behaves in get ().
>>
> Let's first decide whether we want these methods before fixing esoteric
> bugs (throwing InterruptedException without clearing the interrupt status,
> really?)
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150304/52b2598e/attachment.html>

From thurston at nomagicsoftware.com  Thu Mar  5 01:58:56 2015
From: thurston at nomagicsoftware.com (thurstonn)
Date: Wed, 4 Mar 2015 23:58:56 -0700 (MST)
Subject: [concurrency-interest] Future.isCompletedNormally
	Future.isCompletedAbnormally
In-Reply-To: <CA+kOe08Gz+Mdje7W1GwCXz7VF1oTxQbV=uC0Y2oFD3-L0+We0w@mail.gmail.com>
References: <CA+kOe08Gz+Mdje7W1GwCXz7VF1oTxQbV=uC0Y2oFD3-L0+We0w@mail.gmail.com>
Message-ID: <1425538736511-12397.post@n7.nabble.com>

What would be the memory consistency effects of 

isCompletedNormally() ==> true      be?


I assume the "asynchronous computation" would happen-before in that case.


Also, just for discussion, what if you have some :

DefaultFuture<T>(default:T, () -> T) implements Future<T>

what would the semantics of isCompletedAbnormally() be in such a case? 
Might burden the implementation unnecessarily in such a case to strictly
implement it.

I'm not sure how FJP uses FJT's methods, but it doesn't necessarily follow
that those methods should be promoted to the base interface.



--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/Future-isCompletedNormally-Future-isCompletedAbnormally-tp12386p12397.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From martinrb at google.com  Thu Mar  5 10:27:05 2015
From: martinrb at google.com (Martin Buchholz)
Date: Thu, 5 Mar 2015 07:27:05 -0800
Subject: [concurrency-interest] Future.isCompletedNormally
	Future.isCompletedAbnormally
In-Reply-To: <1425538736511-12397.post@n7.nabble.com>
References: <CA+kOe08Gz+Mdje7W1GwCXz7VF1oTxQbV=uC0Y2oFD3-L0+We0w@mail.gmail.com>
	<1425538736511-12397.post@n7.nabble.com>
Message-ID: <CA+kOe08hdSQ=UONbM6WsxcPfxsAx45Z=0eF3ejo7Ubk04tF8Zg@mail.gmail.com>

Even isDone is not (yet) mentioned in
Memory Consistency Properties

in
http://docs.oracle.com/javase/8/docs/api/java/util/concurrent/package-summary.html

All of the is* methods would (obviously) have the same happens-before
semantics.

On Wed, Mar 4, 2015 at 10:58 PM, thurstonn <thurston at nomagicsoftware.com>
wrote:

> What would be the memory consistency effects of
>
> isCompletedNormally() ==> true      be?
>
>
> I assume the "asynchronous computation" would happen-before in that case.
>
>
> Also, just for discussion, what if you have some :
>
> DefaultFuture<T>(default:T, () -> T) implements Future<T>
>
> what would the semantics of isCompletedAbnormally() be in such a case?
> Might burden the implementation unnecessarily in such a case to strictly
> implement it.
>
> I'm not sure how FJP uses FJT's methods, but it doesn't necessarily follow
> that those methods should be promoted to the base interface.
>
>
>
> --
> View this message in context:
> http://jsr166-concurrency.10961.n7.nabble.com/Future-isCompletedNormally-Future-isCompletedAbnormally-tp12386p12397.html
> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150305/0d586116/attachment.html>

From thurston at nomagicsoftware.com  Thu Mar  5 14:07:33 2015
From: thurston at nomagicsoftware.com (thurstonn)
Date: Thu, 5 Mar 2015 12:07:33 -0700 (MST)
Subject: [concurrency-interest] Future.isCompletedNormally
	Future.isCompletedAbnormally
In-Reply-To: <CA+kOe08hdSQ=UONbM6WsxcPfxsAx45Z=0eF3ejo7Ubk04tF8Zg@mail.gmail.com>
References: <CA+kOe08Gz+Mdje7W1GwCXz7VF1oTxQbV=uC0Y2oFD3-L0+We0w@mail.gmail.com>
	<1425538736511-12397.post@n7.nabble.com>
	<CA+kOe08hdSQ=UONbM6WsxcPfxsAx45Z=0eF3ejo7Ubk04tF8Zg@mail.gmail.com>
Message-ID: <1425582453145-12399.post@n7.nabble.com>

Martin Buchholz-3 wrote
> Even isDone is not (yet) mentioned in
> Memory Consistency Properties
> 
> in
> http://docs.oracle.com/javase/8/docs/api/java/util/concurrent/package-summary.html
> 
> All of the is* methods would (obviously) have the same happens-before
> semantics.

But that's deliberate - isDone() can't have any specified memory consistency
rules, since 
the following **doesn't** hold:
 isDone == true iff isCompletedNormally() == true

The usefulness of isCompletedNormally() would lie exactly in this, e,g,

if (f.isCompletedNormally())
   do something  //all done
else if (waitedlongenough())
   f.cancel()
   schedule a backup/replacement execution

That would be useful, especially given that get() throws checked exceptions,
which makes the above presently very unwieldy.
isCompletedNormally should have the same memory consistency effects as a
"successful" return from f.get().  In fact, I'd prefer it be renamed to
wasSuccessful() (but what's in a name?)








--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/Future-isCompletedNormally-Future-isCompletedAbnormally-tp12386p12399.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From martinrb at google.com  Thu Mar  5 14:58:09 2015
From: martinrb at google.com (Martin Buchholz)
Date: Thu, 5 Mar 2015 11:58:09 -0800
Subject: [concurrency-interest] Future.isCompletedNormally
	Future.isCompletedAbnormally
In-Reply-To: <1425582453145-12399.post@n7.nabble.com>
References: <CA+kOe08Gz+Mdje7W1GwCXz7VF1oTxQbV=uC0Y2oFD3-L0+We0w@mail.gmail.com>
	<1425538736511-12397.post@n7.nabble.com>
	<CA+kOe08hdSQ=UONbM6WsxcPfxsAx45Z=0eF3ejo7Ubk04tF8Zg@mail.gmail.com>
	<1425582453145-12399.post@n7.nabble.com>
Message-ID: <CA+kOe0_d3Z0rG4tcUQ2ne6piCZM-8Wz4zJrLdHXi7KaNCAY4AQ@mail.gmail.com>

Perhaps Doug can explain why isCompletedNormally and isCompletedAbnormally
landed in ForkJoinTask but no other Future implementation.

On Thu, Mar 5, 2015 at 11:07 AM, thurstonn <thurston at nomagicsoftware.com>
wrote:

>
> > All of the is* methods would (obviously) have the same happens-before
> > semantics.
>
> But that's deliberate - isDone() can't have any specified memory
> consistency
> rules, since
> the following **doesn't** hold:
>  isDone == true iff isCompletedNormally() == true
>
>
I don't see why that's true.  I expect the action in the thread that makes
the future done (setting result or exceptional value) happens-before
actions isDone returning true


> The usefulness of isCompletedNormally() would lie exactly in this, e,g,
>
> if (f.isCompletedNormally())
>    do something  //all done
> else if (waitedlongenough())
>    f.cancel()
>    schedule a backup/replacement execution
>
>
I don't think eliding the try/catch block is good motivation.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150305/972ea2b4/attachment.html>

From philipp.haller at a3.epfl.ch  Fri Mar  6 11:55:20 2015
From: philipp.haller at a3.epfl.ch (Philipp Haller)
Date: Fri, 6 Mar 2015 17:55:20 +0100
Subject: [concurrency-interest] 2nd CFP: Scala Symposium 2015
Message-ID: <CALEKNwYbycgiKxd-M_MDfLJWkfymvwt_-8MAZYeWp-Lg_eJQOw@mail.gmail.com>

The submission deadline for the 2015 Scala Symposium (formerly, Scala
Workshop) is approaching. Please consider submitting research papers,
experience reports, tool demos, or academic student talks. A lot of
the work on concurrency on/for Java/the JVM is directly relevant to
concurrency in Scala, so we'd be particularly interested in the work
of this community.

The symposium is co-located with PLDI on June 13-14 in Portland, Oregon.

Cheers,
Philipp



========================================================================
                         Scala Symposium 2015

                      co-located with PLDI 2015
                        Portland, Oregon, USA
                           June 13-14, 2015

                        SECOND CALL FOR PAPERS

                 http://lamp.epfl.ch/~hmiller/scala2015
========================================================================

Scala is a general-purpose programming language designed to express common
programming patterns in a concise, elegant, and type-safe way. It smoothly
integrates features of object-oriented and functional languages.

This symposium is a forum for researchers and practitioners to share new ideas
and results of interest to the Scala community.


Important Dates
===============

* Abstract submission:   March 26, 2015
* Paper/talk submission: April 2, 2015
* Author notification:   April 27, 2015
* Final papers due:      May 7, 2015

All deadlines are at 23:59 Baker Island, USA (UTC-12).


Scope
=====

We seek papers on topics related to Scala, including (but not limited to):

- Language design and implementation -- language extensions, optimization, and
  performance evaluation.
- Library design and implementation patterns for extending Scala -- embedded
  domain-specific languages, combining language features, generic and
meta-programming.
- Formal techniques for Scala-like programs -- formalizations of the language,
  type system, and semantics, formalizing proposed language extensions and
  variants, dependent object types, type and effect systems.
- Concurrent and distributed programming -- libraries, frameworks, language
  extensions, programming paradigms: (actors, STM, ...), performance
  evaluation, experimental results.
- Safety and reliability -- pluggable type systems, contracts, static analysis
  and verification, runtime monitoring.
- Tools -- development environments, debuggers, refactoring tools, testing
  frameworks.
- Case studies, experience reports, and pearls.

Submitted papers should describe new ideas, experimental results, or projects
related to Scala. In order to encourage lively discussion, submitted papers
may describe work in progress. All papers will be judged on a combination of
correctness, significance, novelty, clarity, and interest to the community.

In general, papers should explain their original contributions,
identifying what has been accomplished, explaining why it is
significant, and relating it to previous work (also for other
languages where appropriate). Papers in the last category of the list
above need not necessarily report original research results; they may
instead, for example, report practical experience that will be useful
to others, new Scala idioms, or programming pearls. In all cases, such
a paper must make a contribution which is of interest to the Scala
community, or from which other members of the Scala community can
benefit.

KEYWORDS: Library Design and Implementation, Language Design and
Implementation, Applications, Formal Techniques, Parallelism and
Concurrency, Distributed Programming, Tools, Experience Reports,
Empirical Studies


Academic Student Talks
======================

In addition to regular papers and tool demos, we also solicit short student
talks by bachelor/master/PhD students. A student talk is not accompanied by a
paper (it is sufficient to submit a short abstract of the talk in plain text).
Student talks are about 5-10 minutes long, presenting ongoing or completed
research related to Scala. In previous years, each student with an accepted
student talk received a grant (donated by our sponsors) covering registration
and/or travel costs.


Open Source Talks
=================

We will also accept a limited number of short talks about open-source projects
using Scala presented by contributors. An open-source talk is not accompanied
by a paper (it is sufficient to submit a short abstract of the talk in plain
text). Open-source talks are about 10 minutes long, presenting or announcing
an open-source project that is of interest to the Scala community.


Proceedings
===========

It is planned to publish accepted papers in the ACM Digital Library. Authors
must transfer copyright to ACM upon acceptance (for government work,
to the extent transferable), but retain various rights (see ACM Copyright
Policy). Authors are encouraged to publish auxiliary material with their paper
(source code, test data, etc.); they retain copyright of auxiliary material.


Submission Details
==================

Submitted papers should be in portable document format (PDF), formatted using
the standard ACM SIGPLAN two-column conference style (10pt format). Regular
research papers must not exceed 10 pages, tool demonstration papers and short
papers must not exceed 4 pages. "Tool Demos" and "Short Papers" should be
marked as such with those words in the title at time of submission.
Each paper submission must adhere to ACM SIGPLAN's republication policy, as
explained on the web.

Note: "Short Papers" differ from "Tool Demos" in that "Short Papers" are
approached as short research papers. "Short Papers" are expected to carry some
new insights or contribution, and to compare with related work, as with any
normal research paper. They are simply shorter versions of full research
papers. "Tool Demos" on the other hand are about showcasing a well-developed,
well-documented tool, live, before the symposium. Papers corresponding
to "Tool Demos"
are meant to contain an overview of the tool and methodology for the
tool's use. Tool demo papers are less concerned about providing new research
insights, or thoroughly comparing with related work. The Scala Symposium PC
will approach tool demos in the same way as the PEPM'14 Workshop PC, detailed in
PEPM's Tool Paper Evaluation Criteria
(see http://www.program-transformation.org/PEPM14/ToolPaperAdvice).

Student talks and open-source talks are not accompanied by papers. Therefore,
it is sufficient to only submit a plain-text abstract. Both "Student Talks"
and "Open Source Talks" should be marked as such with those words in the title
at time of submission.

Submission see: http://lampwww.epfl.ch/~hmiller/scala2015


Program Committee
=================

* Oscar Boykin, Twitter
* Dave Clarke, Uppsala University
* Doug Lea, State University of New York (SUNY) Oswego
* Ondrej Lhotak, University of Waterloo
* Matt Might, University of Utah
* Adriaan Moors, Typesafe
* Nate Nystrom, University of Lugano
* Bruno Oliveira, University of Hong Kong
* Martin Odersky, EPFL
* Tiark Rompf, Purdue University
* Guido Salvaneschi, TU Darmstadt
* Daniel Spiewak, RichRelevance
* Lex Spoon, Semmle
* Jan Vitek, Northeastern University
* Damien Zufferey, MIT

Organizers
==========

* Philipp Haller, KTH Royal Institute of Technology (Co-chair)
* Heather Miller, EPFL (Co-chair)
* Martin Odersky, EPFL and Typesafe


Links
=====

* The Scala Symposium 2015 website: http://lampwww.epfl.ch/~hmiller/scala2015
* The PLDI 2015 website: http://conf.researchr.org/home/pldi2015

From martinrb at google.com  Fri Mar  6 14:35:51 2015
From: martinrb at google.com (Martin Buchholz)
Date: Fri, 6 Mar 2015 11:35:51 -0800
Subject: [concurrency-interest] Why not "weakNanoTime" for jdk9?
Message-ID: <CA+kOe08ypd93wW4GrcwhUrzGsqx_DnEYXWG6pKnDhs5bHUfb2w@mail.gmail.com>

We all know that nanoTime has problems.  Many (most?) users of nanoTime,
especially for benchmarking or concurrent algorithms, don't really care
about the monotonicity guarantee and just want a "best-effort"
"as-efficient-as-possible" version of nanoTime.  Sure, some algorithms in
java.util.concurrent might need to change slightly to accommodate nanoTime
going backwards, but we already do this to some extent.

http://shipilev.net/blog/2014/nanotrusting-nanotime/

Some users have already created such a private nanoTime.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150306/79f835c9/attachment.html>

From aph at redhat.com  Fri Mar  6 15:14:31 2015
From: aph at redhat.com (Andrew Haley)
Date: Fri, 06 Mar 2015 20:14:31 +0000
Subject: [concurrency-interest] Why not "weakNanoTime" for jdk9?
In-Reply-To: <CA+kOe08ypd93wW4GrcwhUrzGsqx_DnEYXWG6pKnDhs5bHUfb2w@mail.gmail.com>
References: <CA+kOe08ypd93wW4GrcwhUrzGsqx_DnEYXWG6pKnDhs5bHUfb2w@mail.gmail.com>
Message-ID: <54FA0AA7.5090208@redhat.com>

On 03/06/2015 07:35 PM, Martin Buchholz wrote:
> We all know that nanoTime has problems.  Many (most?) users of nanoTime,
> especially for benchmarking or concurrent algorithms, don't really care
> about the monotonicity guarantee and just want a "best-effort"
> "as-efficient-as-possible" version of nanoTime.  Sure, some algorithms in
> java.util.concurrent might need to change slightly to accommodate nanoTime
> going backwards, but we already do this to some extent.
> 
> http://shipilev.net/blog/2014/nanotrusting-nanotime/
> 
> Some users have already created such a private nanoTime.

Indeed.  See
http://developerblog.redhat.com/2014/06/24/ultra-lightweight-high-precision-logger-for-openjdk/
for an example of an interval timer with sub-nanosecond resolution and
about 30-nanosecond latency.

It'd be nice to be able to do something close to that in pure Java.

I'm not at all sure how Aleksey managed to get 30ns latency from
System.nanoTime().  Maybe the implementation has changed since I
tried it.

Andrew.

From aleksey.shipilev at oracle.com  Fri Mar  6 18:27:04 2015
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Sat, 07 Mar 2015 02:27:04 +0300
Subject: [concurrency-interest] Why not "weakNanoTime" for jdk9?
In-Reply-To: <54FA0AA7.5090208@redhat.com>
References: <CA+kOe08ypd93wW4GrcwhUrzGsqx_DnEYXWG6pKnDhs5bHUfb2w@mail.gmail.com>
	<54FA0AA7.5090208@redhat.com>
Message-ID: <54FA37C8.3090800@oracle.com>

Hi guys,

On 03/06/2015 11:14 PM, Andrew Haley wrote:
> On 03/06/2015 07:35 PM, Martin Buchholz wrote:
>> We all know that nanoTime has problems. 

It would really help if you list what problems weakNanoTime is supposed
to solve.

> Many (most?) users of nanoTime,
>> especially for benchmarking or concurrent algorithms, don't really care
>> about the monotonicity guarantee and just want a "best-effort"
>> "as-efficient-as-possible" version of nanoTime. 

I disagree on this premise. In the absence of proper API for CPU
affinity and CPU frequency control, we *do* need monotonicity and
linearity guarantees. Efficiency is a second-order concern here; you can
measure garbage with what you perceive as the nanosecond
precision/latency, but not exactly nanosecond accuracy. And you really
want accuracy, not precision.

Personally, I have doubts you need the single-digit-nanosecond
timestamping. The amount of in-flight ops modern CPUs do at those
timescales greatly blurs the meaning of measured time.


>> Some users have already created such a private nanoTime.
> 
> Indeed.  See
> http://developerblog.redhat.com/2014/06/24/ultra-lightweight-high-precision-logger-for-openjdk/
> for an example of an interval timer with sub-nanosecond resolution and
> about 30-nanosecond latency.

I'm curious what does it mean to have sub-nanosecond resolution with 30
nanosecond latency...


> I'm not at all sure how Aleksey managed to get 30ns latency from
> System.nanoTime().  Maybe the implementation has changed since I
> tried it.

As of now, C2 inlines the System.nanoTime() call into the call to
os::javaTimeNanos in OS-specific runtime code, which is then calling
clock_gettime in glibc, which is then turns to Linux's vDSO
__vdso_clock_gettime, and quickly returns.

Juggling the clock type in os::javaTimeNanos...

CLOCK_MONOTONIC (default now):
 Benchmark                         Mode  Cnt       Score   Error  Units
 TimersBench.granularity_nanotime  avgt    5      22.424 ? 0.609  ns/op
 TimersBench.latency_nanotime      avgt    5      21.569 ? 0.341  ns/op

CLOCK_MONOTONIC_RAW:
 Benchmark                         Mode  Cnt       Score    Error  Units
 TimersBench.granularity_nanotime  avgt    5     328.502 ?  7.631  ns/op
 TimersBench.latency_nanotime      avgt    5     332.489 ? 11.013  ns/op

CLOCK_MONOTONIC_COARSE:
 Benchmark                         Mode  Cnt        Score   Error  Units
 TimersBench.granularity_nanotime  avgt    5  3999733.222 ? 0.403  ns/op
 TimersBench.latency_nanotime      avgt    5        7.046 ? 0.328  ns/op

CLOCK_MONOTONIC_RAW is apparently unsupported by my kernel, and goes to
fallback code and real syscall. Maybe what Andrew was seeing before was
the absence of vDSO opts for CLOCK_MONOTONIC.


Thanks,
-Aleksey.


-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 819 bytes
Desc: OpenPGP digital signature
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150307/5dba1fca/attachment.bin>

From jsampson at guidewire.com  Fri Mar  6 19:41:45 2015
From: jsampson at guidewire.com (Justin Sampson)
Date: Sat, 7 Mar 2015 00:41:45 +0000
Subject: [concurrency-interest] Why not "weakNanoTime" for jdk9?
In-Reply-To: <54FA37C8.3090800@oracle.com>
References: <CA+kOe08ypd93wW4GrcwhUrzGsqx_DnEYXWG6pKnDhs5bHUfb2w@mail.gmail.com>
	<54FA0AA7.5090208@redhat.com> <54FA37C8.3090800@oracle.com>
Message-ID: <0FD072A166C6DC4C851F6115F37DDD279AEED75A@sm-ex-01-vm.guidewire.com>

Aleksey Shipilev wrote:

> It would really help if you list what problems weakNanoTime is
> supposed to solve.

I was talking to Martin about this idea recently so I'll take a shot
at describing why it's appealing to me (with the usual disclaimer
that I know I'm much less of an expert than most other folks here).

The main case I'm interested in is handling timeout calculations in
concurrent operations. The usual case should be that the operation
succeeds without timing out, and if it _does_ time out it's often
after waiting several seconds or minutes, in which case being off
by, say, a few microseconds is not a big deal.

Given those assumptions, we really want the usual case (success) to
be as fast as possible, and especially not to impose any additional
synchronization or volatile accesses. Since strict monotonicity
requires checking some kind of centrally synchronized clock state,
it fails that use case.

Furthermore, in this particular use case, it's trivial to have the
appearance of monotonicity _within_ a particular operation: Just
keep a local variable with the last time seen, and only update it if
the next time seen is greater than the last time seen. No extra
synchronization is required.

The semantics I'm imagining would be for a very fast timer that is
_usually_ monotonic, as long as the current thread stays on one
processor, with occasional blips when switching between processors.
We would still want those blips to be as small as practically
achievable, so I guess there would still have to be some occasional
synchronization to keep the fast timer within some tolerance of the
central system clock.

The way I see it, good concurrency semantics are about acknowledging
the reality of the hardware, and a strictly monotonic clock is
simply not the reality of the hardware when there's more than one
processor involved.

Actually, come to think of it, given an underlying non-monotonic
timer, the weakNanoTime method could easily provide monotonicity on
a per-thread basis without any synchronization overhead. That would
mean most concurrent code wouldn't even have to change to become
tolerant of non-monotonicity. It would just have to be made really
clear to users that different threads might see timer values out of
order relative to each other, though still within some best-effort
tolerance.

Cheers,
Justin


From davidcholmes at aapt.net.au  Fri Mar  6 20:44:24 2015
From: davidcholmes at aapt.net.au (David Holmes)
Date: Sat, 7 Mar 2015 11:44:24 +1000
Subject: [concurrency-interest] Why not "weakNanoTime" for jdk9?
In-Reply-To: <0FD072A166C6DC4C851F6115F37DDD279AEED75A@sm-ex-01-vm.guidewire.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCAECKKOAA.davidcholmes@aapt.net.au>

The VM uses whatever high-resolution "clock" the OS provides it. If
experience shows that clock is not monotonic when it is meant to be then we
added internal guards to ensure time doesn't go backwards (which means
sometimes it stands still which has its own issues). As time goes by and
things like frequency-stable, cross-core synchronized TSC becomes available,
the need for the extra monotonicity guard is reduced, and for those
situations when you know what your platform provides we could have a VM flag
to say "trust the OS timer". I know we have an open bug for that on Solaris.
A new API that simply returns whatever the OS returns may be useful for some
people. However I would not support trying to implement something direct to
the hardware in the VM.

Just be aware that the implementation of these timers varies greatly
depending on the hardware and the OS. And once you throw in virtualization
many more problems arise.

Also we'll start using CLOCK_MONOTONIC_RAW on linux at some point. (There's
a bug open for that too.)

David

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Justin
> Sampson
> Sent: Saturday, 7 March 2015 10:42 AM
> To: Aleksey Shipilev; Andrew Haley; Martin Buchholz;
> concurrency-interest; core-libs-dev
> Subject: Re: [concurrency-interest] Why not "weakNanoTime" for jdk9?
>
>
> Aleksey Shipilev wrote:
>
> > It would really help if you list what problems weakNanoTime is
> > supposed to solve.
>
> I was talking to Martin about this idea recently so I'll take a shot
> at describing why it's appealing to me (with the usual disclaimer
> that I know I'm much less of an expert than most other folks here).
>
> The main case I'm interested in is handling timeout calculations in
> concurrent operations. The usual case should be that the operation
> succeeds without timing out, and if it _does_ time out it's often
> after waiting several seconds or minutes, in which case being off
> by, say, a few microseconds is not a big deal.
>
> Given those assumptions, we really want the usual case (success) to
> be as fast as possible, and especially not to impose any additional
> synchronization or volatile accesses. Since strict monotonicity
> requires checking some kind of centrally synchronized clock state,
> it fails that use case.
>
> Furthermore, in this particular use case, it's trivial to have the
> appearance of monotonicity _within_ a particular operation: Just
> keep a local variable with the last time seen, and only update it if
> the next time seen is greater than the last time seen. No extra
> synchronization is required.
>
> The semantics I'm imagining would be for a very fast timer that is
> _usually_ monotonic, as long as the current thread stays on one
> processor, with occasional blips when switching between processors.
> We would still want those blips to be as small as practically
> achievable, so I guess there would still have to be some occasional
> synchronization to keep the fast timer within some tolerance of the
> central system clock.
>
> The way I see it, good concurrency semantics are about acknowledging
> the reality of the hardware, and a strictly monotonic clock is
> simply not the reality of the hardware when there's more than one
> processor involved.
>
> Actually, come to think of it, given an underlying non-monotonic
> timer, the weakNanoTime method could easily provide monotonicity on
> a per-thread basis without any synchronization overhead. That would
> mean most concurrent code wouldn't even have to change to become
> tolerant of non-monotonicity. It would just have to be made really
> clear to users that different threads might see timer values out of
> order relative to each other, though still within some best-effort
> tolerance.
>
> Cheers,
> Justin
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From lukeisandberg at gmail.com  Sat Mar  7 17:27:38 2015
From: lukeisandberg at gmail.com (Luke Sandberg)
Date: Sat, 7 Mar 2015 14:27:38 -0800
Subject: [concurrency-interest] Future.isCompletedNormally
	Future.isCompletedAbnormally
In-Reply-To: <CA+kOe0_d3Z0rG4tcUQ2ne6piCZM-8Wz4zJrLdHXi7KaNCAY4AQ@mail.gmail.com>
References: <CA+kOe08Gz+Mdje7W1GwCXz7VF1oTxQbV=uC0Y2oFD3-L0+We0w@mail.gmail.com>
	<1425538736511-12397.post@n7.nabble.com>
	<CA+kOe08hdSQ=UONbM6WsxcPfxsAx45Z=0eF3ejo7Ubk04tF8Zg@mail.gmail.com>
	<1425582453145-12399.post@n7.nabble.com>
	<CA+kOe0_d3Z0rG4tcUQ2ne6piCZM-8Wz4zJrLdHXi7KaNCAY4AQ@mail.gmail.com>
Message-ID: <CAO9V1M+t6w-zOgaPgTMSPx=jhHxnHrTrzCoK_E4v8QvF=tJe0w@mail.gmail.com>

I think these would be pretty useful (isCancelled() is pretty useful), I
would also be interested in efficient methods to get the failure cause of a
future (without paying the cost of an ExecutionException).  A lot of time
the executionexception contains no useful information (if a library is
dereferencing a users future), and so i end up discarding the EE or
replacing it with something more useful.

On Thu, Mar 5, 2015 at 11:58 AM, Martin Buchholz <martinrb at google.com>
wrote:

> Perhaps Doug can explain why isCompletedNormally and isCompletedAbnormally
> landed in ForkJoinTask but no other Future implementation.
>
> On Thu, Mar 5, 2015 at 11:07 AM, thurstonn <thurston at nomagicsoftware.com>
> wrote:
>
>>
>> > All of the is* methods would (obviously) have the same happens-before
>> > semantics.
>>
>> But that's deliberate - isDone() can't have any specified memory
>> consistency
>> rules, since
>> the following **doesn't** hold:
>>  isDone == true iff isCompletedNormally() == true
>>
>>
> I don't see why that's true.  I expect the action in the thread that makes
> the future done (setting result or exceptional value) happens-before
> actions isDone returning true
>
>
>> The usefulness of isCompletedNormally() would lie exactly in this, e,g,
>>
>> if (f.isCompletedNormally())
>>    do something  //all done
>> else if (waitedlongenough())
>>    f.cancel()
>>    schedule a backup/replacement execution
>>
>>
> I don't think eliding the try/catch block is good motivation.
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150307/6b7f1d7a/attachment.html>

From joe.bowbeer at gmail.com  Sat Mar  7 18:32:58 2015
From: joe.bowbeer at gmail.com (Joe Bowbeer)
Date: Sat, 7 Mar 2015 15:32:58 -0800
Subject: [concurrency-interest] Future.isCompletedNormally
	Future.isCompletedAbnormally
In-Reply-To: <CAO9V1M+t6w-zOgaPgTMSPx=jhHxnHrTrzCoK_E4v8QvF=tJe0w@mail.gmail.com>
References: <CA+kOe08Gz+Mdje7W1GwCXz7VF1oTxQbV=uC0Y2oFD3-L0+We0w@mail.gmail.com>
	<1425538736511-12397.post@n7.nabble.com>
	<CA+kOe08hdSQ=UONbM6WsxcPfxsAx45Z=0eF3ejo7Ubk04tF8Zg@mail.gmail.com>
	<1425582453145-12399.post@n7.nabble.com>
	<CA+kOe0_d3Z0rG4tcUQ2ne6piCZM-8Wz4zJrLdHXi7KaNCAY4AQ@mail.gmail.com>
	<CAO9V1M+t6w-zOgaPgTMSPx=jhHxnHrTrzCoK_E4v8QvF=tJe0w@mail.gmail.com>
Message-ID: <CAHzJPErUqJ3B8bjDY64r1rL2W-W1A3rz=acfBjLSGwxtQZXVuQ@mail.gmail.com>

More comments and suggestions

1. I think "isCompletedAbnormally" is confusing in relation to the existing
method isCancelled.  Is cancellation an abnormal completion?  I think this
is debatable and therefore a point of confusion.  I think these names are
clearer:

hasFailed()
hasCompletedWithException()

2. hasCompletedNormally instead of isCompletedNormally?

3. Would a non-blocking sibling of get() be a more basic building block for
these methods? For example:

getOptional() returns an Optional that is present if f.isDone, empty if
!f.isDone, and otherwise throws the usual exceptions?  (Except it does
*not* throw InterruptedException?)

--Joe

On Sat, Mar 7, 2015 at 2:27 PM, Luke Sandberg <lukeisandberg at gmail.com>
wrote:

> I think these would be pretty useful (isCancelled() is pretty useful), I
> would also be interested in efficient methods to get the failure cause of a
> future (without paying the cost of an ExecutionException).  A lot of time
> the executionexception contains no useful information (if a library is
> dereferencing a users future), and so i end up discarding the EE or
> replacing it with something more useful.
>
> On Thu, Mar 5, 2015 at 11:58 AM, Martin Buchholz <martinrb at google.com>
> wrote:
>
>> Perhaps Doug can explain why isCompletedNormally and
>> isCompletedAbnormally landed in ForkJoinTask but no other Future
>> implementation.
>>
>> On Thu, Mar 5, 2015 at 11:07 AM, thurstonn <thurston at nomagicsoftware.com>
>> wrote:
>>
>>>
>>> > All of the is* methods would (obviously) have the same happens-before
>>> > semantics.
>>>
>>> But that's deliberate - isDone() can't have any specified memory
>>> consistency
>>> rules, since
>>> the following **doesn't** hold:
>>>  isDone == true iff isCompletedNormally() == true
>>>
>>>
>> I don't see why that's true.  I expect the action in the thread that
>> makes the future done (setting result or exceptional value) happens-before
>> actions isDone returning true
>>
>>
>>> The usefulness of isCompletedNormally() would lie exactly in this, e,g,
>>>
>>> if (f.isCompletedNormally())
>>>    do something  //all done
>>> else if (waitedlongenough())
>>>    f.cancel()
>>>    schedule a backup/replacement execution
>>>
>>>
>> I don't think eliding the try/catch block is good motivation.
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150307/415d351e/attachment.html>

From chris.purcell.39 at gmail.com  Mon Mar  9 05:10:15 2015
From: chris.purcell.39 at gmail.com (Chris Purcell)
Date: Mon, 09 Mar 2015 09:10:15 +0000
Subject: [concurrency-interest] Future.isCompletedNormally
	Future.isCompletedAbnormally
Message-ID: <CAJUoZVY1FnyD4bTBWzkf3yX5Rn93CUmzt7yrt0740esu5ikKuA@mail.gmail.com>

Possible alternative implementation:

default boolean isDoneSuccessfully() {
  try {
    get(this, 0, SECONDS);
    return true;
  } catch (CancellationException
      | ExecutionException
      | TimeoutException e) {
    return false;
  } catch (InterruptedException e) {
    currentThread().interrupt();
    return false;
  }
}

My understanding of hotspot optimisation (limited) suggests (a) the
InterruptedExceptions, CancellationExceptions and TimeoutExceptions will be
interned, (b) the optimiser is more likely to trigger (and hopefully kill
the exceptions entirely?) if we keep this small.

The benefit of a default implementation over doing this by hand, apart from
discoverability and overrideability, is that this is more likely to be
optimised, being a common method.

(I'm assuming interruptions only occur for incomplete futures. I think
that's a valid assumption, otherwise there's no non-blocking way to get a
future's result.)

Chris
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150309/a1c43d17/attachment.html>

From martinrb at google.com  Mon Mar  9 12:45:40 2015
From: martinrb at google.com (Martin Buchholz)
Date: Mon, 9 Mar 2015 09:45:40 -0700
Subject: [concurrency-interest] Future.isCompletedNormally
	Future.isCompletedAbnormally
In-Reply-To: <CAHzJPErUqJ3B8bjDY64r1rL2W-W1A3rz=acfBjLSGwxtQZXVuQ@mail.gmail.com>
References: <CA+kOe08Gz+Mdje7W1GwCXz7VF1oTxQbV=uC0Y2oFD3-L0+We0w@mail.gmail.com>
	<1425538736511-12397.post@n7.nabble.com>
	<CA+kOe08hdSQ=UONbM6WsxcPfxsAx45Z=0eF3ejo7Ubk04tF8Zg@mail.gmail.com>
	<1425582453145-12399.post@n7.nabble.com>
	<CA+kOe0_d3Z0rG4tcUQ2ne6piCZM-8Wz4zJrLdHXi7KaNCAY4AQ@mail.gmail.com>
	<CAO9V1M+t6w-zOgaPgTMSPx=jhHxnHrTrzCoK_E4v8QvF=tJe0w@mail.gmail.com>
	<CAHzJPErUqJ3B8bjDY64r1rL2W-W1A3rz=acfBjLSGwxtQZXVuQ@mail.gmail.com>
Message-ID: <CA+kOe0_fEH-sccaCL_cTy8vM4KCNBg_Twu40fXCB+9uTpRQihA@mail.gmail.com>

I don't think the names of the existing methods is great, but it's not
terrible, and the time for bike shedding the names is past.
(We should have objected when Doug introduced these methods)
http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/CompletableFuture.html#getNow-T-
http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/ForkJoinTask.html#isCompletedNormally--
I just noticed this method that Luke wants:
http://gee.cs.oswego.edu/dl/jsr166/dist/docs/java/util/concurrent/ForkJoinTask.html#getException--



On Sat, Mar 7, 2015 at 3:32 PM, Joe Bowbeer <joe.bowbeer at gmail.com> wrote:

> More comments and suggestions
>
> 1. I think "isCompletedAbnormally" is confusing in relation to the
> existing method isCancelled.  Is cancellation an abnormal completion?  I
> think this is debatable and therefore a point of confusion.  I think these
> names are clearer:
>
> hasFailed()
> hasCompletedWithException()
>
> 2. hasCompletedNormally instead of isCompletedNormally?
>
> 3. Would a non-blocking sibling of get() be a more basic building block
> for these methods? For example:
>
> getOptional() returns an Optional that is present if f.isDone, empty if
> !f.isDone, and otherwise throws the usual exceptions?  (Except it does
> *not* throw InterruptedException?)
>
> --Joe
>
> On Sat, Mar 7, 2015 at 2:27 PM, Luke Sandberg <lukeisandberg at gmail.com>
> wrote:
>
>> I think these would be pretty useful (isCancelled() is pretty useful), I
>> would also be interested in efficient methods to get the failure cause of a
>> future (without paying the cost of an ExecutionException).  A lot of time
>> the executionexception contains no useful information (if a library is
>> dereferencing a users future), and so i end up discarding the EE or
>> replacing it with something more useful.
>>
>> On Thu, Mar 5, 2015 at 11:58 AM, Martin Buchholz <martinrb at google.com>
>> wrote:
>>
>>> Perhaps Doug can explain why isCompletedNormally and
>>> isCompletedAbnormally landed in ForkJoinTask but no other Future
>>> implementation.
>>>
>>> On Thu, Mar 5, 2015 at 11:07 AM, thurstonn <thurston at nomagicsoftware.com
>>> > wrote:
>>>
>>>>
>>>> > All of the is* methods would (obviously) have the same happens-before
>>>> > semantics.
>>>>
>>>> But that's deliberate - isDone() can't have any specified memory
>>>> consistency
>>>> rules, since
>>>> the following **doesn't** hold:
>>>>  isDone == true iff isCompletedNormally() == true
>>>>
>>>>
>>> I don't see why that's true.  I expect the action in the thread that
>>> makes the future done (setting result or exceptional value) happens-before
>>> actions isDone returning true
>>>
>>>
>>>> The usefulness of isCompletedNormally() would lie exactly in this, e,g,
>>>>
>>>> if (f.isCompletedNormally())
>>>>    do something  //all done
>>>> else if (waitedlongenough())
>>>>    f.cancel()
>>>>    schedule a backup/replacement execution
>>>>
>>>>
>>> I don't think eliding the try/catch block is good motivation.
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150309/fe1646d8/attachment.html>

From martinrb at google.com  Mon Mar  9 14:31:20 2015
From: martinrb at google.com (Martin Buchholz)
Date: Mon, 9 Mar 2015 11:31:20 -0700
Subject: [concurrency-interest] Future.isCompletedNormally
	Future.isCompletedAbnormally
In-Reply-To: <CA+kOe0_fEH-sccaCL_cTy8vM4KCNBg_Twu40fXCB+9uTpRQihA@mail.gmail.com>
References: <CA+kOe08Gz+Mdje7W1GwCXz7VF1oTxQbV=uC0Y2oFD3-L0+We0w@mail.gmail.com>
	<1425538736511-12397.post@n7.nabble.com>
	<CA+kOe08hdSQ=UONbM6WsxcPfxsAx45Z=0eF3ejo7Ubk04tF8Zg@mail.gmail.com>
	<1425582453145-12399.post@n7.nabble.com>
	<CA+kOe0_d3Z0rG4tcUQ2ne6piCZM-8Wz4zJrLdHXi7KaNCAY4AQ@mail.gmail.com>
	<CAO9V1M+t6w-zOgaPgTMSPx=jhHxnHrTrzCoK_E4v8QvF=tJe0w@mail.gmail.com>
	<CAHzJPErUqJ3B8bjDY64r1rL2W-W1A3rz=acfBjLSGwxtQZXVuQ@mail.gmail.com>
	<CA+kOe0_fEH-sccaCL_cTy8vM4KCNBg_Twu40fXCB+9uTpRQihA@mail.gmail.com>
Message-ID: <CA+kOe09hH8YT8YK3oHyMquOHrASiAXc3f1rq+xsjBTJm+45HDQ@mail.gmail.com>

I just realized to my horror that existing
CompletableFuture.isCompletedExceptionally and
ForkJoinTask.isCompletedAbnormally have (more or less) the same semantics,
and return true on cancellation.
There's no existing method that returns true on abnormal completion but not
on cancellation.

Existing spec for ForkJoinTask states:

The execution status of tasks may be queried at several levels of detail:
isDone()
<https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ForkJoinTask.html#isDone-->
is true if a task completed in any way (including the case where a task was
cancelled without executing); isCompletedNormally()
<https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ForkJoinTask.html#isCompletedNormally-->
is true if a task completed without cancellation or encountering an
exception; isCancelled()
<https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ForkJoinTask.html#isCancelled-->
is true if the task was cancelled (in which case getException()
<https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ForkJoinTask.html#getException-->
returns a CancellationException
<https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/CancellationException.html>);
and isCompletedAbnormally()
<https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ForkJoinTask.html#isCompletedAbnormally-->
is true if a task was either cancelled or encountered an exception, in
which case getException()
<https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ForkJoinTask.html#getException-->
will return either the encountered exception or CancellationException
<https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/CancellationException.html>.


Can we achieve general consensus that propagating some of these small
convenience methods to more Future implementations is useful?

v 0.2

Index: java/util/concurrent/CompletableFuture.java
===================================================================
RCS file:
/export/home/jsr166/jsr166/jsr166/src/main/java/util/concurrent/CompletableFuture.java,v
retrieving revision 1.158
diff -u -r1.158 CompletableFuture.java
--- java/util/concurrent/CompletableFuture.java    17 Jan 2015 20:05:02
-0000    1.158
+++ java/util/concurrent/CompletableFuture.java    9 Mar 2015 18:12:39 -0000
@@ -2314,6 +2314,29 @@
     }

     /**
+     * Returns {@code true} if this CompletableFuture completed
exceptionally or was cancelled.
+     *
+     * @return {@code true} if this CompletableFuture completed
exceptionally or was cancelled
+     */
+    public final boolean isCompletedAbnormally() {
+        return isCompletedExceptionally();
+    }
+
+    /**
+     * Returns {@code true} if this CompletableFuture completed without
throwing an
+     * exception and was not cancelled.
+     *
+     * @return {@code true} if this CompletableFuture completed without
throwing an
+     * exception and was not cancelled
+     */
+    public final boolean isCompletedNormally() {
+        Object r;
+        return (r = result) != null
+            && (r == NIL
+                || !(r instanceof AltResult));
+    }
+
+    /**
      * Forcibly sets or resets the value subsequently returned by
      * method {@link #get()} and related methods, whether or not
      * already completed. This method is designed for use only in
Index: java/util/concurrent/Future.java
===================================================================
RCS file:
/export/home/jsr166/jsr166/jsr166/src/main/java/util/concurrent/Future.java,v
retrieving revision 1.40
diff -u -r1.40 Future.java
--- java/util/concurrent/Future.java    17 Feb 2015 18:55:39 -0000    1.40
+++ java/util/concurrent/Future.java    9 Mar 2015 18:12:39 -0000
@@ -110,6 +110,74 @@
     boolean isDone();

     /**
+     * Returns {@code true} if this task completed by throwing an
exception or was cancelled.
+     *
+     * @return {@code true} if this task completed by throwing an
exception or was cancelled
+     */
+    default boolean isCompletedAbnormally() {
+        if (isDone()) {
+            if (isCancelled())
+                return true;
+            // Support Future implementations that throw
+            // InterruptedException even when result is available.
+            boolean interrupted = false;
+            try {
+                retry: for (;;) {
+                    try {
+                        get();
+                    } catch (ExecutionException e) {
+                        return true;
+                    } catch (InterruptedException e) {
+                        interrupted = true;
+                        continue retry;
+                    } catch (CancellationException hmmm) {
+                        // Unexpected, given that isCancelled() returned
false
+                    }
+                    break;
+                }
+            } finally {
+                if (interrupted)
+                    Thread.currentThread().interrupt();
+            }
+        }
+        return false;
+    }
+
+    /**
+     * Returns {@code true} if this task completed without throwing an
+     * exception and was not cancelled.
+     *
+     * @return {@code true} if this task completed without throwing an
+     * exception and was not cancelled
+     */
+    default boolean isCompletedNormally() {
+        if (isDone() && !isCancelled()) {
+            // Support Future implementations that throw
+            // InterruptedException even when result is available.
+            boolean interrupted = false;
+            try {
+                retry: for (;;) {
+                    try {
+                        get();
+                        return true;
+                    } catch (ExecutionException e) {
+                    } catch (InterruptedException e) {
+                        interrupted = true;
+                        continue retry;
+                    } catch (CancellationException hmmm) {
+                        // Unexpected, given that isCancelled() returned
false
+                    }
+                    break;
+                }
+            } finally {
+                if (interrupted)
+                    Thread.currentThread().interrupt();
+            }
+        }
+        return false;
+    }
+
+    /**
      * Waits if necessary for the computation to complete, and then
      * retrieves its result.
      *
Index: java/util/concurrent/FutureTask.java
===================================================================
RCS file:
/export/home/jsr166/jsr166/jsr166/src/main/java/util/concurrent/FutureTask.java,v
retrieving revision 1.111
diff -u -r1.111 FutureTask.java
--- java/util/concurrent/FutureTask.java    5 Mar 2015 00:32:52 -0000
1.111
+++ java/util/concurrent/FutureTask.java    9 Mar 2015 18:12:39 -0000
@@ -133,6 +133,32 @@
         return state != NEW;
     }

+    /**
+     * Returns {@code true} if this task completed by throwing an
exception or was cancelled.
+     *
+     * @return {@code true} if this task completed by throwing an
exception or was cancelled
+     */
+    public final boolean isCompletedAbnormally() {
+        int s;
+        while ((s = state) == COMPLETING)
+            Thread.yield();
+        return s == EXCEPTIONAL || s >= CANCELLED;
+    }
+
+    /**
+     * Returns {@code true} if this task completed without throwing an
+     * exception and was not cancelled.
+     *
+     * @return {@code true} if this task completed without throwing an
+     * exception and was not cancelled
+     */
+    public final boolean isCompletedNormally() {
+        int s;
+        while ((s = state) == COMPLETING)
+            Thread.yield();
+        return s == NORMAL;
+    }
+
     public boolean cancel(boolean mayInterruptIfRunning) {
         if (!(state == NEW &&
               U.compareAndSwapInt(this, STATE, NEW,
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150309/f6f84913/attachment-0001.html>

From martinrb at google.com  Mon Mar  9 16:30:26 2015
From: martinrb at google.com (Martin Buchholz)
Date: Mon, 9 Mar 2015 13:30:26 -0700
Subject: [concurrency-interest] Why not "weakNanoTime" for jdk9?
In-Reply-To: <NFBBKALFDCPFIDBNKAPCAECKKOAA.davidcholmes@aapt.net.au>
References: <0FD072A166C6DC4C851F6115F37DDD279AEED75A@sm-ex-01-vm.guidewire.com>
	<NFBBKALFDCPFIDBNKAPCAECKKOAA.davidcholmes@aapt.net.au>
Message-ID: <CA+kOe0_osR6ax9DsU4BbH7w4yVdQAi-nw3JikKFY4L3Nz2vaQA@mail.gmail.com>

Perhaps I shouldn't have tried to do anything on this front, given the
cross-system difficulties that especially David has battled for years.  I'm
surprised Aleksey is not a fan - I expected Aleksey to want a version of
nanoTime more suitable for benchmarking.

I learned about the various "clocks" on linux/posix.  I'm surprised the
low-level CLOCKs also promise monotonicity (CLOCK_MONOTONIC).  Perhaps what
was really originally intended was that CLOCK_MONOTONIC was immune to the
effects of sysadmin action (and also adjustments by NTP would be gradual
and preserve monotonicity).

Anyways, having a version of nanoTime that does not promise monotonicity is
only useful if there is a significant benefit, probably in reduced overhead.

No consensus - giving up.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150309/7a0c91b9/attachment.html>

From pavel.rappo at gmail.com  Wed Mar 11 12:11:03 2015
From: pavel.rappo at gmail.com (Pavel Rappo)
Date: Wed, 11 Mar 2015 16:11:03 +0000
Subject: [concurrency-interest] CompletableFuture.supply
Message-ID: <CAChcVu=8Or+4SCHTxUSFQ+zz_OE99Z9YSK0LBSgC=7nMhchU8g@mail.gmail.com>

Hi, given [1] I wonder if anyone thinks this could be a useful method to be
added to j.u.c.CompletableFuture:

    public static <U> CompletableFuture<U> supply(Supplier<U> supplier)

Just a completable future that is to be completed by one of those who claim its
result. Right now I simulate it like this:

    CompletableFuture.supply(supplier, Runnable::run)       (A)

or a less flexible way [2] like this:

    CompletableFuture.complete( supplier.get() )            (B)

My concern is that it's not very pretty. Also it is the only method which
missing its non-async counterpart (not that I have any kind of method-symmetry
OCD, but anyway).

-------------------------------------------------------------------------------
[1] http://cs.oswego.edu/pipermail/concurrency-interest/2015-January/013600.html
[2] In case "B" I'm starting (unconditionally and synchronously) a possibly
    heavy computation. Which may not be needed at all (e.g. theAcceptEither).

From kasperni at gmail.com  Wed Mar 11 13:02:44 2015
From: kasperni at gmail.com (Kasper Nielsen)
Date: Wed, 11 Mar 2015 18:02:44 +0100
Subject: [concurrency-interest] CompletableFuture.supply
In-Reply-To: <CAChcVu=8Or+4SCHTxUSFQ+zz_OE99Z9YSK0LBSgC=7nMhchU8g@mail.gmail.com>
References: <CAChcVu=8Or+4SCHTxUSFQ+zz_OE99Z9YSK0LBSgC=7nMhchU8g@mail.gmail.com>
Message-ID: <CAPs6151jWOrHbPcAKeyRAdA+tstVY5FQv5VM9Y116nQmyHL4WA@mail.gmail.com>

Hi Pavel,

This method has already been added in latest and will (most likely)  be
included in Java 9.
public static <U> CompletableFuture<U> supplyAsync(Supplier<U> supplier)

see
http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/CompletableFuture.java?revision=1.158&view=markup

- Kasper

On Wed, Mar 11, 2015 at 5:11 PM, Pavel Rappo <pavel.rappo at gmail.com> wrote:

> Hi, given [1] I wonder if anyone thinks this could be a useful method to be
> added to j.u.c.CompletableFuture:
>
>     public static <U> CompletableFuture<U> supply(Supplier<U> supplier)
>
> Just a completable future that is to be completed by one of those who
> claim its
> result. Right now I simulate it like this:
>
>     CompletableFuture.supply(supplier, Runnable::run)       (A)
>
> or a less flexible way [2] like this:
>
>     CompletableFuture.complete( supplier.get() )            (B)
>
> My concern is that it's not very pretty. Also it is the only method which
> missing its non-async counterpart (not that I have any kind of
> method-symmetry
> OCD, but anyway).
>
>
> -------------------------------------------------------------------------------
> [1]
> http://cs.oswego.edu/pipermail/concurrency-interest/2015-January/013600.html
> [2] In case "B" I'm starting (unconditionally and synchronously) a possibly
>     heavy computation. Which may not be needed at all (e.g.
> theAcceptEither).
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150311/ab93ed9a/attachment.html>

From pavel.rappo at gmail.com  Wed Mar 11 13:16:06 2015
From: pavel.rappo at gmail.com (Pavel Rappo)
Date: Wed, 11 Mar 2015 17:16:06 +0000
Subject: [concurrency-interest] CompletableFuture.supply
In-Reply-To: <CAPs6151jWOrHbPcAKeyRAdA+tstVY5FQv5VM9Y116nQmyHL4WA@mail.gmail.com>
References: <CAChcVu=8Or+4SCHTxUSFQ+zz_OE99Z9YSK0LBSgC=7nMhchU8g@mail.gmail.com>
	<CAPs6151jWOrHbPcAKeyRAdA+tstVY5FQv5VM9Y116nQmyHL4WA@mail.gmail.com>
Message-ID: <CAChcVu=fn5cN8O37Jr385kkaXFhpQ9rJRAGjqf9ck8FhZ35Ohw@mail.gmail.com>

Hey Kasper,

You're right. CompletableFuture.supplyAsync is there. What I'm talking about is
a _synchronous_ version of "supplying" functionality.
Unfortunately I've made a typo in my previous mail, so this might have confused
you. It should have been written like this:

    CompletableFuture.supplyAsync(supplier, Runnable::run)  (A)

instead of:

    CompletableFuture.supply(supplier, Runnable::run)       (A)

Any chance this clarifies my previous email?


On Wed, Mar 11, 2015 at 5:02 PM, Kasper Nielsen <kasperni at gmail.com> wrote:
> Hi Pavel,
>
> This method has already been added in latest and will (most likely)  be
> included in Java 9.
> public static <U> CompletableFuture<U> supplyAsync(Supplier<U> supplier)
>
> see
> http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/CompletableFuture.java?revision=1.158&view=markup
>
> - Kasper
>
> On Wed, Mar 11, 2015 at 5:11 PM, Pavel Rappo <pavel.rappo at gmail.com> wrote:
>>
>> Hi, given [1] I wonder if anyone thinks this could be a useful method to
>> be
>> added to j.u.c.CompletableFuture:
>>
>>     public static <U> CompletableFuture<U> supply(Supplier<U> supplier)
>>
>> Just a completable future that is to be completed by one of those who
>> claim its
>> result. Right now I simulate it like this:
>>
>>     CompletableFuture.supply(supplier, Runnable::run)       (A)
>>
>> or a less flexible way [2] like this:
>>
>>     CompletableFuture.complete( supplier.get() )            (B)
>>
>> My concern is that it's not very pretty. Also it is the only method which
>> missing its non-async counterpart (not that I have any kind of
>> method-symmetry
>> OCD, but anyway).
>>
>>
>> -------------------------------------------------------------------------------
>> [1]
>> http://cs.oswego.edu/pipermail/concurrency-interest/2015-January/013600.html
>> [2] In case "B" I'm starting (unconditionally and synchronously) a
>> possibly
>>     heavy computation. Which may not be needed at all (e.g.
>> theAcceptEither).
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>

From dl at cs.oswego.edu  Wed Mar 11 19:23:39 2015
From: dl at cs.oswego.edu (Doug Lea)
Date: Wed, 11 Mar 2015 19:23:39 -0400
Subject: [concurrency-interest] CompletableFuture.supply
In-Reply-To: <CAChcVu=8Or+4SCHTxUSFQ+zz_OE99Z9YSK0LBSgC=7nMhchU8g@mail.gmail.com>
References: <CAChcVu=8Or+4SCHTxUSFQ+zz_OE99Z9YSK0LBSgC=7nMhchU8g@mail.gmail.com>
Message-ID: <5500CE7B.8070100@cs.oswego.edu>

On 03/11/2015 12:11 PM, Pavel Rappo wrote:
> Hi, given [1] I wonder if anyone thinks this could be a useful method to be
> added to j.u.c.CompletableFuture:
>
>      public static <U> CompletableFuture<U> supply(Supplier<U> supplier)
>
> Just a completable future that is to be completed by one of those who claim its
> result. Right now I simulate it like this:
>
>      CompletableFuture.supply(supplier, Runnable::run)       (A)
>
> or a less flexible way [2] like this:
>
>      CompletableFuture.complete( supplier.get() )            (B)

Or:
    CompletableFuture.completedFuture(supplier.get())

One reason for not defining non-async supply was that one of the
above would always be applicable. But if there's a missing use case,
please argue for adding this.

-Doug


>
> My concern is that it's not very pretty. Also it is the only method which
> missing its non-async counterpart (not that I have any kind of method-symmetry
> OCD, but anyway).
>
> -------------------------------------------------------------------------------
> [1] http://cs.oswego.edu/pipermail/concurrency-interest/2015-January/013600.html
> [2] In case "B" I'm starting (unconditionally and synchronously) a possibly
>      heavy computation. Which may not be needed at all (e.g. theAcceptEither).
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>


From pavel.rappo at gmail.com  Thu Mar 12 07:56:35 2015
From: pavel.rappo at gmail.com (Pavel Rappo)
Date: Thu, 12 Mar 2015 11:56:35 +0000
Subject: [concurrency-interest] CompletableFuture.supply
In-Reply-To: <5500CE7B.8070100@cs.oswego.edu>
References: <CAChcVu=8Or+4SCHTxUSFQ+zz_OE99Z9YSK0LBSgC=7nMhchU8g@mail.gmail.com>
	<5500CE7B.8070100@cs.oswego.edu>
Message-ID: <CAChcVum=RKtdjLKQ9LX=03+-3L+u7Y4QdtJmU6ZWi+oLdvBcPQ@mail.gmail.com>

Doug, I don't think there's a missing use case. As you've said it could be done
with methods we already have. Though a similar argument can be applied to
existing methods. For instance `allOf` and `anyOf`. We could have written
something like this every time we needed their functionality [*]:

    public static CompletableFuture<Void> allOf(CompletableFuture<?>... cfs) {
        Objects.requireNonNull(cfs);
        if (cfs.length == 0) {
            return CompletableFuture.completedFuture(null);
        }
        CompletableFuture<Void> allOf;
        allOf = cfs[0].thenRunAsync(() -> {
        });
        for (int i = 1; i < cfs.length; i++) {
            allOf = allOf.runAfterBothAsync(cfs[i], () -> { });
        }
        return allOf;
    }

Agreed, this use case occurs much more frequently and the code to address it is
more error prone. Also implementing it internally in CompletableFuture gives you
a chance to make it more performant than the example above.

Probably you're right. Maybe there's no need for such a simple thing as
CompletableFuture.supply. So on rare occasions I need _synchronous_ supply/run
I will write:

        CompletableFuture.supplyAsync(supplier, Runnable::run);
and
        CompletableFuture.runAsync(command, Runnable::run);

accordingly. Thanks.

-------------------------------------------------------------------------------
[*] Just a sketch for illustrative purposes, may have nasty bugs

On Wed, Mar 11, 2015 at 11:23 PM, Doug Lea <dl at cs.oswego.edu> wrote:
> On 03/11/2015 12:11 PM, Pavel Rappo wrote:
>>
>> Hi, given [1] I wonder if anyone thinks this could be a useful method to
>> be
>> added to j.u.c.CompletableFuture:
>>
>>      public static <U> CompletableFuture<U> supply(Supplier<U> supplier)
>>
>> Just a completable future that is to be completed by one of those who
>> claim its
>> result. Right now I simulate it like this:
>>
>>      CompletableFuture.supply(supplier, Runnable::run)       (A)
>>
>> or a less flexible way [2] like this:
>>
>>      CompletableFuture.complete( supplier.get() )            (B)
>
>
> Or:
>    CompletableFuture.completedFuture(supplier.get())
>
> One reason for not defining non-async supply was that one of the
> above would always be applicable. But if there's a missing use case,
> please argue for adding this.
>
> -Doug
>
>
>>
>> My concern is that it's not very pretty. Also it is the only method which
>> missing its non-async counterpart (not that I have any kind of
>> method-symmetry
>> OCD, but anyway).
>>
>>
>> -------------------------------------------------------------------------------
>> [1]
>> http://cs.oswego.edu/pipermail/concurrency-interest/2015-January/013600.html
>> [2] In case "B" I'm starting (unconditionally and synchronously) a
>> possibly
>>      heavy computation. Which may not be needed at all (e.g.
>> theAcceptEither).
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

From pavel.rappo at gmail.com  Thu Mar 12 11:00:22 2015
From: pavel.rappo at gmail.com (Pavel Rappo)
Date: Thu, 12 Mar 2015 15:00:22 +0000
Subject: [concurrency-interest] CompletableFuture.anyOf
Message-ID: <CAChcVum7O+agGMVbe2u0qJ5w9cCrDxvz0Mm3VpqMkRSHUhXafw@mail.gmail.com>

Hi,

Could anybody please explain to me one of the design decisions behind
CompletableFuture.anyOf? Namely an explicit difference between semantics of
`allOf` and `anyOf` methods.

public static CompletableFuture<Void> allOf(CompletableFuture<?>... cfs):

    "...If no CompletableFutures are provided, returns a CompletableFuture
    completed with the value {@code null}..."

public static CompletableFuture<Object> anyOf(CompletableFuture<?>... cfs):

    "...If no CompletableFutures are provided, returns an incomplete
    CompletableFuture..."

I suppose this difference is due to difference in returned values. `allOf` can
safely return CompletableFuture.completedFuture(null) because the result is
always `Void` and it has nothing to do with results of any of the conjuncts.
On the other hand, `anyOf` is bound to return "a new CompletableFuture that is
completed with _the result or exception of any of the given CompletableFutures_
when one completes". Thus if "empty call" would have returned `null` it would
violate its contract. Because no CompletableFuture would have been actually
completed. Moreover we wouldn't be able to distinguish between this and the
situation when chosen disjunct completed with a null result. If my guess is
correct then the design is no doubt very consistent. At the same time, does an
"empty call" (or a call with a possibly zero-length array)

    CompletableFuture.anyOf()

have any value for API clients and not masking a bug? If not, then wouldn't it
be appropriate to have a signature like this:

    CompletableFuture<Void> allOf(CompletableFuture<?> mandatory,
                                  CompletableFuture<?>... optional):

Thanks.

From ron.pressler at gmail.com  Thu Mar 12 11:04:03 2015
From: ron.pressler at gmail.com (Ron Pressler)
Date: Thu, 12 Mar 2015 17:04:03 +0200
Subject: [concurrency-interest] Java IO Benchmark: Quasar vs. Async
	ForkJoinPool vs. managedBlock
Message-ID: <CABg6-qg4P7Ug0tf2vOjGRyaEmsKeozwUGfwrdMAZjyFyf=6N5w@mail.gmail.com>

Hi.
I'm not the author of the post and can't vouch for the benchmark's validity
(though I am Quasar's main author) -- especially since when it comes to IO
I mostly test sockets and this benchmark measures file IO --  but I thought
some people here might find it interesting:

http://blog.takipi.com/java-io-benchmark-quasar-vs-async-forkjoinpool-vs-managedblock/

Ron
?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150312/4bd518fe/attachment-0001.html>

From vikas.vksingh at gmail.com  Thu Mar 12 18:10:18 2015
From: vikas.vksingh at gmail.com (vikas)
Date: Thu, 12 Mar 2015 15:10:18 -0700 (MST)
Subject: [concurrency-interest] DCL using Fence Intrinsics
Message-ID: <1426198218728-12420.post@n7.nabble.com>

Hi,

  I am trying to understand the fence intrinsic api.
  Pershing has showw how to write DCL in C++ in his blog
  http://preshing.com/20130930/double-checked-locking-is-fixed-in-cpp11/

  I was trying to have a similar thing in Java (*Code1*) 
  
     sun.misc.Unsafe U;
     Singleton instance = null

     Singleton getInstance() {
          Singleton tmp = instance;
         * U.loadFence();*
          if(tmp == null) {
              synchronized(Singleton.class) {
                   tmp = instance;
                   if(tmp == null) {
                       tmp = new Singleton();
                       *U.storeFence();*
                       instance = tmp;
                  }
              }
           }
       return tmp;
     } 
                                    *Code1*
     
   * Will the above Code1 works? *
   
   
------------------------------------------------------------------------------
  
    On similar lines i have another doubt. See below *Code2*.
    if * a* and *b* are normal variables with initial value 0

       T1                                                     T2
     a = 1;                                     
while(unsafe.getIntVolatile(b)!=1);
     unsafe.putIntOrdered(b,1);         assert(a==1); // *will always pass*
                                       
                                     *Code2*
   
    Code2 works because putXXXOrdered and getXXXVolatile forms a happens
before edge.
    i.e. assert in Thread T2 will always pass.

   
-------------------------------------------------------------------------------
    But can we say the same thing for below code (*Code3*)
   
       T1                                                        T2
     a = 1;                                               while(b!=1);
     unsafe.storeFence();                           unsafe.loadFence();
     b = 1;                                               assert(a==1); 
                                     *Code3*

  * /What  prevents the compiler to optimize the while loop in *Code3* to an
infinte loop./*
   So does *Code3 *works? If not, then is there anyway we can achieve the 
   expected behavior using fences. 

   thanks
   vikas
  
   

  
    



--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/DCL-using-Fence-Intrinsics-tp12420.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From vitalyd at gmail.com  Thu Mar 12 19:01:05 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Thu, 12 Mar 2015 19:01:05 -0400
Subject: [concurrency-interest] DCL using Fence Intrinsics
In-Reply-To: <1426198218728-12420.post@n7.nabble.com>
References: <1426198218728-12420.post@n7.nabble.com>
Message-ID: <CAHjP37HvtQu6-4ccDZHLyfiD0dNs9vVwetW_F5Kq_YiEAL+eBA@mail.gmail.com>

1 works, and I can't see why you even need the loadFence.

2 and 3 won't (always) work.  In 2, compiler can move a=1 after the loop.
For 3, if you put loadFence inside the while loop it will work.

sent from my phone
On Mar 12, 2015 6:43 PM, "vikas" <vikas.vksingh at gmail.com> wrote:

> Hi,
>
>   I am trying to understand the fence intrinsic api.
>   Pershing has showw how to write DCL in C++ in his blog
>   http://preshing.com/20130930/double-checked-locking-is-fixed-in-cpp11/
>
>   I was trying to have a similar thing in Java (*Code1*)
>
>      sun.misc.Unsafe U;
>      Singleton instance = null
>
>      Singleton getInstance() {
>           Singleton tmp = instance;
>          * U.loadFence();*
>           if(tmp == null) {
>               synchronized(Singleton.class) {
>                    tmp = instance;
>                    if(tmp == null) {
>                        tmp = new Singleton();
>                        *U.storeFence();*
>                        instance = tmp;
>                   }
>               }
>            }
>        return tmp;
>      }
>                                     *Code1*
>
>    * Will the above Code1 works? *
>
>
>
> ------------------------------------------------------------------------------
>
>     On similar lines i have another doubt. See below *Code2*.
>     if * a* and *b* are normal variables with initial value 0
>
>        T1                                                     T2
>      a = 1;
> while(unsafe.getIntVolatile(b)!=1);
>      unsafe.putIntOrdered(b,1);         assert(a==1); // *will always pass*
>
>                                      *Code2*
>
>     Code2 works because putXXXOrdered and getXXXVolatile forms a happens
> before edge.
>     i.e. assert in Thread T2 will always pass.
>
>
>
> -------------------------------------------------------------------------------
>     But can we say the same thing for below code (*Code3*)
>
>        T1                                                        T2
>      a = 1;                                               while(b!=1);
>      unsafe.storeFence();                           unsafe.loadFence();
>      b = 1;                                               assert(a==1);
>                                      *Code3*
>
>   * /What  prevents the compiler to optimize the while loop in *Code3* to
> an
> infinte loop./*
>    So does *Code3 *works? If not, then is there anyway we can achieve the
>    expected behavior using fences.
>
>    thanks
>    vikas
>
>
>
>
>
>
>
>
> --
> View this message in context:
> http://jsr166-concurrency.10961.n7.nabble.com/DCL-using-Fence-Intrinsics-tp12420.html
> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150312/0b2694a7/attachment.html>

From vikas.vksingh at gmail.com  Thu Mar 12 19:48:16 2015
From: vikas.vksingh at gmail.com (vikas)
Date: Thu, 12 Mar 2015 16:48:16 -0700 (MST)
Subject: [concurrency-interest] DCL using Fence Intrinsics
In-Reply-To: <CAHjP37HvtQu6-4ccDZHLyfiD0dNs9vVwetW_F5Kq_YiEAL+eBA@mail.gmail.com>
References: <1426198218728-12420.post@n7.nabble.com>
	<CAHjP37HvtQu6-4ccDZHLyfiD0dNs9vVwetW_F5Kq_YiEAL+eBA@mail.gmail.com>
Message-ID: <1426204096664-12422.post@n7.nabble.com>

*In 2, compiler can move a=1 after the loop*

Not sure what you meant here a==1 is a read operation and already after the
loop

*For 3, if you put loadFence inside the while loop it will work*
 Not sure why it will work 

*I can't see why you even need the loadFence.*
Probably without load fence you may not see all fields of Singleton fully
initialized.
There is not happens before relation between storeFence and reading of
instance variable.





--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/DCL-using-Fence-Intrinsics-tp12420p12422.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From vikas.vksingh at gmail.com  Thu Mar 12 19:56:03 2015
From: vikas.vksingh at gmail.com (vikas)
Date: Thu, 12 Mar 2015 16:56:03 -0700 (MST)
Subject: [concurrency-interest] DCL using Fence Intrinsics
In-Reply-To: <1426204096664-12422.post@n7.nabble.com>
References: <1426198218728-12420.post@n7.nabble.com>
	<CAHjP37HvtQu6-4ccDZHLyfiD0dNs9vVwetW_F5Kq_YiEAL+eBA@mail.gmail.com>
	<1426204096664-12422.post@n7.nabble.com>
Message-ID: <1426204563513-12423.post@n7.nabble.com>

 / >>For 3, if you put loadFence inside the while loop it will work/ 

  *Yahh i got it, yes it will work if i put loadFence inside the loop*




--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/DCL-using-Fence-Intrinsics-tp12420p12423.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From vitalyd at gmail.com  Thu Mar 12 20:52:22 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Thu, 12 Mar 2015 20:52:22 -0400
Subject: [concurrency-interest] DCL using Fence Intrinsics
In-Reply-To: <1426204096664-12422.post@n7.nabble.com>
References: <1426198218728-12420.post@n7.nabble.com>
	<CAHjP37HvtQu6-4ccDZHLyfiD0dNs9vVwetW_F5Kq_YiEAL+eBA@mail.gmail.com>
	<1426204096664-12422.post@n7.nabble.com>
Message-ID: <CAHjP37GxTbjHOPqQCRTvpRhBWW+fKR7iqLiQ8AFwHFzVe6kn_w@mail.gmail.com>

I'm talking about the assignment of 1 to a done by T1.  The formatting is a
bit off, so perhaps I'm misreading it.

In 1, the writer has a storeFence between constructor and assignment to
field which prevents reader from seeing the reference before initializing
stores are complete.  So, I think the reader sees either null or a fully
constructed object without loadFence.  This is basically mimicing what
happens when you publish an instance with final fields racily.

sent from my phone
On Mar 12, 2015 8:21 PM, "vikas" <vikas.vksingh at gmail.com> wrote:

> *In 2, compiler can move a=1 after the loop*
>
> Not sure what you meant here a==1 is a read operation and already after the
> loop
>
> *For 3, if you put loadFence inside the while loop it will work*
>  Not sure why it will work
>
> *I can't see why you even need the loadFence.*
> Probably without load fence you may not see all fields of Singleton fully
> initialized.
> There is not happens before relation between storeFence and reading of
> instance variable.
>
>
>
>
>
> --
> View this message in context:
> http://jsr166-concurrency.10961.n7.nabble.com/DCL-using-Fence-Intrinsics-tp12420p12422.html
> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150312/6a94a2bd/attachment.html>

From oleksandr.otenko at oracle.com  Fri Mar 13 09:18:24 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Fri, 13 Mar 2015 13:18:24 +0000
Subject: [concurrency-interest] DCL using Fence Intrinsics
In-Reply-To: <1426204096664-12422.post@n7.nabble.com>
References: <1426198218728-12420.post@n7.nabble.com>	<CAHjP37HvtQu6-4ccDZHLyfiD0dNs9vVwetW_F5Kq_YiEAL+eBA@mail.gmail.com>
	<1426204096664-12422.post@n7.nabble.com>
Message-ID: <5502E3A0.7000708@oracle.com>

I think there is a formatting issue. The loop for Code 2 appears in T1, 
but you probably meant T2.

Alex

On 12/03/2015 23:48, vikas wrote:
> *In 2, compiler can move a=1 after the loop*
>
> Not sure what you meant here a==1 is a read operation and already after the
> loop
>
> *For 3, if you put loadFence inside the while loop it will work*
>   Not sure why it will work
>
> *I can't see why you even need the loadFence.*
> Probably without load fence you may not see all fields of Singleton fully
> initialized.
> There is not happens before relation between storeFence and reading of
> instance variable.
>
>
>
>
>
> --
> View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/DCL-using-Fence-Intrinsics-tp12420p12422.html
> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From oleksandr.otenko at oracle.com  Fri Mar 13 09:50:52 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Fri, 13 Mar 2015 13:50:52 +0000
Subject: [concurrency-interest] DCL using Fence Intrinsics
In-Reply-To: <CAHjP37HvtQu6-4ccDZHLyfiD0dNs9vVwetW_F5Kq_YiEAL+eBA@mail.gmail.com>
References: <1426198218728-12420.post@n7.nabble.com>
	<CAHjP37HvtQu6-4ccDZHLyfiD0dNs9vVwetW_F5Kq_YiEAL+eBA@mail.gmail.com>
Message-ID: <5502EB3C.40407@oracle.com>

On 12/03/2015 23:01, Vitaly Davidovich wrote:
>
> 1 works, and I can't see why you even need the loadFence.
>
> 2 and 3 won't (always) work.  In 2, compiler can move a=1 after the 
> loop.  For 3, if you put loadFence inside the while loop it will work.
>

If we assume the loop in 2 was meant to be in T2, then it will work.

For 3, you need to have loadFence inside the loop /and/ after the loop.

Alex

> sent from my phone
>
> On Mar 12, 2015 6:43 PM, "vikas" <vikas.vksingh at gmail.com 
> <mailto:vikas.vksingh at gmail.com>> wrote:
>
>     Hi,
>
>       I am trying to understand the fence intrinsic api.
>       Pershing has showw how to write DCL in C++ in his blog
>     http://preshing.com/20130930/double-checked-locking-is-fixed-in-cpp11/
>
>       I was trying to have a similar thing in Java (*Code1*)
>
>          sun.misc.Unsafe U;
>          Singleton instance = null
>
>          Singleton getInstance() {
>               Singleton tmp = instance;
>              * U.loadFence();*
>               if(tmp == null) {
>                   synchronized(Singleton.class) {
>                        tmp = instance;
>                        if(tmp == null) {
>                            tmp = new Singleton();
>                            *U.storeFence();*
>                            instance = tmp;
>                       }
>                   }
>                }
>            return tmp;
>          }
>                                         *Code1*
>
>        * Will the above Code1 works? *
>
>
>     ------------------------------------------------------------------------------
>
>         On similar lines i have another doubt. See below *Code2*.
>         if * a* and *b* are normal variables with initial value 0
>
>            T1  T2
>          a = 1;
>     while(unsafe.getIntVolatile(b)!=1);
>          unsafe.putIntOrdered(b,1);         assert(a==1); // *will
>     always pass*
>
>                                          *Code2*
>
>         Code2 works because putXXXOrdered and getXXXVolatile forms a
>     happens
>     before edge.
>         i.e. assert in Thread T2 will always pass.
>
>
>     -------------------------------------------------------------------------------
>         But can we say the same thing for below code (*Code3*)
>
>            T1   T2
>          a = 1;  while(b!=1);
>          unsafe.storeFence();  unsafe.loadFence();
>          b = 1;  assert(a==1);
>                                          *Code3*
>
>       * /What  prevents the compiler to optimize the while loop in
>     *Code3* to an
>     infinte loop./*
>        So does *Code3 *works? If not, then is there anyway we can
>     achieve the
>        expected behavior using fences.
>
>        thanks
>        vikas
>
>
>
>
>
>
>
>
>     --
>     View this message in context:
>     http://jsr166-concurrency.10961.n7.nabble.com/DCL-using-Fence-Intrinsics-tp12420.html
>     Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150313/afb40437/attachment-0001.html>

From vitalyd at gmail.com  Fri Mar 13 10:10:03 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Fri, 13 Mar 2015 10:10:03 -0400
Subject: [concurrency-interest] DCL using Fence Intrinsics
In-Reply-To: <5502EB3C.40407@oracle.com>
References: <1426198218728-12420.post@n7.nabble.com>
	<CAHjP37HvtQu6-4ccDZHLyfiD0dNs9vVwetW_F5Kq_YiEAL+eBA@mail.gmail.com>
	<5502EB3C.40407@oracle.com>
Message-ID: <CAHjP37FnGORdO-4WbW0_P7ivxF9KLkr3O4syejv34mNErNMeyw@mail.gmail.com>

Yeah, I read #2 as the while loop being in T1, but if it's T2, then yes,
it's fine and will work.

Thanks for clarifying #3 -- I meant to keep existing code as is but stuff a
loadFence into the loop, but re-reading my reply, I do see how it can be
interpreted as moving the existing one.

On Fri, Mar 13, 2015 at 9:50 AM, Oleksandr Otenko <
oleksandr.otenko at oracle.com> wrote:

>  On 12/03/2015 23:01, Vitaly Davidovich wrote:
>
> 1 works, and I can't see why you even need the loadFence.
>
> 2 and 3 won't (always) work.  In 2, compiler can move a=1 after the loop.
> For 3, if you put loadFence inside the while loop it will work.
>
>
> If we assume the loop in 2 was meant to be in T2, then it will work.
>
> For 3, you need to have loadFence inside the loop *and* after the loop.
>
> Alex
>
>
>  sent from my phone
> On Mar 12, 2015 6:43 PM, "vikas" <vikas.vksingh at gmail.com> wrote:
>
>> Hi,
>>
>>   I am trying to understand the fence intrinsic api.
>>   Pershing has showw how to write DCL in C++ in his blog
>>   http://preshing.com/20130930/double-checked-locking-is-fixed-in-cpp11/
>>
>>   I was trying to have a similar thing in Java (*Code1*)
>>
>>      sun.misc.Unsafe U;
>>      Singleton instance = null
>>
>>      Singleton getInstance() {
>>           Singleton tmp = instance;
>>          * U.loadFence();*
>>           if(tmp == null) {
>>               synchronized(Singleton.class) {
>>                    tmp = instance;
>>                    if(tmp == null) {
>>                        tmp = new Singleton();
>>                        *U.storeFence();*
>>                        instance = tmp;
>>                   }
>>               }
>>            }
>>        return tmp;
>>      }
>>                                     *Code1*
>>
>>    * Will the above Code1 works? *
>>
>>
>>
>> ------------------------------------------------------------------------------
>>
>>     On similar lines i have another doubt. See below *Code2*.
>>     if * a* and *b* are normal variables with initial value 0
>>
>>        T1                                                     T2
>>      a = 1;
>> while(unsafe.getIntVolatile(b)!=1);
>>      unsafe.putIntOrdered(b,1);         assert(a==1); // *will always
>> pass*
>>
>>                                      *Code2*
>>
>>     Code2 works because putXXXOrdered and getXXXVolatile forms a happens
>> before edge.
>>     i.e. assert in Thread T2 will always pass.
>>
>>
>>
>> -------------------------------------------------------------------------------
>>     But can we say the same thing for below code (*Code3*)
>>
>>        T1                                                        T2
>>      a = 1;                                               while(b!=1);
>>      unsafe.storeFence();                           unsafe.loadFence();
>>      b = 1;                                               assert(a==1);
>>                                      *Code3*
>>
>>   * /What  prevents the compiler to optimize the while loop in *Code3* to
>> an
>> infinte loop./*
>>    So does *Code3 *works? If not, then is there anyway we can achieve the
>>    expected behavior using fences.
>>
>>    thanks
>>    vikas
>>
>>
>>
>>
>>
>>
>>
>>
>> --
>> View this message in context:
>> http://jsr166-concurrency.10961.n7.nabble.com/DCL-using-Fence-Intrinsics-tp12420.html
>> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
> _______________________________________________
> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150313/224420a0/attachment.html>

From vitalyd at gmail.com  Fri Mar 13 10:23:43 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Fri, 13 Mar 2015 10:23:43 -0400
Subject: [concurrency-interest] DCL using Fence Intrinsics
In-Reply-To: <CAHjP37FnGORdO-4WbW0_P7ivxF9KLkr3O4syejv34mNErNMeyw@mail.gmail.com>
References: <1426198218728-12420.post@n7.nabble.com>
	<CAHjP37HvtQu6-4ccDZHLyfiD0dNs9vVwetW_F5Kq_YiEAL+eBA@mail.gmail.com>
	<5502EB3C.40407@oracle.com>
	<CAHjP37FnGORdO-4WbW0_P7ivxF9KLkr3O4syejv34mNErNMeyw@mail.gmail.com>
Message-ID: <CAHjP37E2gCf-3baeKeUYs43EuG8McebopLF7YGcqJz47ByHCEQ@mail.gmail.com>

btw, for #3, you'd probably want to rewrite T2 as:

if (b==1) {
   U.loadFence();
} else {
    do {
       U.loadFence();
    }while(b!=1);
}

assert(a==1);

This would avoid an additional load fence upon exiting the while loop (if
the while loop was actually entered).


On Fri, Mar 13, 2015 at 10:10 AM, Vitaly Davidovich <vitalyd at gmail.com>
wrote:

> Yeah, I read #2 as the while loop being in T1, but if it's T2, then yes,
> it's fine and will work.
>
> Thanks for clarifying #3 -- I meant to keep existing code as is but stuff
> a loadFence into the loop, but re-reading my reply, I do see how it can be
> interpreted as moving the existing one.
>
> On Fri, Mar 13, 2015 at 9:50 AM, Oleksandr Otenko <
> oleksandr.otenko at oracle.com> wrote:
>
>>  On 12/03/2015 23:01, Vitaly Davidovich wrote:
>>
>> 1 works, and I can't see why you even need the loadFence.
>>
>> 2 and 3 won't (always) work.  In 2, compiler can move a=1 after the
>> loop.  For 3, if you put loadFence inside the while loop it will work.
>>
>>
>> If we assume the loop in 2 was meant to be in T2, then it will work.
>>
>> For 3, you need to have loadFence inside the loop *and* after the loop.
>>
>> Alex
>>
>>
>>  sent from my phone
>> On Mar 12, 2015 6:43 PM, "vikas" <vikas.vksingh at gmail.com> wrote:
>>
>>> Hi,
>>>
>>>   I am trying to understand the fence intrinsic api.
>>>   Pershing has showw how to write DCL in C++ in his blog
>>>   http://preshing.com/20130930/double-checked-locking-is-fixed-in-cpp11/
>>>
>>>   I was trying to have a similar thing in Java (*Code1*)
>>>
>>>      sun.misc.Unsafe U;
>>>      Singleton instance = null
>>>
>>>      Singleton getInstance() {
>>>           Singleton tmp = instance;
>>>          * U.loadFence();*
>>>           if(tmp == null) {
>>>               synchronized(Singleton.class) {
>>>                    tmp = instance;
>>>                    if(tmp == null) {
>>>                        tmp = new Singleton();
>>>                        *U.storeFence();*
>>>                        instance = tmp;
>>>                   }
>>>               }
>>>            }
>>>        return tmp;
>>>      }
>>>                                     *Code1*
>>>
>>>    * Will the above Code1 works? *
>>>
>>>
>>>
>>> ------------------------------------------------------------------------------
>>>
>>>     On similar lines i have another doubt. See below *Code2*.
>>>     if * a* and *b* are normal variables with initial value 0
>>>
>>>        T1                                                     T2
>>>      a = 1;
>>> while(unsafe.getIntVolatile(b)!=1);
>>>      unsafe.putIntOrdered(b,1);         assert(a==1); // *will always
>>> pass*
>>>
>>>                                      *Code2*
>>>
>>>     Code2 works because putXXXOrdered and getXXXVolatile forms a happens
>>> before edge.
>>>     i.e. assert in Thread T2 will always pass.
>>>
>>>
>>>
>>> -------------------------------------------------------------------------------
>>>     But can we say the same thing for below code (*Code3*)
>>>
>>>        T1                                                        T2
>>>      a = 1;                                               while(b!=1);
>>>      unsafe.storeFence();                           unsafe.loadFence();
>>>      b = 1;                                               assert(a==1);
>>>                                      *Code3*
>>>
>>>   * /What  prevents the compiler to optimize the while loop in *Code3*
>>> to an
>>> infinte loop./*
>>>    So does *Code3 *works? If not, then is there anyway we can achieve the
>>>    expected behavior using fences.
>>>
>>>    thanks
>>>    vikas
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>> --
>>> View this message in context:
>>> http://jsr166-concurrency.10961.n7.nabble.com/DCL-using-Fence-Intrinsics-tp12420.html
>>> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150313/6f860f2d/attachment-0001.html>

From oleksandr.otenko at oracle.com  Fri Mar 13 10:33:11 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Fri, 13 Mar 2015 14:33:11 +0000
Subject: [concurrency-interest] DCL using Fence Intrinsics
In-Reply-To: <CAHjP37E2gCf-3baeKeUYs43EuG8McebopLF7YGcqJz47ByHCEQ@mail.gmail.com>
References: <1426198218728-12420.post@n7.nabble.com>	<CAHjP37HvtQu6-4ccDZHLyfiD0dNs9vVwetW_F5Kq_YiEAL+eBA@mail.gmail.com>	<5502EB3C.40407@oracle.com>	<CAHjP37FnGORdO-4WbW0_P7ivxF9KLkr3O4syejv34mNErNMeyw@mail.gmail.com>
	<CAHjP37E2gCf-3baeKeUYs43EuG8McebopLF7YGcqJz47ByHCEQ@mail.gmail.com>
Message-ID: <5502F527.5080002@oracle.com>

No, you have just shown that you don't need a loadFence after the loop, 
which is wrong.

You need a loadFence between the last load of b and the load of a, to 
preserve the order of loading a after loading b. Then you need a 
loadFence between loads of b, so you keep re-loading b on each iteration.

Alex

On 13/03/2015 14:23, Vitaly Davidovich wrote:
> btw, for #3, you'd probably want to rewrite T2 as:
>
> if (b==1) {
>    U.loadFence();
> } else {
>     do {
>        U.loadFence();
>     }while(b!=1);
> }
>
> assert(a==1);
>
> This would avoid an additional load fence upon exiting the while loop 
> (if the while loop was actually entered).
>
>
> On Fri, Mar 13, 2015 at 10:10 AM, Vitaly Davidovich <vitalyd at gmail.com 
> <mailto:vitalyd at gmail.com>> wrote:
>
>     Yeah, I read #2 as the while loop being in T1, but if it's T2,
>     then yes, it's fine and will work.
>
>     Thanks for clarifying #3 -- I meant to keep existing code as is
>     but stuff a loadFence into the loop, but re-reading my reply, I do
>     see how it can be interpreted as moving the existing one.
>
>     On Fri, Mar 13, 2015 at 9:50 AM, Oleksandr Otenko
>     <oleksandr.otenko at oracle.com <mailto:oleksandr.otenko at oracle.com>>
>     wrote:
>
>         On 12/03/2015 23:01, Vitaly Davidovich wrote:
>>
>>         1 works, and I can't see why you even need the loadFence.
>>
>>         2 and 3 won't (always) work. In 2, compiler can move a=1
>>         after the loop.  For 3, if you put loadFence inside the while
>>         loop it will work.
>>
>
>         If we assume the loop in 2 was meant to be in T2, then it will
>         work.
>
>         For 3, you need to have loadFence inside the loop /and/ after
>         the loop.
>
>         Alex
>
>
>>         sent from my phone
>>
>>         On Mar 12, 2015 6:43 PM, "vikas" <vikas.vksingh at gmail.com
>>         <mailto:vikas.vksingh at gmail.com>> wrote:
>>
>>             Hi,
>>
>>               I am trying to understand the fence intrinsic api.
>>               Pershing has showw how to write DCL in C++ in his blog
>>             http://preshing.com/20130930/double-checked-locking-is-fixed-in-cpp11/
>>
>>               I was trying to have a similar thing in Java (*Code1*)
>>
>>                  sun.misc.Unsafe U;
>>                  Singleton instance = null
>>
>>                  Singleton getInstance() {
>>                       Singleton tmp = instance;
>>                      * U.loadFence();*
>>                       if(tmp == null) {
>>             synchronized(Singleton.class) {
>>                                tmp = instance;
>>                                if(tmp == null) {
>>                                    tmp = new Singleton();
>>              *U.storeFence();*
>>                                    instance = tmp;
>>                               }
>>                           }
>>                        }
>>                    return tmp;
>>                  }
>>             *Code1*
>>
>>                * Will the above Code1 works? *
>>
>>
>>             ------------------------------------------------------------------------------
>>
>>                 On similar lines i have another doubt. See below *Code2*.
>>                 if * a* and *b* are normal variables with initial value 0
>>
>>                    T1                        T2
>>                  a = 1;
>>             while(unsafe.getIntVolatile(b)!=1);
>>                  unsafe.putIntOrdered(b,1);  assert(a==1); // *will
>>             always pass*
>>
>>              *Code2*
>>
>>                 Code2 works because putXXXOrdered and getXXXVolatile
>>             forms a happens
>>             before edge.
>>                 i.e. assert in Thread T2 will always pass.
>>
>>
>>             -------------------------------------------------------------------------------
>>                 But can we say the same thing for below code (*Code3*)
>>
>>                    T1                           T2
>>                  a = 1;                    while(b!=1);
>>                  unsafe.storeFence();              unsafe.loadFence();
>>                  b = 1;                    assert(a==1);
>>              *Code3*
>>
>>               * /What  prevents the compiler to optimize the while
>>             loop in *Code3* to an
>>             infinte loop./*
>>                So does *Code3 *works? If not, then is there anyway we
>>             can achieve the
>>                expected behavior using fences.
>>
>>                thanks
>>                vikas
>>
>>
>>
>>
>>
>>
>>
>>
>>             --
>>             View this message in context:
>>             http://jsr166-concurrency.10961.n7.nabble.com/DCL-using-Fence-Intrinsics-tp12420.html
>>             Sent from the JSR166 Concurrency mailing list archive at
>>             Nabble.com.
>>             _______________________________________________
>>             Concurrency-interest mailing list
>>             Concurrency-interest at cs.oswego.edu
>>             <mailto:Concurrency-interest at cs.oswego.edu>
>>             http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>>         _______________________________________________
>>         Concurrency-interest mailing list
>>         Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150313/8293ea81/attachment.html>

From vitalyd at gmail.com  Fri Mar 13 10:55:07 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Fri, 13 Mar 2015 10:55:07 -0400
Subject: [concurrency-interest] DCL using Fence Intrinsics
In-Reply-To: <5502F527.5080002@oracle.com>
References: <1426198218728-12420.post@n7.nabble.com>
	<CAHjP37HvtQu6-4ccDZHLyfiD0dNs9vVwetW_F5Kq_YiEAL+eBA@mail.gmail.com>
	<5502EB3C.40407@oracle.com>
	<CAHjP37FnGORdO-4WbW0_P7ivxF9KLkr3O4syejv34mNErNMeyw@mail.gmail.com>
	<CAHjP37E2gCf-3baeKeUYs43EuG8McebopLF7YGcqJz47ByHCEQ@mail.gmail.com>
	<5502F527.5080002@oracle.com>
Message-ID: <CAHjP37GNuddw5-yxJ1PFrzpAG2_rK2HSqQdj8vQRHVL8DvrtzQ@mail.gmail.com>

So I thought it might be shady, but I can't come up with a *legitimate*
case where it breaks.  One possibility is following reordering:

else {
    do {
       U.loadFence();
        // sink the 'a' read into here, it's still 0, then 'b' reads 1 and
we break
    }while(b!=1);

I can't immediately see why such a transformation would take place because
for compiler to do that, it would have to prove that the loop always
executes only once (otherwise it's moving a load ahead of a loadFence).
It's also making a loop invariant read into a variant one.  I guess it
could clone the code into 2 separate versions, one for looping and one for
not, but seems weird and useless.  I suppose CPU could speculate somehow
here, but again, not immediately clear to me why it would speculate ahead
of 'b' when 'b' is read possibly many times and 'a' is read just once.

But you're right, this "trick" isn't reliable.



On Fri, Mar 13, 2015 at 10:33 AM, Oleksandr Otenko <
oleksandr.otenko at oracle.com> wrote:

>  No, you have just shown that you don't need a loadFence after the loop,
> which is wrong.
>
> You need a loadFence between the last load of b and the load of a, to
> preserve the order of loading a after loading b. Then you need a loadFence
> between loads of b, so you keep re-loading b on each iteration.
>
> Alex
>
>
> On 13/03/2015 14:23, Vitaly Davidovich wrote:
>
> btw, for #3, you'd probably want to rewrite T2 as:
>
>  if (b==1) {
>    U.loadFence();
> } else {
>     do {
>        U.loadFence();
>     }while(b!=1);
> }
>
>  assert(a==1);
>
>  This would avoid an additional load fence upon exiting the while loop
> (if the while loop was actually entered).
>
>
> On Fri, Mar 13, 2015 at 10:10 AM, Vitaly Davidovich <vitalyd at gmail.com>
> wrote:
>
>> Yeah, I read #2 as the while loop being in T1, but if it's T2, then yes,
>> it's fine and will work.
>>
>>  Thanks for clarifying #3 -- I meant to keep existing code as is but
>> stuff a loadFence into the loop, but re-reading my reply, I do see how it
>> can be interpreted as moving the existing one.
>>
>> On Fri, Mar 13, 2015 at 9:50 AM, Oleksandr Otenko <
>> oleksandr.otenko at oracle.com> wrote:
>>
>>>  On 12/03/2015 23:01, Vitaly Davidovich wrote:
>>>
>>> 1 works, and I can't see why you even need the loadFence.
>>>
>>> 2 and 3 won't (always) work.  In 2, compiler can move a=1 after the
>>> loop.  For 3, if you put loadFence inside the while loop it will work.
>>>
>>>
>>>  If we assume the loop in 2 was meant to be in T2, then it will work.
>>>
>>> For 3, you need to have loadFence inside the loop *and* after the loop.
>>>
>>> Alex
>>>
>>>
>>>  sent from my phone
>>> On Mar 12, 2015 6:43 PM, "vikas" <vikas.vksingh at gmail.com> wrote:
>>>
>>>> Hi,
>>>>
>>>>   I am trying to understand the fence intrinsic api.
>>>>   Pershing has showw how to write DCL in C++ in his blog
>>>>
>>>> http://preshing.com/20130930/double-checked-locking-is-fixed-in-cpp11/
>>>>
>>>>   I was trying to have a similar thing in Java (*Code1*)
>>>>
>>>>      sun.misc.Unsafe U;
>>>>      Singleton instance = null
>>>>
>>>>      Singleton getInstance() {
>>>>           Singleton tmp = instance;
>>>>          * U.loadFence();*
>>>>           if(tmp == null) {
>>>>               synchronized(Singleton.class) {
>>>>                    tmp = instance;
>>>>                    if(tmp == null) {
>>>>                        tmp = new Singleton();
>>>>                        *U.storeFence();*
>>>>                        instance = tmp;
>>>>                   }
>>>>               }
>>>>            }
>>>>        return tmp;
>>>>      }
>>>>                                     *Code1*
>>>>
>>>>    * Will the above Code1 works? *
>>>>
>>>>
>>>>
>>>> ------------------------------------------------------------------------------
>>>>
>>>>     On similar lines i have another doubt. See below *Code2*.
>>>>     if * a* and *b* are normal variables with initial value 0
>>>>
>>>>        T1                                                     T2
>>>>      a = 1;
>>>> while(unsafe.getIntVolatile(b)!=1);
>>>>      unsafe.putIntOrdered(b,1);         assert(a==1); // *will always
>>>> pass*
>>>>
>>>>                                      *Code2*
>>>>
>>>>     Code2 works because putXXXOrdered and getXXXVolatile forms a happens
>>>> before edge.
>>>>     i.e. assert in Thread T2 will always pass.
>>>>
>>>>
>>>>
>>>> -------------------------------------------------------------------------------
>>>>     But can we say the same thing for below code (*Code3*)
>>>>
>>>>        T1                                                        T2
>>>>      a = 1;                                               while(b!=1);
>>>>      unsafe.storeFence();                           unsafe.loadFence();
>>>>      b = 1;                                               assert(a==1);
>>>>                                      *Code3*
>>>>
>>>>   * /What  prevents the compiler to optimize the while loop in *Code3*
>>>> to an
>>>> infinte loop./*
>>>>    So does *Code3 *works? If not, then is there anyway we can achieve
>>>> the
>>>>    expected behavior using fences.
>>>>
>>>>    thanks
>>>>    vikas
>>>>
>>>>
>>>>
>>>>
>>>>
>>>>
>>>>
>>>>
>>>> --
>>>> View this message in context:
>>>> http://jsr166-concurrency.10961.n7.nabble.com/DCL-using-Fence-Intrinsics-tp12420.html
>>>> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>>
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150313/abb0b141/attachment-0001.html>

From oleksandr.otenko at oracle.com  Fri Mar 13 11:02:50 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Fri, 13 Mar 2015 15:02:50 +0000
Subject: [concurrency-interest] DCL using Fence Intrinsics
In-Reply-To: <CAHjP37GNuddw5-yxJ1PFrzpAG2_rK2HSqQdj8vQRHVL8DvrtzQ@mail.gmail.com>
References: <1426198218728-12420.post@n7.nabble.com>	<CAHjP37HvtQu6-4ccDZHLyfiD0dNs9vVwetW_F5Kq_YiEAL+eBA@mail.gmail.com>	<5502EB3C.40407@oracle.com>	<CAHjP37FnGORdO-4WbW0_P7ivxF9KLkr3O4syejv34mNErNMeyw@mail.gmail.com>	<CAHjP37E2gCf-3baeKeUYs43EuG8McebopLF7YGcqJz47ByHCEQ@mail.gmail.com>	<5502F527.5080002@oracle.com>
	<CAHjP37GNuddw5-yxJ1PFrzpAG2_rK2HSqQdj8vQRHVL8DvrtzQ@mail.gmail.com>
Message-ID: <5502FC1A.6080503@oracle.com>

First you need to justify the saving of loadFence. You can't assume the 
saving is significant (first it must be predictable) and at the same 
time assume the load of a / breaking the loop is not predictable.

Alex

On 13/03/2015 14:55, Vitaly Davidovich wrote:
> So I thought it might be shady, but I can't come up with a 
> *legitimate* case where it breaks.  One possibility is following 
> reordering:
>
> else {
>     do {
>  U.loadFence();
>         // sink the 'a' read into here, it's still 0, then 'b' reads 1 
> and we break
>     }while(b!=1);
>
> I can't immediately see why such a transformation would take place 
> because for compiler to do that, it would have to prove that the loop 
> always executes only once (otherwise it's moving a load ahead of a 
> loadFence).  It's also making a loop invariant read into a variant 
> one.  I guess it could clone the code into 2 separate versions, one 
> for looping and one for not, but seems weird and useless.  I suppose 
> CPU could speculate somehow here, but again, not immediately clear to 
> me why it would speculate ahead of 'b' when 'b' is read possibly many 
> times and 'a' is read just once.
>
> But you're right, this "trick" isn't reliable.
>
>
>
> On Fri, Mar 13, 2015 at 10:33 AM, Oleksandr Otenko 
> <oleksandr.otenko at oracle.com <mailto:oleksandr.otenko at oracle.com>> wrote:
>
>     No, you have just shown that you don't need a loadFence after the
>     loop, which is wrong.
>
>     You need a loadFence between the last load of b and the load of a,
>     to preserve the order of loading a after loading b. Then you need
>     a loadFence between loads of b, so you keep re-loading b on each
>     iteration.
>
>     Alex
>
>
>     On 13/03/2015 14:23, Vitaly Davidovich wrote:
>>     btw, for #3, you'd probably want to rewrite T2 as:
>>
>>     if (b==1) {
>>        U.loadFence();
>>     } else {
>>         do {
>>            U.loadFence();
>>         }while(b!=1);
>>     }
>>
>>     assert(a==1);
>>
>>     This would avoid an additional load fence upon exiting the while
>>     loop (if the while loop was actually entered).
>>
>>
>>     On Fri, Mar 13, 2015 at 10:10 AM, Vitaly Davidovich
>>     <vitalyd at gmail.com <mailto:vitalyd at gmail.com>> wrote:
>>
>>         Yeah, I read #2 as the while loop being in T1, but if it's
>>         T2, then yes, it's fine and will work.
>>
>>         Thanks for clarifying #3 -- I meant to keep existing code as
>>         is but stuff a loadFence into the loop, but re-reading my
>>         reply, I do see how it can be interpreted as moving the
>>         existing one.
>>
>>         On Fri, Mar 13, 2015 at 9:50 AM, Oleksandr Otenko
>>         <oleksandr.otenko at oracle.com
>>         <mailto:oleksandr.otenko at oracle.com>> wrote:
>>
>>             On 12/03/2015 23:01, Vitaly Davidovich wrote:
>>>
>>>             1 works, and I can't see why you even need the loadFence.
>>>
>>>             2 and 3 won't (always) work.  In 2, compiler can move
>>>             a=1 after the loop.  For 3, if you put loadFence inside
>>>             the while loop it will work.
>>>
>>
>>             If we assume the loop in 2 was meant to be in T2, then it
>>             will work.
>>
>>             For 3, you need to have loadFence inside the loop /and/
>>             after the loop.
>>
>>             Alex
>>
>>
>>>             sent from my phone
>>>
>>>             On Mar 12, 2015 6:43 PM, "vikas"
>>>             <vikas.vksingh at gmail.com
>>>             <mailto:vikas.vksingh at gmail.com>> wrote:
>>>
>>>                 Hi,
>>>
>>>                   I am trying to understand the fence intrinsic api.
>>>                   Pershing has showw how to write DCL in C++ in his blog
>>>                 http://preshing.com/20130930/double-checked-locking-is-fixed-in-cpp11/
>>>
>>>                   I was trying to have a similar thing in Java (*Code1*)
>>>
>>>                      sun.misc.Unsafe U;
>>>                      Singleton instance = null
>>>
>>>                      Singleton getInstance() {
>>>                           Singleton tmp = instance;
>>>                          * U.loadFence();*
>>>                           if(tmp == null) {
>>>                 synchronized(Singleton.class) {
>>>                                    tmp = instance;
>>>                  if(tmp == null) {
>>>                  tmp = new Singleton();
>>>                  *U.storeFence();*
>>>                  instance = tmp;
>>>                                   }
>>>                               }
>>>                            }
>>>                        return tmp;
>>>                      }
>>>                             *Code1*
>>>
>>>                    * Will the above Code1 works? *
>>>
>>>
>>>                 ------------------------------------------------------------------------------
>>>
>>>                     On similar lines i have another doubt. See below
>>>                 *Code2*.
>>>                     if * a* and *b* are normal variables with
>>>                 initial value 0
>>>
>>>                        T1              T2
>>>                      a = 1;
>>>                 while(unsafe.getIntVolatile(b)!=1);
>>>                  unsafe.putIntOrdered(b,1);        assert(a==1); //
>>>                 *will always pass*
>>>
>>>                              *Code2*
>>>
>>>                     Code2 works because putXXXOrdered and
>>>                 getXXXVolatile forms a happens
>>>                 before edge.
>>>                     i.e. assert in Thread T2 will always pass.
>>>
>>>
>>>                 -------------------------------------------------------------------------------
>>>                     But can we say the same thing for below code
>>>                 (*Code3*)
>>>
>>>                        T1                 T2
>>>                      a = 1;          while(b!=1);
>>>                  unsafe.storeFence();  unsafe.loadFence();
>>>                      b = 1;          assert(a==1);
>>>                              *Code3*
>>>
>>>                   * /What  prevents the compiler to optimize the
>>>                 while loop in *Code3* to an
>>>                 infinte loop./*
>>>                    So does *Code3 *works? If not, then is there
>>>                 anyway we can achieve the
>>>                    expected behavior using fences.
>>>
>>>                    thanks
>>>                    vikas
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>                 --
>>>                 View this message in context:
>>>                 http://jsr166-concurrency.10961.n7.nabble.com/DCL-using-Fence-Intrinsics-tp12420.html
>>>                 Sent from the JSR166 Concurrency mailing list
>>>                 archive at Nabble.com.
>>>                 _______________________________________________
>>>                 Concurrency-interest mailing list
>>>                 Concurrency-interest at cs.oswego.edu
>>>                 <mailto:Concurrency-interest at cs.oswego.edu>
>>>                 http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>>
>>>             _______________________________________________
>>>             Concurrency-interest mailing list
>>>             Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>>             http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150313/b9390d77/attachment-0001.html>

From vitalyd at gmail.com  Fri Mar 13 11:06:44 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Fri, 13 Mar 2015 11:06:44 -0400
Subject: [concurrency-interest] DCL using Fence Intrinsics
In-Reply-To: <5502FC1A.6080503@oracle.com>
References: <1426198218728-12420.post@n7.nabble.com>
	<CAHjP37HvtQu6-4ccDZHLyfiD0dNs9vVwetW_F5Kq_YiEAL+eBA@mail.gmail.com>
	<5502EB3C.40407@oracle.com>
	<CAHjP37FnGORdO-4WbW0_P7ivxF9KLkr3O4syejv34mNErNMeyw@mail.gmail.com>
	<CAHjP37E2gCf-3baeKeUYs43EuG8McebopLF7YGcqJz47ByHCEQ@mail.gmail.com>
	<5502F527.5080002@oracle.com>
	<CAHjP37GNuddw5-yxJ1PFrzpAG2_rK2HSqQdj8vQRHVL8DvrtzQ@mail.gmail.com>
	<5502FC1A.6080503@oracle.com>
Message-ID: <CAHjP37FN3pigOEPRkcKfqdrd09u8MFioWVPTD+fQOZ+bk+hUiQ@mail.gmail.com>

Saving is easy to justify: it's at best a heavy compiler fence and at worst
both compiler and cpu.  All loop optimizers (generally) try to move loop
invariant stuff out, not in.

sent from my phone
On Mar 13, 2015 11:03 AM, "Oleksandr Otenko" <oleksandr.otenko at oracle.com>
wrote:

>  First you need to justify the saving of loadFence. You can't assume the
> saving is significant (first it must be predictable) and at the same time
> assume the load of a / breaking the loop is not predictable.
>
> Alex
>
> On 13/03/2015 14:55, Vitaly Davidovich wrote:
>
>  So I thought it might be shady, but I can't come up with a *legitimate*
> case where it breaks.  One possibility is following reordering:
>
>  else {
>     do {
>        U.loadFence();
>         // sink the 'a' read into here, it's still 0, then 'b' reads 1 and
> we break
>     }while(b!=1);
>
>  I can't immediately see why such a transformation would take place
> because for compiler to do that, it would have to prove that the loop
> always executes only once (otherwise it's moving a load ahead of a
> loadFence).  It's also making a loop invariant read into a variant one.  I
> guess it could clone the code into 2 separate versions, one for looping and
> one for not, but seems weird and useless.  I suppose CPU could speculate
> somehow here, but again, not immediately clear to me why it would speculate
> ahead of 'b' when 'b' is read possibly many times and 'a' is read just once.
>
>  But you're right, this "trick" isn't reliable.
>
>
>
> On Fri, Mar 13, 2015 at 10:33 AM, Oleksandr Otenko <
> oleksandr.otenko at oracle.com> wrote:
>
>>  No, you have just shown that you don't need a loadFence after the loop,
>> which is wrong.
>>
>> You need a loadFence between the last load of b and the load of a, to
>> preserve the order of loading a after loading b. Then you need a loadFence
>> between loads of b, so you keep re-loading b on each iteration.
>>
>> Alex
>>
>>
>> On 13/03/2015 14:23, Vitaly Davidovich wrote:
>>
>> btw, for #3, you'd probably want to rewrite T2 as:
>>
>>  if (b==1) {
>>    U.loadFence();
>> } else {
>>     do {
>>        U.loadFence();
>>     }while(b!=1);
>> }
>>
>>  assert(a==1);
>>
>>  This would avoid an additional load fence upon exiting the while loop
>> (if the while loop was actually entered).
>>
>>
>> On Fri, Mar 13, 2015 at 10:10 AM, Vitaly Davidovich <vitalyd at gmail.com>
>> wrote:
>>
>>> Yeah, I read #2 as the while loop being in T1, but if it's T2, then yes,
>>> it's fine and will work.
>>>
>>>  Thanks for clarifying #3 -- I meant to keep existing code as is but
>>> stuff a loadFence into the loop, but re-reading my reply, I do see how it
>>> can be interpreted as moving the existing one.
>>>
>>> On Fri, Mar 13, 2015 at 9:50 AM, Oleksandr Otenko <
>>> oleksandr.otenko at oracle.com> wrote:
>>>
>>>>  On 12/03/2015 23:01, Vitaly Davidovich wrote:
>>>>
>>>> 1 works, and I can't see why you even need the loadFence.
>>>>
>>>> 2 and 3 won't (always) work.  In 2, compiler can move a=1 after the
>>>> loop.  For 3, if you put loadFence inside the while loop it will work.
>>>>
>>>>
>>>>  If we assume the loop in 2 was meant to be in T2, then it will work.
>>>>
>>>> For 3, you need to have loadFence inside the loop *and* after the loop.
>>>>
>>>> Alex
>>>>
>>>>
>>>>  sent from my phone
>>>> On Mar 12, 2015 6:43 PM, "vikas" <vikas.vksingh at gmail.com> wrote:
>>>>
>>>>> Hi,
>>>>>
>>>>>   I am trying to understand the fence intrinsic api.
>>>>>   Pershing has showw how to write DCL in C++ in his blog
>>>>>
>>>>> http://preshing.com/20130930/double-checked-locking-is-fixed-in-cpp11/
>>>>>
>>>>>   I was trying to have a similar thing in Java (*Code1*)
>>>>>
>>>>>      sun.misc.Unsafe U;
>>>>>      Singleton instance = null
>>>>>
>>>>>      Singleton getInstance() {
>>>>>           Singleton tmp = instance;
>>>>>          * U.loadFence();*
>>>>>           if(tmp == null) {
>>>>>               synchronized(Singleton.class) {
>>>>>                    tmp = instance;
>>>>>                    if(tmp == null) {
>>>>>                        tmp = new Singleton();
>>>>>                        *U.storeFence();*
>>>>>                        instance = tmp;
>>>>>                   }
>>>>>               }
>>>>>            }
>>>>>        return tmp;
>>>>>      }
>>>>>                                     *Code1*
>>>>>
>>>>>    * Will the above Code1 works? *
>>>>>
>>>>>
>>>>>
>>>>> ------------------------------------------------------------------------------
>>>>>
>>>>>     On similar lines i have another doubt. See below *Code2*.
>>>>>     if * a* and *b* are normal variables with initial value 0
>>>>>
>>>>>        T1                                                     T2
>>>>>      a = 1;
>>>>> while(unsafe.getIntVolatile(b)!=1);
>>>>>      unsafe.putIntOrdered(b,1);         assert(a==1); // *will always
>>>>> pass*
>>>>>
>>>>>                                      *Code2*
>>>>>
>>>>>     Code2 works because putXXXOrdered and getXXXVolatile forms a
>>>>> happens
>>>>> before edge.
>>>>>     i.e. assert in Thread T2 will always pass.
>>>>>
>>>>>
>>>>>
>>>>> -------------------------------------------------------------------------------
>>>>>     But can we say the same thing for below code (*Code3*)
>>>>>
>>>>>        T1                                                        T2
>>>>>      a = 1;                                               while(b!=1);
>>>>>      unsafe.storeFence();                           unsafe.loadFence();
>>>>>      b = 1;                                               assert(a==1);
>>>>>                                      *Code3*
>>>>>
>>>>>   * /What  prevents the compiler to optimize the while loop in *Code3*
>>>>> to an
>>>>> infinte loop./*
>>>>>    So does *Code3 *works? If not, then is there anyway we can achieve
>>>>> the
>>>>>    expected behavior using fences.
>>>>>
>>>>>    thanks
>>>>>    vikas
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>
>>>>> --
>>>>> View this message in context:
>>>>> http://jsr166-concurrency.10961.n7.nabble.com/DCL-using-Fence-Intrinsics-tp12420.html
>>>>> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at cs.oswego.edu
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>>
>>>
>>
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150313/48df09a0/attachment-0001.html>

From oleksandr.otenko at oracle.com  Fri Mar 13 12:28:17 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Fri, 13 Mar 2015 16:28:17 +0000
Subject: [concurrency-interest] DCL using Fence Intrinsics
In-Reply-To: <CAHjP37FN3pigOEPRkcKfqdrd09u8MFioWVPTD+fQOZ+bk+hUiQ@mail.gmail.com>
References: <1426198218728-12420.post@n7.nabble.com>	<CAHjP37HvtQu6-4ccDZHLyfiD0dNs9vVwetW_F5Kq_YiEAL+eBA@mail.gmail.com>	<5502EB3C.40407@oracle.com>	<CAHjP37FnGORdO-4WbW0_P7ivxF9KLkr3O4syejv34mNErNMeyw@mail.gmail.com>	<CAHjP37E2gCf-3baeKeUYs43EuG8McebopLF7YGcqJz47ByHCEQ@mail.gmail.com>	<5502F527.5080002@oracle.com>	<CAHjP37GNuddw5-yxJ1PFrzpAG2_rK2HSqQdj8vQRHVL8DvrtzQ@mail.gmail.com>	<5502FC1A.6080503@oracle.com>
	<CAHjP37FN3pigOEPRkcKfqdrd09u8MFioWVPTD+fQOZ+bk+hUiQ@mail.gmail.com>
Message-ID: <55031021.6000904@oracle.com>

No, you look at it wrong way around.

Compare the effort of proving the correctness of the resulting code to 
the performance improvement.

Alex


On 13/03/2015 15:06, Vitaly Davidovich wrote:
>
> Saving is easy to justify: it's at best a heavy compiler fence and at 
> worst both compiler and cpu.  All loop optimizers (generally) try to 
> move loop invariant stuff out, not in.
>
> sent from my phone
>
> On Mar 13, 2015 11:03 AM, "Oleksandr Otenko" 
> <oleksandr.otenko at oracle.com <mailto:oleksandr.otenko at oracle.com>> wrote:
>
>     First you need to justify the saving of loadFence. You can't
>     assume the saving is significant (first it must be predictable)
>     and at the same time assume the load of a / breaking the loop is
>     not predictable.
>
>     Alex
>
>     On 13/03/2015 14:55, Vitaly Davidovich wrote:
>>     So I thought it might be shady, but I can't come up with a
>>     *legitimate* case where it breaks.  One possibility is following
>>     reordering:
>>
>>     else {
>>         do {
>>      U.loadFence();
>>             // sink the 'a' read into here, it's still 0, then 'b'
>>     reads 1 and we break
>>     }while(b!=1);
>>
>>     I can't immediately see why such a transformation would take
>>     place because for compiler to do that, it would have to prove
>>     that the loop always executes only once (otherwise it's moving a
>>     load ahead of a loadFence). It's also making a loop invariant
>>     read into a variant one.  I guess it could clone the code into 2
>>     separate versions, one for looping and one for not, but seems
>>     weird and useless.  I suppose CPU could speculate somehow here,
>>     but again, not immediately clear to me why it would speculate
>>     ahead of 'b' when 'b' is read possibly many times and 'a' is read
>>     just once.
>>
>>     But you're right, this "trick" isn't reliable.
>>
>>
>>
>>     On Fri, Mar 13, 2015 at 10:33 AM, Oleksandr Otenko
>>     <oleksandr.otenko at oracle.com
>>     <mailto:oleksandr.otenko at oracle.com>> wrote:
>>
>>         No, you have just shown that you don't need a loadFence after
>>         the loop, which is wrong.
>>
>>         You need a loadFence between the last load of b and the load
>>         of a, to preserve the order of loading a after loading b.
>>         Then you need a loadFence between loads of b, so you keep
>>         re-loading b on each iteration.
>>
>>         Alex
>>
>>
>>         On 13/03/2015 14:23, Vitaly Davidovich wrote:
>>>         btw, for #3, you'd probably want to rewrite T2 as:
>>>
>>>         if (b==1) {
>>>            U.loadFence();
>>>         } else {
>>>             do {
>>>                U.loadFence();
>>>             }while(b!=1);
>>>         }
>>>
>>>         assert(a==1);
>>>
>>>         This would avoid an additional load fence upon exiting the
>>>         while loop (if the while loop was actually entered).
>>>
>>>
>>>         On Fri, Mar 13, 2015 at 10:10 AM, Vitaly Davidovich
>>>         <vitalyd at gmail.com <mailto:vitalyd at gmail.com>> wrote:
>>>
>>>             Yeah, I read #2 as the while loop being in T1, but if
>>>             it's T2, then yes, it's fine and will work.
>>>
>>>             Thanks for clarifying #3 -- I meant to keep existing
>>>             code as is but stuff a loadFence into the loop, but
>>>             re-reading my reply, I do see how it can be interpreted
>>>             as moving the existing one.
>>>
>>>             On Fri, Mar 13, 2015 at 9:50 AM, Oleksandr Otenko
>>>             <oleksandr.otenko at oracle.com
>>>             <mailto:oleksandr.otenko at oracle.com>> wrote:
>>>
>>>                 On 12/03/2015 23:01, Vitaly Davidovich wrote:
>>>>
>>>>                 1 works, and I can't see why you even need the
>>>>                 loadFence.
>>>>
>>>>                 2 and 3 won't (always) work.  In 2, compiler can
>>>>                 move a=1 after the loop. For 3, if you put
>>>>                 loadFence inside the while loop it will work.
>>>>
>>>
>>>                 If we assume the loop in 2 was meant to be in T2,
>>>                 then it will work.
>>>
>>>                 For 3, you need to have loadFence inside the loop
>>>                 /and/ after the loop.
>>>
>>>                 Alex
>>>
>>>
>>>>                 sent from my phone
>>>>
>>>>                 On Mar 12, 2015 6:43 PM, "vikas"
>>>>                 <vikas.vksingh at gmail.com
>>>>                 <mailto:vikas.vksingh at gmail.com>> wrote:
>>>>
>>>>                     Hi,
>>>>
>>>>                       I am trying to understand the fence intrinsic
>>>>                     api.
>>>>                       Pershing has showw how to write DCL in C++ in
>>>>                     his blog
>>>>                     http://preshing.com/20130930/double-checked-locking-is-fixed-in-cpp11/
>>>>
>>>>                       I was trying to have a similar thing in Java
>>>>                     (*Code1*)
>>>>
>>>>                      sun.misc.Unsafe U;
>>>>                          Singleton instance = null
>>>>
>>>>                          Singleton getInstance() {
>>>>                     Singleton tmp = instance;
>>>>                              * U.loadFence();*
>>>>                               if(tmp == null) {
>>>>                     synchronized(Singleton.class) {
>>>>                        tmp = instance;
>>>>                        if(tmp == null) {
>>>>                            tmp = new Singleton();
>>>>                      *U.storeFence();*
>>>>                            instance = tmp;
>>>>                       }
>>>>                                   }
>>>>                                }
>>>>                            return tmp;
>>>>                          }
>>>>                         *Code1*
>>>>
>>>>                        * Will the above Code1 works? *
>>>>
>>>>
>>>>                     ------------------------------------------------------------------------------
>>>>
>>>>                         On similar lines i have another doubt. See
>>>>                     below *Code2*.
>>>>                         if * a* and *b* are normal variables with
>>>>                     initial value 0
>>>>
>>>>                            T1              T2
>>>>                          a = 1;
>>>>                     while(unsafe.getIntVolatile(b)!=1);
>>>>                      unsafe.putIntOrdered(b,1);  assert(a==1); //
>>>>                     *will always pass*
>>>>
>>>>                          *Code2*
>>>>
>>>>                         Code2 works because putXXXOrdered and
>>>>                     getXXXVolatile forms a happens
>>>>                     before edge.
>>>>                         i.e. assert in Thread T2 will always pass.
>>>>
>>>>
>>>>                     -------------------------------------------------------------------------------
>>>>                         But can we say the same thing for below
>>>>                     code (*Code3*)
>>>>
>>>>                            T1 T2
>>>>                          a = 1;  while(b!=1);
>>>>                      unsafe.storeFence();  unsafe.loadFence();
>>>>                          b = 1;  assert(a==1);
>>>>                          *Code3*
>>>>
>>>>                       * /What prevents the compiler to optimize the
>>>>                     while loop in *Code3* to an
>>>>                     infinte loop./*
>>>>                        So does *Code3 *works? If not, then is there
>>>>                     anyway we can achieve the
>>>>                        expected behavior using fences.
>>>>
>>>>                        thanks
>>>>                        vikas
>>>>
>>>>
>>>>
>>>>
>>>>
>>>>
>>>>
>>>>
>>>>                     --
>>>>                     View this message in context:
>>>>                     http://jsr166-concurrency.10961.n7.nabble.com/DCL-using-Fence-Intrinsics-tp12420.html
>>>>                     Sent from the JSR166 Concurrency mailing list
>>>>                     archive at Nabble.com.
>>>>                     _______________________________________________
>>>>                     Concurrency-interest mailing list
>>>>                     Concurrency-interest at cs.oswego.edu
>>>>                     <mailto:Concurrency-interest at cs.oswego.edu>
>>>>                     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>>>
>>>>                 _______________________________________________
>>>>                 Concurrency-interest mailing list
>>>>                 Concurrency-interest at cs.oswego.edu  <mailto:Concurrency-interest at cs.oswego.edu>
>>>>                 http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>>
>>
>>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150313/f0a5abe3/attachment-0001.html>

From vitalyd at gmail.com  Fri Mar 13 12:38:14 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Fri, 13 Mar 2015 12:38:14 -0400
Subject: [concurrency-interest] DCL using Fence Intrinsics
In-Reply-To: <55031021.6000904@oracle.com>
References: <1426198218728-12420.post@n7.nabble.com>
	<CAHjP37HvtQu6-4ccDZHLyfiD0dNs9vVwetW_F5Kq_YiEAL+eBA@mail.gmail.com>
	<5502EB3C.40407@oracle.com>
	<CAHjP37FnGORdO-4WbW0_P7ivxF9KLkr3O4syejv34mNErNMeyw@mail.gmail.com>
	<CAHjP37E2gCf-3baeKeUYs43EuG8McebopLF7YGcqJz47ByHCEQ@mail.gmail.com>
	<5502F527.5080002@oracle.com>
	<CAHjP37GNuddw5-yxJ1PFrzpAG2_rK2HSqQdj8vQRHVL8DvrtzQ@mail.gmail.com>
	<5502FC1A.6080503@oracle.com>
	<CAHjP37FN3pigOEPRkcKfqdrd09u8MFioWVPTD+fQOZ+bk+hUiQ@mail.gmail.com>
	<55031021.6000904@oracle.com>
Message-ID: <CAHjP37GFqpt4TH=00Kg5cRb5sBCnXFQyyt66v-8fVkbsgk_EvQ@mail.gmail.com>

As I mentioned a few replies earlier, I'm not advocating this.  What I was
trying to establish, purely for educational purpose, is a real example of
either compiler and/or cpu transform that would invalidate it.

On Fri, Mar 13, 2015 at 12:28 PM, Oleksandr Otenko <
oleksandr.otenko at oracle.com> wrote:

>  No, you look at it wrong way around.
>
> Compare the effort of proving the correctness of the resulting code to the
> performance improvement.
>
> Alex
>
>
>
> On 13/03/2015 15:06, Vitaly Davidovich wrote:
>
> Saving is easy to justify: it's at best a heavy compiler fence and at
> worst both compiler and cpu.  All loop optimizers (generally) try to move
> loop invariant stuff out, not in.
>
> sent from my phone
> On Mar 13, 2015 11:03 AM, "Oleksandr Otenko" <oleksandr.otenko at oracle.com>
> wrote:
>
>>  First you need to justify the saving of loadFence. You can't assume the
>> saving is significant (first it must be predictable) and at the same time
>> assume the load of a / breaking the loop is not predictable.
>>
>> Alex
>>
>> On 13/03/2015 14:55, Vitaly Davidovich wrote:
>>
>>  So I thought it might be shady, but I can't come up with a *legitimate*
>> case where it breaks.  One possibility is following reordering:
>>
>>  else {
>>     do {
>>        U.loadFence();
>>         // sink the 'a' read into here, it's still 0, then 'b' reads 1
>> and we break
>>     }while(b!=1);
>>
>>  I can't immediately see why such a transformation would take place
>> because for compiler to do that, it would have to prove that the loop
>> always executes only once (otherwise it's moving a load ahead of a
>> loadFence).  It's also making a loop invariant read into a variant one.  I
>> guess it could clone the code into 2 separate versions, one for looping and
>> one for not, but seems weird and useless.  I suppose CPU could speculate
>> somehow here, but again, not immediately clear to me why it would speculate
>> ahead of 'b' when 'b' is read possibly many times and 'a' is read just once.
>>
>>  But you're right, this "trick" isn't reliable.
>>
>>
>>
>> On Fri, Mar 13, 2015 at 10:33 AM, Oleksandr Otenko <
>> oleksandr.otenko at oracle.com> wrote:
>>
>>>  No, you have just shown that you don't need a loadFence after the loop,
>>> which is wrong.
>>>
>>> You need a loadFence between the last load of b and the load of a, to
>>> preserve the order of loading a after loading b. Then you need a loadFence
>>> between loads of b, so you keep re-loading b on each iteration.
>>>
>>> Alex
>>>
>>>
>>> On 13/03/2015 14:23, Vitaly Davidovich wrote:
>>>
>>> btw, for #3, you'd probably want to rewrite T2 as:
>>>
>>>  if (b==1) {
>>>    U.loadFence();
>>> } else {
>>>     do {
>>>        U.loadFence();
>>>     }while(b!=1);
>>> }
>>>
>>>  assert(a==1);
>>>
>>>  This would avoid an additional load fence upon exiting the while loop
>>> (if the while loop was actually entered).
>>>
>>>
>>> On Fri, Mar 13, 2015 at 10:10 AM, Vitaly Davidovich <vitalyd at gmail.com>
>>> wrote:
>>>
>>>> Yeah, I read #2 as the while loop being in T1, but if it's T2, then
>>>> yes, it's fine and will work.
>>>>
>>>>  Thanks for clarifying #3 -- I meant to keep existing code as is but
>>>> stuff a loadFence into the loop, but re-reading my reply, I do see how it
>>>> can be interpreted as moving the existing one.
>>>>
>>>> On Fri, Mar 13, 2015 at 9:50 AM, Oleksandr Otenko <
>>>> oleksandr.otenko at oracle.com> wrote:
>>>>
>>>>>  On 12/03/2015 23:01, Vitaly Davidovich wrote:
>>>>>
>>>>> 1 works, and I can't see why you even need the loadFence.
>>>>>
>>>>> 2 and 3 won't (always) work.  In 2, compiler can move a=1 after the
>>>>> loop.  For 3, if you put loadFence inside the while loop it will work.
>>>>>
>>>>>
>>>>>  If we assume the loop in 2 was meant to be in T2, then it will work.
>>>>>
>>>>> For 3, you need to have loadFence inside the loop *and* after the
>>>>> loop.
>>>>>
>>>>> Alex
>>>>>
>>>>>
>>>>>  sent from my phone
>>>>> On Mar 12, 2015 6:43 PM, "vikas" <vikas.vksingh at gmail.com> wrote:
>>>>>
>>>>>> Hi,
>>>>>>
>>>>>>   I am trying to understand the fence intrinsic api.
>>>>>>   Pershing has showw how to write DCL in C++ in his blog
>>>>>>
>>>>>> http://preshing.com/20130930/double-checked-locking-is-fixed-in-cpp11/
>>>>>>
>>>>>>   I was trying to have a similar thing in Java (*Code1*)
>>>>>>
>>>>>>      sun.misc.Unsafe U;
>>>>>>      Singleton instance = null
>>>>>>
>>>>>>      Singleton getInstance() {
>>>>>>           Singleton tmp = instance;
>>>>>>          * U.loadFence();*
>>>>>>           if(tmp == null) {
>>>>>>               synchronized(Singleton.class) {
>>>>>>                    tmp = instance;
>>>>>>                    if(tmp == null) {
>>>>>>                        tmp = new Singleton();
>>>>>>                        *U.storeFence();*
>>>>>>                        instance = tmp;
>>>>>>                   }
>>>>>>               }
>>>>>>            }
>>>>>>        return tmp;
>>>>>>      }
>>>>>>                                     *Code1*
>>>>>>
>>>>>>    * Will the above Code1 works? *
>>>>>>
>>>>>>
>>>>>>
>>>>>> ------------------------------------------------------------------------------
>>>>>>
>>>>>>     On similar lines i have another doubt. See below *Code2*.
>>>>>>     if * a* and *b* are normal variables with initial value 0
>>>>>>
>>>>>>        T1                                                     T2
>>>>>>      a = 1;
>>>>>> while(unsafe.getIntVolatile(b)!=1);
>>>>>>      unsafe.putIntOrdered(b,1);         assert(a==1); // *will always
>>>>>> pass*
>>>>>>
>>>>>>                                      *Code2*
>>>>>>
>>>>>>     Code2 works because putXXXOrdered and getXXXVolatile forms a
>>>>>> happens
>>>>>> before edge.
>>>>>>     i.e. assert in Thread T2 will always pass.
>>>>>>
>>>>>>
>>>>>>
>>>>>> -------------------------------------------------------------------------------
>>>>>>     But can we say the same thing for below code (*Code3*)
>>>>>>
>>>>>>        T1                                                        T2
>>>>>>      a = 1;                                               while(b!=1);
>>>>>>      unsafe.storeFence();
>>>>>>  unsafe.loadFence();
>>>>>>      b = 1;
>>>>>>  assert(a==1);
>>>>>>                                      *Code3*
>>>>>>
>>>>>>   * /What  prevents the compiler to optimize the while loop in
>>>>>> *Code3* to an
>>>>>> infinte loop./*
>>>>>>    So does *Code3 *works? If not, then is there anyway we can achieve
>>>>>> the
>>>>>>    expected behavior using fences.
>>>>>>
>>>>>>    thanks
>>>>>>    vikas
>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>> --
>>>>>> View this message in context:
>>>>>> http://jsr166-concurrency.10961.n7.nabble.com/DCL-using-Fence-Intrinsics-tp12420.html
>>>>>> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
>>>>>> _______________________________________________
>>>>>> Concurrency-interest mailing list
>>>>>> Concurrency-interest at cs.oswego.edu
>>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>>
>>>>>
>>>>>
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing listConcurrency-interest at cs.oswego.eduhttp://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>>
>>>>>
>>>>
>>>
>>>
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150313/837c52db/attachment-0001.html>

From vikas.vksingh at gmail.com  Fri Mar 13 13:27:43 2015
From: vikas.vksingh at gmail.com (vikas)
Date: Fri, 13 Mar 2015 10:27:43 -0700 (MST)
Subject: [concurrency-interest] DCL using Fence Intrinsics
In-Reply-To: <CAHjP37GFqpt4TH=00Kg5cRb5sBCnXFQyyt66v-8fVkbsgk_EvQ@mail.gmail.com>
References: <CAHjP37HvtQu6-4ccDZHLyfiD0dNs9vVwetW_F5Kq_YiEAL+eBA@mail.gmail.com>
	<5502EB3C.40407@oracle.com>
	<CAHjP37FnGORdO-4WbW0_P7ivxF9KLkr3O4syejv34mNErNMeyw@mail.gmail.com>
	<CAHjP37E2gCf-3baeKeUYs43EuG8McebopLF7YGcqJz47ByHCEQ@mail.gmail.com>
	<5502F527.5080002@oracle.com>
	<CAHjP37GNuddw5-yxJ1PFrzpAG2_rK2HSqQdj8vQRHVL8DvrtzQ@mail.gmail.com>
	<5502FC1A.6080503@oracle.com>
	<CAHjP37FN3pigOEPRkcKfqdrd09u8MFioWVPTD+fQOZ+bk+hUiQ@mail.gmail.com>
	<55031021.6000904@oracle.com>
	<CAHjP37GFqpt4TH=00Kg5cRb5sBCnXFQyyt66v-8fVkbsgk_EvQ@mail.gmail.com>
Message-ID: <1426267663897-12435.post@n7.nabble.com>

Thanks Vitaly,

and sorry for the improper formatting.

on the second note i was wondering why i wouldn't need loadFence in *Code1*
DCL Example

JMM cookbook suggest to insert LoadLoad barrier before final field access
(in processor where data dependency is not respected), my example of DCL
added LoadFence only because of this.

Also C++ example does need both the fences 
http://preshing.com/20130930/double-checked-locking-is-fixed-in-cpp11/

I can think of one reason on why It may work in java without LoadFence is
benign data race-like construct
are kind of allowed in Java whereas they are not allowed in C++.

Also so below *Code4* works for DCL Singleton pattern ?


                                                    *Code4*
  
     sun.misc.Unsafe *U*; 
     Singleton instance = null 

     Singleton getInstance() { 
          Singleton tmp = instance;  // no fence while reading 
          if(tmp == null) { 
              synchronized(Singleton.class) { 
                   tmp = instance; 
                   if(tmp == null) { 
                       tmp = new Singleton(); 
                      * U.storeFence();* // only need StoreFence
                       instance = tmp; 
                  } 
              } 
           } 
       return tmp; 
     } 


 





--
View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/DCL-using-Fence-Intrinsics-tp12420p12435.html
Sent from the JSR166 Concurrency mailing list archive at Nabble.com.

From oleksandr.otenko at oracle.com  Fri Mar 13 14:38:30 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Fri, 13 Mar 2015 18:38:30 +0000
Subject: [concurrency-interest] DCL using Fence Intrinsics
In-Reply-To: <1426267663897-12435.post@n7.nabble.com>
References: <CAHjP37HvtQu6-4ccDZHLyfiD0dNs9vVwetW_F5Kq_YiEAL+eBA@mail.gmail.com>	<5502EB3C.40407@oracle.com>	<CAHjP37FnGORdO-4WbW0_P7ivxF9KLkr3O4syejv34mNErNMeyw@mail.gmail.com>	<CAHjP37E2gCf-3baeKeUYs43EuG8McebopLF7YGcqJz47ByHCEQ@mail.gmail.com>	<5502F527.5080002@oracle.com>	<CAHjP37GNuddw5-yxJ1PFrzpAG2_rK2HSqQdj8vQRHVL8DvrtzQ@mail.gmail.com>	<5502FC1A.6080503@oracle.com>	<CAHjP37FN3pigOEPRkcKfqdrd09u8MFioWVPTD+fQOZ+bk+hUiQ@mail.gmail.com>	<55031021.6000904@oracle.com>	<CAHjP37GFqpt4TH=00Kg5cRb5sBCnXFQyyt66v-8fVkbsgk_EvQ@mail.gmail.com>
	<1426267663897-12435.post@n7.nabble.com>
Message-ID: <55032EA6.8030907@oracle.com>

Vitaly is wrong. The loadFence in Code1 is needed. Without it, it is 
possible to access the uninitialized fields of the singleton. (the loads 
may occur before the load of instance)


Alex


On 13/03/2015 17:27, vikas wrote:
> Thanks Vitaly,
>
> and sorry for the improper formatting.
>
> on the second note i was wondering why i wouldn't need loadFence in *Code1*
> DCL Example
>
> JMM cookbook suggest to insert LoadLoad barrier before final field access
> (in processor where data dependency is not respected), my example of DCL
> added LoadFence only because of this.
>
> Also C++ example does need both the fences
> http://preshing.com/20130930/double-checked-locking-is-fixed-in-cpp11/
>
> I can think of one reason on why It may work in java without LoadFence is
> benign data race-like construct
> are kind of allowed in Java whereas they are not allowed in C++.
>
> Also so below *Code4* works for DCL Singleton pattern ?
>
>
>                                                      *Code4*
>    
>       sun.misc.Unsafe *U*;
>       Singleton instance = null
>
>       Singleton getInstance() {
>            Singleton tmp = instance;  // no fence while reading
>            if(tmp == null) {
>                synchronized(Singleton.class) {
>                     tmp = instance;
>                     if(tmp == null) {
>                         tmp = new Singleton();
>                        * U.storeFence();* // only need StoreFence
>                         instance = tmp;
>                    }
>                }
>             }
>         return tmp;
>       }
>
>
>   
>
>
>
>
>
> --
> View this message in context: http://jsr166-concurrency.10961.n7.nabble.com/DCL-using-Fence-Intrinsics-tp12420p12435.html
> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From vitalyd at gmail.com  Fri Mar 13 14:57:41 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Fri, 13 Mar 2015 14:57:41 -0400
Subject: [concurrency-interest] DCL using Fence Intrinsics
In-Reply-To: <1426267663897-12435.post@n7.nabble.com>
References: <CAHjP37HvtQu6-4ccDZHLyfiD0dNs9vVwetW_F5Kq_YiEAL+eBA@mail.gmail.com>
	<5502EB3C.40407@oracle.com>
	<CAHjP37FnGORdO-4WbW0_P7ivxF9KLkr3O4syejv34mNErNMeyw@mail.gmail.com>
	<CAHjP37E2gCf-3baeKeUYs43EuG8McebopLF7YGcqJz47ByHCEQ@mail.gmail.com>
	<5502F527.5080002@oracle.com>
	<CAHjP37GNuddw5-yxJ1PFrzpAG2_rK2HSqQdj8vQRHVL8DvrtzQ@mail.gmail.com>
	<5502FC1A.6080503@oracle.com>
	<CAHjP37FN3pigOEPRkcKfqdrd09u8MFioWVPTD+fQOZ+bk+hUiQ@mail.gmail.com>
	<55031021.6000904@oracle.com>
	<CAHjP37GFqpt4TH=00Kg5cRb5sBCnXFQyyt66v-8fVkbsgk_EvQ@mail.gmail.com>
	<1426267663897-12435.post@n7.nabble.com>
Message-ID: <CAHjP37FWTa3AvTG_apSwuLS-TPkne_hWZHEQjY7g83mwDkbsPA@mail.gmail.com>

>
> JMM cookbook suggest to insert LoadLoad barrier before final field access
> (in processor where data dependency is not respected), my example of DCL
> added LoadFence only because of this.


Right, the only such processor (that doesn't respect indirection) I've
heard of is Alpha, but AFAIK, that's not a supported platform (for Oracle
Hotspot, at least).

Also C++ example does need both the fences
> http://preshing.com/20130930/double-checked-locking-is-fixed-in-cpp11/


> I can think of one reason on why It may work in java without LoadFence is
> benign data race-like construct
> are kind of allowed in Java whereas they are not allowed in C++.


> Also so below *Code4* works for DCL Singleton pattern ?


One aspect is that java disallows introducing phantom reads when you've
loaded a piece of memory into a temp and are using the temp -- C++ allows
for this, which breaks racy attempts that would succeed in java.

I could be wrong, but my thinking is that Code4 is basically what happens
when you invoke a constructor with final fields (as I mentioned before) and
then publish the reference racily.  Technically speaking, if you were to
take things like the Alpha into account, you most definitely would need a
load fence.  The one doubt in my head is in this snippet of your code:

tmp = instance;
                   if(tmp == null) {
                       tmp = new Singleton();
                      * U.storeFence();* // only need StoreFence
                       instance = tmp;
                  }

It's theoretically conceivable for a compiler to realize that it doesn't
need to store to 'tmp' here and just store to the field directly.  If there
were no storeFence() there, then that definitely isn't right (it's
basically the broken DCL scenario).  With the storeFence() placed where it
is, there's a clear (in my mind, at least) barrier between where
allocation+construction is done and field assignment.  If that's true, and
taking things like Alpha out of the equation, I believe you don't need the
loadFence.

On Fri, Mar 13, 2015 at 1:27 PM, vikas <vikas.vksingh at gmail.com> wrote:

> Thanks Vitaly,
>
> and sorry for the improper formatting.
>
> on the second note i was wondering why i wouldn't need loadFence in *Code1*
> DCL Example
>
> JMM cookbook suggest to insert LoadLoad barrier before final field access
> (in processor where data dependency is not respected), my example of DCL
> added LoadFence only because of this.
>
> Also C++ example does need both the fences
> http://preshing.com/20130930/double-checked-locking-is-fixed-in-cpp11/
>
> I can think of one reason on why It may work in java without LoadFence is
> benign data race-like construct
> are kind of allowed in Java whereas they are not allowed in C++.
>
> Also so below *Code4* works for DCL Singleton pattern ?
>
>
>                                                     *Code4*
>
>      sun.misc.Unsafe *U*;
>      Singleton instance = null
>
>      Singleton getInstance() {
>           Singleton tmp = instance;  // no fence while reading
>           if(tmp == null) {
>               synchronized(Singleton.class) {
>                    tmp = instance;
>                    if(tmp == null) {
>                        tmp = new Singleton();
>                       * U.storeFence();* // only need StoreFence
>                        instance = tmp;
>                   }
>               }
>            }
>        return tmp;
>      }
>
>
>
>
>
>
>
>
> --
> View this message in context:
> http://jsr166-concurrency.10961.n7.nabble.com/DCL-using-Fence-Intrinsics-tp12420p12435.html
> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150313/d6f15abb/attachment.html>

From vitalyd at gmail.com  Fri Mar 13 15:31:11 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Fri, 13 Mar 2015 15:31:11 -0400
Subject: [concurrency-interest] DCL using Fence Intrinsics
In-Reply-To: <55032EA6.8030907@oracle.com>
References: <CAHjP37HvtQu6-4ccDZHLyfiD0dNs9vVwetW_F5Kq_YiEAL+eBA@mail.gmail.com>
	<5502EB3C.40407@oracle.com>
	<CAHjP37FnGORdO-4WbW0_P7ivxF9KLkr3O4syejv34mNErNMeyw@mail.gmail.com>
	<CAHjP37E2gCf-3baeKeUYs43EuG8McebopLF7YGcqJz47ByHCEQ@mail.gmail.com>
	<5502F527.5080002@oracle.com>
	<CAHjP37GNuddw5-yxJ1PFrzpAG2_rK2HSqQdj8vQRHVL8DvrtzQ@mail.gmail.com>
	<5502FC1A.6080503@oracle.com>
	<CAHjP37FN3pigOEPRkcKfqdrd09u8MFioWVPTD+fQOZ+bk+hUiQ@mail.gmail.com>
	<55031021.6000904@oracle.com>
	<CAHjP37GFqpt4TH=00Kg5cRb5sBCnXFQyyt66v-8fVkbsgk_EvQ@mail.gmail.com>
	<1426267663897-12435.post@n7.nabble.com>
	<55032EA6.8030907@oracle.com>
Message-ID: <CAHjP37H+bcHC3DFKpmLksKJENhvQYYKCwmoO663P6FDvdYNMMA@mail.gmail.com>

I mentioned that in my previous reply, but I'm not aware of any JVM running
on platforms that allow such reordering.  I also highly doubt that such a
platform would ever be ported to as bug tail would be very long, along with
JVM having to insert LoadLoad barriers in lots of places where refs are
read of classes with at least one final field.  If you have a
concrete/real/practical example of where this reordering can take place,
I'd love to know about it.

On Fri, Mar 13, 2015 at 2:38 PM, Oleksandr Otenko <
oleksandr.otenko at oracle.com> wrote:

> Vitaly is wrong. The loadFence in Code1 is needed. Without it, it is
> possible to access the uninitialized fields of the singleton. (the loads
> may occur before the load of instance)
>
>
> Alex
>
>
>
> On 13/03/2015 17:27, vikas wrote:
>
>> Thanks Vitaly,
>>
>> and sorry for the improper formatting.
>>
>> on the second note i was wondering why i wouldn't need loadFence in
>> *Code1*
>> DCL Example
>>
>> JMM cookbook suggest to insert LoadLoad barrier before final field access
>> (in processor where data dependency is not respected), my example of DCL
>> added LoadFence only because of this.
>>
>> Also C++ example does need both the fences
>> http://preshing.com/20130930/double-checked-locking-is-fixed-in-cpp11/
>>
>> I can think of one reason on why It may work in java without LoadFence is
>> benign data race-like construct
>> are kind of allowed in Java whereas they are not allowed in C++.
>>
>> Also so below *Code4* works for DCL Singleton pattern ?
>>
>>
>>                                                      *Code4*
>>          sun.misc.Unsafe *U*;
>>       Singleton instance = null
>>
>>       Singleton getInstance() {
>>            Singleton tmp = instance;  // no fence while reading
>>            if(tmp == null) {
>>                synchronized(Singleton.class) {
>>                     tmp = instance;
>>                     if(tmp == null) {
>>                         tmp = new Singleton();
>>                        * U.storeFence();* // only need StoreFence
>>                         instance = tmp;
>>                    }
>>                }
>>             }
>>         return tmp;
>>       }
>>
>>
>>
>>
>>
>>
>>
>> --
>> View this message in context: http://jsr166-concurrency.
>> 10961.n7.nabble.com/DCL-using-Fence-Intrinsics-tp12420p12435.html
>> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150313/f0962ae0/attachment-0001.html>

From oleksandr.otenko at oracle.com  Fri Mar 13 16:22:19 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Fri, 13 Mar 2015 20:22:19 +0000
Subject: [concurrency-interest] DCL using Fence Intrinsics
In-Reply-To: <CAHjP37H+bcHC3DFKpmLksKJENhvQYYKCwmoO663P6FDvdYNMMA@mail.gmail.com>
References: <CAHjP37HvtQu6-4ccDZHLyfiD0dNs9vVwetW_F5Kq_YiEAL+eBA@mail.gmail.com>	<5502EB3C.40407@oracle.com>	<CAHjP37FnGORdO-4WbW0_P7ivxF9KLkr3O4syejv34mNErNMeyw@mail.gmail.com>	<CAHjP37E2gCf-3baeKeUYs43EuG8McebopLF7YGcqJz47ByHCEQ@mail.gmail.com>	<5502F527.5080002@oracle.com>	<CAHjP37GNuddw5-yxJ1PFrzpAG2_rK2HSqQdj8vQRHVL8DvrtzQ@mail.gmail.com>	<5502FC1A.6080503@oracle.com>	<CAHjP37FN3pigOEPRkcKfqdrd09u8MFioWVPTD+fQOZ+bk+hUiQ@mail.gmail.com>	<55031021.6000904@oracle.com>	<CAHjP37GFqpt4TH=00Kg5cRb5sBCnXFQyyt66v-8fVkbsgk_EvQ@mail.gmail.com>	<1426267663897-12435.post@n7.nabble.com>	<55032EA6.8030907@oracle.com>
	<CAHjP37H+bcHC3DFKpmLksKJENhvQYYKCwmoO663P6FDvdYNMMA@mail.gmail.com>
Message-ID: <550346FB.3080401@oracle.com>

Wasn't there a recent thread with a reference to a platform which can 
load even dependent data out of order?

http://en.wikipedia.org/wiki/Memory_ordering
>
>   * Dependent loads can be reordered (this is unique for Alpha). If
>     the processor fetches a pointer to some data after this
>     reordering, it might not fetch the data itself but use stale data
>     which it has already cached and not yet invalidated. Allowing this
>     relaxation makes cache hardware simpler and faster but leads to
>     the requirement of memory barriers for readers and writers.^[5]
>     <http://en.wikipedia.org/wiki/Memory_ordering#cite_note-5>
>


Alex

On 13/03/2015 19:31, Vitaly Davidovich wrote:
> I mentioned that in my previous reply, but I'm not aware of any JVM 
> running on platforms that allow such reordering.  I also highly doubt 
> that such a platform would ever be ported to as bug tail would be very 
> long, along with JVM having to insert LoadLoad barriers in lots of 
> places where refs are read of classes with at least one final field.  
> If you have a concrete/real/practical example of where this reordering 
> can take place, I'd love to know about it.
>
> On Fri, Mar 13, 2015 at 2:38 PM, Oleksandr Otenko 
> <oleksandr.otenko at oracle.com <mailto:oleksandr.otenko at oracle.com>> wrote:
>
>     Vitaly is wrong. The loadFence in Code1 is needed. Without it, it
>     is possible to access the uninitialized fields of the singleton.
>     (the loads may occur before the load of instance)
>
>
>     Alex
>
>
>
>     On 13/03/2015 17:27, vikas wrote:
>
>         Thanks Vitaly,
>
>         and sorry for the improper formatting.
>
>         on the second note i was wondering why i wouldn't need
>         loadFence in *Code1*
>         DCL Example
>
>         JMM cookbook suggest to insert LoadLoad barrier before final
>         field access
>         (in processor where data dependency is not respected), my
>         example of DCL
>         added LoadFence only because of this.
>
>         Also C++ example does need both the fences
>         http://preshing.com/20130930/double-checked-locking-is-fixed-in-cpp11/
>
>         I can think of one reason on why It may work in java without
>         LoadFence is
>         benign data race-like construct
>         are kind of allowed in Java whereas they are not allowed in C++.
>
>         Also so below *Code4* works for DCL Singleton pattern ?
>
>
>          *Code4*
>                  sun.misc.Unsafe *U*;
>               Singleton instance = null
>
>               Singleton getInstance() {
>                    Singleton tmp = instance;  // no fence while reading
>                    if(tmp == null) {
>                        synchronized(Singleton.class) {
>                             tmp = instance;
>                             if(tmp == null) {
>                                 tmp = new Singleton();
>                                * U.storeFence();* // only need StoreFence
>                                 instance = tmp;
>                            }
>                        }
>                     }
>                 return tmp;
>               }
>
>
>
>
>
>
>
>         --
>         View this message in context:
>         http://jsr166-concurrency.10961.n7.nabble.com/DCL-using-Fence-Intrinsics-tp12420p12435.html
>         Sent from the JSR166 Concurrency mailing list archive at
>         Nabble.com.
>         _______________________________________________
>         Concurrency-interest mailing list
>         Concurrency-interest at cs.oswego.edu
>         <mailto:Concurrency-interest at cs.oswego.edu>
>         http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>     _______________________________________________
>     Concurrency-interest mailing list
>     Concurrency-interest at cs.oswego.edu
>     <mailto:Concurrency-interest at cs.oswego.edu>
>     http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150313/ee0070a3/attachment.html>

From vitalyd at gmail.com  Fri Mar 13 17:28:44 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Fri, 13 Mar 2015 17:28:44 -0400
Subject: [concurrency-interest] DCL using Fence Intrinsics
In-Reply-To: <550346FB.3080401@oracle.com>
References: <CAHjP37HvtQu6-4ccDZHLyfiD0dNs9vVwetW_F5Kq_YiEAL+eBA@mail.gmail.com>
	<5502EB3C.40407@oracle.com>
	<CAHjP37FnGORdO-4WbW0_P7ivxF9KLkr3O4syejv34mNErNMeyw@mail.gmail.com>
	<CAHjP37E2gCf-3baeKeUYs43EuG8McebopLF7YGcqJz47ByHCEQ@mail.gmail.com>
	<5502F527.5080002@oracle.com>
	<CAHjP37GNuddw5-yxJ1PFrzpAG2_rK2HSqQdj8vQRHVL8DvrtzQ@mail.gmail.com>
	<5502FC1A.6080503@oracle.com>
	<CAHjP37FN3pigOEPRkcKfqdrd09u8MFioWVPTD+fQOZ+bk+hUiQ@mail.gmail.com>
	<55031021.6000904@oracle.com>
	<CAHjP37GFqpt4TH=00Kg5cRb5sBCnXFQyyt66v-8fVkbsgk_EvQ@mail.gmail.com>
	<1426267663897-12435.post@n7.nabble.com>
	<55032EA6.8030907@oracle.com>
	<CAHjP37H+bcHC3DFKpmLksKJENhvQYYKCwmoO663P6FDvdYNMMA@mail.gmail.com>
	<550346FB.3080401@oracle.com>
Message-ID: <CAHjP37GP28_y2j8-O4O=mxTnYMw8FEUL5ZqR0OYkwXYjjbqrdQ@mail.gmail.com>

As I mentioned, Alpha is the only processor I know of that allows for
that.  But getting back to java, it's inconceivable to support such a cpu
since every load of a reference to a class with final fields would entail
cpu fences on that architecture.  So practically speaking, we can eliminate
such a reordering possibility at the hardware level (modulo cpu errata or
whatever).  Now we have the compiler left, and I can't immediately see a
transformation that would invalidate that code -- it loads the field once
into a local, and then only deals with the local (except final
assignment).  On the read path, given it's loaded into a local, compiler
cannot introduce re-reads of that field, so there's no chance of returning
null from the method.  And I can't figure out what type of compiler
transformation would allow observing uninitialized object state.

The reason I mentioned possibility of removing loadFence is because it's a
performance drain on the fast path (once the singleton is established, we
want the reads to be quick).  If you were really keen on supporting
Alpha-like weak memory models, you'd want to introduce a read barrier
specifically for data dependence, which would be a noop for everyone else
(this is akin to linux kernel's smp_read_barrier_depends, which AFAIK only
does anything on Alpha).

So yes, theoretically you always want to put a loadFence there (assuming
that we have only the existing fence intrinsics), but I don't think it's
practically (now or in the future) necessary.

On Fri, Mar 13, 2015 at 4:22 PM, Oleksandr Otenko <
oleksandr.otenko at oracle.com> wrote:

>  Wasn't there a recent thread with a reference to a platform which can
> load even dependent data out of order?
>
> http://en.wikipedia.org/wiki/Memory_ordering
>
>
>    - Dependent loads can be reordered (this is unique for Alpha). If the
>    processor fetches a pointer to some data after this reordering, it might
>    not fetch the data itself but use stale data which it has already cached
>    and not yet invalidated. Allowing this relaxation makes cache hardware
>    simpler and faster but leads to the requirement of memory barriers for
>    readers and writers.[5]
>    <http://en.wikipedia.org/wiki/Memory_ordering#cite_note-5>
>
>
>
> Alex
>
>
> On 13/03/2015 19:31, Vitaly Davidovich wrote:
>
> I mentioned that in my previous reply, but I'm not aware of any JVM
> running on platforms that allow such reordering.  I also highly doubt that
> such a platform would ever be ported to as bug tail would be very long,
> along with JVM having to insert LoadLoad barriers in lots of places where
> refs are read of classes with at least one final field.  If you have a
> concrete/real/practical example of where this reordering can take place,
> I'd love to know about it.
>
> On Fri, Mar 13, 2015 at 2:38 PM, Oleksandr Otenko <
> oleksandr.otenko at oracle.com> wrote:
>
>> Vitaly is wrong. The loadFence in Code1 is needed. Without it, it is
>> possible to access the uninitialized fields of the singleton. (the loads
>> may occur before the load of instance)
>>
>>
>> Alex
>>
>>
>>
>> On 13/03/2015 17:27, vikas wrote:
>>
>>> Thanks Vitaly,
>>>
>>> and sorry for the improper formatting.
>>>
>>> on the second note i was wondering why i wouldn't need loadFence in
>>> *Code1*
>>> DCL Example
>>>
>>> JMM cookbook suggest to insert LoadLoad barrier before final field access
>>> (in processor where data dependency is not respected), my example of DCL
>>> added LoadFence only because of this.
>>>
>>> Also C++ example does need both the fences
>>> http://preshing.com/20130930/double-checked-locking-is-fixed-in-cpp11/
>>>
>>> I can think of one reason on why It may work in java without LoadFence is
>>> benign data race-like construct
>>> are kind of allowed in Java whereas they are not allowed in C++.
>>>
>>> Also so below *Code4* works for DCL Singleton pattern ?
>>>
>>>
>>>                                                      *Code4*
>>>          sun.misc.Unsafe *U*;
>>>       Singleton instance = null
>>>
>>>       Singleton getInstance() {
>>>            Singleton tmp = instance;  // no fence while reading
>>>            if(tmp == null) {
>>>                synchronized(Singleton.class) {
>>>                     tmp = instance;
>>>                     if(tmp == null) {
>>>                         tmp = new Singleton();
>>>                        * U.storeFence();* // only need StoreFence
>>>                         instance = tmp;
>>>                    }
>>>                }
>>>             }
>>>         return tmp;
>>>       }
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>> --
>>> View this message in context:
>>> http://jsr166-concurrency.10961.n7.nabble.com/DCL-using-Fence-Intrinsics-tp12420p12435.html
>>> Sent from the JSR166 Concurrency mailing list archive at Nabble.com.
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150313/b8e813b3/attachment-0001.html>

From jsampson at guidewire.com  Sat Mar 14 00:37:25 2015
From: jsampson at guidewire.com (Justin Sampson)
Date: Sat, 14 Mar 2015 04:37:25 +0000
Subject: [concurrency-interest] DCL using Fence Intrinsics
In-Reply-To: <550346FB.3080401@oracle.com>
References: <CAHjP37HvtQu6-4ccDZHLyfiD0dNs9vVwetW_F5Kq_YiEAL+eBA@mail.gmail.com>
	<5502EB3C.40407@oracle.com>
	<CAHjP37FnGORdO-4WbW0_P7ivxF9KLkr3O4syejv34mNErNMeyw@mail.gmail.com>
	<CAHjP37E2gCf-3baeKeUYs43EuG8McebopLF7YGcqJz47ByHCEQ@mail.gmail.com>
	<5502F527.5080002@oracle.com>
	<CAHjP37GNuddw5-yxJ1PFrzpAG2_rK2HSqQdj8vQRHVL8DvrtzQ@mail.gmail.com>
	<5502FC1A.6080503@oracle.com>
	<CAHjP37FN3pigOEPRkcKfqdrd09u8MFioWVPTD+fQOZ+bk+hUiQ@mail.gmail.com>
	<55031021.6000904@oracle.com>
	<CAHjP37GFqpt4TH=00Kg5cRb5sBCnXFQyyt66v-8fVkbsgk_EvQ@mail.gmail.com>
	<1426267663897-12435.post@n7.nabble.com>	<55032EA6.8030907@oracle.com>
	<CAHjP37H+bcHC3DFKpmLksKJENhvQYYKCwmoO663P6FDvdYNMMA@mail.gmail.com>
	<550346FB.3080401@oracle.com>
Message-ID: <0FD072A166C6DC4C851F6115F37DDD279AEEF269@sm-ex-01-vm.guidewire.com>

Vitaly wrote:

> So yes, theoretically you always want to put a loadFence there
> (assuming that we have only the existing fence intrinsics), but I
> don't think it's practically (now or in the future) necessary.

As a mere mortal trying to write reliable, portable code without
knowing the details of every processor that Oracle is planning to
support in HotSpot, "theoretically" is pretty important to me.

That said, what actually counts as a dependent load? Is it purely a
matter of dereferencing pointers? That works for final field
semantics, since they are carefully defined in the JMM in terms of
reachability (i.e. kind of like happens-before but not transitive
with other happens-before edges). So that covers Vikas's Code1 vs.
Code4, as long as all he cares about is anything reachable from that
specific object. But in Vikas's Code2/Code3, are 'a' and 'b'
actually "dependent" in any way? Neither one is a pointer, so
there's no question of reachability. The processor doesn't know of
any relationship between them. So surely we do need the load fence
both inside the loop and after it, right? Regardless of instruction
reordering, don't we have to worry about a variety of cache effects
as well?

Cheers,
Justin


From vitalyd at gmail.com  Sat Mar 14 01:05:59 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Sat, 14 Mar 2015 01:05:59 -0400
Subject: [concurrency-interest] DCL using Fence Intrinsics
In-Reply-To: <0FD072A166C6DC4C851F6115F37DDD279AEEF269@sm-ex-01-vm.guidewire.com>
References: <CAHjP37HvtQu6-4ccDZHLyfiD0dNs9vVwetW_F5Kq_YiEAL+eBA@mail.gmail.com>
	<5502EB3C.40407@oracle.com>
	<CAHjP37FnGORdO-4WbW0_P7ivxF9KLkr3O4syejv34mNErNMeyw@mail.gmail.com>
	<CAHjP37E2gCf-3baeKeUYs43EuG8McebopLF7YGcqJz47ByHCEQ@mail.gmail.com>
	<5502F527.5080002@oracle.com>
	<CAHjP37GNuddw5-yxJ1PFrzpAG2_rK2HSqQdj8vQRHVL8DvrtzQ@mail.gmail.com>
	<5502FC1A.6080503@oracle.com>
	<CAHjP37FN3pigOEPRkcKfqdrd09u8MFioWVPTD+fQOZ+bk+hUiQ@mail.gmail.com>
	<55031021.6000904@oracle.com>
	<CAHjP37GFqpt4TH=00Kg5cRb5sBCnXFQyyt66v-8fVkbsgk_EvQ@mail.gmail.com>
	<1426267663897-12435.post@n7.nabble.com>
	<55032EA6.8030907@oracle.com>
	<CAHjP37H+bcHC3DFKpmLksKJENhvQYYKCwmoO663P6FDvdYNMMA@mail.gmail.com>
	<550346FB.3080401@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD279AEEF269@sm-ex-01-vm.guidewire.com>
Message-ID: <CAHjP37FxN_ShR0BZ3dgP-LmTpLdzb+TLV2g-mFyhOjCnvVj8HQ@mail.gmail.com>

No disagreement with your first paragraph.  Ultimately it's up to the
authors of code to decide if and where they want to go off the beaten path.

Data dependence is pointer dereference and arithmetic.  In java, this
really means loading a reference to a heap object (base address) and then
accessing its fields (base address + offset_to_field, i.e. pointer
arithmetic).

The reason I mentioned things like Alpha not being practical for final
field semantics is the following.  If the JVM (interpreter or compiler)
sees a read of some heap object that contains final fields followed by a
read of some field on that object, how does it guarantee that this data
dependence is enforced? It can't allow the read to observe initial/"zero"
state of those final fields due to this reordering.  So, it would have to
insert LoadLoad fence in between.  On Alpha, that's a cpu fence.  So now
we're talking about a cpu level fence on all such reads - it's
impractical.  The reason this is tractable  in, say, C (e.g. linux kernel
supports alpha) land is because there isn't a language construct/notion of
final fields.

'a' and 'b' are independent, so yes, there's no data dependence as far as
compiler or cpu is concerned.  My attempt to avoid back to back loadFence
calls when exiting the loop was a thought experiment as to whether it still
works with that exact code structure that I gave; I stated my reasoning,
but admit it's brittle.

sent from my phone
On Mar 14, 2015 12:37 AM, "Justin Sampson" <jsampson at guidewire.com> wrote:

> Vitaly wrote:
>
> > So yes, theoretically you always want to put a loadFence there
> > (assuming that we have only the existing fence intrinsics), but I
> > don't think it's practically (now or in the future) necessary.
>
> As a mere mortal trying to write reliable, portable code without
> knowing the details of every processor that Oracle is planning to
> support in HotSpot, "theoretically" is pretty important to me.
>
> That said, what actually counts as a dependent load? Is it purely a
> matter of dereferencing pointers? That works for final field
> semantics, since they are carefully defined in the JMM in terms of
> reachability (i.e. kind of like happens-before but not transitive
> with other happens-before edges). So that covers Vikas's Code1 vs.
> Code4, as long as all he cares about is anything reachable from that
> specific object. But in Vikas's Code2/Code3, are 'a' and 'b'
> actually "dependent" in any way? Neither one is a pointer, so
> there's no question of reachability. The processor doesn't know of
> any relationship between them. So surely we do need the load fence
> both inside the loop and after it, right? Regardless of instruction
> reordering, don't we have to worry about a variety of cache effects
> as well?
>
> Cheers,
> Justin
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150314/f6d9655b/attachment.html>

From stephan.diestelhorst at gmail.com  Sat Mar 14 05:54:39 2015
From: stephan.diestelhorst at gmail.com (Stephan Diestelhorst)
Date: Sat, 14 Mar 2015 09:54:39 +0000
Subject: [concurrency-interest] DCL using Fence Intrinsics
In-Reply-To: <CAHjP37FxN_ShR0BZ3dgP-LmTpLdzb+TLV2g-mFyhOjCnvVj8HQ@mail.gmail.com>
References: <CAHjP37HvtQu6-4ccDZHLyfiD0dNs9vVwetW_F5Kq_YiEAL+eBA@mail.gmail.com>
	<5502EB3C.40407@oracle.com>
	<CAHjP37FnGORdO-4WbW0_P7ivxF9KLkr3O4syejv34mNErNMeyw@mail.gmail.com>
	<CAHjP37E2gCf-3baeKeUYs43EuG8McebopLF7YGcqJz47ByHCEQ@mail.gmail.com>
	<5502F527.5080002@oracle.com>
	<CAHjP37GNuddw5-yxJ1PFrzpAG2_rK2HSqQdj8vQRHVL8DvrtzQ@mail.gmail.com>
	<5502FC1A.6080503@oracle.com>
	<CAHjP37FN3pigOEPRkcKfqdrd09u8MFioWVPTD+fQOZ+bk+hUiQ@mail.gmail.com>
	<55031021.6000904@oracle.com>
	<CAHjP37GFqpt4TH=00Kg5cRb5sBCnXFQyyt66v-8fVkbsgk_EvQ@mail.gmail.com>
	<1426267663897-12435.post@n7.nabble.com>
	<55032EA6.8030907@oracle.com>
	<CAHjP37H+bcHC3DFKpmLksKJENhvQYYKCwmoO663P6FDvdYNMMA@mail.gmail.com>
	<550346FB.3080401@oracle.com>
	<0FD072A166C6DC4C851F6115F37DDD279AEEF269@sm-ex-01-vm.guidewire.com>
	<CAHjP37FxN_ShR0BZ3dgP-LmTpLdzb+TLV2g-mFyhOjCnvVj8HQ@mail.gmail.com>
Message-ID: <CAJR39Ey+0M-=zbaehMODvefTmpnanESHGj3cXbNy+cbm4g6baQ@mail.gmail.com>

On 14 Mar 2015 05:28, "Vitaly Davidovich" <vitalyd at gmail.com> wrote:
> The reason I mentioned things like Alpha not being practical for final
field semantics is the following.  If the JVM (interpreter or compiler)
sees a read of some heap object that contains final fields followed by a
read of some field on that object, how does it guarantee that this data
dependence is enforced? It can't allow the read to observe initial/"zero"
state of those final fields due to this reordering.  So, it would have to
insert LoadLoad fence in between.  On Alpha, that's a cpu fence.  So now
we're talking about a cpu level fence on all such reads - it's impractical.

I fail to see why adding such a fence is "impractical".  If it is
performance, then such a fence could be built equally fast as the current
dependency-observing architectures.  If it is instruction bloat, annotate
the second load directly.  The only impracticalities IMHO are if (1) the
compiler has a hard time figuring out if it needs a fence or not, and (2)
if these fences would constitute a large fraction of all accesses.

AFAICS, neither applies here.  So please stop assuming things about the
HW.  IIRC, some IBM architecture did not honour cancelling out
data-dependencies; not relevant here, but it shows that people are looking
for weakened load load ordering.

Cheers,
  Stephan
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150314/9c3b30d2/attachment.html>

From marko at hazelcast.com  Mon Mar 16 13:00:07 2015
From: marko at hazelcast.com (Marko Topolnik)
Date: Mon, 16 Mar 2015 18:00:07 +0100
Subject: [concurrency-interest] Enforcing total sync order on modern hardware
Message-ID: <CALtZ-o47wWjeVXtn=zx8t4vau6D5T-qFnA3Vm2AGcFUm2S_PVw@mail.gmail.com>

In an old post I drew the diagram below, which showed happens-before edges
between three threads. The thread shown at the bottom was a
"clock-updating" thread, continuously updating a volatile "current time"
value. The other two threads read the time and were additionally coupled
through a shared volatile variable with one writer and one reader thread.

My point was that the threads could behave in a paradoxical way from the
standpoint of "global time": the reader could observe a late time value and
nevertheless not be prevented from observing an early write to the shared
var.

The JMM actually prevents this behavior with the enforcement of a total
sync order.

However, during a private discussion with Martin Thompson, it seemed
unclear how exactly a runtime would actually enforce total sync order
without hurting performance. Given that, since Nehalem, cores communicate
point-to-point over QPI and don't lock the global front-side bus, the CPU
doesn't naturally offer a total ordering of all lock operations. I would be
very interested to learn how exactly this goes on, or perhaps whether
Martin and I were missing something and this is actually not an issue.

Here is the diagram reposted:


                                /--> Rrt6 --/-> Rrt9 --> Rv0
    ---------------------------+------------+--------/
  /                            |     ------/
Wv0 ---> Rwt3 -> Wv1 --> Rwt6  |    /
        /                /   --|   /
       |                | /       /
       T3 ------------> T6 ----> T9


T3, T6, T9 -- writes to currentTime
Rwt0, Rwt1 -- reads of currentTime by writing thread
Rrt1, Rrt2 -- reads of currentTime by reading thread
Wv0, Wv1   -- writes to the sharedVar
Rv0        -- read of the sharedVar

initially t = 0 ms;
T3 writes t = 3 ms;
T6 writes t = 6 ms;
T9 writes t = 9 ms.

The program outputs

Writing 0 at t = 0 ms
Writing 1 at t = 3 ms
Reading 0 at t = 9 ms


---
Marko Topolnik
Hazelcast
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150316/05139042/attachment.html>

From vitalyd at gmail.com  Mon Mar 16 14:35:55 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Mon, 16 Mar 2015 14:35:55 -0400
Subject: [concurrency-interest] Enforcing total sync order on modern
	hardware
In-Reply-To: <CALtZ-o47wWjeVXtn=zx8t4vau6D5T-qFnA3Vm2AGcFUm2S_PVw@mail.gmail.com>
References: <CALtZ-o47wWjeVXtn=zx8t4vau6D5T-qFnA3Vm2AGcFUm2S_PVw@mail.gmail.com>
Message-ID: <CAHjP37H2qM+BGaHaZOTYB_VuRtOS6ZiS7SVmE+WpjJLniXj7kA@mail.gmail.com>

Marko,

Can't scheduling alone cause a situation where reader reads t=9ms, looks at
the shared value, and sees something old that hasn't been updated yet since
the writer hasn't observed the t=9ms yet (due to scheduling)? Nehalem is a
TSO (total store order) architecture, which means each core's writes appear
in the same order to all other cores, but there's no global order across
all cores.  So your clock-updating thread's stores will appear to the
reader/writer in the same order, but what can reader/writer say about the
other if each reads t = X? I think nothing of consequence given they're not
synchronizing-with the clock-updating thread, but simply observing that
value in a "racy" (with respect to each other) manner.

I'm probably misunderstanding your question/point though, so please correct
me.

On Mon, Mar 16, 2015 at 1:00 PM, Marko Topolnik <marko at hazelcast.com> wrote:

> In an old post I drew the diagram below, which showed happens-before edges
> between three threads. The thread shown at the bottom was a
> "clock-updating" thread, continuously updating a volatile "current time"
> value. The other two threads read the time and were additionally coupled
> through a shared volatile variable with one writer and one reader thread.
>
> My point was that the threads could behave in a paradoxical way from the
> standpoint of "global time": the reader could observe a late time value and
> nevertheless not be prevented from observing an early write to the shared
> var.
>
> The JMM actually prevents this behavior with the enforcement of a total
> sync order.
>
> However, during a private discussion with Martin Thompson, it seemed
> unclear how exactly a runtime would actually enforce total sync order
> without hurting performance. Given that, since Nehalem, cores communicate
> point-to-point over QPI and don't lock the global front-side bus, the CPU
> doesn't naturally offer a total ordering of all lock operations. I would be
> very interested to learn how exactly this goes on, or perhaps whether
> Martin and I were missing something and this is actually not an issue.
>
> Here is the diagram reposted:
>
>
>                                 /--> Rrt6 --/-> Rrt9 --> Rv0
>     ---------------------------+------------+--------/
>   /                            |     ------/
> Wv0 ---> Rwt3 -> Wv1 --> Rwt6  |    /
>         /                /   --|   /
>        |                | /       /
>        T3 ------------> T6 ----> T9
>
>
> T3, T6, T9 -- writes to currentTime
> Rwt0, Rwt1 -- reads of currentTime by writing thread
> Rrt1, Rrt2 -- reads of currentTime by reading thread
> Wv0, Wv1   -- writes to the sharedVar
> Rv0        -- read of the sharedVar
>
> initially t = 0 ms;
> T3 writes t = 3 ms;
> T6 writes t = 6 ms;
> T9 writes t = 9 ms.
>
> The program outputs
>
> Writing 0 at t = 0 ms
> Writing 1 at t = 3 ms
> Reading 0 at t = 9 ms
>
>
> ---
> Marko Topolnik
> Hazelcast
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150316/a73ccf3b/attachment.html>

From aph at redhat.com  Mon Mar 16 14:44:37 2015
From: aph at redhat.com (Andrew Haley)
Date: Mon, 16 Mar 2015 18:44:37 +0000
Subject: [concurrency-interest] Enforcing total sync order on modern
	hardware
In-Reply-To: <CALtZ-o47wWjeVXtn=zx8t4vau6D5T-qFnA3Vm2AGcFUm2S_PVw@mail.gmail.com>
References: <CALtZ-o47wWjeVXtn=zx8t4vau6D5T-qFnA3Vm2AGcFUm2S_PVw@mail.gmail.com>
Message-ID: <55072495.2060701@redhat.com>

On 03/16/2015 05:00 PM, Marko Topolnik wrote:

> Given that, since Nehalem, cores communicate point-to-point over QPI
> and don't lock the global front-side bus, the CPU doesn't naturally
> offer a total ordering of all lock operations.

Intel do actually guarantee

    Locked instructions have a total order.

so this is a hardware problem, not a software one.  How exactly the
hardware people do this on a large network of processors is some of
the most Secret Sauce, but I can imagine some kind of combining
network in hardware.

Andrew.

[1]  Intel? 64 and IA-32 Architectures Software Developer?s Manual
Volume 3 (3A, 3B & 3C): System Programming Guide 8.2.2, Memory
Ordering in P6 and More Recent Processor Families

From marko at hazelcast.com  Mon Mar 16 14:48:05 2015
From: marko at hazelcast.com (Marko Topolnik)
Date: Mon, 16 Mar 2015 19:48:05 +0100
Subject: [concurrency-interest] Enforcing total sync order on modern
	hardware
In-Reply-To: <CAHjP37H2qM+BGaHaZOTYB_VuRtOS6ZiS7SVmE+WpjJLniXj7kA@mail.gmail.com>
References: <CALtZ-o47wWjeVXtn=zx8t4vau6D5T-qFnA3Vm2AGcFUm2S_PVw@mail.gmail.com>
	<CAHjP37H2qM+BGaHaZOTYB_VuRtOS6ZiS7SVmE+WpjJLniXj7kA@mail.gmail.com>
Message-ID: <CALtZ-o5HsACO982EwEK3U8E1iDqZ6yLxmND8jENa1hSOnsRFuw@mail.gmail.com>

Vitaly,

if you refer back to the diagram, the writer thread:

1. wrote 0 to the shared var;
2. observed t = 3ms;
3. wrote 1 to the shared var;
4. observed t = 6 ms.

The reader thread:
1. observed t = 6 ms;
2. observed t = 9 ms;
3. read 0 from the shared var.

This should eliminate the effects of scheduling.

The point is, the above is in violation of the Java Memory Model because no
total synchronization order is possible under which this result would be
obtained. On the other hand, it is happens before-consistent.

On Mon, Mar 16, 2015 at 7:35 PM, Vitaly Davidovich <vitalyd at gmail.com>
wrote:

> Marko,
>
> Can't scheduling alone cause a situation where reader reads t=9ms, looks
> at the shared value, and sees something old that hasn't been updated yet
> since the writer hasn't observed the t=9ms yet (due to scheduling)? Nehalem
> is a TSO (total store order) architecture, which means each core's writes
> appear in the same order to all other cores, but there's no global order
> across all cores.  So your clock-updating thread's stores will appear to
> the reader/writer in the same order, but what can reader/writer say about
> the other if each reads t = X? I think nothing of consequence given they're
> not synchronizing-with the clock-updating thread, but simply observing that
> value in a "racy" (with respect to each other) manner.
>
> I'm probably misunderstanding your question/point though, so please
> correct me.
>
> On Mon, Mar 16, 2015 at 1:00 PM, Marko Topolnik <marko at hazelcast.com>
> wrote:
>
>> In an old post I drew the diagram below, which showed happens-before
>> edges between three threads. The thread shown at the bottom was a
>> "clock-updating" thread, continuously updating a volatile "current time"
>> value. The other two threads read the time and were additionally coupled
>> through a shared volatile variable with one writer and one reader thread.
>>
>> My point was that the threads could behave in a paradoxical way from the
>> standpoint of "global time": the reader could observe a late time value and
>> nevertheless not be prevented from observing an early write to the shared
>> var.
>>
>> The JMM actually prevents this behavior with the enforcement of a total
>> sync order.
>>
>> However, during a private discussion with Martin Thompson, it seemed
>> unclear how exactly a runtime would actually enforce total sync order
>> without hurting performance. Given that, since Nehalem, cores communicate
>> point-to-point over QPI and don't lock the global front-side bus, the CPU
>> doesn't naturally offer a total ordering of all lock operations. I would be
>> very interested to learn how exactly this goes on, or perhaps whether
>> Martin and I were missing something and this is actually not an issue.
>>
>> Here is the diagram reposted:
>>
>>
>>                                 /--> Rrt6 --/-> Rrt9 --> Rv0
>>     ---------------------------+------------+--------/
>>   /                            |     ------/
>> Wv0 ---> Rwt3 -> Wv1 --> Rwt6  |    /
>>         /                /   --|   /
>>        |                | /       /
>>        T3 ------------> T6 ----> T9
>>
>>
>> T3, T6, T9 -- writes to currentTime
>> Rwt0, Rwt1 -- reads of currentTime by writing thread
>> Rrt1, Rrt2 -- reads of currentTime by reading thread
>> Wv0, Wv1   -- writes to the sharedVar
>> Rv0        -- read of the sharedVar
>>
>> initially t = 0 ms;
>> T3 writes t = 3 ms;
>> T6 writes t = 6 ms;
>> T9 writes t = 9 ms.
>>
>> The program outputs
>>
>> Writing 0 at t = 0 ms
>> Writing 1 at t = 3 ms
>> Reading 0 at t = 9 ms
>>
>>
>> ---
>> Marko Topolnik
>> Hazelcast
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150316/e6aabd35/attachment-0001.html>

From marko at hazelcast.com  Mon Mar 16 15:05:59 2015
From: marko at hazelcast.com (Marko Topolnik)
Date: Mon, 16 Mar 2015 20:05:59 +0100
Subject: [concurrency-interest] Enforcing total sync order on modern
	hardware
In-Reply-To: <55072495.2060701@redhat.com>
References: <CALtZ-o47wWjeVXtn=zx8t4vau6D5T-qFnA3Vm2AGcFUm2S_PVw@mail.gmail.com>
	<55072495.2060701@redhat.com>
Message-ID: <CALtZ-o4r9JNRD5gr=eiqc9ukfgO2g4UgFzv66FtoWsN9480oWA@mail.gmail.com>

Andrew,

thank you for the reference, this answers the dilemma in full. I didn't
know this guarantee existed on x86.

---
Marko

On Mon, Mar 16, 2015 at 7:44 PM, Andrew Haley <aph at redhat.com> wrote:

> On 03/16/2015 05:00 PM, Marko Topolnik wrote:
>
> > Given that, since Nehalem, cores communicate point-to-point over QPI
> > and don't lock the global front-side bus, the CPU doesn't naturally
> > offer a total ordering of all lock operations.
>
> Intel do actually guarantee
>
>     Locked instructions have a total order.
>
> so this is a hardware problem, not a software one.  How exactly the
> hardware people do this on a large network of processors is some of
> the most Secret Sauce, but I can imagine some kind of combining
> network in hardware.
>
> Andrew.
>
> [1]  Intel? 64 and IA-32 Architectures Software Developer?s Manual
> Volume 3 (3A, 3B & 3C): System Programming Guide 8.2.2, Memory
> Ordering in P6 and More Recent Processor Families
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150316/37086852/attachment.html>

From vitalyd at gmail.com  Mon Mar 16 15:40:21 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Mon, 16 Mar 2015 15:40:21 -0400
Subject: [concurrency-interest] Enforcing total sync order on modern
	hardware
In-Reply-To: <CALtZ-o4r9JNRD5gr=eiqc9ukfgO2g4UgFzv66FtoWsN9480oWA@mail.gmail.com>
References: <CALtZ-o47wWjeVXtn=zx8t4vau6D5T-qFnA3Vm2AGcFUm2S_PVw@mail.gmail.com>
	<55072495.2060701@redhat.com>
	<CALtZ-o4r9JNRD5gr=eiqc9ukfgO2g4UgFzv66FtoWsN9480oWA@mail.gmail.com>
Message-ID: <CAHjP37H+bQYLebX-LrA5gE_d+UjFKkUiTTOwGYyJ=dnboHk=sQ@mail.gmail.com>

Why were you concerned with lock instructions specifically? At one point in
the past, volatile writes were done using mfence, IIRC.

sent from my phone
On Mar 16, 2015 3:28 PM, "Marko Topolnik" <marko at hazelcast.com> wrote:

> Andrew,
>
> thank you for the reference, this answers the dilemma in full. I didn't
> know this guarantee existed on x86.
>
> ---
> Marko
>
> On Mon, Mar 16, 2015 at 7:44 PM, Andrew Haley <aph at redhat.com> wrote:
>
>> On 03/16/2015 05:00 PM, Marko Topolnik wrote:
>>
>> > Given that, since Nehalem, cores communicate point-to-point over QPI
>> > and don't lock the global front-side bus, the CPU doesn't naturally
>> > offer a total ordering of all lock operations.
>>
>> Intel do actually guarantee
>>
>>     Locked instructions have a total order.
>>
>> so this is a hardware problem, not a software one.  How exactly the
>> hardware people do this on a large network of processors is some of
>> the most Secret Sauce, but I can imagine some kind of combining
>> network in hardware.
>>
>> Andrew.
>>
>> [1]  Intel? 64 and IA-32 Architectures Software Developer?s Manual
>> Volume 3 (3A, 3B & 3C): System Programming Guide 8.2.2, Memory
>> Ordering in P6 and More Recent Processor Families
>>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150316/4ecd13da/attachment.html>

From marko at hazelcast.com  Mon Mar 16 15:46:15 2015
From: marko at hazelcast.com (Marko Topolnik)
Date: Mon, 16 Mar 2015 20:46:15 +0100
Subject: [concurrency-interest] Enforcing total sync order on modern
	hardware
In-Reply-To: <CAHjP37H+bQYLebX-LrA5gE_d+UjFKkUiTTOwGYyJ=dnboHk=sQ@mail.gmail.com>
References: <CALtZ-o47wWjeVXtn=zx8t4vau6D5T-qFnA3Vm2AGcFUm2S_PVw@mail.gmail.com>
	<55072495.2060701@redhat.com>
	<CALtZ-o4r9JNRD5gr=eiqc9ukfgO2g4UgFzv66FtoWsN9480oWA@mail.gmail.com>
	<CAHjP37H+bQYLebX-LrA5gE_d+UjFKkUiTTOwGYyJ=dnboHk=sQ@mail.gmail.com>
Message-ID: <CALtZ-o6J9GEtjmvL4aR30GNUnpX7Jjk0t_FeLCE1oAgvLMwQow@mail.gmail.com>

What is important is that there be _some_ way of guaranteeing total sync
order at the CPU level. It is less important whether this is achieved by
mfence or lock instruction.

-Marko

On Mon, Mar 16, 2015 at 8:40 PM, Vitaly Davidovich <vitalyd at gmail.com>
wrote:

> Why were you concerned with lock instructions specifically? At one point
> in the past, volatile writes were done using mfence, IIRC.
>
> sent from my phone
> On Mar 16, 2015 3:28 PM, "Marko Topolnik" <marko at hazelcast.com> wrote:
>
>> Andrew,
>>
>> thank you for the reference, this answers the dilemma in full. I didn't
>> know this guarantee existed on x86.
>>
>> ---
>> Marko
>>
>> On Mon, Mar 16, 2015 at 7:44 PM, Andrew Haley <aph at redhat.com> wrote:
>>
>>> On 03/16/2015 05:00 PM, Marko Topolnik wrote:
>>>
>>> > Given that, since Nehalem, cores communicate point-to-point over QPI
>>> > and don't lock the global front-side bus, the CPU doesn't naturally
>>> > offer a total ordering of all lock operations.
>>>
>>> Intel do actually guarantee
>>>
>>>     Locked instructions have a total order.
>>>
>>> so this is a hardware problem, not a software one.  How exactly the
>>> hardware people do this on a large network of processors is some of
>>> the most Secret Sauce, but I can imagine some kind of combining
>>> network in hardware.
>>>
>>> Andrew.
>>>
>>> [1]  Intel? 64 and IA-32 Architectures Software Developer?s Manual
>>> Volume 3 (3A, 3B & 3C): System Programming Guide 8.2.2, Memory
>>> Ordering in P6 and More Recent Processor Families
>>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150316/f952ce76/attachment.html>

From vitalyd at gmail.com  Mon Mar 16 16:02:52 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Mon, 16 Mar 2015 16:02:52 -0400
Subject: [concurrency-interest] Enforcing total sync order on modern
	hardware
In-Reply-To: <CALtZ-o6J9GEtjmvL4aR30GNUnpX7Jjk0t_FeLCE1oAgvLMwQow@mail.gmail.com>
References: <CALtZ-o47wWjeVXtn=zx8t4vau6D5T-qFnA3Vm2AGcFUm2S_PVw@mail.gmail.com>
	<55072495.2060701@redhat.com>
	<CALtZ-o4r9JNRD5gr=eiqc9ukfgO2g4UgFzv66FtoWsN9480oWA@mail.gmail.com>
	<CAHjP37H+bQYLebX-LrA5gE_d+UjFKkUiTTOwGYyJ=dnboHk=sQ@mail.gmail.com>
	<CALtZ-o6J9GEtjmvL4aR30GNUnpX7Jjk0t_FeLCE1oAgvLMwQow@mail.gmail.com>
Message-ID: <CAHjP37GuG76JTbdpVfWi+GUZbaBReonFh3HmOEBbp84=5KKX=g@mail.gmail.com>

By "total sync order at the CPU level" do you mean sync order of that cpu
itself or some global order across all CPUs? The total order of lock
instructions is across all CPUs, whereas mfence, AFAIK, orders only the
local CPU's memory operations.  Sorry, maybe I'm being dense today, but I
still don't get why knowing that lock instructions have total order somehow
answers your question.  Were you simply asking whether there are
instructions available to ensure memory operations are done (or appear to
be) in program (minus compiler code motion) order on a per-cpu basis?

On Mon, Mar 16, 2015 at 3:46 PM, Marko Topolnik <marko at hazelcast.com> wrote:

> What is important is that there be _some_ way of guaranteeing total sync
> order at the CPU level. It is less important whether this is achieved by
> mfence or lock instruction.
>
> -Marko
>
> On Mon, Mar 16, 2015 at 8:40 PM, Vitaly Davidovich <vitalyd at gmail.com>
> wrote:
>
>> Why were you concerned with lock instructions specifically? At one point
>> in the past, volatile writes were done using mfence, IIRC.
>>
>> sent from my phone
>> On Mar 16, 2015 3:28 PM, "Marko Topolnik" <marko at hazelcast.com> wrote:
>>
>>> Andrew,
>>>
>>> thank you for the reference, this answers the dilemma in full. I didn't
>>> know this guarantee existed on x86.
>>>
>>> ---
>>> Marko
>>>
>>> On Mon, Mar 16, 2015 at 7:44 PM, Andrew Haley <aph at redhat.com> wrote:
>>>
>>>> On 03/16/2015 05:00 PM, Marko Topolnik wrote:
>>>>
>>>> > Given that, since Nehalem, cores communicate point-to-point over QPI
>>>> > and don't lock the global front-side bus, the CPU doesn't naturally
>>>> > offer a total ordering of all lock operations.
>>>>
>>>> Intel do actually guarantee
>>>>
>>>>     Locked instructions have a total order.
>>>>
>>>> so this is a hardware problem, not a software one.  How exactly the
>>>> hardware people do this on a large network of processors is some of
>>>> the most Secret Sauce, but I can imagine some kind of combining
>>>> network in hardware.
>>>>
>>>> Andrew.
>>>>
>>>> [1]  Intel? 64 and IA-32 Architectures Software Developer?s Manual
>>>> Volume 3 (3A, 3B & 3C): System Programming Guide 8.2.2, Memory
>>>> Ordering in P6 and More Recent Processor Families
>>>>
>>>
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150316/285c7ecf/attachment.html>

From marko at hazelcast.com  Mon Mar 16 16:22:22 2015
From: marko at hazelcast.com (Marko Topolnik)
Date: Mon, 16 Mar 2015 21:22:22 +0100
Subject: [concurrency-interest] Enforcing total sync order on modern
	hardware
In-Reply-To: <CAHjP37GuG76JTbdpVfWi+GUZbaBReonFh3HmOEBbp84=5KKX=g@mail.gmail.com>
References: <CALtZ-o47wWjeVXtn=zx8t4vau6D5T-qFnA3Vm2AGcFUm2S_PVw@mail.gmail.com>
	<55072495.2060701@redhat.com>
	<CALtZ-o4r9JNRD5gr=eiqc9ukfgO2g4UgFzv66FtoWsN9480oWA@mail.gmail.com>
	<CAHjP37H+bQYLebX-LrA5gE_d+UjFKkUiTTOwGYyJ=dnboHk=sQ@mail.gmail.com>
	<CALtZ-o6J9GEtjmvL4aR30GNUnpX7Jjk0t_FeLCE1oAgvLMwQow@mail.gmail.com>
	<CAHjP37GuG76JTbdpVfWi+GUZbaBReonFh3HmOEBbp84=5KKX=g@mail.gmail.com>
Message-ID: <CALtZ-o78Nsh=rC7O7Yj1p6tJPrjQRfevNpgnKRucym8LvE2pJQ@mail.gmail.com>

I was wondering how a JVM implementation would be able to enforce global
sync order across all CPUs in a performant way. So, given the total
ordering on lock instructions, I would assume that the implementation of
any given synchronizing action would have to involve a lock instruction at
some point.

On Mon, Mar 16, 2015 at 9:02 PM, Vitaly Davidovich <vitalyd at gmail.com>
wrote:

> By "total sync order at the CPU level" do you mean sync order of that cpu
> itself or some global order across all CPUs? The total order of lock
> instructions is across all CPUs, whereas mfence, AFAIK, orders only the
> local CPU's memory operations.  Sorry, maybe I'm being dense today, but I
> still don't get why knowing that lock instructions have total order somehow
> answers your question.  Were you simply asking whether there are
> instructions available to ensure memory operations are done (or appear to
> be) in program (minus compiler code motion) order on a per-cpu basis?
>
> On Mon, Mar 16, 2015 at 3:46 PM, Marko Topolnik <marko at hazelcast.com>
> wrote:
>
>> What is important is that there be _some_ way of guaranteeing total sync
>> order at the CPU level. It is less important whether this is achieved by
>> mfence or lock instruction.
>>
>> -Marko
>>
>> On Mon, Mar 16, 2015 at 8:40 PM, Vitaly Davidovich <vitalyd at gmail.com>
>> wrote:
>>
>>> Why were you concerned with lock instructions specifically? At one point
>>> in the past, volatile writes were done using mfence, IIRC.
>>>
>>> sent from my phone
>>> On Mar 16, 2015 3:28 PM, "Marko Topolnik" <marko at hazelcast.com> wrote:
>>>
>>>> Andrew,
>>>>
>>>> thank you for the reference, this answers the dilemma in full. I didn't
>>>> know this guarantee existed on x86.
>>>>
>>>> ---
>>>> Marko
>>>>
>>>> On Mon, Mar 16, 2015 at 7:44 PM, Andrew Haley <aph at redhat.com> wrote:
>>>>
>>>>> On 03/16/2015 05:00 PM, Marko Topolnik wrote:
>>>>>
>>>>> > Given that, since Nehalem, cores communicate point-to-point over QPI
>>>>> > and don't lock the global front-side bus, the CPU doesn't naturally
>>>>> > offer a total ordering of all lock operations.
>>>>>
>>>>> Intel do actually guarantee
>>>>>
>>>>>     Locked instructions have a total order.
>>>>>
>>>>> so this is a hardware problem, not a software one.  How exactly the
>>>>> hardware people do this on a large network of processors is some of
>>>>> the most Secret Sauce, but I can imagine some kind of combining
>>>>> network in hardware.
>>>>>
>>>>> Andrew.
>>>>>
>>>>> [1]  Intel? 64 and IA-32 Architectures Software Developer?s Manual
>>>>> Volume 3 (3A, 3B & 3C): System Programming Guide 8.2.2, Memory
>>>>> Ordering in P6 and More Recent Processor Families
>>>>>
>>>>
>>>>
>>>> _______________________________________________
>>>> Concurrency-interest mailing list
>>>> Concurrency-interest at cs.oswego.edu
>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>
>>>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150316/f39a9349/attachment-0001.html>

From marko at hazelcast.com  Tue Mar 17 02:31:09 2015
From: marko at hazelcast.com (Marko Topolnik)
Date: Tue, 17 Mar 2015 07:31:09 +0100
Subject: [concurrency-interest] Enforcing total sync order on modern
	hardware
In-Reply-To: <CALtZ-o78Nsh=rC7O7Yj1p6tJPrjQRfevNpgnKRucym8LvE2pJQ@mail.gmail.com>
References: <CALtZ-o47wWjeVXtn=zx8t4vau6D5T-qFnA3Vm2AGcFUm2S_PVw@mail.gmail.com>
	<55072495.2060701@redhat.com>
	<CALtZ-o4r9JNRD5gr=eiqc9ukfgO2g4UgFzv66FtoWsN9480oWA@mail.gmail.com>
	<CAHjP37H+bQYLebX-LrA5gE_d+UjFKkUiTTOwGYyJ=dnboHk=sQ@mail.gmail.com>
	<CALtZ-o6J9GEtjmvL4aR30GNUnpX7Jjk0t_FeLCE1oAgvLMwQow@mail.gmail.com>
	<CAHjP37GuG76JTbdpVfWi+GUZbaBReonFh3HmOEBbp84=5KKX=g@mail.gmail.com>
	<CALtZ-o78Nsh=rC7O7Yj1p6tJPrjQRfevNpgnKRucym8LvE2pJQ@mail.gmail.com>
Message-ID: <CALtZ-o6qur6Qc5Vj6NBrPfFNyd3DB-U8CZuXZ3znm7-XHZoXjg@mail.gmail.com>

There is another concern that may be interesting to reconsider. Given the
lack of total sync order when just using memory barriers, is the JSR-133
Cookbook wrong/outdated in this respect? It doesn't at all deal with the
issue of the sync order, just with the visibility of inter-thread actions.

If a JVM just followed the rules put forth by the Cookbook, would an
invalid execution as outlined in my diagram actually be possible?

On Mon, Mar 16, 2015 at 9:22 PM, Marko Topolnik <marko at hazelcast.com> wrote:

> I was wondering how a JVM implementation would be able to enforce global
> sync order across all CPUs in a performant way. So, given the total
> ordering on lock instructions, I would assume that the implementation of
> any given synchronizing action would have to involve a lock instruction at
> some point.
>
> On Mon, Mar 16, 2015 at 9:02 PM, Vitaly Davidovich <vitalyd at gmail.com>
> wrote:
>
>> By "total sync order at the CPU level" do you mean sync order of that cpu
>> itself or some global order across all CPUs? The total order of lock
>> instructions is across all CPUs, whereas mfence, AFAIK, orders only the
>> local CPU's memory operations.  Sorry, maybe I'm being dense today, but I
>> still don't get why knowing that lock instructions have total order somehow
>> answers your question.  Were you simply asking whether there are
>> instructions available to ensure memory operations are done (or appear to
>> be) in program (minus compiler code motion) order on a per-cpu basis?
>>
>> On Mon, Mar 16, 2015 at 3:46 PM, Marko Topolnik <marko at hazelcast.com>
>> wrote:
>>
>>> What is important is that there be _some_ way of guaranteeing total sync
>>> order at the CPU level. It is less important whether this is achieved by
>>> mfence or lock instruction.
>>>
>>> -Marko
>>>
>>> On Mon, Mar 16, 2015 at 8:40 PM, Vitaly Davidovich <vitalyd at gmail.com>
>>> wrote:
>>>
>>>> Why were you concerned with lock instructions specifically? At one
>>>> point in the past, volatile writes were done using mfence, IIRC.
>>>>
>>>> sent from my phone
>>>> On Mar 16, 2015 3:28 PM, "Marko Topolnik" <marko at hazelcast.com> wrote:
>>>>
>>>>> Andrew,
>>>>>
>>>>> thank you for the reference, this answers the dilemma in full. I
>>>>> didn't know this guarantee existed on x86.
>>>>>
>>>>> ---
>>>>> Marko
>>>>>
>>>>> On Mon, Mar 16, 2015 at 7:44 PM, Andrew Haley <aph at redhat.com> wrote:
>>>>>
>>>>>> On 03/16/2015 05:00 PM, Marko Topolnik wrote:
>>>>>>
>>>>>> > Given that, since Nehalem, cores communicate point-to-point over QPI
>>>>>> > and don't lock the global front-side bus, the CPU doesn't naturally
>>>>>> > offer a total ordering of all lock operations.
>>>>>>
>>>>>> Intel do actually guarantee
>>>>>>
>>>>>>     Locked instructions have a total order.
>>>>>>
>>>>>> so this is a hardware problem, not a software one.  How exactly the
>>>>>> hardware people do this on a large network of processors is some of
>>>>>> the most Secret Sauce, but I can imagine some kind of combining
>>>>>> network in hardware.
>>>>>>
>>>>>> Andrew.
>>>>>>
>>>>>> [1]  Intel? 64 and IA-32 Architectures Software Developer?s Manual
>>>>>> Volume 3 (3A, 3B & 3C): System Programming Guide 8.2.2, Memory
>>>>>> Ordering in P6 and More Recent Processor Families
>>>>>>
>>>>>
>>>>>
>>>>> _______________________________________________
>>>>> Concurrency-interest mailing list
>>>>> Concurrency-interest at cs.oswego.edu
>>>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>>>
>>>>>
>>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150317/0eabf156/attachment.html>

From aleksey.shipilev at oracle.com  Tue Mar 17 06:46:05 2015
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Tue, 17 Mar 2015 13:46:05 +0300
Subject: [concurrency-interest] Enforcing total sync order on modern
	hardware
In-Reply-To: <CALtZ-o6qur6Qc5Vj6NBrPfFNyd3DB-U8CZuXZ3znm7-XHZoXjg@mail.gmail.com>
References: <CALtZ-o47wWjeVXtn=zx8t4vau6D5T-qFnA3Vm2AGcFUm2S_PVw@mail.gmail.com>	<55072495.2060701@redhat.com>	<CALtZ-o4r9JNRD5gr=eiqc9ukfgO2g4UgFzv66FtoWsN9480oWA@mail.gmail.com>	<CAHjP37H+bQYLebX-LrA5gE_d+UjFKkUiTTOwGYyJ=dnboHk=sQ@mail.gmail.com>	<CALtZ-o6J9GEtjmvL4aR30GNUnpX7Jjk0t_FeLCE1oAgvLMwQow@mail.gmail.com>	<CAHjP37GuG76JTbdpVfWi+GUZbaBReonFh3HmOEBbp84=5KKX=g@mail.gmail.com>	<CALtZ-o78Nsh=rC7O7Yj1p6tJPrjQRfevNpgnKRucym8LvE2pJQ@mail.gmail.com>
	<CALtZ-o6qur6Qc5Vj6NBrPfFNyd3DB-U8CZuXZ3znm7-XHZoXjg@mail.gmail.com>
Message-ID: <550805ED.30003@oracle.com>

On 17.03.2015 9:31, Marko Topolnik wrote:
> There is another concern that may be interesting to reconsider. Given
> the lack of total sync order when just using memory barriers, is the
> JSR-133 Cookbook wrong/outdated in this respect? It doesn't at all deal
> with the issue of the sync order, just with the visibility of
> inter-thread actions.

Sigh. The rule of thumb in dealing with concurrency: assume you are
misunderstanding something before you think something agreed upon is
incorrect. In fact, in this thread, you confused yourselves with "memory
barriers are lacking total order". Memory barriers *are* regaining the
total ordering when used correctly. E.g. on x86, this is a sequentially
consistent code:

  SeqCst store:
    mov %blah, (memory)   ; volatile store
    lock addl (%esp), 0

  SeqCst load:
    mov (memory), %blah   ; volatile load

The alternative also works:

  SeqCst store:
    mov %blah, (memory)   ; volatile store
    mfence

  SeqCst load:
    mov (memory), %blah   ; volatile load


The mental model I am having in my head is as follows:

  a) Cache-coherent systems maintain the consistent (coherent) view of
each memory location at any given moment. In fact, most coherency
protocols provide the total order for the operations on a *single*
location. Regardless how the actual interconnect is operating, the cache
coherency protocols are to maintain that illusion. MESI-like protocols
are by nature message-based, and so they do not require shared bus to
begin with, so no problems with QPI.

  b) CPUs can do dirty work either doing multiple accesses at once, or
even without consulting the coherency. That has a potential to break the
invariants the coherency is trying to maintain: either through
overlapping the accesses, or not exposing the values right away. Memory
barriers are here to instruct hardware to stop being smart and expose
the operations to coherency in the instruction order.

  c) The easiest way to provide the memory fence effect is force CPU to
wait for completion of fenced operations, without doing anything else.
What fences are doing is (almost) exactly that: they let coherency to
sort out the final value of the memory location without overlapping the
access with anything else.

E.g. on x86, where total store order is provided by hardware, the
fencing is required to avoid store forwarding effects. With store
forwarding and no fencing, the writer CPU can "see" the written value
before anything else sees it, and can go on pretending everyone else saw
that value as well. This shortcut sets us up for potentially observing
the non-total ordering:

Dekker idiom:

   volatile int x, y;
 -----------------------
  x = 1        y = 1
  r1 = y       r2 = x

(r1, r2) = (0, 0) is forbidden, non-SC.

Imagine CPU1 does the "x = 1" store, and it is on its way to coherency,
now in store buffer. CPU1 then proceeds to read "y", and imagine it
reads "0", because CPU2 is lagging behind. Now, CPU2 does "y = 1" store.
Then CPU2 does "r2 = x" load, and reads "0" (!), because the "x" value
is still stuck in CPU1's store buffer. This is a "transient" condition
when coherency has no idea CPUs have different ideas about the value of
"x", because all values have not yet reached coherency. Locked
instruction / mfence after "x = 1" write basically says "until we are
sure "x = 1" had completed, freeze and don't make any moves."

Even without store buffers, when CPUs can start writing "x = 1" and "y =
1" at the same time, waiting for their local operations to complete
saves us from failures due to overlapped in-flight updates of "x" and
"y". In other words, CPU1 would not be able to start loading "y" before
it completed "x = 1", which means that whatever value of "y" it reads
after completing the write of "x" while CPU1's write of "y = 1" is still
pending, CPU2 is ought to observe "x = 1". Memory barriers provide the
"lock-step" behavior here.

The same line of reasoning applies to non-TSO hardware,
non-multiple-copy-atomic hardware, etc. Memory barriers are routinely
used to regain the order of operations. These things are
well-researched, so instead of relying on mailing list hearsays, see the
authoritative sources here:

http://www.rdrop.com/users/paulmck/scalability/paper/whymb.2010.07.23a.pdf
  http://www.cl.cam.ac.uk/~pes20/weakmemory/cacm.pdf
  http://www.cl.cam.ac.uk/~pes20/ppc-supplemental/test7.pdf
  http://www.cl.cam.ac.uk/~pes20/cpp/cpp0xmappings.html


> If a JVM just followed the rules put forth by the Cookbook, would an
> invalid execution as outlined in my diagram actually be possible?

This is the monospace-friendly version:
 http://cs.oswego.edu/pipermail/concurrency-interest/2015-March/014118.html

Thinking on hardware level is full of surprises, and one should only do
that very carefully.

So, if only the "currentTime" is volatile (sequentially consistent), you
can still read "0" from non-volatile "sharedVar" in Rv0. The easiest way
to achieve that hardware wise is to get Wv1 to stuck in store buffer,
voila. It does not depend on whether the loads and stores to
"currentTime" are totally ordered or not. Even if they are, Wv1 can
still move past Rwt6.

If "sharedVar" is also volatile (sequentially consistent), then Wv1
would complete before reading Rwt6. Reading Rwt6 after the write means
the write is observable near tick 6: it is plausible the clock ticked 6
before we were writing; it is plausible the clock ticked 6 right after
we did the write. Which *really* means the write is guaranteed to be
observable at the *next* tick, T9, since "currentTime" reads/writes are
totally ordered. Therefore, once the reader thread observed t=9, it
should also observe the Wv1, rendering Rv0 reading "0" incorrect.

                                Rrt9 ---> Rv0
  Wv0 --> Wv1 --> Rwt6           ^
         .---------^         .---/
       T6 ---------------> T9

 "global time" -------------------------------->


Notice how this relies on the writer thread to observe Rwt6! That's a
reference frame for you. If writer was to observe Rwt9, you might have
plausibly inferred the Wv1 may be not visible at Rv0:

            Rrt9 ---> Rv0
              ^
  Wv0 --------+---------> Wv1 --> Rwt9
         .----/--------------------^
       T9

 "global time" -------------------------------->

It inherently relies on T6 updates to wait for everyone to see the
update. In other words, guaranteeing every thread sees the consistent
progression of values in some "global time".

Thanks,
-Aleksey.


-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 819 bytes
Desc: OpenPGP digital signature
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150317/f9ca0d9c/attachment.bin>

From marko at hazelcast.com  Tue Mar 17 09:39:32 2015
From: marko at hazelcast.com (Marko Topolnik)
Date: Tue, 17 Mar 2015 14:39:32 +0100
Subject: [concurrency-interest] Enforcing total sync order on modern
	hardware
In-Reply-To: <550805ED.30003@oracle.com>
References: <CALtZ-o47wWjeVXtn=zx8t4vau6D5T-qFnA3Vm2AGcFUm2S_PVw@mail.gmail.com>
	<55072495.2060701@redhat.com>
	<CALtZ-o4r9JNRD5gr=eiqc9ukfgO2g4UgFzv66FtoWsN9480oWA@mail.gmail.com>
	<CAHjP37H+bQYLebX-LrA5gE_d+UjFKkUiTTOwGYyJ=dnboHk=sQ@mail.gmail.com>
	<CALtZ-o6J9GEtjmvL4aR30GNUnpX7Jjk0t_FeLCE1oAgvLMwQow@mail.gmail.com>
	<CAHjP37GuG76JTbdpVfWi+GUZbaBReonFh3HmOEBbp84=5KKX=g@mail.gmail.com>
	<CALtZ-o78Nsh=rC7O7Yj1p6tJPrjQRfevNpgnKRucym8LvE2pJQ@mail.gmail.com>
	<CALtZ-o6qur6Qc5Vj6NBrPfFNyd3DB-U8CZuXZ3znm7-XHZoXjg@mail.gmail.com>
	<550805ED.30003@oracle.com>
Message-ID: <CALtZ-o7TCBcFhv3BpjCWQsGoYh+rFbB83sutZa8DXCtB2V16HA@mail.gmail.com>

On Tue, Mar 17, 2015 at 11:46 AM, Aleksey Shipilev <
aleksey.shipilev at oracle.com> wrote:

> On 17.03.2015 9:31, Marko Topolnik wrote:
> > There is another concern that may be interesting to reconsider. Given
> > the lack of total sync order when just using memory barriers, is the
> > JSR-133 Cookbook wrong/outdated in this respect? It doesn't at all deal
> > with the issue of the sync order, just with the visibility of
> > inter-thread actions.
>
> The mental model I am having in my head is as follows:
>
>   a) Cache-coherent systems maintain the consistent (coherent) view of
> each memory location at any given moment. In fact, most coherency
> protocols provide the total order for the operations on a *single*
> location. Regardless how the actual interconnect is operating, the cache
> coherency protocols are to maintain that illusion. MESI-like protocols
> are by nature message-based, and so they do not require shared bus to
> begin with, so no problems with QPI.
>

So let's fix the following total order on currentTime:

T3 -> Rwt3 -> T6 -> Rwt6 -> Rrt6 -> T9 -> Rrt9


> If "sharedVar" is also volatile (sequentially consistent), then Wv1
> would complete before reading Rwt6.


OK, but this wouldn't necessarily happen on a unique global timescale: the
"writing" thread would have the ordering Wv1 -> Rwt6; there would be an
_independent_ total order of actions on currentTime, and a third, again
independent order of actions by the "reading" thread. Due to the
distributed nature of coherence the fact that, on one core, Wv1 precedes
Rwt6 does not enforce Rrt6 -> Rv1 on another core. It is not obvious that
there is transitivity between these individual orders.

Particularly note this statement in
http://www.cl.cam.ac.uk/~pes20/weakmemory/cacm.pdf:

"[the CPU vendor specifications] admit the IRIW behaviour above but, under
reasonable assumptions on the strongest x86 memory barrier, MFENCE, adding
MFENCEs would not suffice to recover sequential consistency (instead, one
would have to make liberal use of x86 LOCK?d instructions). Here the
specifications seem to be much looser than the behaviour of implemented
processors: to the best of our knowledge, and following some testing, IRIW
is not observable in practice, even without MFENCEs. It appears that some
JVM implementations depend on this fact, and would not be correct if one
assumed only the IWP/AMD3.14/x86-CC architecture."

Also, for the newer revision of Intel's specification, ?P6. In a
multiprocessor system, stores to the same location have a total order? has
been replaced by: ?Any two stores are seen in a consistent order by
processors other than those performing the stores.?

So here's a consistent order seen by all the processors except those
running the two writing threads:

Wv0 -> T3 -> T6 -> T9 -> Wv1

This also respects the total ordering for each individual site, and a total
ordering of each individual processor's stores. The "reading" thread
inserts its Rv0 between T9 and Wv1.



> Reading Rwt6 after the write means
> the write is observable near tick 6: it is plausible the clock ticked 6
> before we were writing; it is plausible the clock ticked 6 right after
> we did the write. Which *really* means the write is guaranteed to be
> observable at the *next* tick, T9, since "currentTime" reads/writes are
> totally ordered. Therefore, once the reader thread observed t=9, it
> should also observe the Wv1, rendering Rv0 reading "0" incorrect.
>
>                                 Rrt9 ---> Rv0
>   Wv0 --> Wv1 --> Rwt6           ^
>          .---------^         .---/
>        T6 ---------------> T9
>
>  "global time" -------------------------------->
>
>
> Notice how this relies on the writer thread to observe Rwt6! That's a
> reference frame for you. If writer was to observe Rwt9, you might have
> plausibly inferred the Wv1 may be not visible at Rv0:
>

Thanks, that was precisely my motivation to add Rwt6 :)

---
Marko
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150317/52826738/attachment-0001.html>

From vitalyd at gmail.com  Tue Mar 17 10:18:55 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Tue, 17 Mar 2015 10:18:55 -0400
Subject: [concurrency-interest] Enforcing total sync order on modern
	hardware
In-Reply-To: <CALtZ-o7TCBcFhv3BpjCWQsGoYh+rFbB83sutZa8DXCtB2V16HA@mail.gmail.com>
References: <CALtZ-o47wWjeVXtn=zx8t4vau6D5T-qFnA3Vm2AGcFUm2S_PVw@mail.gmail.com>
	<55072495.2060701@redhat.com>
	<CALtZ-o4r9JNRD5gr=eiqc9ukfgO2g4UgFzv66FtoWsN9480oWA@mail.gmail.com>
	<CAHjP37H+bQYLebX-LrA5gE_d+UjFKkUiTTOwGYyJ=dnboHk=sQ@mail.gmail.com>
	<CALtZ-o6J9GEtjmvL4aR30GNUnpX7Jjk0t_FeLCE1oAgvLMwQow@mail.gmail.com>
	<CAHjP37GuG76JTbdpVfWi+GUZbaBReonFh3HmOEBbp84=5KKX=g@mail.gmail.com>
	<CALtZ-o78Nsh=rC7O7Yj1p6tJPrjQRfevNpgnKRucym8LvE2pJQ@mail.gmail.com>
	<CALtZ-o6qur6Qc5Vj6NBrPfFNyd3DB-U8CZuXZ3znm7-XHZoXjg@mail.gmail.com>
	<550805ED.30003@oracle.com>
	<CALtZ-o7TCBcFhv3BpjCWQsGoYh+rFbB83sutZa8DXCtB2V16HA@mail.gmail.com>
Message-ID: <CAHjP37HYFWvdV0iMah7h4jLqX8miQcW=Prg_f9EPd4Lr5=keWQ@mail.gmail.com>

The Intel spec update is to account for store buffers in the absence of
fences.

In your "inserts a Rv0" example aren't we back to scheduling? There's no
happens before relationship between the time value and the two other
threads.  Making the time writes volatile translates into those writes
being visible to all other cores before the time thread progresses - when
that time value is observed is up to those other cores.  If the two threads
communicated via piggybacking on the time value, say writer writes Svar=9
only when t=9 then reader is guaranteed to see at least Svar=9 if they also
see t=9 (on x86), assuming Svar is volatile.  But as your example stands,
you have some value being written globally every so often, and then two
other threads peeking and poking at it while doing their own thing.

sent from my phone
On Mar 17, 2015 10:03 AM, "Marko Topolnik" <marko at hazelcast.com> wrote:

> On Tue, Mar 17, 2015 at 11:46 AM, Aleksey Shipilev <
> aleksey.shipilev at oracle.com> wrote:
>
>> On 17.03.2015 9:31, Marko Topolnik wrote:
>> > There is another concern that may be interesting to reconsider. Given
>> > the lack of total sync order when just using memory barriers, is the
>> > JSR-133 Cookbook wrong/outdated in this respect? It doesn't at all deal
>> > with the issue of the sync order, just with the visibility of
>> > inter-thread actions.
>>
>> The mental model I am having in my head is as follows:
>>
>>   a) Cache-coherent systems maintain the consistent (coherent) view of
>> each memory location at any given moment. In fact, most coherency
>> protocols provide the total order for the operations on a *single*
>> location. Regardless how the actual interconnect is operating, the cache
>> coherency protocols are to maintain that illusion. MESI-like protocols
>> are by nature message-based, and so they do not require shared bus to
>> begin with, so no problems with QPI.
>>
>
> So let's fix the following total order on currentTime:
>
> T3 -> Rwt3 -> T6 -> Rwt6 -> Rrt6 -> T9 -> Rrt9
>
>
>> If "sharedVar" is also volatile (sequentially consistent), then Wv1
>> would complete before reading Rwt6.
>
>
> OK, but this wouldn't necessarily happen on a unique global timescale: the
> "writing" thread would have the ordering Wv1 -> Rwt6; there would be an
> _independent_ total order of actions on currentTime, and a third, again
> independent order of actions by the "reading" thread. Due to the
> distributed nature of coherence the fact that, on one core, Wv1 precedes
> Rwt6 does not enforce Rrt6 -> Rv1 on another core. It is not obvious that
> there is transitivity between these individual orders.
>
> Particularly note this statement in
> http://www.cl.cam.ac.uk/~pes20/weakmemory/cacm.pdf:
>
> "[the CPU vendor specifications] admit the IRIW behaviour above but, under
> reasonable assumptions on the strongest x86 memory barrier, MFENCE, adding
> MFENCEs would not suffice to recover sequential consistency (instead, one
> would have to make liberal use of x86 LOCK?d instructions). Here the
> specifications seem to be much looser than the behaviour of implemented
> processors: to the best of our knowledge, and following some testing, IRIW
> is not observable in practice, even without MFENCEs. It appears that some
> JVM implementations depend on this fact, and would not be correct if one
> assumed only the IWP/AMD3.14/x86-CC architecture."
>
> Also, for the newer revision of Intel's specification, ?P6. In a
> multiprocessor system, stores to the same location have a total order? has
> been replaced by: ?Any two stores are seen in a consistent order by
> processors other than those performing the stores.?
>
> So here's a consistent order seen by all the processors except those
> running the two writing threads:
>
> Wv0 -> T3 -> T6 -> T9 -> Wv1
>
> This also respects the total ordering for each individual site, and a
> total ordering of each individual processor's stores. The "reading" thread
> inserts its Rv0 between T9 and Wv1.
>
>
>
>> Reading Rwt6 after the write means
>> the write is observable near tick 6: it is plausible the clock ticked 6
>> before we were writing; it is plausible the clock ticked 6 right after
>> we did the write. Which *really* means the write is guaranteed to be
>> observable at the *next* tick, T9, since "currentTime" reads/writes are
>> totally ordered. Therefore, once the reader thread observed t=9, it
>> should also observe the Wv1, rendering Rv0 reading "0" incorrect.
>>
>>                                 Rrt9 ---> Rv0
>>   Wv0 --> Wv1 --> Rwt6           ^
>>          .---------^         .---/
>>        T6 ---------------> T9
>>
>>  "global time" -------------------------------->
>>
>>
>> Notice how this relies on the writer thread to observe Rwt6! That's a
>> reference frame for you. If writer was to observe Rwt9, you might have
>> plausibly inferred the Wv1 may be not visible at Rv0:
>>
>
> Thanks, that was precisely my motivation to add Rwt6 :)
>
> ---
> Marko
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150317/4a5b1805/attachment.html>

From aleksey.shipilev at oracle.com  Tue Mar 17 10:56:39 2015
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Tue, 17 Mar 2015 17:56:39 +0300
Subject: [concurrency-interest] Enforcing total sync order on modern
	hardware
In-Reply-To: <CALtZ-o7TCBcFhv3BpjCWQsGoYh+rFbB83sutZa8DXCtB2V16HA@mail.gmail.com>
References: <CALtZ-o47wWjeVXtn=zx8t4vau6D5T-qFnA3Vm2AGcFUm2S_PVw@mail.gmail.com>	<55072495.2060701@redhat.com>	<CALtZ-o4r9JNRD5gr=eiqc9ukfgO2g4UgFzv66FtoWsN9480oWA@mail.gmail.com>	<CAHjP37H+bQYLebX-LrA5gE_d+UjFKkUiTTOwGYyJ=dnboHk=sQ@mail.gmail.com>	<CALtZ-o6J9GEtjmvL4aR30GNUnpX7Jjk0t_FeLCE1oAgvLMwQow@mail.gmail.com>	<CAHjP37GuG76JTbdpVfWi+GUZbaBReonFh3HmOEBbp84=5KKX=g@mail.gmail.com>	<CALtZ-o78Nsh=rC7O7Yj1p6tJPrjQRfevNpgnKRucym8LvE2pJQ@mail.gmail.com>	<CALtZ-o6qur6Qc5Vj6NBrPfFNyd3DB-U8CZuXZ3znm7-XHZoXjg@mail.gmail.com>	<550805ED.30003@oracle.com>
	<CALtZ-o7TCBcFhv3BpjCWQsGoYh+rFbB83sutZa8DXCtB2V16HA@mail.gmail.com>
Message-ID: <550840A7.4080501@oracle.com>

On 03/17/2015 04:39 PM, Marko Topolnik wrote:
> On Tue, Mar 17, 2015 at 11:46 AM, Aleksey Shipilev
> <aleksey.shipilev at oracle.com <mailto:aleksey.shipilev at oracle.com>> wrote:
> So let's fix the following total order on currentTime:
> 
> T3 -> Rwt3 -> T6 -> Rwt6 -> Rrt6 -> T9 -> Rrt9
>  
> 
>     If "sharedVar" is also volatile (sequentially consistent), then Wv1
>     would complete before reading Rwt6. 
> 
> 
> OK, but this wouldn't necessarily happen on a unique global timescale:
> the "writing" thread would have the ordering Wv1 -> Rwt6; there would be
> an _independent_ total order of actions on currentTime, and a third,
> again independent order of actions by the "reading" thread. Due to the
> distributed nature of coherence the fact that, on one core, Wv1 precedes
> Rwt6 does not enforce Rrt6 -> Rv1 on another core. It is not obvious
> that there is transitivity between these individual orders.

No one fully understands how exactly the hardware concurrency works. All
the academic papers you and me cited before are trying to recover the
actual semantics from the experiments and anecdotal examples. Therefore,
lacking the clearly-specified model, we can only come up with some model
and explain the behavior there.

I offered the simplistic mental model where fences expose the values to
global coherency in instruction order, and thus regain the total order.
When you say "wouldn't necessarily happen on a unique global timescale"
or "due to distributed nature of coherence", you seem to be implying
some other model, but that definition is too vague to be useful. Indeed,
I can come up with an arbitrarily weak model that allows pretty much
anything.


> Particularly note this statement
> in http://www.cl.cam.ac.uk/~pes20/weakmemory/cacm.pdf
> <http://www.cl.cam.ac.uk/%7Epes20/weakmemory/cacm.pdf>:
> 
> "[the CPU vendor specifications] admit the IRIW behaviour above but,
> under reasonable assumptions on the strongest x86 memory barrier,
> MFENCE, adding MFENCEs would not suffice to recover sequential
> consistency (instead, one would have to make liberal use of x86 LOCK?d
> instructions). Here the specifications seem to be much looser than the
> behaviour of implemented processors: to the best of our knowledge, and
> following some testing, IRIW is not observable in practice, even without
> MFENCEs. It appears that some JVM implementations depend on this fact,
> and would not be correct if one assumed only the IWP/AMD3.14/x86-CC
> architecture."


http://www0.cs.ucl.ac.uk/staff/j.alglave/papers/popl09.pdf has a
discussion about the suspected semantics of MFENCE, and they argue
MFENCEs are not enough if you are being paranoid about this. They also
argue Intel SDM can be read as if MFENCEs are actually serialized.

My example above also implied the fences are globally serialized. This
dubs the actual practice in both C/C++11 and JVM mappings, that assume
x86 MFENCE-s are globally serialized and MFENCE/LOCK-insns can be used
interchangeably.

POWER's hwsync seems to provide SC guarantees in the similar fashion,
"Regaining sequential consistency (SC) using sync: If one adds a sync
between every program-order pair of instructions (creating tests
SB+syncs, MP+syncs, WRC+syncs, and IRIW+syncs), then all the non-SC
results above are forbidden,"
(http://www.cl.cam.ac.uk/~pes20/ppc-supplemental/pldi105-sarkar.pdf).

These intricate details on hardware concurrency is one of the reasons we
have JMM. JMM explicitly requires the total order on synchronization
actions, and JVM implementors are responsible to figure out how to map
it back on hardware, let it be fences or locked instructions or what.

Aside: The IRIW example on POWER requires hwsyncs before the volatile
reads, which incurs performance penalties. The discussion on whether
IRIW (and more broadly, total order that mandates IRIW) is something
that we should care about in a language memory model, is way, way above
most people's heads, including mine. Until there is an expert consensus
on the topic, laymans we are should not try to confuse ourselves with
it, before reading up and fully understanding the academic trail on the
topic.


> Also, for the newer revision of Intel's specification, ?P6. In a
> multiprocessor system, stores to the same location have a total order?
> has been replaced by: ?Any two stores are seen in a consistent order by
> processors other than those performing the stores.?

I think that provision is to allow store forwarding, as I described in
my original reply with Dekker idiom. MFENCE should bring the total store
order guarantees back even for the "writer" processors, since as per
Intel SDM:

"This serializing operation guarantees that every load and store
instruction that precedes the MFENCE instruction in program order
becomes *globally visible* before any load or store instruction that
follows the MFENCE instruction."

In other words, the relaxation for "writer" processors is the relaxation
for the regular TSO, not when you are using fences to force the order.


> So here's a consistent order seen by all the processors except those
> running the two writing threads:
> 
> Wv0 -> T3 -> T6 -> T9 -> Wv1
> 
> This also respects the total ordering for each individual site, and a
> total ordering of each individual processor's stores. The "reading"
> thread inserts its Rv0 between T9 and Wv1.

I don't understand this example. If Wv1 is serialized with fence, and
there is a read from "t" following it, that read cannot be Rwt6 then, as
it should see t=9. That pretty much deconstructs the original "execution".


-Aleksey.

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 819 bytes
Desc: OpenPGP digital signature
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150317/9b42fa53/attachment.bin>

From vitalyd at gmail.com  Tue Mar 17 11:33:42 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Tue, 17 Mar 2015 11:33:42 -0400
Subject: [concurrency-interest] Enforcing total sync order on modern
	hardware
In-Reply-To: <550840A7.4080501@oracle.com>
References: <CALtZ-o47wWjeVXtn=zx8t4vau6D5T-qFnA3Vm2AGcFUm2S_PVw@mail.gmail.com>
	<55072495.2060701@redhat.com>
	<CALtZ-o4r9JNRD5gr=eiqc9ukfgO2g4UgFzv66FtoWsN9480oWA@mail.gmail.com>
	<CAHjP37H+bQYLebX-LrA5gE_d+UjFKkUiTTOwGYyJ=dnboHk=sQ@mail.gmail.com>
	<CALtZ-o6J9GEtjmvL4aR30GNUnpX7Jjk0t_FeLCE1oAgvLMwQow@mail.gmail.com>
	<CAHjP37GuG76JTbdpVfWi+GUZbaBReonFh3HmOEBbp84=5KKX=g@mail.gmail.com>
	<CALtZ-o78Nsh=rC7O7Yj1p6tJPrjQRfevNpgnKRucym8LvE2pJQ@mail.gmail.com>
	<CALtZ-o6qur6Qc5Vj6NBrPfFNyd3DB-U8CZuXZ3znm7-XHZoXjg@mail.gmail.com>
	<550805ED.30003@oracle.com>
	<CALtZ-o7TCBcFhv3BpjCWQsGoYh+rFbB83sutZa8DXCtB2V16HA@mail.gmail.com>
	<550840A7.4080501@oracle.com>
Message-ID: <CAHjP37GVk7m-mcX+gYTBd8uU6iNus-L9V2-EOGSkQW5Po+z8yg@mail.gmail.com>

I suspect that part of the confusion is caused by Intel talking about
locked instructions as having total order.  I'm not sure why they call them
out like this, but I suspect it's because the CPUs have to arbitrate at who
acquires the cacheline in exclusive mode if this happens to run at exactly
the same time.  Otherwise, I don't see it any different than normal cache
coherence mechanics that prevent multiple CPUs performing an update to the
same line concurrently.

Also, total order means each CPUs stores appear in program (and same) order
to all other CPUs (modulo store buffer forwarding).  But to me this means
there're multiple ordered streams proceeding concurrently - everyone
observing these stores sees them at the same time (if read at the same
time) and in same order.  But the system isn't moving in some wall-clock
lock step manner unless code uses happens before conditions to make
progress by observing phase changes.

sent from my phone
On Mar 17, 2015 11:20 AM, "Aleksey Shipilev" <aleksey.shipilev at oracle.com>
wrote:

> On 03/17/2015 04:39 PM, Marko Topolnik wrote:
> > On Tue, Mar 17, 2015 at 11:46 AM, Aleksey Shipilev
> > <aleksey.shipilev at oracle.com <mailto:aleksey.shipilev at oracle.com>>
> wrote:
> > So let's fix the following total order on currentTime:
> >
> > T3 -> Rwt3 -> T6 -> Rwt6 -> Rrt6 -> T9 -> Rrt9
> >
> >
> >     If "sharedVar" is also volatile (sequentially consistent), then Wv1
> >     would complete before reading Rwt6.
> >
> >
> > OK, but this wouldn't necessarily happen on a unique global timescale:
> > the "writing" thread would have the ordering Wv1 -> Rwt6; there would be
> > an _independent_ total order of actions on currentTime, and a third,
> > again independent order of actions by the "reading" thread. Due to the
> > distributed nature of coherence the fact that, on one core, Wv1 precedes
> > Rwt6 does not enforce Rrt6 -> Rv1 on another core. It is not obvious
> > that there is transitivity between these individual orders.
>
> No one fully understands how exactly the hardware concurrency works. All
> the academic papers you and me cited before are trying to recover the
> actual semantics from the experiments and anecdotal examples. Therefore,
> lacking the clearly-specified model, we can only come up with some model
> and explain the behavior there.
>
> I offered the simplistic mental model where fences expose the values to
> global coherency in instruction order, and thus regain the total order.
> When you say "wouldn't necessarily happen on a unique global timescale"
> or "due to distributed nature of coherence", you seem to be implying
> some other model, but that definition is too vague to be useful. Indeed,
> I can come up with an arbitrarily weak model that allows pretty much
> anything.
>
>
> > Particularly note this statement
> > in http://www.cl.cam.ac.uk/~pes20/weakmemory/cacm.pdf
> > <http://www.cl.cam.ac.uk/%7Epes20/weakmemory/cacm.pdf>:
> >
> > "[the CPU vendor specifications] admit the IRIW behaviour above but,
> > under reasonable assumptions on the strongest x86 memory barrier,
> > MFENCE, adding MFENCEs would not suffice to recover sequential
> > consistency (instead, one would have to make liberal use of x86 LOCK?d
> > instructions). Here the specifications seem to be much looser than the
> > behaviour of implemented processors: to the best of our knowledge, and
> > following some testing, IRIW is not observable in practice, even without
> > MFENCEs. It appears that some JVM implementations depend on this fact,
> > and would not be correct if one assumed only the IWP/AMD3.14/x86-CC
> > architecture."
>
>
> http://www0.cs.ucl.ac.uk/staff/j.alglave/papers/popl09.pdf has a
> discussion about the suspected semantics of MFENCE, and they argue
> MFENCEs are not enough if you are being paranoid about this. They also
> argue Intel SDM can be read as if MFENCEs are actually serialized.
>
> My example above also implied the fences are globally serialized. This
> dubs the actual practice in both C/C++11 and JVM mappings, that assume
> x86 MFENCE-s are globally serialized and MFENCE/LOCK-insns can be used
> interchangeably.
>
> POWER's hwsync seems to provide SC guarantees in the similar fashion,
> "Regaining sequential consistency (SC) using sync: If one adds a sync
> between every program-order pair of instructions (creating tests
> SB+syncs, MP+syncs, WRC+syncs, and IRIW+syncs), then all the non-SC
> results above are forbidden,"
> (http://www.cl.cam.ac.uk/~pes20/ppc-supplemental/pldi105-sarkar.pdf).
>
> These intricate details on hardware concurrency is one of the reasons we
> have JMM. JMM explicitly requires the total order on synchronization
> actions, and JVM implementors are responsible to figure out how to map
> it back on hardware, let it be fences or locked instructions or what.
>
> Aside: The IRIW example on POWER requires hwsyncs before the volatile
> reads, which incurs performance penalties. The discussion on whether
> IRIW (and more broadly, total order that mandates IRIW) is something
> that we should care about in a language memory model, is way, way above
> most people's heads, including mine. Until there is an expert consensus
> on the topic, laymans we are should not try to confuse ourselves with
> it, before reading up and fully understanding the academic trail on the
> topic.
>
>
> > Also, for the newer revision of Intel's specification, ?P6. In a
> > multiprocessor system, stores to the same location have a total order?
> > has been replaced by: ?Any two stores are seen in a consistent order by
> > processors other than those performing the stores.?
>
> I think that provision is to allow store forwarding, as I described in
> my original reply with Dekker idiom. MFENCE should bring the total store
> order guarantees back even for the "writer" processors, since as per
> Intel SDM:
>
> "This serializing operation guarantees that every load and store
> instruction that precedes the MFENCE instruction in program order
> becomes *globally visible* before any load or store instruction that
> follows the MFENCE instruction."
>
> In other words, the relaxation for "writer" processors is the relaxation
> for the regular TSO, not when you are using fences to force the order.
>
>
> > So here's a consistent order seen by all the processors except those
> > running the two writing threads:
> >
> > Wv0 -> T3 -> T6 -> T9 -> Wv1
> >
> > This also respects the total ordering for each individual site, and a
> > total ordering of each individual processor's stores. The "reading"
> > thread inserts its Rv0 between T9 and Wv1.
>
> I don't understand this example. If Wv1 is serialized with fence, and
> there is a read from "t" following it, that read cannot be Rwt6 then, as
> it should see t=9. That pretty much deconstructs the original "execution".
>
>
> -Aleksey.
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150317/eb166221/attachment-0001.html>

From marko at hazelcast.com  Tue Mar 17 11:57:39 2015
From: marko at hazelcast.com (Marko Topolnik)
Date: Tue, 17 Mar 2015 16:57:39 +0100
Subject: [concurrency-interest] Enforcing total sync order on modern
	hardware
In-Reply-To: <550840A7.4080501@oracle.com>
References: <CALtZ-o47wWjeVXtn=zx8t4vau6D5T-qFnA3Vm2AGcFUm2S_PVw@mail.gmail.com>
	<55072495.2060701@redhat.com>
	<CALtZ-o4r9JNRD5gr=eiqc9ukfgO2g4UgFzv66FtoWsN9480oWA@mail.gmail.com>
	<CAHjP37H+bQYLebX-LrA5gE_d+UjFKkUiTTOwGYyJ=dnboHk=sQ@mail.gmail.com>
	<CALtZ-o6J9GEtjmvL4aR30GNUnpX7Jjk0t_FeLCE1oAgvLMwQow@mail.gmail.com>
	<CAHjP37GuG76JTbdpVfWi+GUZbaBReonFh3HmOEBbp84=5KKX=g@mail.gmail.com>
	<CALtZ-o78Nsh=rC7O7Yj1p6tJPrjQRfevNpgnKRucym8LvE2pJQ@mail.gmail.com>
	<CALtZ-o6qur6Qc5Vj6NBrPfFNyd3DB-U8CZuXZ3znm7-XHZoXjg@mail.gmail.com>
	<550805ED.30003@oracle.com>
	<CALtZ-o7TCBcFhv3BpjCWQsGoYh+rFbB83sutZa8DXCtB2V16HA@mail.gmail.com>
	<550840A7.4080501@oracle.com>
Message-ID: <CALtZ-o41-6QZJma_0aG3k6B42ZELPZ7Yr_w1aTqrU6_vcG5OBw@mail.gmail.com>

On Tue, Mar 17, 2015 at 3:56 PM, Aleksey Shipilev <
aleksey.shipilev at oracle.com> wrote:
>
>
> I offered the simplistic mental model where fences expose the values to
> global coherency in instruction order, and thus regain the total order.
> When you say "wouldn't necessarily happen on a unique global timescale"
> or "due to distributed nature of coherence", you seem to be implying
> some other model, but that definition is too vague to be useful. Indeed,
> I can come up with an arbitrarily weak model that allows pretty much
> anything.


I am trying to work my way starting from specified guarantees, taking care
not to introduce tacit assumptions such as the transitivity of separately
guaranteed total orders. But yes, at some point empirical evidence takes
over the documented guarantees.

> These intricate details on hardware concurrency is one of the reasons we
> have JMM. JMM explicitly requires the total order on synchronization
> actions, and JVM implementors are responsible to figure out how to map
> it back on hardware, let it be fences or locked instructions or what.

Sure, that's exactly what I was asking about: how does JMM efficiently
translate into hardware given the ever more distributed nature of the HW
architecture.

> > Also, for the newer revision of Intel's specification, ?P6. In a
> > multiprocessor system, stores to the same location have a total order?
> > has been replaced by: ?Any two stores are seen in a consistent order by
> > processors other than those performing the stores.?
>
> I think that provision is to allow store forwarding, as I described in
> my original reply with Dekker idiom. MFENCE should bring the total store
> order guarantees back even for the "writer" processors

Note that the older phrase talked only about the _same_ location, therefore
not applying to IRIW.

> > So here's a consistent order seen by all the processors except those
> > running the two writing threads:
> >
> > Wv0 -> T3 -> T6 -> T9 -> Wv1
> >
> > This also respects the total ordering for each individual site, and a
> > total ordering of each individual processor's stores. The "reading"
> > thread inserts its Rv0 between T9 and Wv1.
>
> I don't understand this example. If Wv1 is serialized with fence, and
> there is a read from "t" following it, that read cannot be Rwt6 then, as
> it should see t=9. That pretty much deconstructs the original "execution".

I didn't see the need to imply global serialization: I maintained the order
of writes by each thread and the total order of actions on a single
location. Only a global ordering between reads of one location and writes
to another location forces Wv1 to be sandwiched between T3 and T6.
Apparently, though, MFENCE does force it.

-Marko
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150317/2e7d2950/attachment.html>

From aleksey.shipilev at oracle.com  Tue Mar 17 12:42:38 2015
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Tue, 17 Mar 2015 19:42:38 +0300
Subject: [concurrency-interest] Enforcing total sync order on modern
	hardware
In-Reply-To: <CALtZ-o41-6QZJma_0aG3k6B42ZELPZ7Yr_w1aTqrU6_vcG5OBw@mail.gmail.com>
References: <CALtZ-o47wWjeVXtn=zx8t4vau6D5T-qFnA3Vm2AGcFUm2S_PVw@mail.gmail.com>	<55072495.2060701@redhat.com>	<CALtZ-o4r9JNRD5gr=eiqc9ukfgO2g4UgFzv66FtoWsN9480oWA@mail.gmail.com>	<CAHjP37H+bQYLebX-LrA5gE_d+UjFKkUiTTOwGYyJ=dnboHk=sQ@mail.gmail.com>	<CALtZ-o6J9GEtjmvL4aR30GNUnpX7Jjk0t_FeLCE1oAgvLMwQow@mail.gmail.com>	<CAHjP37GuG76JTbdpVfWi+GUZbaBReonFh3HmOEBbp84=5KKX=g@mail.gmail.com>	<CALtZ-o78Nsh=rC7O7Yj1p6tJPrjQRfevNpgnKRucym8LvE2pJQ@mail.gmail.com>	<CALtZ-o6qur6Qc5Vj6NBrPfFNyd3DB-U8CZuXZ3znm7-XHZoXjg@mail.gmail.com>	<550805ED.30003@oracle.com>	<CALtZ-o7TCBcFhv3BpjCWQsGoYh+rFbB83sutZa8DXCtB2V16HA@mail.gmail.com>	<550840A7.4080501@oracle.com>
	<CALtZ-o41-6QZJma_0aG3k6B42ZELPZ7Yr_w1aTqrU6_vcG5OBw@mail.gmail.com>
Message-ID: <5508597E.7030307@oracle.com>

On 03/17/2015 06:57 PM, Marko Topolnik wrote:
> On Tue, Mar 17, 2015 at 3:56 PM, Aleksey Shipilev
> <aleksey.shipilev at oracle.com <mailto:aleksey.shipilev at oracle.com>> wrote:
> I am trying to work my way starting from specified guarantees, taking
> care not to introduce tacit assumptions such as the transitivity of
> separately guaranteed total orders. But yes, at some point empirical
> evidence takes over the documented guarantees.

If you are looking for documented guarantees that include hardware
memory consistency, the full JMM consistency rules, and explain any
phenomena you throw at it, then you are in for a
<strike>surprise</strike> long haul!



>> These intricate details on hardware concurrency is one of the reasons we
>> have JMM. JMM explicitly requires the total order on synchronization
>> actions, and JVM implementors are responsible to figure out how to map
>> it back on hardware, let it be fences or locked instructions or what.
> 
> Sure, that's exactly what I was asking about: how does JMM efficiently
> translate into hardware given the ever more distributed nature of the HW
> architecture.

So, big picture view: hardware does provide the totally ordered
(sequentially consistent) operations. locked insns and/or fences on x86;
hwsyncs/etc on POWER; dmb/ll/sc on ARM, etc. JMM implementations, as
well as C/C++11 implementations almost directly map their total ordered
sync ops to those hardware primitives.

It is a hardware problem how to maintain total ordering. The simple
mental model that fits simplified message-passing coherency protocol is
relatively easy to construct. It deliberately does not talk about
intricate topologies, and other bells and whistles, because the original
question was about maintaining the total order in the absence of a
globally lockable shared bus.

Thanks,
-Aleksey


-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 819 bytes
Desc: OpenPGP digital signature
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150317/3e56bcef/attachment.bin>

From stephan.diestelhorst at gmail.com  Tue Mar 17 17:42:17 2015
From: stephan.diestelhorst at gmail.com (Stephan Diestelhorst)
Date: Tue, 17 Mar 2015 21:42:17 +0000
Subject: [concurrency-interest] Enforcing total sync order on modern
	hardware
In-Reply-To: <CALtZ-o7TCBcFhv3BpjCWQsGoYh+rFbB83sutZa8DXCtB2V16HA@mail.gmail.com>
References: <CALtZ-o47wWjeVXtn=zx8t4vau6D5T-qFnA3Vm2AGcFUm2S_PVw@mail.gmail.com>
	<550805ED.30003@oracle.com>
	<CALtZ-o7TCBcFhv3BpjCWQsGoYh+rFbB83sutZa8DXCtB2V16HA@mail.gmail.com>
Message-ID: <1501970.3n0TOdWmYW@d-allen>

Am Dienstag, 17. M?rz 2015, 14:39:32 schrieb Marko Topolnik:
> On Tue, Mar 17, 2015 at 11:46 AM, Aleksey Shipilev <
> aleksey.shipilev at oracle.com> wrote:
> 
> > If "sharedVar" is also volatile (sequentially consistent), then Wv1
> > would complete before reading Rwt6.
> 
> 
> OK, but this wouldn't necessarily happen on a unique global timescale: the
> "writing" thread would have the ordering Wv1 -> Rwt6; there would be an
> _independent_ total order of actions on currentTime, and a third, again
> independent order of actions by the "reading" thread. Due to the
> distributed nature of coherence the fact that, on one core, Wv1 precedes
> Rwt6 does not enforce Rrt6 -> Rv1 on another core. It is not obvious that
> there is transitivity between these individual orders.
> 
> Particularly note this statement in
> http://www.cl.cam.ac.uk/~pes20/weakmemory/cacm.pdf:
> 
> "[the CPU vendor specifications] admit the IRIW behaviour above but, under
> reasonable assumptions on the strongest x86 memory barrier, MFENCE, adding
> MFENCEs would not suffice to recover sequential consistency (instead, one
> would have to make liberal use of x86 LOCK?d instructions). Here the
> specifications seem to be much looser than the behaviour of implemented
> processors: to the best of our knowledge, and following some testing, IRIW
> is not observable in practice, even without MFENCEs. It appears that some
> JVM implementations depend on this fact, and would not be correct if one
> assumed only the IWP/AMD3.14/x86-CC architecture."
> 
> Also, for the newer revision of Intel's specification, ?P6. In a
> multiprocessor system, stores to the same location have a total order? has
> been replaced by: ?Any two stores are seen in a consistent order by
> processors other than those performing the stores.?

IRIW is ruled out by both AMD and Intel.  Not a problem.
Weak architectures, such as ARM, talk about multi-copy atomicity, which
is what you are after.  On these architectures, fences (DMBs in the ARM
case) do restore global order through an elaborate construction of who
saw what etc.

Nothing to see here, I presume (unless you were talking about a real
closk, such as the TSC on x86.  But that has interesting semantics and I
will not feed the trolls.  Some naive notes:
http://rp-www.cs.usyd.edu.au/~gramoli/events/wttm4/papers/diestelhorst.pdf )

Thanks,
  Stephan



From oleksandr.otenko at oracle.com  Tue Mar 17 19:29:31 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Tue, 17 Mar 2015 23:29:31 +0000
Subject: [concurrency-interest] Enforcing total sync order on modern
	hardware
In-Reply-To: <1501970.3n0TOdWmYW@d-allen>
References: <CALtZ-o47wWjeVXtn=zx8t4vau6D5T-qFnA3Vm2AGcFUm2S_PVw@mail.gmail.com>	<550805ED.30003@oracle.com>	<CALtZ-o7TCBcFhv3BpjCWQsGoYh+rFbB83sutZa8DXCtB2V16HA@mail.gmail.com>
	<1501970.3n0TOdWmYW@d-allen>
Message-ID: <5508B8DB.2020405@oracle.com>

> IRIW is ruled out by both AMD and Intel.  Not a problem.

Does this "ruled out" mean "the IRIW results forbidden by JMM will not 
be observed", or does this mean "the IRIW results forbidden by JMM are 
not supported as not a problem"?

Alex



On 17/03/2015 21:42, Stephan Diestelhorst wrote:
> Am Dienstag, 17. M?rz 2015, 14:39:32 schrieb Marko Topolnik:
>> On Tue, Mar 17, 2015 at 11:46 AM, Aleksey Shipilev <
>> aleksey.shipilev at oracle.com> wrote:
>>
>>> If "sharedVar" is also volatile (sequentially consistent), then Wv1
>>> would complete before reading Rwt6.
>>
>> OK, but this wouldn't necessarily happen on a unique global timescale: the
>> "writing" thread would have the ordering Wv1 -> Rwt6; there would be an
>> _independent_ total order of actions on currentTime, and a third, again
>> independent order of actions by the "reading" thread. Due to the
>> distributed nature of coherence the fact that, on one core, Wv1 precedes
>> Rwt6 does not enforce Rrt6 -> Rv1 on another core. It is not obvious that
>> there is transitivity between these individual orders.
>>
>> Particularly note this statement in
>> http://www.cl.cam.ac.uk/~pes20/weakmemory/cacm.pdf:
>>
>> "[the CPU vendor specifications] admit the IRIW behaviour above but, under
>> reasonable assumptions on the strongest x86 memory barrier, MFENCE, adding
>> MFENCEs would not suffice to recover sequential consistency (instead, one
>> would have to make liberal use of x86 LOCK?d instructions). Here the
>> specifications seem to be much looser than the behaviour of implemented
>> processors: to the best of our knowledge, and following some testing, IRIW
>> is not observable in practice, even without MFENCEs. It appears that some
>> JVM implementations depend on this fact, and would not be correct if one
>> assumed only the IWP/AMD3.14/x86-CC architecture."
>>
>> Also, for the newer revision of Intel's specification, ?P6. In a
>> multiprocessor system, stores to the same location have a total order? has
>> been replaced by: ?Any two stores are seen in a consistent order by
>> processors other than those performing the stores.?
> IRIW is ruled out by both AMD and Intel.  Not a problem.
> Weak architectures, such as ARM, talk about multi-copy atomicity, which
> is what you are after.  On these architectures, fences (DMBs in the ARM
> case) do restore global order through an elaborate construction of who
> saw what etc.
>
> Nothing to see here, I presume (unless you were talking about a real
> closk, such as the TSC on x86.  But that has interesting semantics and I
> will not feed the trolls.  Some naive notes:
> http://rp-www.cs.usyd.edu.au/~gramoli/events/wttm4/papers/diestelhorst.pdf )
>
> Thanks,
>    Stephan
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From marko at hazelcast.com  Thu Mar 19 09:21:02 2015
From: marko at hazelcast.com (Marko Topolnik)
Date: Thu, 19 Mar 2015 14:21:02 +0100
Subject: [concurrency-interest] Enforcing total sync order on modern
	hardware
In-Reply-To: <1501970.3n0TOdWmYW@d-allen>
References: <CALtZ-o47wWjeVXtn=zx8t4vau6D5T-qFnA3Vm2AGcFUm2S_PVw@mail.gmail.com>
	<550805ED.30003@oracle.com>
	<CALtZ-o7TCBcFhv3BpjCWQsGoYh+rFbB83sutZa8DXCtB2V16HA@mail.gmail.com>
	<1501970.3n0TOdWmYW@d-allen>
Message-ID: <CALtZ-o5vvaA0=LrAfaVS2mYqKOfctBhGGQMj4oMckpq-_cg0zQ@mail.gmail.com>

On Tue, Mar 17, 2015 at 10:42 PM, Stephan Diestelhorst <
stephan.diestelhorst at gmail.com> wrote:

>
> IRIW is ruled out by both AMD and Intel.  Not a problem.
>

My scenario is not quite IRIW. There's one pure writer, one pure reader,
and one reader-writer. There aren't any two reading processors which
conflict in their observation of the store order. For the reader-writer,
its stores and loads are independent so, without a fence, the visibility of
the store may be postponed and the processor is allowed to observe its own
stores sooner than all others.

It seems that the JVM will emit such a fence after a volatile store, which
will ensure that the store has become visible to everyone before attempting
any further loads. That would cause the Rwt6 load to impose an indirect
happens-before to the Rrt9 load by the reading processor, transitively
forcing Wv1 to happen before Rv0, preventing that load to observe the old
value of 0.

On the other hand, the compiler might be tempted to optimize by eliminating
some of the fences for the case of independent stores and loads. In our
example just relying on TSO would be enough to satisfy happens-before
consistency for actions done by the reading-writing processor. Studying
Aleksey's Nanotrusting the Nanotime we find that doing some navel-gazing
CPU work after a volatile write eliminates the volatile overheads,
indicating that computation was able to continue without waiting for the
store to complete. It's just that the total sync order would be very
difficult to achieve if the computation involved volatile reads (even if
they were independent).


> Weak architectures, such as ARM, talk about multi-copy atomicity, which
> is what you are after.  On these architectures, fences (DMBs in the ARM
> case) do restore global order through an elaborate construction of who
> saw what etc.


A naive idea would be that a single global memory location was CASed on
each sync operation, allowing everything to piggyback on the total ordering
of CAS operations. I guess in reality there are smarter tricks :)

--
Marko
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150319/633f6290/attachment.html>

From oleksandr.otenko at oracle.com  Thu Mar 19 13:07:17 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Thu, 19 Mar 2015 17:07:17 +0000
Subject: [concurrency-interest] Enforcing total sync order on modern
	hardware
In-Reply-To: <CALtZ-o5vvaA0=LrAfaVS2mYqKOfctBhGGQMj4oMckpq-_cg0zQ@mail.gmail.com>
References: <CALtZ-o47wWjeVXtn=zx8t4vau6D5T-qFnA3Vm2AGcFUm2S_PVw@mail.gmail.com>	<550805ED.30003@oracle.com>	<CALtZ-o7TCBcFhv3BpjCWQsGoYh+rFbB83sutZa8DXCtB2V16HA@mail.gmail.com>	<1501970.3n0TOdWmYW@d-allen>
	<CALtZ-o5vvaA0=LrAfaVS2mYqKOfctBhGGQMj4oMckpq-_cg0zQ@mail.gmail.com>
Message-ID: <550B0245.50606@oracle.com>

Your scenario is exactly IRIW.

T9 is x=1
Wv1 is y=1
Rrt9 is r0=x
Rv0 is r1=y

T6 is initial x=0
Rwt6 is a constraint that you want to consider only total orders with 
y=1 preceding x=1
It is equivalent to the independent reader thread doing r2=y, then r3=x, 
and observing r2==1, r3==0. That is, you have reformulated IRIW in such 
a way so that we consider only one specific outcome, not all possible 
outcomes.

T1:
x=1

T2:
y=1

T3:
r0=x;
r1=y;

T4:
r2=y;
r3=x;

JMM forbids the outcome (r0,r1,r2,r3) = (1,0,1,0) - ie one thread 
thinking x=1 happened before y=1, the other thread thinking y=1 happened 
before x=1.

What you are missing from the diagram here:

http://cs.oswego.edu/pipermail/concurrency-interest/2015-March/014118.html

is a synchronizes-with edge from Wv1 to Rv0 - because you didn't 
convince yourself that one must exist. If it does exist, then Rv0 will 
observe the latest write from synchronization order it synchronizes with.

The presence of that edge is proven in the same way as for IRIW: we 
demonstrate an even stronger statement that Rwt6 is before T9 in 
synchronization order; otherwise, Rwt6 must observe T9 (the rule about 
consistency of read operations - reads observe the last write they 
synchronize-with). Then from program orders it follows that Wv1 appears 
before Rv0 in synchronization order, so Rv0 synchronizes-with Wv1 and 
observes v==1 as the last write it synchronizes-with.

Alex



On 19/03/2015 13:21, Marko Topolnik wrote:
>
> On Tue, Mar 17, 2015 at 10:42 PM, Stephan Diestelhorst 
> <stephan.diestelhorst at gmail.com 
> <mailto:stephan.diestelhorst at gmail.com>> wrote:
>
>
>     IRIW is ruled out by both AMD and Intel.  Not a problem.
>
>
> My scenario is not quite IRIW.
...
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150319/77fc0650/attachment.html>

From marko at hazelcast.com  Thu Mar 19 14:02:14 2015
From: marko at hazelcast.com (Marko Topolnik)
Date: Thu, 19 Mar 2015 19:02:14 +0100
Subject: [concurrency-interest] Enforcing total sync order on modern
	hardware
In-Reply-To: <550B0245.50606@oracle.com>
References: <CALtZ-o47wWjeVXtn=zx8t4vau6D5T-qFnA3Vm2AGcFUm2S_PVw@mail.gmail.com>
	<550805ED.30003@oracle.com>
	<CALtZ-o7TCBcFhv3BpjCWQsGoYh+rFbB83sutZa8DXCtB2V16HA@mail.gmail.com>
	<1501970.3n0TOdWmYW@d-allen>
	<CALtZ-o5vvaA0=LrAfaVS2mYqKOfctBhGGQMj4oMckpq-_cg0zQ@mail.gmail.com>
	<550B0245.50606@oracle.com>
Message-ID: <CALtZ-o4K8i2QrS3OzApai=CCv9oFpbtZDZgN0V8M4cZRy5ocTg@mail.gmail.com>

On Thu, Mar 19, 2015 at 6:07 PM, Oleksandr Otenko <
oleksandr.otenko at oracle.com> wrote:

>  Your scenario is exactly IRIW.
>
> T9 is x=1
> Wv1 is y=1
> Rrt9 is r0=x
> Rv0 is r1=y
>
> T6 is initial x=0
>


Rwt6 is a constraint that you want to consider only total orders with y=1
> preceding x=1
> It is equivalent to the independent reader thread doing r2=y, then r3=x,
> and observing r2==1, r3==0. That is, you have reformulated IRIW in such a
> way so that we consider only one specific outcome, not all possible
> outcomes.
>

No, for a correct analogy to IRIW you need to introduce an Rv1' between Wv1
and Rwt6.

Now,

T9 is x=1
Wv1 is y=1
Rrt9 is r0=x
Rv0 is r1=y
Rv1' is r2=y
Rwt6 is r3=x

Now we can clearly see that the JMM does not allow (r0,r1,r2,r3) =
(1,0,1,0) even though this is a happens before-consistent result. In other
words, JMM gives this the exact same treatment as in IRIW. However, on a
low level there is a key difference in that it involves the store buffers
in the timing, and does it asymetrically, just for one of the two reading
threads. This variation is potentially more interesting because it is
specifically excluded from the guarantees of the Intel specification.
Therefore it is harder to meet by the JIT compiler.


> T1:
> x=1
>
> T2:
> y=1
>
> T3:
> r0=x;
> r1=y;
>
> T4:
> r2=y;
> r3=x;
>
> JMM forbids the outcome (r0,r1,r2,r3) = (1,0,1,0) - ie one thread thinking
> x=1 happened before y=1, the other thread thinking y=1 happened before x=1.
>

---
Marko
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150319/e5dd9746/attachment.html>

From oleksandr.otenko at oracle.com  Thu Mar 19 14:14:58 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Thu, 19 Mar 2015 18:14:58 +0000
Subject: [concurrency-interest] Enforcing total sync order on modern
	hardware
In-Reply-To: <CALtZ-o4K8i2QrS3OzApai=CCv9oFpbtZDZgN0V8M4cZRy5ocTg@mail.gmail.com>
References: <CALtZ-o47wWjeVXtn=zx8t4vau6D5T-qFnA3Vm2AGcFUm2S_PVw@mail.gmail.com>	<550805ED.30003@oracle.com>	<CALtZ-o7TCBcFhv3BpjCWQsGoYh+rFbB83sutZa8DXCtB2V16HA@mail.gmail.com>	<1501970.3n0TOdWmYW@d-allen>	<CALtZ-o5vvaA0=LrAfaVS2mYqKOfctBhGGQMj4oMckpq-_cg0zQ@mail.gmail.com>	<550B0245.50606@oracle.com>
	<CALtZ-o4K8i2QrS3OzApai=CCv9oFpbtZDZgN0V8M4cZRy5ocTg@mail.gmail.com>
Message-ID: <550B1222.3060704@oracle.com>

No, you don't need Rv1'.

No, happens-before consistency is not the only rule for correctness of 
the outcome. The "read must see the last write it synchronizes-with" is 
also part of the rule set. You must either show the proof that Wv1 
appears after Rv0, or accept Wv1 appears before Rv0 in synchronization 
order.

Alex

On 19/03/2015 18:02, Marko Topolnik wrote:
>
>
> On Thu, Mar 19, 2015 at 6:07 PM, Oleksandr Otenko 
> <oleksandr.otenko at oracle.com <mailto:oleksandr.otenko at oracle.com>> wrote:
>
>     Your scenario is exactly IRIW.
>
>     T9 is x=1
>     Wv1 is y=1
>     Rrt9 is r0=x
>     Rv0 is r1=y
>
>     T6 is initial x=0
>
>     Rwt6 is a constraint that you want to consider only total orders
>     with y=1 preceding x=1
>     It is equivalent to the independent reader thread doing r2=y, then
>     r3=x, and observing r2==1, r3==0. That is, you have reformulated
>     IRIW in such a way so that we consider only one specific outcome,
>     not all possible outcomes.
>
>
> No, for a correct analogy to IRIW you need to introduce an Rv1' 
> between Wv1 and Rwt6.
>
> Now,
>
> T9 is x=1
> Wv1 is y=1
> Rrt9 is r0=x
> Rv0 is r1=y
> Rv1' is r2=y
> Rwt6 is r3=x
>
> Now we can clearly see that the JMM does not allow (r0,r1,r2,r3) = 
> (1,0,1,0) even though this is a happens before-consistent result. In 
> other words, JMM gives this the exact same treatment as in IRIW. 
> However, on a low level there is a key difference in that it involves 
> the store buffers in the timing, and does it asymetrically, just for 
> one of the two reading threads. This variation is potentially more 
> interesting because it is specifically excluded from the guarantees of 
> the Intel specification. Therefore it is harder to meet by the JIT 
> compiler.
>
>     T1:
>     x=1
>
>     T2:
>     y=1
>
>     T3:
>     r0=x;
>     r1=y;
>
>     T4:
>     r2=y;
>     r3=x;
>
>     JMM forbids the outcome (r0,r1,r2,r3) = (1,0,1,0) - ie one thread
>     thinking x=1 happened before y=1, the other thread thinking y=1
>     happened before x=1.
>
>
> ---
> Marko

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150319/4bc819e5/attachment.html>

From oleksandr.otenko at oracle.com  Thu Mar 19 14:32:09 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Thu, 19 Mar 2015 18:32:09 +0000
Subject: [concurrency-interest] Enforcing total sync order on modern
	hardware
In-Reply-To: <550B1222.3060704@oracle.com>
References: <CALtZ-o47wWjeVXtn=zx8t4vau6D5T-qFnA3Vm2AGcFUm2S_PVw@mail.gmail.com>	<550805ED.30003@oracle.com>	<CALtZ-o7TCBcFhv3BpjCWQsGoYh+rFbB83sutZa8DXCtB2V16HA@mail.gmail.com>	<1501970.3n0TOdWmYW@d-allen>	<CALtZ-o5vvaA0=LrAfaVS2mYqKOfctBhGGQMj4oMckpq-_cg0zQ@mail.gmail.com>	<550B0245.50606@oracle.com>
	<CALtZ-o4K8i2QrS3OzApai=CCv9oFpbtZDZgN0V8M4cZRy5ocTg@mail.gmail.com>
	<550B1222.3060704@oracle.com>
Message-ID: <550B1629.50203@oracle.com>

Looking from another angle, you have claimed it is "happens-before 
consistent" without showing that you have found */all/* happens-before 
edges.

I have explained why there definitely is another happens-before edge. I 
can provide another constructive proof, if it will look easier:

Suppose, Rwt6 observed T6 and not T9. Suppose, Wv1 appears after Rv0 in 
synchronization order, so Rv0 sees the write of Wv0, and does not see 
Wv1. Then Rwt6 appears after Rrt9 and T9 in synchronization order, and 
Rwt6 must observe T9, not T6 - contradiction. Thus by contradiction the 
two assumptions cannot exist together. Placing Rwt6 after T9 or Wv1 
before Rv0 exposes existence of one more happens-before edge, which is 
not on the diagram. Therefore, if we assume Rwt6 sees T6 and not T9, 
then Rv0 sees Wv1.


Alex

On 19/03/2015 18:14, Oleksandr Otenko wrote:
> No, you don't need Rv1'.
>
> No, happens-before consistency is not the only rule for correctness of 
> the outcome. The "read must see the last write it synchronizes-with" 
> is also part of the rule set. You must either show the proof that Wv1 
> appears after Rv0, or accept Wv1 appears before Rv0 in synchronization 
> order.
>
> Alex
>
> On 19/03/2015 18:02, Marko Topolnik wrote:
>>
>>
>> On Thu, Mar 19, 2015 at 6:07 PM, Oleksandr Otenko 
>> <oleksandr.otenko at oracle.com <mailto:oleksandr.otenko at oracle.com>> wrote:
>>
>>     Your scenario is exactly IRIW.
>>
>>     T9 is x=1
>>     Wv1 is y=1
>>     Rrt9 is r0=x
>>     Rv0 is r1=y
>>
>>     T6 is initial x=0
>>
>>     Rwt6 is a constraint that you want to consider only total orders
>>     with y=1 preceding x=1
>>     It is equivalent to the independent reader thread doing r2=y,
>>     then r3=x, and observing r2==1, r3==0. That is, you have
>>     reformulated IRIW in such a way so that we consider only one
>>     specific outcome, not all possible outcomes.
>>
>>
>> No, for a correct analogy to IRIW you need to introduce an Rv1' 
>> between Wv1 and Rwt6.
>>
>> Now,
>>
>> T9 is x=1
>> Wv1 is y=1
>> Rrt9 is r0=x
>> Rv0 is r1=y
>> Rv1' is r2=y
>> Rwt6 is r3=x
>>
>> Now we can clearly see that the JMM does not allow (r0,r1,r2,r3) = 
>> (1,0,1,0) even though this is a happens before-consistent result. In 
>> other words, JMM gives this the exact same treatment as in IRIW. 
>> However, on a low level there is a key difference in that it involves 
>> the store buffers in the timing, and does it asymetrically, just for 
>> one of the two reading threads. This variation is potentially more 
>> interesting because it is specifically excluded from the guarantees 
>> of the Intel specification. Therefore it is harder to meet by the JIT 
>> compiler.
>>
>>     T1:
>>     x=1
>>
>>     T2:
>>     y=1
>>
>>     T3:
>>     r0=x;
>>     r1=y;
>>
>>     T4:
>>     r2=y;
>>     r3=x;
>>
>>     JMM forbids the outcome (r0,r1,r2,r3) = (1,0,1,0) - ie one thread
>>     thinking x=1 happened before y=1, the other thread thinking y=1
>>     happened before x=1.
>>
>>
>> ---
>> Marko
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150319/ea8fe9d4/attachment-0001.html>

From marko at hazelcast.com  Thu Mar 19 15:34:06 2015
From: marko at hazelcast.com (Marko Topolnik)
Date: Thu, 19 Mar 2015 20:34:06 +0100
Subject: [concurrency-interest] Enforcing total sync order on modern
	hardware
In-Reply-To: <550B1629.50203@oracle.com>
References: <CALtZ-o47wWjeVXtn=zx8t4vau6D5T-qFnA3Vm2AGcFUm2S_PVw@mail.gmail.com>
	<550805ED.30003@oracle.com>
	<CALtZ-o7TCBcFhv3BpjCWQsGoYh+rFbB83sutZa8DXCtB2V16HA@mail.gmail.com>
	<1501970.3n0TOdWmYW@d-allen>
	<CALtZ-o5vvaA0=LrAfaVS2mYqKOfctBhGGQMj4oMckpq-_cg0zQ@mail.gmail.com>
	<550B0245.50606@oracle.com>
	<CALtZ-o4K8i2QrS3OzApai=CCv9oFpbtZDZgN0V8M4cZRy5ocTg@mail.gmail.com>
	<550B1222.3060704@oracle.com> <550B1629.50203@oracle.com>
Message-ID: <CALtZ-o5ike96VAMeCUM1coqZtyB6OH20qpA-0Dn+qzAJPDgy8A@mail.gmail.com>

Your fallacy is in assuming total sync order as a part of the
happens-before consistency. It is not. JLS 17.4.5:

"A set of actions A is happens-before consistent if for all reads r in A,
where W(r) is the write action seen by r, it is not the case that either
hb(r, W(r)) or that there exists a write w in A such that w.v = r.v and
hb(W(r), w) and hb(w, r)."

"In a happens-before consistent set of actions, each read sees a write that
it is allowed to see by the happens-before ordering."

That's all there is to it, and my diagram clearly satisfies this
definition. Now, you may argue that the happens-before order is itself
constrained by the total sync order, but before you do that I urge you to
consider lazySet(). This method's semantics are exactly that of a volatile
write which is exempted from the total sync order. In its presence the
incidental correspondence between sync order and happens-before order
breaks.

---
Marko

On Thu, Mar 19, 2015 at 7:32 PM, Oleksandr Otenko <
oleksandr.otenko at oracle.com> wrote:
>
> Looking from another angle, you have claimed it is "happens-before
consistent" without showing that you have found all happens-before edges.
>
> I have explained why there definitely is another happens-before edge. I
can provide another constructive proof, if it will look easier:
>
> Suppose, Rwt6 observed T6 and not T9. Suppose, Wv1 appears after Rv0 in
synchronization order, so Rv0 sees the write of Wv0, and does not see Wv1.
Then Rwt6 appears after Rrt9 and T9 in synchronization order, and Rwt6 must
observe T9, not T6 - contradiction. Thus by contradiction the two
assumptions cannot exist together. Placing Rwt6 after T9 or Wv1 before Rv0
exposes existence of one more happens-before edge, which is not on the
diagram. Therefore, if we assume Rwt6 sees T6 and not T9, then Rv0 sees Wv1.
>
>
> Alex
>
>
> On 19/03/2015 18:14, Oleksandr Otenko wrote:
>
> No, you don't need Rv1'.
>
> No, happens-before consistency is not the only rule for correctness of
the outcome. The "read must see the last write it synchronizes-with" is
also part of the rule set. You must either show the proof that Wv1 appears
after Rv0, or accept Wv1 appears before Rv0 in synchronization order.
>
> Alex
>
> On 19/03/2015 18:02, Marko Topolnik wrote:
>
>
>
> On Thu, Mar 19, 2015 at 6:07 PM, Oleksandr Otenko <
oleksandr.otenko at oracle.com> wrote:
>>
>> Your scenario is exactly IRIW.
>>
>> T9 is x=1
>> Wv1 is y=1
>> Rrt9 is r0=x
>> Rv0 is r1=y
>>
>> T6 is initial x=0
>>
>>
>>
>> Rwt6 is a constraint that you want to consider only total orders with
y=1 preceding x=1
>> It is equivalent to the independent reader thread doing r2=y, then r3=x,
and observing r2==1, r3==0. That is, you have reformulated IRIW in such a
way so that we consider only one specific outcome, not all possible
outcomes.
>
>
> No, for a correct analogy to IRIW you need to introduce an Rv1' between
Wv1 and Rwt6.
>
> Now,
>
> T9 is x=1
> Wv1 is y=1
> Rrt9 is r0=x
> Rv0 is r1=y
> Rv1' is r2=y
> Rwt6 is r3=x
>
> Now we can clearly see that the JMM does not allow (r0,r1,r2,r3) =
(1,0,1,0) even though this is a happens before-consistent result. In other
words, JMM gives this the exact same treatment as in IRIW. However, on a
low level there is a key difference in that it involves the store buffers
in the timing, and does it asymetrically, just for one of the two reading
threads. This variation is potentially more interesting because it is
specifically excluded from the guarantees of the Intel specification.
Therefore it is harder to meet by the JIT compiler.
>
>>
>> T1:
>> x=1
>>
>> T2:
>> y=1
>>
>> T3:
>> r0=x;
>> r1=y;
>>
>> T4:
>> r2=y;
>> r3=x;
>>
>> JMM forbids the outcome (r0,r1,r2,r3) = (1,0,1,0) - ie one thread
thinking x=1 happened before y=1, the other thread thinking y=1 happened
before x=1.
>
>
> ---
> Marko
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150319/055d8e16/attachment.html>

From dmitry.zaslavsky at gmail.com  Thu Mar 19 16:02:48 2015
From: dmitry.zaslavsky at gmail.com (Dmitry Zaslavsky)
Date: Thu, 19 Mar 2015 16:02:48 -0400
Subject: [concurrency-interest] ReentrantLock bug?
Message-ID: <18DB6EF6-3568-462B-B064-A8415948A31C@gmail.com>

Apologies for such a silly report / question.

    I have an application that out 100s of machines, every couple of runs would get into a state that?s only possible if ReentrantLock has a bug.
    While it would be hard to repro locally. On the grid it?s quite easy to repeat.
    Of course it?s possible I am doing something something very wrong.
    One time I saw RL in a locked state but the owner was null.
    I replaced all the uses of _lock.lock() try {} finally { _lock.unlock(); } with synchronized(_lock) and the problem went away.

   Still using jdk 7 u21
   Any suggestions?



From aleksey.shipilev at oracle.com  Thu Mar 19 16:32:30 2015
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Thu, 19 Mar 2015 23:32:30 +0300
Subject: [concurrency-interest] ReentrantLock bug?
In-Reply-To: <18DB6EF6-3568-462B-B064-A8415948A31C@gmail.com>
References: <18DB6EF6-3568-462B-B064-A8415948A31C@gmail.com>
Message-ID: <550B325E.1010207@oracle.com>

On 03/19/2015 11:02 PM, Dmitry Zaslavsky wrote:
> Apologies for such a silly report / question.
> 
>     I have an application that out 100s of machines, every couple of runs would get into a state that?s only possible if ReentrantLock has a bug.
>     While it would be hard to repro locally. On the grid it?s quite easy to repeat.
>     Of course it?s possible I am doing something something very wrong.
>     One time I saw RL in a locked state but the owner was null.
>     I replaced all the uses of _lock.lock() try {} finally { _lock.unlock(); } with synchronized(_lock) and the problem went away.
> 
>    Still using jdk 7 u21
>    Any suggestions?

The usual things:

 a) Update the JDK to at least 7u40 (or even better, JDK 8u40);
 b) -Xbootclasspath/p: the latest jsr166.jar (only works with JDK 8, I
think);

We have experimental RL tests in jcstress [1], you may want to try those
on your target machines.

-Aleksey.

[1]
http://hg.openjdk.java.net/code-tools/jcstress/file/5fcd4f948639/tests-custom/src/main/java/org/openjdk/jcstress/tests/locks


-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 819 bytes
Desc: OpenPGP digital signature
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150319/577cd713/attachment.bin>

From sitnikov.vladimir at gmail.com  Thu Mar 19 16:50:02 2015
From: sitnikov.vladimir at gmail.com (Vladimir Sitnikov)
Date: Thu, 19 Mar 2015 23:50:02 +0300
Subject: [concurrency-interest] ReentrantLock bug?
In-Reply-To: <550B325E.1010207@oracle.com>
References: <18DB6EF6-3568-462B-B064-A8415948A31C@gmail.com>
	<550B325E.1010207@oracle.com>
Message-ID: <CAB=Je-Hh=edL9a1W=NZ2c+=Y13ApRBUw2SeBE9XY7_VcP1OAxg@mail.gmail.com>

I typically observe 'invalid' state of ReentrantLock due to
StackOverflowError/OutOfMemoryError.

Can you try your application with extended stack size and/or check for
SOE/OOM?

Regards,
Vladimir Sitnikov
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150319/d257266d/attachment.html>

From dmitry.zaslavsky at gmail.com  Thu Mar 19 16:52:58 2015
From: dmitry.zaslavsky at gmail.com (Dmitry Zaslavsky)
Date: Thu, 19 Mar 2015 16:52:58 -0400
Subject: [concurrency-interest] ReentrantLock bug?
In-Reply-To: <CAB=Je-Hh=edL9a1W=NZ2c+=Y13ApRBUw2SeBE9XY7_VcP1OAxg@mail.gmail.com>
References: <18DB6EF6-3568-462B-B064-A8415948A31C@gmail.com>
	<550B325E.1010207@oracle.com>
	<CAB=Je-Hh=edL9a1W=NZ2c+=Y13ApRBUw2SeBE9XY7_VcP1OAxg@mail.gmail.com>
Message-ID: <613E1B57-4408-4C7C-BC57-DAE6B18EB9B9@gmail.com>

I doubt it's one of those conditions.
I would expect the process to die but it's perfectly fine otherwise. Due to this issue it looks like a deadlocked process

Sent from mobile device

> On Mar 19, 2015, at 4:50 PM, Vladimir Sitnikov <sitnikov.vladimir at gmail.com> wrote:
> 
> I typically observe 'invalid' state of ReentrantLock due to StackOverflowError/OutOfMemoryError.
> 
> Can you try your application with extended stack size and/or check for SOE/OOM?
> 
> Regards,
> Vladimir Sitnikov
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150319/b656e382/attachment.html>

From dmitry.zaslavsky at gmail.com  Thu Mar 19 16:54:15 2015
From: dmitry.zaslavsky at gmail.com (Dmitry Zaslavsky)
Date: Thu, 19 Mar 2015 16:54:15 -0400
Subject: [concurrency-interest] ReentrantLock bug?
In-Reply-To: <550B325E.1010207@oracle.com>
References: <18DB6EF6-3568-462B-B064-A8415948A31C@gmail.com>
	<550B325E.1010207@oracle.com>
Message-ID: <A9D7D14E-E298-4453-8DEC-CA3FEFE9CD4A@gmail.com>

I'll try asking for later jdk but it's not under my control :(

Would you happen to know of any relevant specific changes

Thanks

Sent from mobile device

> On Mar 19, 2015, at 4:32 PM, Aleksey Shipilev <aleksey.shipilev at oracle.com> wrote:
> 
>> On 03/19/2015 11:02 PM, Dmitry Zaslavsky wrote:
>> Apologies for such a silly report / question.
>> 
>>    I have an application that out 100s of machines, every couple of runs would get into a state that?s only possible if ReentrantLock has a bug.
>>    While it would be hard to repro locally. On the grid it?s quite easy to repeat.
>>    Of course it?s possible I am doing something something very wrong.
>>    One time I saw RL in a locked state but the owner was null.
>>    I replaced all the uses of _lock.lock() try {} finally { _lock.unlock(); } with synchronized(_lock) and the problem went away.
>> 
>>   Still using jdk 7 u21
>>   Any suggestions?
> 
> The usual things:
> 
> a) Update the JDK to at least 7u40 (or even better, JDK 8u40);
> b) -Xbootclasspath/p: the latest jsr166.jar (only works with JDK 8, I
> think);
> 
> We have experimental RL tests in jcstress [1], you may want to try those
> on your target machines.
> 
> -Aleksey.
> 
> [1]
> http://hg.openjdk.java.net/code-tools/jcstress/file/5fcd4f948639/tests-custom/src/main/java/org/openjdk/jcstress/tests/locks
> 
> 


From oleksandr.otenko at oracle.com  Thu Mar 19 16:58:52 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Thu, 19 Mar 2015 20:58:52 +0000
Subject: [concurrency-interest] Enforcing total sync order on modern
	hardware
In-Reply-To: <CALtZ-o5ike96VAMeCUM1coqZtyB6OH20qpA-0Dn+qzAJPDgy8A@mail.gmail.com>
References: <CALtZ-o47wWjeVXtn=zx8t4vau6D5T-qFnA3Vm2AGcFUm2S_PVw@mail.gmail.com>	<550805ED.30003@oracle.com>	<CALtZ-o7TCBcFhv3BpjCWQsGoYh+rFbB83sutZa8DXCtB2V16HA@mail.gmail.com>	<1501970.3n0TOdWmYW@d-allen>	<CALtZ-o5vvaA0=LrAfaVS2mYqKOfctBhGGQMj4oMckpq-_cg0zQ@mail.gmail.com>	<550B0245.50606@oracle.com>	<CALtZ-o4K8i2QrS3OzApai=CCv9oFpbtZDZgN0V8M4cZRy5ocTg@mail.gmail.com>	<550B1222.3060704@oracle.com>	<550B1629.50203@oracle.com>
	<CALtZ-o5ike96VAMeCUM1coqZtyB6OH20qpA-0Dn+qzAJPDgy8A@mail.gmail.com>
Message-ID: <550B388C.2070808@oracle.com>

No.

17.4.5. In plain words, happens-before order is a combination of program 
order and synchronizes-with. Then 17.4.4. In plain words, 
synchronizes-with comes from synchronization order: a volatile write 
synchronizes-with all volatile reads (of the same variable) appearing 
subsequently in synchronization order.

So you can't construct a happens-before by ignoring synchronization order.

Alex

On 19/03/2015 19:34, Marko Topolnik wrote:
> Your fallacy is in assuming total sync order as a part of the 
> happens-before consistency. It is not. JLS 17.4.5:
>
> "A set of actions A is happens-before consistent if for all reads r in 
> A, where W(r) is the write action seen by r, it is not the case that 
> either hb(r, W(r)) or that there exists a write w in A such that w.v = 
> r.v and hb(W(r), w) and hb(w, r)."
>
> "In a happens-before consistent set of actions, each read sees a write 
> that it is allowed to see by the happens-before ordering."
>
> That's all there is to it, and my diagram clearly satisfies this 
> definition. Now, you may argue that the happens-before order is itself 
> constrained by the total sync order, but before you do that I urge you 
> to consider lazySet(). This method's semantics are exactly that of a 
> volatile write which is exempted from the total sync order. In its 
> presence the incidental correspondence between sync order and 
> happens-before order breaks.
>
> ---
> Marko
>
> On Thu, Mar 19, 2015 at 7:32 PM, Oleksandr Otenko 
> <oleksandr.otenko at oracle.com <mailto:oleksandr.otenko at oracle.com>> wrote:
> >
> > Looking from another angle, you have claimed it is "happens-before 
> consistent" without showing that you have found all happens-before edges.
> >
> > I have explained why there definitely is another happens-before 
> edge. I can provide another constructive proof, if it will look easier:
> >
> > Suppose, Rwt6 observed T6 and not T9. Suppose, Wv1 appears after Rv0 
> in synchronization order, so Rv0 sees the write of Wv0, and does not 
> see Wv1. Then Rwt6 appears after Rrt9 and T9 in synchronization order, 
> and Rwt6 must observe T9, not T6 - contradiction. Thus by 
> contradiction the two assumptions cannot exist together. Placing Rwt6 
> after T9 or Wv1 before Rv0 exposes existence of one more 
> happens-before edge, which is not on the diagram. Therefore, if we 
> assume Rwt6 sees T6 and not T9, then Rv0 sees Wv1.
> >
> >
> > Alex
> >
> >
> > On 19/03/2015 18:14, Oleksandr Otenko wrote:
> >
> > No, you don't need Rv1'.
> >
> > No, happens-before consistency is not the only rule for correctness 
> of the outcome. The "read must see the last write it 
> synchronizes-with" is also part of the rule set. You must either show 
> the proof that Wv1 appears after Rv0, or accept Wv1 appears before Rv0 
> in synchronization order.
> >
> > Alex
> >
> > On 19/03/2015 18:02, Marko Topolnik wrote:
> >
> >
> >
> > On Thu, Mar 19, 2015 at 6:07 PM, Oleksandr Otenko 
> <oleksandr.otenko at oracle.com <mailto:oleksandr.otenko at oracle.com>> wrote:
> >>
> >> Your scenario is exactly IRIW.
> >>
> >> T9 is x=1
> >> Wv1 is y=1
> >> Rrt9 is r0=x
> >> Rv0 is r1=y
> >>
> >> T6 is initial x=0
> >>
> >>
> >>
> >> Rwt6 is a constraint that you want to consider only total orders 
> with y=1 preceding x=1
> >> It is equivalent to the independent reader thread doing r2=y, then 
> r3=x, and observing r2==1, r3==0. That is, you have reformulated IRIW 
> in such a way so that we consider only one specific outcome, not all 
> possible outcomes.
> >
> >
> > No, for a correct analogy to IRIW you need to introduce an Rv1' 
> between Wv1 and Rwt6.
> >
> > Now,
> >
> > T9 is x=1
> > Wv1 is y=1
> > Rrt9 is r0=x
> > Rv0 is r1=y
> > Rv1' is r2=y
> > Rwt6 is r3=x
> >
> > Now we can clearly see that the JMM does not allow (r0,r1,r2,r3) = 
> (1,0,1,0) even though this is a happens before-consistent result. In 
> other words, JMM gives this the exact same treatment as in IRIW. 
> However, on a low level there is a key difference in that it involves 
> the store buffers in the timing, and does it asymetrically, just for 
> one of the two reading threads. This variation is potentially more 
> interesting because it is specifically excluded from the guarantees of 
> the Intel specification. Therefore it is harder to meet by the JIT 
> compiler.
> >
> >>
> >> T1:
> >> x=1
> >>
> >> T2:
> >> y=1
> >>
> >> T3:
> >> r0=x;
> >> r1=y;
> >>
> >> T4:
> >> r2=y;
> >> r3=x;
> >>
> >> JMM forbids the outcome (r0,r1,r2,r3) = (1,0,1,0) - ie one thread 
> thinking x=1 happened before y=1, the other thread thinking y=1 
> happened before x=1.
> >
> >
> > ---
> > Marko
> >
> >
> >

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150319/09a19616/attachment.html>

From heinz at javaspecialists.eu  Thu Mar 19 17:17:23 2015
From: heinz at javaspecialists.eu (Dr Heinz M. Kabutz)
Date: Thu, 19 Mar 2015 23:17:23 +0200
Subject: [concurrency-interest] ReentrantLock bug?
In-Reply-To: <18DB6EF6-3568-462B-B064-A8415948A31C@gmail.com>
References: <18DB6EF6-3568-462B-B064-A8415948A31C@gmail.com>
Message-ID: <550B3CE3.1070807@javaspecialists.eu>

Hi Dmitry,

is it possible to send me a sanitized stack trace of what state the 
thread would be in if RL caused a hang?  Someone reported a very strange 
behaviour a long time ago on Linux with JDK 7, where you could sometimes 
get a thread that was waiting for a lock, even though it was Unlocked.  
The lock they used was a direct copy of ReentrantLock, with the only 
difference being that it had been not been loaded in the system class 
loader.  I managed to reproduce it, although not sure of the exact 
version of Java 7.  After several day of chasing it, I ran out of time 
and left it, but I've always had the niggling suspicion that it might be 
a bug that lives fine and well in the ReentrantLock.

This was for Java 1.7.0_40, -server.  I thus do not share Aleksey's 
optimism that moving over to that version is going to make the problem 
go away.  I could not reproduce it on 1.8, but that might have just been 
a coincidence.  It was quite difficult to reproduce.

BTW, not a silly report / question at all.  Very important indeed, 
especially considering how many classes in JDK use ReentrantLock 
internally :-(((

Regards

Heinz
-- 
Dr Heinz M. Kabutz (PhD CompSci)
Author of "The Java(tm) Specialists' Newsletter"
Sun/Oracle Java Champion since 2005
JavaOne Rock Star Speaker 2012
http://www.javaspecialists.eu
Tel: +30 69 75 595 262
Skype: kabutz



Dmitry Zaslavsky wrote:
> Apologies for such a silly report / question.
>
>     I have an application that out 100s of machines, every couple of runs would get into a state that?s only possible if ReentrantLock has a bug.
>     While it would be hard to repro locally. On the grid it?s quite easy to repeat.
>     Of course it?s possible I am doing something something very wrong.
>     One time I saw RL in a locked state but the owner was null.
>     I replaced all the uses of _lock.lock() try {} finally { _lock.unlock(); } with synchronized(_lock) and the problem went away.
>
>    Still using jdk 7 u21
>    Any suggestions?
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>   

From heinz at javaspecialists.eu  Thu Mar 19 17:34:00 2015
From: heinz at javaspecialists.eu (Dr Heinz M. Kabutz)
Date: Thu, 19 Mar 2015 23:34:00 +0200
Subject: [concurrency-interest] ReentrantLock bug?
In-Reply-To: <550B3CE3.1070807@javaspecialists.eu>
References: <18DB6EF6-3568-462B-B064-A8415948A31C@gmail.com>
	<550B3CE3.1070807@javaspecialists.eu>
Message-ID: <550B40C8.20902@javaspecialists.eu>

The initial discovery was shown here: https://github.com/rafaelbrandao/msc

Regards

Heinz
-- 
Dr Heinz M. Kabutz (PhD CompSci)
Author of "The Java(tm) Specialists' Newsletter"
Sun/Oracle Java Champion since 2005
JavaOne Rock Star Speaker 2012
http://www.javaspecialists.eu
Tel: +30 69 75 595 262
Skype: kabutz



Dr Heinz M. Kabutz wrote:
> Hi Dmitry,
>
> is it possible to send me a sanitized stack trace of what state the 
> thread would be in if RL caused a hang?  Someone reported a very 
> strange behaviour a long time ago on Linux with JDK 7, where you could 
> sometimes get a thread that was waiting for a lock, even though it was 
> Unlocked.  The lock they used was a direct copy of ReentrantLock, with 
> the only difference being that it had been not been loaded in the 
> system class loader.  I managed to reproduce it, although not sure of 
> the exact version of Java 7.  After several day of chasing it, I ran 
> out of time and left it, but I've always had the niggling suspicion 
> that it might be a bug that lives fine and well in the ReentrantLock.
>
> This was for Java 1.7.0_40, -server.  I thus do not share Aleksey's 
> optimism that moving over to that version is going to make the problem 
> go away.  I could not reproduce it on 1.8, but that might have just 
> been a coincidence.  It was quite difficult to reproduce.
>
> BTW, not a silly report / question at all.  Very important indeed, 
> especially considering how many classes in JDK use ReentrantLock 
> internally :-(((
>
> Regards
>
> Heinz

From vitalyd at gmail.com  Thu Mar 19 18:03:24 2015
From: vitalyd at gmail.com (Vitaly Davidovich)
Date: Thu, 19 Mar 2015 18:03:24 -0400
Subject: [concurrency-interest] ReentrantLock bug?
In-Reply-To: <550B40C8.20902@javaspecialists.eu>
References: <18DB6EF6-3568-462B-B064-A8415948A31C@gmail.com>
	<550B3CE3.1070807@javaspecialists.eu>
	<550B40C8.20902@javaspecialists.eu>
Message-ID: <CAHjP37E7uEGRzMLNWLbr7J88iGHV1MUFR4oN=gsTSgzCUxCvPg@mail.gmail.com>

Hmm, the symptoms seem very similar to the set of bugs (should've been
fixed by 7u21 time though, AFAIR) that were filed due to missing barriers
in the native Parker class.  In those cases, -XX:+UseMembar masked the
problem and things worked -- has that been tried with the copy of RL?

Also, what exactly are the differences between the JDK RL and the "nearly
unmodified" one?

On Thu, Mar 19, 2015 at 5:34 PM, Dr Heinz M. Kabutz <
heinz at javaspecialists.eu> wrote:

> The initial discovery was shown here: https://github.com/rafaelbrandao/msc
>
> Regards
>
> Heinz
> --
> Dr Heinz M. Kabutz (PhD CompSci)
> Author of "The Java(tm) Specialists' Newsletter"
> Sun/Oracle Java Champion since 2005
> JavaOne Rock Star Speaker 2012
> http://www.javaspecialists.eu
> Tel: +30 69 75 595 262
> Skype: kabutz
>
>
>
> Dr Heinz M. Kabutz wrote:
>
>> Hi Dmitry,
>>
>> is it possible to send me a sanitized stack trace of what state the
>> thread would be in if RL caused a hang?  Someone reported a very strange
>> behaviour a long time ago on Linux with JDK 7, where you could sometimes
>> get a thread that was waiting for a lock, even though it was Unlocked.  The
>> lock they used was a direct copy of ReentrantLock, with the only difference
>> being that it had been not been loaded in the system class loader.  I
>> managed to reproduce it, although not sure of the exact version of Java 7.
>> After several day of chasing it, I ran out of time and left it, but I've
>> always had the niggling suspicion that it might be a bug that lives fine
>> and well in the ReentrantLock.
>>
>> This was for Java 1.7.0_40, -server.  I thus do not share Aleksey's
>> optimism that moving over to that version is going to make the problem go
>> away.  I could not reproduce it on 1.8, but that might have just been a
>> coincidence.  It was quite difficult to reproduce.
>>
>> BTW, not a silly report / question at all.  Very important indeed,
>> especially considering how many classes in JDK use ReentrantLock internally
>> :-(((
>>
>> Regards
>>
>> Heinz
>>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150319/f0cd9b26/attachment-0001.html>

From heinz at javaspecialists.eu  Thu Mar 19 18:15:21 2015
From: heinz at javaspecialists.eu (Dr Heinz M. Kabutz)
Date: Fri, 20 Mar 2015 00:15:21 +0200
Subject: [concurrency-interest] ReentrantLock bug?
In-Reply-To: <CAHjP37E7uEGRzMLNWLbr7J88iGHV1MUFR4oN=gsTSgzCUxCvPg@mail.gmail.com>
References: <18DB6EF6-3568-462B-B064-A8415948A31C@gmail.com>
	<550B3CE3.1070807@javaspecialists.eu>
	<550B40C8.20902@javaspecialists.eu>
	<CAHjP37E7uEGRzMLNWLbr7J88iGHV1MUFR4oN=gsTSgzCUxCvPg@mail.gmail.com>
Message-ID: <CACLL95otNsoyESi1G6eZfY4fCXM+_yk6s7=G86fM8NoGP4FtPg@mail.gmail.com>

I boiled it down to whether AQS was loaded in the boot class loader or
not.  Go figure. But as I said, I ran out of time to look at it more.

Heinz

On Friday, 20 March 2015, Vitaly Davidovich <vitalyd at gmail.com> wrote:

> Hmm, the symptoms seem very similar to the set of bugs (should've been
> fixed by 7u21 time though, AFAIR) that were filed due to missing barriers
> in the native Parker class.  In those cases, -XX:+UseMembar masked the
> problem and things worked -- has that been tried with the copy of RL?
>
> Also, what exactly are the differences between the JDK RL and the "nearly
> unmodified" one?
>
> On Thu, Mar 19, 2015 at 5:34 PM, Dr Heinz M. Kabutz <
> heinz at javaspecialists.eu
> <javascript:_e(%7B%7D,'cvml','heinz at javaspecialists.eu');>> wrote:
>
>> The initial discovery was shown here: https://github.com/
>> rafaelbrandao/msc
>>
>> Regards
>>
>> Heinz
>> --
>> Dr Heinz M. Kabutz (PhD CompSci)
>> Author of "The Java(tm) Specialists' Newsletter"
>> Sun/Oracle Java Champion since 2005
>> JavaOne Rock Star Speaker 2012
>> http://www.javaspecialists.eu
>> Tel: +30 69 75 595 262
>> Skype: kabutz
>>
>>
>>
>> Dr Heinz M. Kabutz wrote:
>>
>>> Hi Dmitry,
>>>
>>> is it possible to send me a sanitized stack trace of what state the
>>> thread would be in if RL caused a hang?  Someone reported a very strange
>>> behaviour a long time ago on Linux with JDK 7, where you could sometimes
>>> get a thread that was waiting for a lock, even though it was Unlocked.  The
>>> lock they used was a direct copy of ReentrantLock, with the only difference
>>> being that it had been not been loaded in the system class loader.  I
>>> managed to reproduce it, although not sure of the exact version of Java 7.
>>> After several day of chasing it, I ran out of time and left it, but I've
>>> always had the niggling suspicion that it might be a bug that lives fine
>>> and well in the ReentrantLock.
>>>
>>> This was for Java 1.7.0_40, -server.  I thus do not share Aleksey's
>>> optimism that moving over to that version is going to make the problem go
>>> away.  I could not reproduce it on 1.8, but that might have just been a
>>> coincidence.  It was quite difficult to reproduce.
>>>
>>> BTW, not a silly report / question at all.  Very important indeed,
>>> especially considering how many classes in JDK use ReentrantLock internally
>>> :-(((
>>>
>>> Regards
>>>
>>> Heinz
>>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> <javascript:_e(%7B%7D,'cvml','Concurrency-interest at cs.oswego.edu');>
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>
>

-- 
Dr Heinz M. Kabutz (PhD CompSci)
Author of "The Java(tm) Specialists' Newsletter"
Sun/Oracle Java Champion
JavaOne Rockstar Speaker
http://www.javaspecialists.eu
Tel: +30 69 75 595 262
Skype: kabutz
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150320/7f76e327/attachment.html>

From davidcholmes at aapt.net.au  Thu Mar 19 20:56:17 2015
From: davidcholmes at aapt.net.au (David Holmes)
Date: Fri, 20 Mar 2015 10:56:17 +1000
Subject: [concurrency-interest] ReentrantLock bug?
In-Reply-To: <613E1B57-4408-4C7C-BC57-DAE6B18EB9B9@gmail.com>
Message-ID: <NFBBKALFDCPFIDBNKAPCEEIHKOAA.davidcholmes@aapt.net.au>

StackOverflow is know to cause the ReentrantLock to be left in an invalid
state whereby it appears locked but is not. This can lead to hangs during
classloading due to the use of ConcurrentHashMap for maintaining the
classloading locks. The CHM implementation was changed (CHMv8) to use
synchronized instead of ReentrantLock, and so this classloader issue does
not affect JDK 8 or later. But of course ReentrantLock itself is still
affected by this problem.

There were some other fixes in the low-level park/unpark code that went into
7u40 (8004902) but they were found by code inspection not by anyone
encountering the actual bug - as far as we know.

David
  -----Original Message-----
  From: concurrency-interest-bounces at cs.oswego.edu
[mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Dmitry
Zaslavsky
  Sent: Friday, 20 March 2015 6:53 AM
  To: Vladimir Sitnikov
  Cc: concurrency-interest
  Subject: Re: [concurrency-interest] ReentrantLock bug?


  I doubt it's one of those conditions.
  I would expect the process to die but it's perfectly fine otherwise. Due
to this issue it looks like a deadlocked process

  Sent from mobile device

  On Mar 19, 2015, at 4:50 PM, Vladimir Sitnikov
<sitnikov.vladimir at gmail.com> wrote:


    I typically observe 'invalid' state of ReentrantLock due to
StackOverflowError/OutOfMemoryError.

    Can you try your application with extended stack size and/or check for
SOE/OOM?

    Regards,
    Vladimir Sitnikov
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150320/fc7fe657/attachment.html>

From davidcholmes at aapt.net.au  Thu Mar 19 21:03:23 2015
From: davidcholmes at aapt.net.au (David Holmes)
Date: Fri, 20 Mar 2015 11:03:23 +1000
Subject: [concurrency-interest] ReentrantLock bug?
In-Reply-To: <550B3CE3.1070807@javaspecialists.eu>
Message-ID: <NFBBKALFDCPFIDBNKAPCKEIHKOAA.davidcholmes@aapt.net.au>

Hi Heinz,

By strange coincidence Martin Buchholz just uncovered a lost unpark problem involving the system classloader. See the later comments in:

https://bugs.openjdk.java.net/browse/JDK-8074773

David

> -----Original Message-----
> From: concurrency-interest-bounces at cs.oswego.edu
> [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Dr Heinz
> M. Kabutz
> Sent: Friday, 20 March 2015 7:17 AM
> To: Dmitry Zaslavsky
> Cc: Concurrency-interest at cs.oswego.edu
> Subject: Re: [concurrency-interest] ReentrantLock bug?
> 
> 
> Hi Dmitry,
> 
> is it possible to send me a sanitized stack trace of what state the 
> thread would be in if RL caused a hang?  Someone reported a very strange 
> behaviour a long time ago on Linux with JDK 7, where you could sometimes 
> get a thread that was waiting for a lock, even though it was Unlocked.  
> The lock they used was a direct copy of ReentrantLock, with the only 
> difference being that it had been not been loaded in the system class 
> loader.  I managed to reproduce it, although not sure of the exact 
> version of Java 7.  After several day of chasing it, I ran out of time 
> and left it, but I've always had the niggling suspicion that it might be 
> a bug that lives fine and well in the ReentrantLock.
> 
> This was for Java 1.7.0_40, -server.  I thus do not share Aleksey's 
> optimism that moving over to that version is going to make the problem 
> go away.  I could not reproduce it on 1.8, but that might have just been 
> a coincidence.  It was quite difficult to reproduce.
> 
> BTW, not a silly report / question at all.  Very important indeed, 
> especially considering how many classes in JDK use ReentrantLock 
> internally :-(((
> 
> Regards
> 
> Heinz
> -- 
> Dr Heinz M. Kabutz (PhD CompSci)
> Author of "The Java(tm) Specialists' Newsletter"
> Sun/Oracle Java Champion since 2005
> JavaOne Rock Star Speaker 2012
> http://www.javaspecialists.eu
> Tel: +30 69 75 595 262
> Skype: kabutz
> 
> 
> 
> Dmitry Zaslavsky wrote:
> > Apologies for such a silly report / question.
> >
> >     I have an application that out 100s of machines, every 
> couple of runs would get into a state that?s only possible if 
> ReentrantLock has a bug.
> >     While it would be hard to repro locally. On the grid it?s 
> quite easy to repeat.
> >     Of course it?s possible I am doing something something very wrong.
> >     One time I saw RL in a locked state but the owner was null.
> >     I replaced all the uses of _lock.lock() try {} finally { 
> _lock.unlock(); } with synchronized(_lock) and the problem went away.
> >
> >    Still using jdk 7 u21
> >    Any suggestions?
> >
> >
> > _______________________________________________
> > Concurrency-interest mailing list
> > Concurrency-interest at cs.oswego.edu
> > http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> >   
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
> 



From dmitry.zaslavsky at gmail.com  Thu Mar 19 22:30:27 2015
From: dmitry.zaslavsky at gmail.com (Dmitry Zaslavsky)
Date: Thu, 19 Mar 2015 22:30:27 -0400
Subject: [concurrency-interest] ReentrantLock bug?
In-Reply-To: <NFBBKALFDCPFIDBNKAPCEEIHKOAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCEEIHKOAA.davidcholmes@aapt.net.au>
Message-ID: <C116283B-6AE9-4240-980F-3807FAAAC4D8@gmail.com>

Thanks for the info.
I neglected to mention, that I actually have a custom thread pool (borrowing heavily on fj and others :))
So threads would call park/unpark a lot more than just RL. I assumed it?s not an issue since can spuriously wake up anyways.
I will to try tomorrow on 7u40
Thanks again

> On Mar 19, 2015, at 8:56 PM, David Holmes <davidcholmes at aapt.net.au> wrote:
> 
> StackOverflow is know to cause the ReentrantLock to be left in an invalid state whereby it appears locked but is not. This can lead to hangs during classloading due to the use of ConcurrentHashMap for maintaining the classloading locks. The CHM implementation was changed (CHMv8) to use synchronized instead of ReentrantLock, and so this classloader issue does not affect JDK 8 or later. But of course ReentrantLock itself is still affected by this problem.
>  
> There were some other fixes in the low-level park/unpark code that went into 7u40 (8004902) but they were found by code inspection not by anyone encountering the actual bug - as far as we know.
>  
> David
>> -----Original Message-----
>> From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Dmitry Zaslavsky
>> Sent: Friday, 20 March 2015 6:53 AM
>> To: Vladimir Sitnikov
>> Cc: concurrency-interest
>> Subject: Re: [concurrency-interest] ReentrantLock bug?
>> 
>> I doubt it's one of those conditions.
>> I would expect the process to die but it's perfectly fine otherwise. Due to this issue it looks like a deadlocked process
>> 
>> Sent from mobile device
>> 
>> On Mar 19, 2015, at 4:50 PM, Vladimir Sitnikov <sitnikov.vladimir at gmail.com <mailto:sitnikov.vladimir at gmail.com>> wrote:
>> 
>>> I typically observe 'invalid' state of ReentrantLock due to StackOverflowError/OutOfMemoryError.
>>> Can you try your application with extended stack size and/or check for SOE/OOM?
>>> Regards,
>>> Vladimir Sitnikov

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150319/7d784926/attachment-0001.html>

From kirk at kodewerk.com  Fri Mar 20 02:04:51 2015
From: kirk at kodewerk.com (Kirk Pepperdine)
Date: Fri, 20 Mar 2015 07:04:51 +0100
Subject: [concurrency-interest] ReentrantLock bug?
In-Reply-To: <613E1B57-4408-4C7C-BC57-DAE6B18EB9B9@gmail.com>
References: <18DB6EF6-3568-462B-B064-A8415948A31C@gmail.com>
	<550B325E.1010207@oracle.com>
	<CAB=Je-Hh=edL9a1W=NZ2c+=Y13ApRBUw2SeBE9XY7_VcP1OAxg@mail.gmail.com>
	<613E1B57-4408-4C7C-BC57-DAE6B18EB9B9@gmail.com>
Message-ID: <9C4A4607-7532-4FB6-B33B-5B9AC9D99929@kodewerk.com>

I see app?s survive OOME because only the offending thread gets shot down. After the request that causes the OOME is shot down, the rest of the app works fine.. for some definition of fine.

Regards,
Kirk

On Mar 19, 2015, at 9:52 PM, Dmitry Zaslavsky <dmitry.zaslavsky at gmail.com> wrote:

> I doubt it's one of those conditions.
> I would expect the process to die but it's perfectly fine otherwise. Due to this issue it looks like a deadlocked process
> 
> Sent from mobile device
> 
> On Mar 19, 2015, at 4:50 PM, Vladimir Sitnikov <sitnikov.vladimir at gmail.com> wrote:
> 
>> I typically observe 'invalid' state of ReentrantLock due to StackOverflowError/OutOfMemoryError.
>> 
>> Can you try your application with extended stack size and/or check for SOE/OOM?
>> 
>> Regards,
>> Vladimir Sitnikov
>> 
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150320/c7364814/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 496 bytes
Desc: Message signed with OpenPGP using GPGMail
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150320/c7364814/attachment.bin>

From marko at hazelcast.com  Fri Mar 20 04:54:40 2015
From: marko at hazelcast.com (Marko Topolnik)
Date: Fri, 20 Mar 2015 09:54:40 +0100
Subject: [concurrency-interest] Enforcing total sync order on modern
	hardware
In-Reply-To: <550B388C.2070808@oracle.com>
References: <CALtZ-o47wWjeVXtn=zx8t4vau6D5T-qFnA3Vm2AGcFUm2S_PVw@mail.gmail.com>
	<550805ED.30003@oracle.com>
	<CALtZ-o7TCBcFhv3BpjCWQsGoYh+rFbB83sutZa8DXCtB2V16HA@mail.gmail.com>
	<1501970.3n0TOdWmYW@d-allen>
	<CALtZ-o5vvaA0=LrAfaVS2mYqKOfctBhGGQMj4oMckpq-_cg0zQ@mail.gmail.com>
	<550B0245.50606@oracle.com>
	<CALtZ-o4K8i2QrS3OzApai=CCv9oFpbtZDZgN0V8M4cZRy5ocTg@mail.gmail.com>
	<550B1222.3060704@oracle.com> <550B1629.50203@oracle.com>
	<CALtZ-o5ike96VAMeCUM1coqZtyB6OH20qpA-0Dn+qzAJPDgy8A@mail.gmail.com>
	<550B388C.2070808@oracle.com>
Message-ID: <CALtZ-o7Qe9uMX1G_8Gm0DfkpyFOqC9680azEm1Sss3kXGdoiJA@mail.gmail.com>

You have correctly identified the regulations I had in mind with the
previous post; but perhaps I should spend some more time explaining its
point. The current JMM has the following property:

An action can participate in a synchronizes-with relation if and only if it
occurs in the total synchronization order.

In the new revision of JMM the above property will no longer hold: there
will be actions which participate in synchronizes-with without occurring in
the sync order (lazySet) and actions which occur in the sync order without
participating in synchronizes-with (weakCompareAndSet). These changes will
not, however, change anything in the definition of the happens-before
consistency. Under such a revised model there will be scenarios, such as
the one under discussion here, which preserve happens-before consistency,
but are not accountable for in terms of a total sync order.

So in the post above I referred to the essence of happens-before
consistency without tying it to the additional constraints which the JMM
currently imposes. Leslie Lamport's original idea did not include a total
sync order; in fact, its main point was the absence of such order. An
analogy can be set up where the current JMM is Newtonian (global time
exists) whereas Lamport's model is relativistic (each frame of reference
has its own time). The revised model will allow for some of the original
relativism.

---
Marko

On Thu, Mar 19, 2015 at 9:58 PM, Oleksandr Otenko <
oleksandr.otenko at oracle.com> wrote:

>  No.
>
> 17.4.5. In plain words, happens-before order is a combination of program
> order and synchronizes-with. Then 17.4.4. In plain words, synchronizes-with
> comes from synchronization order: a volatile write synchronizes-with all
> volatile reads (of the same variable) appearing subsequently in
> synchronization order.
>
> So you can't construct a happens-before by ignoring synchronization order.
>
> Alex
>
>
> On 19/03/2015 19:34, Marko Topolnik wrote:
>
> Your fallacy is in assuming total sync order as a part of the
> happens-before consistency. It is not. JLS 17.4.5:
>
> "A set of actions A is happens-before consistent if for all reads r in A,
> where W(r) is the write action seen by r, it is not the case that either
> hb(r, W(r)) or that there exists a write w in A such that w.v = r.v and
> hb(W(r), w) and hb(w, r)."
>
> "In a happens-before consistent set of actions, each read sees a write
> that it is allowed to see by the happens-before ordering."
>
>  That's all there is to it, and my diagram clearly satisfies this
> definition. Now, you may argue that the happens-before order is itself
> constrained by the total sync order, but before you do that I urge you to
> consider lazySet(). This method's semantics are exactly that of a volatile
> write which is exempted from the total sync order. In its presence the
> incidental correspondence between sync order and happens-before order
> breaks.
>
>  ---
> Marko
>
> On Thu, Mar 19, 2015 at 7:32 PM, Oleksandr Otenko <
> oleksandr.otenko at oracle.com> wrote:
> >
> > Looking from another angle, you have claimed it is "happens-before
> consistent" without showing that you have found all happens-before edges.
> >
> > I have explained why there definitely is another happens-before edge. I
> can provide another constructive proof, if it will look easier:
> >
> > Suppose, Rwt6 observed T6 and not T9. Suppose, Wv1 appears after Rv0 in
> synchronization order, so Rv0 sees the write of Wv0, and does not see Wv1.
> Then Rwt6 appears after Rrt9 and T9 in synchronization order, and Rwt6 must
> observe T9, not T6 - contradiction. Thus by contradiction the two
> assumptions cannot exist together. Placing Rwt6 after T9 or Wv1 before Rv0
> exposes existence of one more happens-before edge, which is not on the
> diagram. Therefore, if we assume Rwt6 sees T6 and not T9, then Rv0 sees Wv1.
> >
> >
> > Alex
> >
> >
> > On 19/03/2015 18:14, Oleksandr Otenko wrote:
> >
> > No, you don't need Rv1'.
> >
> > No, happens-before consistency is not the only rule for correctness of
> the outcome. The "read must see the last write it synchronizes-with" is
> also part of the rule set. You must either show the proof that Wv1 appears
> after Rv0, or accept Wv1 appears before Rv0 in synchronization order.
> >
> > Alex
> >
> > On 19/03/2015 18:02, Marko Topolnik wrote:
> >
> >
> >
> > On Thu, Mar 19, 2015 at 6:07 PM, Oleksandr Otenko <
> oleksandr.otenko at oracle.com> wrote:
> >>
> >> Your scenario is exactly IRIW.
> >>
> >> T9 is x=1
> >> Wv1 is y=1
> >> Rrt9 is r0=x
> >> Rv0 is r1=y
> >>
> >> T6 is initial x=0
> >>
> >>
> >>
> >> Rwt6 is a constraint that you want to consider only total orders with
> y=1 preceding x=1
> >> It is equivalent to the independent reader thread doing r2=y, then
> r3=x, and observing r2==1, r3==0. That is, you have reformulated IRIW in
> such a way so that we consider only one specific outcome, not all possible
> outcomes.
> >
> >
> > No, for a correct analogy to IRIW you need to introduce an Rv1' between
> Wv1 and Rwt6.
> >
> > Now,
> >
> > T9 is x=1
> > Wv1 is y=1
> > Rrt9 is r0=x
> > Rv0 is r1=y
> > Rv1' is r2=y
> > Rwt6 is r3=x
> >
> > Now we can clearly see that the JMM does not allow (r0,r1,r2,r3) =
> (1,0,1,0) even though this is a happens before-consistent result. In other
> words, JMM gives this the exact same treatment as in IRIW. However, on a
> low level there is a key difference in that it involves the store buffers
> in the timing, and does it asymetrically, just for one of the two reading
> threads. This variation is potentially more interesting because it is
> specifically excluded from the guarantees of the Intel specification.
> Therefore it is harder to meet by the JIT compiler.
> >
> >>
> >> T1:
> >> x=1
> >>
> >> T2:
> >> y=1
> >>
> >> T3:
> >> r0=x;
> >> r1=y;
> >>
> >> T4:
> >> r2=y;
> >> r3=x;
> >>
> >> JMM forbids the outcome (r0,r1,r2,r3) = (1,0,1,0) - ie one thread
> thinking x=1 happened before y=1, the other thread thinking y=1 happened
> before x=1.
> >
> >
> > ---
> > Marko
> >
> >
> >
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150320/0f96a728/attachment.html>

From aleksey.shipilev at oracle.com  Fri Mar 20 05:37:50 2015
From: aleksey.shipilev at oracle.com (Aleksey Shipilev)
Date: Fri, 20 Mar 2015 12:37:50 +0300
Subject: [concurrency-interest] Enforcing total sync order on modern
	hardware
In-Reply-To: <CALtZ-o7Qe9uMX1G_8Gm0DfkpyFOqC9680azEm1Sss3kXGdoiJA@mail.gmail.com>
References: <CALtZ-o47wWjeVXtn=zx8t4vau6D5T-qFnA3Vm2AGcFUm2S_PVw@mail.gmail.com>	<550805ED.30003@oracle.com>	<CALtZ-o7TCBcFhv3BpjCWQsGoYh+rFbB83sutZa8DXCtB2V16HA@mail.gmail.com>	<1501970.3n0TOdWmYW@d-allen>	<CALtZ-o5vvaA0=LrAfaVS2mYqKOfctBhGGQMj4oMckpq-_cg0zQ@mail.gmail.com>	<550B0245.50606@oracle.com>	<CALtZ-o4K8i2QrS3OzApai=CCv9oFpbtZDZgN0V8M4cZRy5ocTg@mail.gmail.com>	<550B1222.3060704@oracle.com>
	<550B1629.50203@oracle.com>	<CALtZ-o5ike96VAMeCUM1coqZtyB6OH20qpA-0Dn+qzAJPDgy8A@mail.gmail.com>	<550B388C.2070808@oracle.com>
	<CALtZ-o7Qe9uMX1G_8Gm0DfkpyFOqC9680azEm1Sss3kXGdoiJA@mail.gmail.com>
Message-ID: <550BEA6E.9030102@oracle.com>

On 20.03.2015 11:54, Marko Topolnik wrote:
> In the new revision of JMM the above property will no longer hold: there
> will be actions which participate in synchronizes-with without occurring
> in the sync order (lazySet) and actions which occur in the sync order
> without participating in synchronizes-with (weakCompareAndSet).

Stop right there. There is no "new revision of JMM" that has this
property yet, and we don't really know what would it look like to absorb
lazySet into formal model. We don't even know if the existing formalisms
are modifiable to include lazySet into JMM, or we need something
completely new.

-Aleksey

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 819 bytes
Desc: OpenPGP digital signature
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150320/e7f27938/attachment.bin>

From oleksandr.otenko at oracle.com  Fri Mar 20 07:42:21 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Fri, 20 Mar 2015 11:42:21 +0000
Subject: [concurrency-interest] Enforcing total sync order on modern
	hardware
In-Reply-To: <CALtZ-o7Qe9uMX1G_8Gm0DfkpyFOqC9680azEm1Sss3kXGdoiJA@mail.gmail.com>
References: <CALtZ-o47wWjeVXtn=zx8t4vau6D5T-qFnA3Vm2AGcFUm2S_PVw@mail.gmail.com>	<550805ED.30003@oracle.com>	<CALtZ-o7TCBcFhv3BpjCWQsGoYh+rFbB83sutZa8DXCtB2V16HA@mail.gmail.com>	<1501970.3n0TOdWmYW@d-allen>	<CALtZ-o5vvaA0=LrAfaVS2mYqKOfctBhGGQMj4oMckpq-_cg0zQ@mail.gmail.com>	<550B0245.50606@oracle.com>	<CALtZ-o4K8i2QrS3OzApai=CCv9oFpbtZDZgN0V8M4cZRy5ocTg@mail.gmail.com>	<550B1222.3060704@oracle.com>	<550B1629.50203@oracle.com>	<CALtZ-o5ike96VAMeCUM1coqZtyB6OH20qpA-0Dn+qzAJPDgy8A@mail.gmail.com>	<550B388C.2070808@oracle.com>
	<CALtZ-o7Qe9uMX1G_8Gm0DfkpyFOqC9680azEm1Sss3kXGdoiJA@mail.gmail.com>
Message-ID: <550C079D.4020407@oracle.com>

Before discussing whether it is an expected result, you'd also need to 
come up with a model of consistency - if you get rid of all total 
orders, do you still have a notion of sequential consistency?

Alex

On 20/03/2015 08:54, Marko Topolnik wrote:
> You have correctly identified the regulations I had in mind with the 
> previous post; but perhaps I should spend some more time explaining 
> its point. The current JMM has the following property:
>
> An action can participate in a synchronizes-with relation if and only 
> if it occurs in the total synchronization order.
>
> In the new revision of JMM the above property will no longer hold: 
> there will be actions which participate in synchronizes-with without 
> occurring in the sync order (lazySet) and actions which occur in the 
> sync order without participating in synchronizes-with 
> (weakCompareAndSet). These changes will not, however, change anything 
> in the definition of the happens-before consistency. Under such a 
> revised model there will be scenarios, such as the one under 
> discussion here, which preserve happens-before consistency, but are 
> not accountable for in terms of a total sync order.
>
> So in the post above I referred to the essence of happens-before 
> consistency without tying it to the additional constraints which the 
> JMM currently imposes. Leslie Lamport's original idea did not include 
> a total sync order; in fact, its main point was the absence of such 
> order. An analogy can be set up where the current JMM is Newtonian 
> (global time exists) whereas Lamport's model is relativistic (each 
> frame of reference has its own time). The revised model will allow for 
> some of the original relativism.
>
> ---
> Marko
>
> On Thu, Mar 19, 2015 at 9:58 PM, Oleksandr Otenko 
> <oleksandr.otenko at oracle.com <mailto:oleksandr.otenko at oracle.com>> wrote:
>
>     No.
>
>     17.4.5. In plain words, happens-before order is a combination of
>     program order and synchronizes-with. Then 17.4.4. In plain words,
>     synchronizes-with comes from synchronization order: a volatile
>     write synchronizes-with all volatile reads (of the same variable)
>     appearing subsequently in synchronization order.
>
>     So you can't construct a happens-before by ignoring
>     synchronization order.
>
>     Alex
>
>
>     On 19/03/2015 19:34, Marko Topolnik wrote:
>>     Your fallacy is in assuming total sync order as a part of the
>>     happens-before consistency. It is not. JLS 17.4.5:
>>
>>     "A set of actions A is happens-before consistent if for all reads
>>     r in A, where W(r) is the write action seen by r, it is not the
>>     case that either hb(r, W(r)) or that there exists a write w in A
>>     such that w.v = r.v and hb(W(r), w) and hb(w, r)."
>>
>>     "In a happens-before consistent set of actions, each read sees a
>>     write that it is allowed to see by the happens-before ordering."
>>
>>     That's all there is to it, and my diagram clearly satisfies this
>>     definition. Now, you may argue that the happens-before order is
>>     itself constrained by the total sync order, but before you do
>>     that I urge you to consider lazySet(). This method's semantics
>>     are exactly that of a volatile write which is exempted from the
>>     total sync order. In its presence the incidental correspondence
>>     between sync order and happens-before order breaks.
>>
>>     ---
>>     Marko
>>
>>     On Thu, Mar 19, 2015 at 7:32 PM, Oleksandr Otenko
>>     <oleksandr.otenko at oracle.com
>>     <mailto:oleksandr.otenko at oracle.com>> wrote:
>>     >
>>     > Looking from another angle, you have claimed it is
>>     "happens-before consistent" without showing that you have found
>>     all happens-before edges.
>>     >
>>     > I have explained why there definitely is another happens-before
>>     edge. I can provide another constructive proof, if it will look
>>     easier:
>>     >
>>     > Suppose, Rwt6 observed T6 and not T9. Suppose, Wv1 appears
>>     after Rv0 in synchronization order, so Rv0 sees the write of Wv0,
>>     and does not see Wv1. Then Rwt6 appears after Rrt9 and T9 in
>>     synchronization order, and Rwt6 must observe T9, not T6 -
>>     contradiction. Thus by contradiction the two assumptions cannot
>>     exist together. Placing Rwt6 after T9 or Wv1 before Rv0 exposes
>>     existence of one more happens-before edge, which is not on the
>>     diagram. Therefore, if we assume Rwt6 sees T6 and not T9, then
>>     Rv0 sees Wv1.
>>     >
>>     >
>>     > Alex
>>     >
>>     >
>>     > On 19/03/2015 18:14, Oleksandr Otenko wrote:
>>     >
>>     > No, you don't need Rv1'.
>>     >
>>     > No, happens-before consistency is not the only rule for
>>     correctness of the outcome. The "read must see the last write it
>>     synchronizes-with" is also part of the rule set. You must either
>>     show the proof that Wv1 appears after Rv0, or accept Wv1 appears
>>     before Rv0 in synchronization order.
>>     >
>>     > Alex
>>     >
>>     > On 19/03/2015 18:02, Marko Topolnik wrote:
>>     >
>>     >
>>     >
>>     > On Thu, Mar 19, 2015 at 6:07 PM, Oleksandr Otenko
>>     <oleksandr.otenko at oracle.com
>>     <mailto:oleksandr.otenko at oracle.com>> wrote:
>>     >>
>>     >> Your scenario is exactly IRIW.
>>     >>
>>     >> T9 is x=1
>>     >> Wv1 is y=1
>>     >> Rrt9 is r0=x
>>     >> Rv0 is r1=y
>>     >>
>>     >> T6 is initial x=0
>>     >>
>>     >>
>>     >>
>>     >> Rwt6 is a constraint that you want to consider only total
>>     orders with y=1 preceding x=1
>>     >> It is equivalent to the independent reader thread doing r2=y,
>>     then r3=x, and observing r2==1, r3==0. That is, you have
>>     reformulated IRIW in such a way so that we consider only one
>>     specific outcome, not all possible outcomes.
>>     >
>>     >
>>     > No, for a correct analogy to IRIW you need to introduce an Rv1'
>>     between Wv1 and Rwt6.
>>     >
>>     > Now,
>>     >
>>     > T9 is x=1
>>     > Wv1 is y=1
>>     > Rrt9 is r0=x
>>     > Rv0 is r1=y
>>     > Rv1' is r2=y
>>     > Rwt6 is r3=x
>>     >
>>     > Now we can clearly see that the JMM does not allow
>>     (r0,r1,r2,r3) = (1,0,1,0) even though this is a happens
>>     before-consistent result. In other words, JMM gives this the
>>     exact same treatment as in IRIW. However, on a low level there is
>>     a key difference in that it involves the store buffers in the
>>     timing, and does it asymetrically, just for one of the two
>>     reading threads. This variation is potentially more interesting
>>     because it is specifically excluded from the guarantees of the
>>     Intel specification. Therefore it is harder to meet by the JIT
>>     compiler.
>>     >
>>     >>
>>     >> T1:
>>     >> x=1
>>     >>
>>     >> T2:
>>     >> y=1
>>     >>
>>     >> T3:
>>     >> r0=x;
>>     >> r1=y;
>>     >>
>>     >> T4:
>>     >> r2=y;
>>     >> r3=x;
>>     >>
>>     >> JMM forbids the outcome (r0,r1,r2,r3) = (1,0,1,0) - ie one
>>     thread thinking x=1 happened before y=1, the other thread
>>     thinking y=1 happened before x=1.
>>     >
>>     >
>>     > ---
>>     > Marko
>>     >
>>     >
>>     >
>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150320/9539873d/attachment-0001.html>

From nathan.reynolds at oracle.com  Fri Mar 20 11:12:41 2015
From: nathan.reynolds at oracle.com (Nathan Reynolds)
Date: Fri, 20 Mar 2015 08:12:41 -0700
Subject: [concurrency-interest] ReentrantLock bug?
In-Reply-To: <9C4A4607-7532-4FB6-B33B-5B9AC9D99929@kodewerk.com>
References: <18DB6EF6-3568-462B-B064-A8415948A31C@gmail.com>	<550B325E.1010207@oracle.com>	<CAB=Je-Hh=edL9a1W=NZ2c+=Y13ApRBUw2SeBE9XY7_VcP1OAxg@mail.gmail.com>	<613E1B57-4408-4C7C-BC57-DAE6B18EB9B9@gmail.com>
	<9C4A4607-7532-4FB6-B33B-5B9AC9D99929@kodewerk.com>
Message-ID: <550C38E9.9050709@oracle.com>

I have hit this ReentrantLock issue a lot in the past.  Since I could 
prove anything about ReentrantLock, I didn't bother filing the bug.  
Sorry.  My workaround was to switch over to synchronized.  I now avoid 
ReentrantLock.  It would be nice to get this fixed.

-Nathan

On 3/19/2015 11:04 PM, Kirk Pepperdine wrote:
> I see app?s survive OOME because only the offending thread gets shot 
> down. After the request that causes the OOME is shot down, the rest of 
> the app works fine.. for some definition of fine.
>
> Regards,
> Kirk
>
> On Mar 19, 2015, at 9:52 PM, Dmitry Zaslavsky 
> <dmitry.zaslavsky at gmail.com <mailto:dmitry.zaslavsky at gmail.com>> wrote:
>
>> I doubt it's one of those conditions.
>> I would expect the process to die but it's perfectly fine otherwise. 
>> Due to this issue it looks like a deadlocked process
>>
>> Sent from mobile device
>>
>> On Mar 19, 2015, at 4:50 PM, Vladimir Sitnikov 
>> <sitnikov.vladimir at gmail.com <mailto:sitnikov.vladimir at gmail.com>> wrote:
>>
>>> I typically observe 'invalid' state of ReentrantLock due to 
>>> StackOverflowError/OutOfMemoryError.
>>>
>>> Can you try your application with extended stack size and/or check 
>>> for SOE/OOM?
>>>
>>> Regards,
>>> Vladimir Sitnikov
>>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu 
>> <mailto:Concurrency-interest at cs.oswego.edu>
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150320/e470fb05/attachment.html>

From arcadiy at ivanov.biz  Fri Mar 20 11:50:25 2015
From: arcadiy at ivanov.biz (Arcadiy Ivanov)
Date: Fri, 20 Mar 2015 11:50:25 -0400
Subject: [concurrency-interest] ReentrantLock bug?
In-Reply-To: <550C38E9.9050709@oracle.com>
References: <18DB6EF6-3568-462B-B064-A8415948A31C@gmail.com>	<550B325E.1010207@oracle.com>	<CAB=Je-Hh=edL9a1W=NZ2c+=Y13ApRBUw2SeBE9XY7_VcP1OAxg@mail.gmail.com>	<613E1B57-4408-4C7C-BC57-DAE6B18EB9B9@gmail.com>	<9C4A4607-7532-4FB6-B33B-5B9AC9D99929@kodewerk.com>
	<550C38E9.9050709@oracle.com>
Message-ID: <550C41C1.2040707@ivanov.biz>

Generally speaking, an application should not attempt to survive an OOME 
(except in rare cases of direct ByteBuffer allocation and other arguably 
controlled OOMEs). I believe even a synchronized monitor can get in the 
inconsistent state on OOME, if you're particularly unlucky that day. 
It's sort of like ThreadDeath error situation - once you get it is very 
hard to know if an application is in the consistent state.

On 2015-03-20 11:12, Nathan Reynolds wrote:
> I have hit this ReentrantLock issue a lot in the past.  Since I could 
> prove anything about ReentrantLock, I didn't bother filing the bug.  
> Sorry.  My workaround was to switch over to synchronized.  I now avoid 
> ReentrantLock.  It would be nice to get this fixed.
> -Nathan
> On 3/19/2015 11:04 PM, Kirk Pepperdine wrote:
>> I see app?s survive OOME because only the offending thread gets shot 
>> down. After the request that causes the OOME is shot down, the rest 
>> of the app works fine.. for some definition of fine.
>>
>> Regards,
>> Kirk
>>
>> On Mar 19, 2015, at 9:52 PM, Dmitry Zaslavsky 
>> <dmitry.zaslavsky at gmail.com <mailto:dmitry.zaslavsky at gmail.com>> wrote:
>>
>>> I doubt it's one of those conditions.
>>> I would expect the process to die but it's perfectly fine otherwise. 
>>> Due to this issue it looks like a deadlocked process
>>>
>>> Sent from mobile device
>>>
>>> On Mar 19, 2015, at 4:50 PM, Vladimir Sitnikov 
>>> <sitnikov.vladimir at gmail.com <mailto:sitnikov.vladimir at gmail.com>> 
>>> wrote:
>>>
>>>> I typically observe 'invalid' state of ReentrantLock due to 
>>>> StackOverflowError/OutOfMemoryError.
>>>>
>>>> Can you try your application with extended stack size and/or check 
>>>> for SOE/OOM?
>>>>
>>>> Regards,
>>>> Vladimir Sitnikov
>>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu 
>>> <mailto:Concurrency-interest at cs.oswego.edu>
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150320/2862751f/attachment.html>

From marko at hazelcast.com  Fri Mar 20 11:53:30 2015
From: marko at hazelcast.com (Marko Topolnik)
Date: Fri, 20 Mar 2015 16:53:30 +0100
Subject: [concurrency-interest] Enforcing total sync order on modern
	hardware
In-Reply-To: <550C079D.4020407@oracle.com>
References: <CALtZ-o47wWjeVXtn=zx8t4vau6D5T-qFnA3Vm2AGcFUm2S_PVw@mail.gmail.com>
	<550805ED.30003@oracle.com>
	<CALtZ-o7TCBcFhv3BpjCWQsGoYh+rFbB83sutZa8DXCtB2V16HA@mail.gmail.com>
	<1501970.3n0TOdWmYW@d-allen>
	<CALtZ-o5vvaA0=LrAfaVS2mYqKOfctBhGGQMj4oMckpq-_cg0zQ@mail.gmail.com>
	<550B0245.50606@oracle.com>
	<CALtZ-o4K8i2QrS3OzApai=CCv9oFpbtZDZgN0V8M4cZRy5ocTg@mail.gmail.com>
	<550B1222.3060704@oracle.com> <550B1629.50203@oracle.com>
	<CALtZ-o5ike96VAMeCUM1coqZtyB6OH20qpA-0Dn+qzAJPDgy8A@mail.gmail.com>
	<550B388C.2070808@oracle.com>
	<CALtZ-o7Qe9uMX1G_8Gm0DfkpyFOqC9680azEm1Sss3kXGdoiJA@mail.gmail.com>
	<550C079D.4020407@oracle.com>
Message-ID: <CALtZ-o7SoNgww5vOsTmcK0Hxo5r2nnGUvby=uSsNve_FLQ0j4Q@mail.gmail.com>

Exactly, sequential consistency is at the heart of this matter. In
Lamport's model it is a non-goal, being replaced by happens-before
consistency. This provides a partial order based on causality, as opposed
to a total order based on global time. For many practical concerns SC is an
overkill and HBC is all you really need to rely on. SC is also associated
with a performance cost which is getting ever larger as CPU architectures
become more distributed. A reflection of that within Java is the growing
need for things like lazySet() which give you happens-before without a
strict SC requirement.

So, to directly respond to your remark: my model of consistency is
happens-before.

---
Marko

On Fri, Mar 20, 2015 at 12:42 PM, Oleksandr Otenko <
oleksandr.otenko at oracle.com> wrote:

>  Before discussing whether it is an expected result, you'd also need to
> come up with a model of consistency - if you get rid of all total orders,
> do you still have a notion of sequential consistency?
>
> Alex
>
>
> On 20/03/2015 08:54, Marko Topolnik wrote:
>
> You have correctly identified the regulations I had in mind with the
> previous post; but perhaps I should spend some more time explaining its
> point. The current JMM has the following property:
>
>  An action can participate in a synchronizes-with relation if and only if
> it occurs in the total synchronization order.
>
>  In the new revision of JMM the above property will no longer hold: there
> will be actions which participate in synchronizes-with without occurring in
> the sync order (lazySet) and actions which occur in the sync order without
> participating in synchronizes-with (weakCompareAndSet). These changes will
> not, however, change anything in the definition of the happens-before
> consistency. Under such a revised model there will be scenarios, such as
> the one under discussion here, which preserve happens-before consistency,
> but are not accountable for in terms of a total sync order.
>
>  So in the post above I referred to the essence of happens-before
> consistency without tying it to the additional constraints which the JMM
> currently imposes. Leslie Lamport's original idea did not include a total
> sync order; in fact, its main point was the absence of such order. An
> analogy can be set up where the current JMM is Newtonian (global time
> exists) whereas Lamport's model is relativistic (each frame of reference
> has its own time). The revised model will allow for some of the original
> relativism.
>
>  ---
> Marko
>
> On Thu, Mar 19, 2015 at 9:58 PM, Oleksandr Otenko <
> oleksandr.otenko at oracle.com> wrote:
>
>>  No.
>>
>> 17.4.5. In plain words, happens-before order is a combination of program
>> order and synchronizes-with. Then 17.4.4. In plain words, synchronizes-with
>> comes from synchronization order: a volatile write synchronizes-with all
>> volatile reads (of the same variable) appearing subsequently in
>> synchronization order.
>>
>> So you can't construct a happens-before by ignoring synchronization order.
>>
>> Alex
>>
>>
>> On 19/03/2015 19:34, Marko Topolnik wrote:
>>
>> Your fallacy is in assuming total sync order as a part of the
>> happens-before consistency. It is not. JLS 17.4.5:
>>
>> "A set of actions A is happens-before consistent if for all reads r in A,
>> where W(r) is the write action seen by r, it is not the case that either
>> hb(r, W(r)) or that there exists a write w in A such that w.v = r.v and
>> hb(W(r), w) and hb(w, r)."
>>
>> "In a happens-before consistent set of actions, each read sees a write
>> that it is allowed to see by the happens-before ordering."
>>
>>  That's all there is to it, and my diagram clearly satisfies this
>> definition. Now, you may argue that the happens-before order is itself
>> constrained by the total sync order, but before you do that I urge you to
>> consider lazySet(). This method's semantics are exactly that of a volatile
>> write which is exempted from the total sync order. In its presence the
>> incidental correspondence between sync order and happens-before order
>> breaks.
>>
>>  ---
>> Marko
>>
>> On Thu, Mar 19, 2015 at 7:32 PM, Oleksandr Otenko <
>> oleksandr.otenko at oracle.com> wrote:
>> >
>> > Looking from another angle, you have claimed it is "happens-before
>> consistent" without showing that you have found all happens-before edges.
>> >
>> > I have explained why there definitely is another happens-before edge. I
>> can provide another constructive proof, if it will look easier:
>> >
>> > Suppose, Rwt6 observed T6 and not T9. Suppose, Wv1 appears after Rv0 in
>> synchronization order, so Rv0 sees the write of Wv0, and does not see Wv1.
>> Then Rwt6 appears after Rrt9 and T9 in synchronization order, and Rwt6 must
>> observe T9, not T6 - contradiction. Thus by contradiction the two
>> assumptions cannot exist together. Placing Rwt6 after T9 or Wv1 before Rv0
>> exposes existence of one more happens-before edge, which is not on the
>> diagram. Therefore, if we assume Rwt6 sees T6 and not T9, then Rv0 sees Wv1.
>> >
>> >
>> > Alex
>> >
>> >
>> > On 19/03/2015 18:14, Oleksandr Otenko wrote:
>> >
>> > No, you don't need Rv1'.
>> >
>> > No, happens-before consistency is not the only rule for correctness of
>> the outcome. The "read must see the last write it synchronizes-with" is
>> also part of the rule set. You must either show the proof that Wv1 appears
>> after Rv0, or accept Wv1 appears before Rv0 in synchronization order.
>> >
>> > Alex
>> >
>> > On 19/03/2015 18:02, Marko Topolnik wrote:
>> >
>> >
>> >
>> > On Thu, Mar 19, 2015 at 6:07 PM, Oleksandr Otenko <
>> oleksandr.otenko at oracle.com> wrote:
>> >>
>> >> Your scenario is exactly IRIW.
>> >>
>> >> T9 is x=1
>> >> Wv1 is y=1
>> >> Rrt9 is r0=x
>> >> Rv0 is r1=y
>> >>
>> >> T6 is initial x=0
>> >>
>> >>
>> >>
>> >> Rwt6 is a constraint that you want to consider only total orders with
>> y=1 preceding x=1
>> >> It is equivalent to the independent reader thread doing r2=y, then
>> r3=x, and observing r2==1, r3==0. That is, you have reformulated IRIW in
>> such a way so that we consider only one specific outcome, not all possible
>> outcomes.
>> >
>> >
>> > No, for a correct analogy to IRIW you need to introduce an Rv1' between
>> Wv1 and Rwt6.
>> >
>> > Now,
>> >
>> > T9 is x=1
>> > Wv1 is y=1
>> > Rrt9 is r0=x
>> > Rv0 is r1=y
>> > Rv1' is r2=y
>> > Rwt6 is r3=x
>> >
>> > Now we can clearly see that the JMM does not allow (r0,r1,r2,r3) =
>> (1,0,1,0) even though this is a happens before-consistent result. In other
>> words, JMM gives this the exact same treatment as in IRIW. However, on a
>> low level there is a key difference in that it involves the store buffers
>> in the timing, and does it asymetrically, just for one of the two reading
>> threads. This variation is potentially more interesting because it is
>> specifically excluded from the guarantees of the Intel specification.
>> Therefore it is harder to meet by the JIT compiler.
>> >
>> >>
>> >> T1:
>> >> x=1
>> >>
>> >> T2:
>> >> y=1
>> >>
>> >> T3:
>> >> r0=x;
>> >> r1=y;
>> >>
>> >> T4:
>> >> r2=y;
>> >> r3=x;
>> >>
>> >> JMM forbids the outcome (r0,r1,r2,r3) = (1,0,1,0) - ie one thread
>> thinking x=1 happened before y=1, the other thread thinking y=1 happened
>> before x=1.
>> >
>> >
>> > ---
>> > Marko
>> >
>> >
>> >
>>
>>
>>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150320/151cee72/attachment-0001.html>

From oleksandr.otenko at oracle.com  Fri Mar 20 12:45:12 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Fri, 20 Mar 2015 16:45:12 +0000
Subject: [concurrency-interest] Enforcing total sync order on modern
	hardware
In-Reply-To: <CALtZ-o7SoNgww5vOsTmcK0Hxo5r2nnGUvby=uSsNve_FLQ0j4Q@mail.gmail.com>
References: <CALtZ-o47wWjeVXtn=zx8t4vau6D5T-qFnA3Vm2AGcFUm2S_PVw@mail.gmail.com>	<550805ED.30003@oracle.com>	<CALtZ-o7TCBcFhv3BpjCWQsGoYh+rFbB83sutZa8DXCtB2V16HA@mail.gmail.com>	<1501970.3n0TOdWmYW@d-allen>	<CALtZ-o5vvaA0=LrAfaVS2mYqKOfctBhGGQMj4oMckpq-_cg0zQ@mail.gmail.com>	<550B0245.50606@oracle.com>	<CALtZ-o4K8i2QrS3OzApai=CCv9oFpbtZDZgN0V8M4cZRy5ocTg@mail.gmail.com>	<550B1222.3060704@oracle.com>	<550B1629.50203@oracle.com>	<CALtZ-o5ike96VAMeCUM1coqZtyB6OH20qpA-0Dn+qzAJPDgy8A@mail.gmail.com>	<550B388C.2070808@oracle.com>	<CALtZ-o7Qe9uMX1G_8Gm0DfkpyFOqC9680azEm1Sss3kXGdoiJA@mail.gmail.com>	<550C079D.4020407@oracle.com>
	<CALtZ-o7SoNgww5vOsTmcK0Hxo5r2nnGUvby=uSsNve_FLQ0j4Q@mail.gmail.com>
Message-ID: <550C4E98.3040208@oracle.com>

No, that doesn't answer the question. You need to modify how 
happens-before is built - because happens-before in JMM and in some 
other model are two different happens-befores. If you get rid of 
synchronization order, then you need to explain which reads the write 
will or will not synchronize-with.

I am only involved in this discussion because you said it isn't IRIW, 
but I see all signs that it is. I remember the discussion here doubting 
that IRIW should be supported, and I appreciate the arguments, but 
without the specification it is difficult to continue a meaningful 
discussion.


Alex


On 20/03/2015 15:53, Marko Topolnik wrote:
> Exactly, sequential consistency is at the heart of this matter. In 
> Lamport's model it is a non-goal, being replaced by happens-before 
> consistency. This provides a partial order based on causality, as 
> opposed to a total order based on global time. For many practical 
> concerns SC is an overkill and HBC is all you really need to rely on. 
> SC is also associated with a performance cost which is getting ever 
> larger as CPU architectures become more distributed. A reflection of 
> that within Java is the growing need for things like lazySet() which 
> give you happens-before without a strict SC requirement.
>
> So, to directly respond to your remark: my model of consistency is 
> happens-before.
>
> ---
> Marko
>
> On Fri, Mar 20, 2015 at 12:42 PM, Oleksandr Otenko 
> <oleksandr.otenko at oracle.com <mailto:oleksandr.otenko at oracle.com>> wrote:
>
>     Before discussing whether it is an expected result, you'd also
>     need to come up with a model of consistency - if you get rid of
>     all total orders, do you still have a notion of sequential
>     consistency?
>
>     Alex
>
>
>     On 20/03/2015 08:54, Marko Topolnik wrote:
>>     You have correctly identified the regulations I had in mind with
>>     the previous post; but perhaps I should spend some more time
>>     explaining its point. The current JMM has the following property:
>>
>>     An action can participate in a synchronizes-with relation if and
>>     only if it occurs in the total synchronization order.
>>
>>     In the new revision of JMM the above property will no longer
>>     hold: there will be actions which participate in
>>     synchronizes-with without occurring in the sync order (lazySet)
>>     and actions which occur in the sync order without participating
>>     in synchronizes-with (weakCompareAndSet). These changes will not,
>>     however, change anything in the definition of the happens-before
>>     consistency. Under such a revised model there will be scenarios,
>>     such as the one under discussion here, which preserve
>>     happens-before consistency, but are not accountable for in terms
>>     of a total sync order.
>>
>>     So in the post above I referred to the essence of happens-before
>>     consistency without tying it to the additional constraints which
>>     the JMM currently imposes. Leslie Lamport's original idea did not
>>     include a total sync order; in fact, its main point was the
>>     absence of such order. An analogy can be set up where the current
>>     JMM is Newtonian (global time exists) whereas Lamport's model is
>>     relativistic (each frame of reference has its own time). The
>>     revised model will allow for some of the original relativism.
>>
>>     ---
>>     Marko
>>
>>     On Thu, Mar 19, 2015 at 9:58 PM, Oleksandr Otenko
>>     <oleksandr.otenko at oracle.com
>>     <mailto:oleksandr.otenko at oracle.com>> wrote:
>>
>>         No.
>>
>>         17.4.5. In plain words, happens-before order is a combination
>>         of program order and synchronizes-with. Then 17.4.4. In plain
>>         words, synchronizes-with comes from synchronization order: a
>>         volatile write synchronizes-with all volatile reads (of the
>>         same variable) appearing subsequently in synchronization order.
>>
>>         So you can't construct a happens-before by ignoring
>>         synchronization order.
>>
>>         Alex
>>
>>
>>         On 19/03/2015 19:34, Marko Topolnik wrote:
>>>         Your fallacy is in assuming total sync order as a part of
>>>         the happens-before consistency. It is not. JLS 17.4.5:
>>>
>>>         "A set of actions A is happens-before consistent if for all
>>>         reads r in A, where W(r) is the write action seen by r, it
>>>         is not the case that either hb(r, W(r)) or that there exists
>>>         a write w in A such that w.v = r.v and hb(W(r), w) and hb(w,
>>>         r)."
>>>
>>>         "In a happens-before consistent set of actions, each read
>>>         sees a write that it is allowed to see by the happens-before
>>>         ordering."
>>>
>>>         That's all there is to it, and my diagram clearly satisfies
>>>         this definition. Now, you may argue that the happens-before
>>>         order is itself constrained by the total sync order, but
>>>         before you do that I urge you to consider lazySet(). This
>>>         method's semantics are exactly that of a volatile write
>>>         which is exempted from the total sync order. In its presence
>>>         the incidental correspondence between sync order and
>>>         happens-before order breaks.
>>>
>>>         ---
>>>         Marko
>>>
>>>         On Thu, Mar 19, 2015 at 7:32 PM, Oleksandr Otenko
>>>         <oleksandr.otenko at oracle.com
>>>         <mailto:oleksandr.otenko at oracle.com>> wrote:
>>>         >
>>>         > Looking from another angle, you have claimed it is
>>>         "happens-before consistent" without showing that you have
>>>         found all happens-before edges.
>>>         >
>>>         > I have explained why there definitely is another
>>>         happens-before edge. I can provide another constructive
>>>         proof, if it will look easier:
>>>         >
>>>         > Suppose, Rwt6 observed T6 and not T9. Suppose, Wv1 appears
>>>         after Rv0 in synchronization order, so Rv0 sees the write of
>>>         Wv0, and does not see Wv1. Then Rwt6 appears after Rrt9 and
>>>         T9 in synchronization order, and Rwt6 must observe T9, not
>>>         T6 - contradiction. Thus by contradiction the two
>>>         assumptions cannot exist together. Placing Rwt6 after T9 or
>>>         Wv1 before Rv0 exposes existence of one more happens-before
>>>         edge, which is not on the diagram. Therefore, if we assume
>>>         Rwt6 sees T6 and not T9, then Rv0 sees Wv1.
>>>         >
>>>         >
>>>         > Alex
>>>         >
>>>         >
>>>         > On 19/03/2015 18:14, Oleksandr Otenko wrote:
>>>         >
>>>         > No, you don't need Rv1'.
>>>         >
>>>         > No, happens-before consistency is not the only rule for
>>>         correctness of the outcome. The "read must see the last
>>>         write it synchronizes-with" is also part of the rule set.
>>>         You must either show the proof that Wv1 appears after Rv0,
>>>         or accept Wv1 appears before Rv0 in synchronization order.
>>>         >
>>>         > Alex
>>>         >
>>>         > On 19/03/2015 18:02, Marko Topolnik wrote:
>>>         >
>>>         >
>>>         >
>>>         > On Thu, Mar 19, 2015 at 6:07 PM, Oleksandr Otenko
>>>         <oleksandr.otenko at oracle.com
>>>         <mailto:oleksandr.otenko at oracle.com>> wrote:
>>>         >>
>>>         >> Your scenario is exactly IRIW.
>>>         >>
>>>         >> T9 is x=1
>>>         >> Wv1 is y=1
>>>         >> Rrt9 is r0=x
>>>         >> Rv0 is r1=y
>>>         >>
>>>         >> T6 is initial x=0
>>>         >>
>>>         >>
>>>         >>
>>>         >> Rwt6 is a constraint that you want to consider only total
>>>         orders with y=1 preceding x=1
>>>         >> It is equivalent to the independent reader thread doing
>>>         r2=y, then r3=x, and observing r2==1, r3==0. That is, you
>>>         have reformulated IRIW in such a way so that we consider
>>>         only one specific outcome, not all possible outcomes.
>>>         >
>>>         >
>>>         > No, for a correct analogy to IRIW you need to introduce an
>>>         Rv1' between Wv1 and Rwt6.
>>>         >
>>>         > Now,
>>>         >
>>>         > T9 is x=1
>>>         > Wv1 is y=1
>>>         > Rrt9 is r0=x
>>>         > Rv0 is r1=y
>>>         > Rv1' is r2=y
>>>         > Rwt6 is r3=x
>>>         >
>>>         > Now we can clearly see that the JMM does not allow
>>>         (r0,r1,r2,r3) = (1,0,1,0) even though this is a happens
>>>         before-consistent result. In other words, JMM gives this the
>>>         exact same treatment as in IRIW. However, on a low level
>>>         there is a key difference in that it involves the store
>>>         buffers in the timing, and does it asymetrically, just for
>>>         one of the two reading threads. This variation is
>>>         potentially more interesting because it is specifically
>>>         excluded from the guarantees of the Intel specification.
>>>         Therefore it is harder to meet by the JIT compiler.
>>>         >
>>>         >>
>>>         >> T1:
>>>         >> x=1
>>>         >>
>>>         >> T2:
>>>         >> y=1
>>>         >>
>>>         >> T3:
>>>         >> r0=x;
>>>         >> r1=y;
>>>         >>
>>>         >> T4:
>>>         >> r2=y;
>>>         >> r3=x;
>>>         >>
>>>         >> JMM forbids the outcome (r0,r1,r2,r3) = (1,0,1,0) - ie
>>>         one thread thinking x=1 happened before y=1, the other
>>>         thread thinking y=1 happened before x=1.
>>>         >
>>>         >
>>>         > ---
>>>         > Marko
>>>         >
>>>         >
>>>         >
>>
>>
>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150320/6f9001fd/attachment-0001.html>

From dmitry.zaslavsky at gmail.com  Fri Mar 20 13:43:22 2015
From: dmitry.zaslavsky at gmail.com (Dmitry Zaslavsky)
Date: Fri, 20 Mar 2015 13:43:22 -0400
Subject: [concurrency-interest] ReentrantLock bug?
In-Reply-To: <NFBBKALFDCPFIDBNKAPCEEIHKOAA.davidcholmes@aapt.net.au>
References: <NFBBKALFDCPFIDBNKAPCEEIHKOAA.davidcholmes@aapt.net.au>
Message-ID: <13F59205-8077-45C1-BCC3-9EB329F2F778@gmail.com>

Was able to repro with 7u40

> On Mar 19, 2015, at 8:56 PM, David Holmes <davidcholmes at aapt.net.au> wrote:
> 
> StackOverflow is know to cause the ReentrantLock to be left in an invalid state whereby it appears locked but is not. This can lead to hangs during classloading due to the use of ConcurrentHashMap for maintaining the classloading locks. The CHM implementation was changed (CHMv8) to use synchronized instead of ReentrantLock, and so this classloader issue does not affect JDK 8 or later. But of course ReentrantLock itself is still affected by this problem.
>  
> There were some other fixes in the low-level park/unpark code that went into 7u40 (8004902) but they were found by code inspection not by anyone encountering the actual bug - as far as we know.
>  
> David
>> -----Original Message-----
>> From: concurrency-interest-bounces at cs.oswego.edu [mailto:concurrency-interest-bounces at cs.oswego.edu]On Behalf Of Dmitry Zaslavsky
>> Sent: Friday, 20 March 2015 6:53 AM
>> To: Vladimir Sitnikov
>> Cc: concurrency-interest
>> Subject: Re: [concurrency-interest] ReentrantLock bug?
>> 
>> I doubt it's one of those conditions.
>> I would expect the process to die but it's perfectly fine otherwise. Due to this issue it looks like a deadlocked process
>> 
>> Sent from mobile device
>> 
>> On Mar 19, 2015, at 4:50 PM, Vladimir Sitnikov <sitnikov.vladimir at gmail.com <mailto:sitnikov.vladimir at gmail.com>> wrote:
>> 
>>> I typically observe 'invalid' state of ReentrantLock due to StackOverflowError/OutOfMemoryError.
>>> Can you try your application with extended stack size and/or check for SOE/OOM?
>>> Regards,
>>> Vladimir Sitnikov

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150320/4c259438/attachment.html>

From marko at hazelcast.com  Fri Mar 20 14:12:57 2015
From: marko at hazelcast.com (Marko Topolnik)
Date: Fri, 20 Mar 2015 19:12:57 +0100
Subject: [concurrency-interest] Enforcing total sync order on modern
	hardware
In-Reply-To: <550C4E98.3040208@oracle.com>
References: <CALtZ-o47wWjeVXtn=zx8t4vau6D5T-qFnA3Vm2AGcFUm2S_PVw@mail.gmail.com>
	<550805ED.30003@oracle.com>
	<CALtZ-o7TCBcFhv3BpjCWQsGoYh+rFbB83sutZa8DXCtB2V16HA@mail.gmail.com>
	<1501970.3n0TOdWmYW@d-allen>
	<CALtZ-o5vvaA0=LrAfaVS2mYqKOfctBhGGQMj4oMckpq-_cg0zQ@mail.gmail.com>
	<550B0245.50606@oracle.com>
	<CALtZ-o4K8i2QrS3OzApai=CCv9oFpbtZDZgN0V8M4cZRy5ocTg@mail.gmail.com>
	<550B1222.3060704@oracle.com> <550B1629.50203@oracle.com>
	<CALtZ-o5ike96VAMeCUM1coqZtyB6OH20qpA-0Dn+qzAJPDgy8A@mail.gmail.com>
	<550B388C.2070808@oracle.com>
	<CALtZ-o7Qe9uMX1G_8Gm0DfkpyFOqC9680azEm1Sss3kXGdoiJA@mail.gmail.com>
	<550C079D.4020407@oracle.com>
	<CALtZ-o7SoNgww5vOsTmcK0Hxo5r2nnGUvby=uSsNve_FLQ0j4Q@mail.gmail.com>
	<550C4E98.3040208@oracle.com>
Message-ID: <CALtZ-o6MCR3EWG5FRc_LNUn1Znt_251pZHM7NVCaha+pJKM1Dg@mail.gmail.com>

On Fri, Mar 20, 2015 at 5:45 PM, Oleksandr Otenko <
oleksandr.otenko at oracle.com> wrote:

>  No, that doesn't answer the question. You need to modify how
> happens-before is built - because happens-before in JMM and in some other
> model are two different happens-befores. If you get rid of synchronization
> order, then you need to explain which reads the write will or will not
> synchronize-with.
>

I think it's quite simple: the read may synchronize-with any write as long
as that doesn't break happens-before consistency.


> I am only involved in this discussion because you said it isn't IRIW, but
> I see all signs that it is. I remember the discussion here doubting that
> IRIW should be supported, and I appreciate the arguments, but without the
> specification it is difficult to continue a meaningful discussion.
>

That's strange to hear since I have pointed out exactly why it's not IRIW:
if we broaden the definition such that it covers my case, then we must
accept that Intel allows IRIW to happen because it explicitly excludes the
writing thread from the guarantee which is supposed to disallow it.

---
Marko
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150320/88906cd0/attachment.html>

From oleksandr.otenko at oracle.com  Fri Mar 20 14:52:03 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Fri, 20 Mar 2015 18:52:03 +0000
Subject: [concurrency-interest] Enforcing total sync order on modern
	hardware
In-Reply-To: <CALtZ-o6MCR3EWG5FRc_LNUn1Znt_251pZHM7NVCaha+pJKM1Dg@mail.gmail.com>
References: <CALtZ-o47wWjeVXtn=zx8t4vau6D5T-qFnA3Vm2AGcFUm2S_PVw@mail.gmail.com>	<550805ED.30003@oracle.com>	<CALtZ-o7TCBcFhv3BpjCWQsGoYh+rFbB83sutZa8DXCtB2V16HA@mail.gmail.com>	<1501970.3n0TOdWmYW@d-allen>	<CALtZ-o5vvaA0=LrAfaVS2mYqKOfctBhGGQMj4oMckpq-_cg0zQ@mail.gmail.com>	<550B0245.50606@oracle.com>	<CALtZ-o4K8i2QrS3OzApai=CCv9oFpbtZDZgN0V8M4cZRy5ocTg@mail.gmail.com>	<550B1222.3060704@oracle.com>	<550B1629.50203@oracle.com>	<CALtZ-o5ike96VAMeCUM1coqZtyB6OH20qpA-0Dn+qzAJPDgy8A@mail.gmail.com>	<550B388C.2070808@oracle.com>	<CALtZ-o7Qe9uMX1G_8Gm0DfkpyFOqC9680azEm1Sss3kXGdoiJA@mail.gmail.com>	<550C079D.4020407@oracle.com>	<CALtZ-o7SoNgww5vOsTmcK0Hxo5r2nnGUvby=uSsNve_FLQ0j4Q@mail.gmail.com>	<550C4E98.3040208@oracle.com>
	<CALtZ-o6MCR3EWG5FRc_LNUn1Znt_251pZHM7NVCaha+pJKM1Dg@mail.gmail.com>
Message-ID: <550C6C53.6040308@oracle.com>

On 20/03/2015 18:12, Marko Topolnik wrote:
> On Fri, Mar 20, 2015 at 5:45 PM, Oleksandr Otenko 
> <oleksandr.otenko at oracle.com <mailto:oleksandr.otenko at oracle.com>> wrote:
>
>     No, that doesn't answer the question. You need to modify how
>     happens-before is built - because happens-before in JMM and in
>     some other model are two different happens-befores. If you get rid
>     of synchronization order, then you need to explain which reads the
>     write will or will not synchronize-with.
>
>
> I think it's quite simple: the read may synchronize-with any write as 
> long as that doesn't break happens-before consistency.

It seems quite naive, too. The problem is that currently the read 
synchronizes-with /all/ writes preceding it, but observes the value set 
by the /last/ write. Here you need to define somehow which write the 
read observes - you need to somehow define which of the writes is "last" 
and what the other readers are allowed to think about it.

It doesn't seem to be explained in one sentence.

>     I am only involved in this discussion because you said it isn't
>     IRIW, but I see all signs that it is. I remember the discussion
>     here doubting that IRIW should be supported, and I appreciate the
>     arguments, but without the specification it is difficult to
>     continue a meaningful discussion.
>
>
> That's strange to hear since I have pointed out exactly why it's not 
> IRIW: if we broaden the definition such that it covers my case, then 
> we must accept that Intel allows IRIW to happen because it explicitly 
> excludes the writing thread from the guarantee which is supposed to 
> disallow it.

Rwt6 and Rrt6 are reduntant. If you remove them, it becomes IRIW. Rwt6 
only witnesses the particular ordering of some operations in IRIW - it 
forbids some of the outcomes from IRIW, but doesn't add new ones. Rrt6 
is meaningless.


Alex

>
> ---
> Marko

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150320/759e960b/attachment.html>

From marko at hazelcast.com  Fri Mar 20 18:05:50 2015
From: marko at hazelcast.com (Marko Topolnik)
Date: Fri, 20 Mar 2015 23:05:50 +0100
Subject: [concurrency-interest] Enforcing total sync order on modern
	hardware
In-Reply-To: <550C6C53.6040308@oracle.com>
References: <CALtZ-o47wWjeVXtn=zx8t4vau6D5T-qFnA3Vm2AGcFUm2S_PVw@mail.gmail.com>
	<550805ED.30003@oracle.com>
	<CALtZ-o7TCBcFhv3BpjCWQsGoYh+rFbB83sutZa8DXCtB2V16HA@mail.gmail.com>
	<1501970.3n0TOdWmYW@d-allen>
	<CALtZ-o5vvaA0=LrAfaVS2mYqKOfctBhGGQMj4oMckpq-_cg0zQ@mail.gmail.com>
	<550B0245.50606@oracle.com>
	<CALtZ-o4K8i2QrS3OzApai=CCv9oFpbtZDZgN0V8M4cZRy5ocTg@mail.gmail.com>
	<550B1222.3060704@oracle.com> <550B1629.50203@oracle.com>
	<CALtZ-o5ike96VAMeCUM1coqZtyB6OH20qpA-0Dn+qzAJPDgy8A@mail.gmail.com>
	<550B388C.2070808@oracle.com>
	<CALtZ-o7Qe9uMX1G_8Gm0DfkpyFOqC9680azEm1Sss3kXGdoiJA@mail.gmail.com>
	<550C079D.4020407@oracle.com>
	<CALtZ-o7SoNgww5vOsTmcK0Hxo5r2nnGUvby=uSsNve_FLQ0j4Q@mail.gmail.com>
	<550C4E98.3040208@oracle.com>
	<CALtZ-o6MCR3EWG5FRc_LNUn1Znt_251pZHM7NVCaha+pJKM1Dg@mail.gmail.com>
	<550C6C53.6040308@oracle.com>
Message-ID: <CALtZ-o7zop9RS4RKaVuSwpr+iuadENYOsv49rGVZv_84n0AQLg@mail.gmail.com>

On Fri, Mar 20, 2015 at 7:52 PM, Oleksandr Otenko <
oleksandr.otenko at oracle.com> wrote:

>  On 20/03/2015 18:12, Marko Topolnik wrote:
>
>  On Fri, Mar 20, 2015 at 5:45 PM, Oleksandr Otenko <
> oleksandr.otenko at oracle.com> wrote:
>
>>  No, that doesn't answer the question. You need to modify how
>> happens-before is built - because happens-before in JMM and in some other
>> model are two different happens-befores. If you get rid of synchronization
>> order, then you need to explain which reads the write will or will not
>> synchronize-with.
>>
>
>  I think it's quite simple: the read may synchronize-with any write as
> long as that doesn't break happens-before consistency.
>
>
> It seems quite naive, too. The problem is that currently the read
> synchronizes-with *all* writes preceding it, but observes the value set
> by the *last* write. Here you need to define somehow which write the read
> observes - you need to somehow define which of the writes is "last" and
> what the other readers are allowed to think about it.
>
> It doesn't seem to be explained in one sentence.
>

It is a quite lightweight exercise to rigorously specify this in terms of
Lamport's clocks; but I concede that, lacking a shared intuition, it will
take more than a sentence to communicate. I hesitate to turn this into a
treatise on the application of Lamport's clocks to the JMM, so I'm letting
it rest.

>  I am only involved in this discussion because you said it isn't IRIW,
>> but I see all signs that it is. I remember the discussion here doubting
>> that IRIW should be supported, and I appreciate the arguments, but without
>> the specification it is difficult to continue a meaningful discussion.
>>
>
>  That's strange to hear since I have pointed out exactly why it's not
> IRIW: if we broaden the definition such that it covers my case, then we
> must accept that Intel allows IRIW to happen because it explicitly excludes
> the writing thread from the guarantee which is supposed to disallow it.
>
>
> Rwt6 and Rrt6 are reduntant. If you remove them, it becomes IRIW. Rwt6
> only witnesses the particular ordering of some operations in IRIW - it
> forbids some of the outcomes from IRIW, but doesn't add new ones. Rrt6 is
> meaningless.
>

IRIW involves four independent threads and six events. My example involves
only three threads, so there must be something wrong in calling it "exactly
IRIW". Apparently you have in mind some quite flexible definition of IRIW,
but I cannot second-guess what it might be.

---
Marko
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150320/6bc98b98/attachment-0001.html>

From oleksandr.otenko at oracle.com  Mon Mar 23 08:53:00 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Mon, 23 Mar 2015 12:53:00 +0000
Subject: [concurrency-interest] Enforcing total sync order on modern
	hardware
In-Reply-To: <CALtZ-o7zop9RS4RKaVuSwpr+iuadENYOsv49rGVZv_84n0AQLg@mail.gmail.com>
References: <CALtZ-o47wWjeVXtn=zx8t4vau6D5T-qFnA3Vm2AGcFUm2S_PVw@mail.gmail.com>	<550805ED.30003@oracle.com>	<CALtZ-o7TCBcFhv3BpjCWQsGoYh+rFbB83sutZa8DXCtB2V16HA@mail.gmail.com>	<1501970.3n0TOdWmYW@d-allen>	<CALtZ-o5vvaA0=LrAfaVS2mYqKOfctBhGGQMj4oMckpq-_cg0zQ@mail.gmail.com>	<550B0245.50606@oracle.com>	<CALtZ-o4K8i2QrS3OzApai=CCv9oFpbtZDZgN0V8M4cZRy5ocTg@mail.gmail.com>	<550B1222.3060704@oracle.com>	<550B1629.50203@oracle.com>	<CALtZ-o5ike96VAMeCUM1coqZtyB6OH20qpA-0Dn+qzAJPDgy8A@mail.gmail.com>	<550B388C.2070808@oracle.com>	<CALtZ-o7Qe9uMX1G_8Gm0DfkpyFOqC9680azEm1Sss3kXGdoiJA@mail.gmail.com>	<550C079D.4020407@oracle.com>	<CALtZ-o7SoNgww5vOsTmcK0Hxo5r2nnGUvby=uSsNve_FLQ0j4Q@mail.gmail.com>	<550C4E98.3040208@oracle.com>	<CALtZ-o6MCR3EWG5FRc_LNUn1Znt_251pZHM7NVCaha+pJKM1Dg@mail.gmail.com>	<550C6C53.6040308@oracle.com>
	<CALtZ-o7zop9RS4RKaVuSwpr+iuadENYOsv49rGVZv_84n0AQLg@mail.gmail.com>
Message-ID: <55100CAC.9000806@oracle.com>

Out of all outcomes IRIW permits, choose those that have the fourth 
thread observe 1 then 0 - ie there exists a thread which observed Wv1 
occur before T9. Now you are looking at your case with three threads. 
Your case does not add more outcomes, only chooses a subset of those in 
IRIW.


Alex

On 20/03/2015 22:05, Marko Topolnik wrote:
> On Fri, Mar 20, 2015 at 7:52 PM, Oleksandr Otenko 
> <oleksandr.otenko at oracle.com <mailto:oleksandr.otenko at oracle.com>> wrote:
>
>     On 20/03/2015 18:12, Marko Topolnik wrote:
>>     On Fri, Mar 20, 2015 at 5:45 PM, Oleksandr Otenko
>>     <oleksandr.otenko at oracle.com
>>     <mailto:oleksandr.otenko at oracle.com>> wrote:
>>
>>         No, that doesn't answer the question. You need to modify how
>>         happens-before is built - because happens-before in JMM and
>>         in some other model are two different happens-befores. If you
>>         get rid of synchronization order, then you need to explain
>>         which reads the write will or will not synchronize-with.
>>
>>
>>     I think it's quite simple: the read may synchronize-with any
>>     write as long as that doesn't break happens-before consistency.
>
>     It seems quite naive, too. The problem is that currently the read
>     synchronizes-with /all/ writes preceding it, but observes the
>     value set by the /last/ write. Here you need to define somehow
>     which write the read observes - you need to somehow define which
>     of the writes is "last" and what the other readers are allowed to
>     think about it.
>
>     It doesn't seem to be explained in one sentence.
>
>
> It is a quite lightweight exercise to rigorously specify this in terms 
> of Lamport's clocks; but I concede that, lacking a shared intuition, 
> it will take more than a sentence to communicate. I hesitate to turn 
> this into a treatise on the application of Lamport's clocks to the 
> JMM, so I'm letting it rest.
>
>>         I am only involved in this discussion because you said it
>>         isn't IRIW, but I see all signs that it is. I remember the
>>         discussion here doubting that IRIW should be supported, and I
>>         appreciate the arguments, but without the specification it is
>>         difficult to continue a meaningful discussion.
>>
>>
>>     That's strange to hear since I have pointed out exactly why it's
>>     not IRIW: if we broaden the definition such that it covers my
>>     case, then we must accept that Intel allows IRIW to happen
>>     because it explicitly excludes the writing thread from the
>>     guarantee which is supposed to disallow it.
>
>     Rwt6 and Rrt6 are reduntant. If you remove them, it becomes IRIW.
>     Rwt6 only witnesses the particular ordering of some operations in
>     IRIW - it forbids some of the outcomes from IRIW, but doesn't add
>     new ones. Rrt6 is meaningless.
>
>
> IRIW involves four independent threads and six events. My example 
> involves only three threads, so there must be something wrong in 
> calling it "exactly IRIW". Apparently you have in mind some quite 
> flexible definition of IRIW, but I cannot second-guess what it might be.
>
> ---
> Marko

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150323/42311e46/attachment.html>

From marko at hazelcast.com  Mon Mar 23 11:56:50 2015
From: marko at hazelcast.com (Marko Topolnik)
Date: Mon, 23 Mar 2015 16:56:50 +0100
Subject: [concurrency-interest] Enforcing total sync order on modern
	hardware
In-Reply-To: <55100CAC.9000806@oracle.com>
References: <CALtZ-o47wWjeVXtn=zx8t4vau6D5T-qFnA3Vm2AGcFUm2S_PVw@mail.gmail.com>
	<550805ED.30003@oracle.com>
	<CALtZ-o7TCBcFhv3BpjCWQsGoYh+rFbB83sutZa8DXCtB2V16HA@mail.gmail.com>
	<1501970.3n0TOdWmYW@d-allen>
	<CALtZ-o5vvaA0=LrAfaVS2mYqKOfctBhGGQMj4oMckpq-_cg0zQ@mail.gmail.com>
	<550B0245.50606@oracle.com>
	<CALtZ-o4K8i2QrS3OzApai=CCv9oFpbtZDZgN0V8M4cZRy5ocTg@mail.gmail.com>
	<550B1222.3060704@oracle.com> <550B1629.50203@oracle.com>
	<CALtZ-o5ike96VAMeCUM1coqZtyB6OH20qpA-0Dn+qzAJPDgy8A@mail.gmail.com>
	<550B388C.2070808@oracle.com>
	<CALtZ-o7Qe9uMX1G_8Gm0DfkpyFOqC9680azEm1Sss3kXGdoiJA@mail.gmail.com>
	<550C079D.4020407@oracle.com>
	<CALtZ-o7SoNgww5vOsTmcK0Hxo5r2nnGUvby=uSsNve_FLQ0j4Q@mail.gmail.com>
	<550C4E98.3040208@oracle.com>
	<CALtZ-o6MCR3EWG5FRc_LNUn1Znt_251pZHM7NVCaha+pJKM1Dg@mail.gmail.com>
	<550C6C53.6040308@oracle.com>
	<CALtZ-o7zop9RS4RKaVuSwpr+iuadENYOsv49rGVZv_84n0AQLg@mail.gmail.com>
	<55100CAC.9000806@oracle.com>
Message-ID: <CALtZ-o5nbO7puxFObMrvAp6X4dWi01g=2GPArUhEbraO-tcxVA@mail.gmail.com>

So your analogy to IRIW is established by introducing a whole new reading
thread. Such an analogy fails to capture the essence of my scenario: I am
interested precisely in the case where the "reading-writing" thread
observes its own writes in addition to the "timer" thread's writes. The
goal is to analyze the tension between the desire to win performance
through store forwarding and the need to stay sequentially consistent. It
was my impression that the distributed nature of QPI messaging would result
in the processors grabbing more of the liberties allowed by Intel's
specification, which specifically excludes my scenario from the ordering
guarantee. As Aleksey pointed out, this is not the case because an MFENCE
instruction provides a stronger guarantee than that: the coherence layer
will have resolved the value at the stored location before the load
instruction asks for its value.

---
Marko

On Mon, Mar 23, 2015 at 1:53 PM, Oleksandr Otenko <
oleksandr.otenko at oracle.com> wrote:

>  Out of all outcomes IRIW permits, choose those that have the fourth
> thread observe 1 then 0 - ie there exists a thread which observed Wv1 occur
> before T9. Now you are looking at your case with three threads. Your case
> does not add more outcomes, only chooses a subset of those in IRIW.
>
>
> Alex
>
>
> On 20/03/2015 22:05, Marko Topolnik wrote:
>
>  On Fri, Mar 20, 2015 at 7:52 PM, Oleksandr Otenko <
> oleksandr.otenko at oracle.com> wrote:
>
>>  On 20/03/2015 18:12, Marko Topolnik wrote:
>>
>>  On Fri, Mar 20, 2015 at 5:45 PM, Oleksandr Otenko <
>> oleksandr.otenko at oracle.com> wrote:
>>
>>>  No, that doesn't answer the question. You need to modify how
>>> happens-before is built - because happens-before in JMM and in some other
>>> model are two different happens-befores. If you get rid of synchronization
>>> order, then you need to explain which reads the write will or will not
>>> synchronize-with.
>>>
>>
>>  I think it's quite simple: the read may synchronize-with any write as
>> long as that doesn't break happens-before consistency.
>>
>>
>>  It seems quite naive, too. The problem is that currently the read
>> synchronizes-with *all* writes preceding it, but observes the value set
>> by the *last* write. Here you need to define somehow which write the
>> read observes - you need to somehow define which of the writes is "last"
>> and what the other readers are allowed to think about it.
>>
>> It doesn't seem to be explained in one sentence.
>>
>
>  It is a quite lightweight exercise to rigorously specify this in terms
> of Lamport's clocks; but I concede that, lacking a shared intuition, it
> will take more than a sentence to communicate. I hesitate to turn this into
> a treatise on the application of Lamport's clocks to the JMM, so I'm
> letting it rest.
>
>>    I am only involved in this discussion because you said it isn't IRIW,
>>> but I see all signs that it is. I remember the discussion here doubting
>>> that IRIW should be supported, and I appreciate the arguments, but without
>>> the specification it is difficult to continue a meaningful discussion.
>>>
>>
>>  That's strange to hear since I have pointed out exactly why it's not
>> IRIW: if we broaden the definition such that it covers my case, then we
>> must accept that Intel allows IRIW to happen because it explicitly excludes
>> the writing thread from the guarantee which is supposed to disallow it.
>>
>>
>>  Rwt6 and Rrt6 are reduntant. If you remove them, it becomes IRIW. Rwt6
>> only witnesses the particular ordering of some operations in IRIW - it
>> forbids some of the outcomes from IRIW, but doesn't add new ones. Rrt6 is
>> meaningless.
>>
>
>  IRIW involves four independent threads and six events. My example
> involves only three threads, so there must be something wrong in calling it
> "exactly IRIW". Apparently you have in mind some quite flexible definition
> of IRIW, but I cannot second-guess what it might be.
>
>  ---
> Marko
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150323/2dcd9e5a/attachment.html>

From oleksandr.otenko at oracle.com  Mon Mar 23 13:00:52 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Mon, 23 Mar 2015 17:00:52 +0000
Subject: [concurrency-interest] Enforcing total sync order on modern
	hardware
In-Reply-To: <CALtZ-o5nbO7puxFObMrvAp6X4dWi01g=2GPArUhEbraO-tcxVA@mail.gmail.com>
References: <CALtZ-o47wWjeVXtn=zx8t4vau6D5T-qFnA3Vm2AGcFUm2S_PVw@mail.gmail.com>	<550805ED.30003@oracle.com>	<CALtZ-o7TCBcFhv3BpjCWQsGoYh+rFbB83sutZa8DXCtB2V16HA@mail.gmail.com>	<1501970.3n0TOdWmYW@d-allen>	<CALtZ-o5vvaA0=LrAfaVS2mYqKOfctBhGGQMj4oMckpq-_cg0zQ@mail.gmail.com>	<550B0245.50606@oracle.com>	<CALtZ-o4K8i2QrS3OzApai=CCv9oFpbtZDZgN0V8M4cZRy5ocTg@mail.gmail.com>	<550B1222.3060704@oracle.com>	<550B1629.50203@oracle.com>	<CALtZ-o5ike96VAMeCUM1coqZtyB6OH20qpA-0Dn+qzAJPDgy8A@mail.gmail.com>	<550B388C.2070808@oracle.com>	<CALtZ-o7Qe9uMX1G_8Gm0DfkpyFOqC9680azEm1Sss3kXGdoiJA@mail.gmail.com>	<550C079D.4020407@oracle.com>	<CALtZ-o7SoNgww5vOsTmcK0Hxo5r2nnGUvby=uSsNve_FLQ0j4Q@mail.gmail.com>	<550C4E98.3040208@oracle.com>	<CALtZ-o6MCR3EWG5FRc_LNUn1Znt_251pZHM7NVCaha+pJKM1Dg@mail.gmail.com>	<550C6C53.6040308@oracle.com>	<CALtZ-o7zop9RS4RKaVuSwpr+iuadENYOsv49rGVZv_84n0AQLg@mail.gmail.com>	<55100CAC.9000806@oracle.com>
	<CALtZ-o5nbO7puxFObMrvAp6X4dWi01g=2GPArUhEbraO-tcxVA@mail.gmail!
	.com>
Message-ID: <551046C4.7020301@oracle.com>

IRIW results apply to /any/ thread doing the reading. The existence of 
the fourth thread only generalizes the result.

It seems this branch of the conversation is pointless.

Alex

On 23/03/2015 15:56, Marko Topolnik wrote:
> So your analogy to IRIW is established by introducing a whole new 
> reading thread. Such an analogy fails to capture the essence of my 
> scenario: I am interested precisely in the case where the 
> "reading-writing" thread observes its own writes in addition to the 
> "timer" thread's writes. The goal is to analyze the tension between 
> the desire to win performance through store forwarding and the need to 
> stay sequentially consistent. It was my impression that the 
> distributed nature of QPI messaging would result in the processors 
> grabbing more of the liberties allowed by Intel's specification, which 
> specifically excludes my scenario from the ordering guarantee. As 
> Aleksey pointed out, this is not the case because an MFENCE 
> instruction provides a stronger guarantee than that: the coherence 
> layer will have resolved the value at the stored location before the 
> load instruction asks for its value.
>
> ---
> Marko
>
> On Mon, Mar 23, 2015 at 1:53 PM, Oleksandr Otenko 
> <oleksandr.otenko at oracle.com <mailto:oleksandr.otenko at oracle.com>> wrote:
>
>     Out of all outcomes IRIW permits, choose those that have the
>     fourth thread observe 1 then 0 - ie there exists a thread which
>     observed Wv1 occur before T9. Now you are looking at your case
>     with three threads. Your case does not add more outcomes, only
>     chooses a subset of those in IRIW.
>
>
>     Alex
>
>
>     On 20/03/2015 22:05, Marko Topolnik wrote:
>>     On Fri, Mar 20, 2015 at 7:52 PM, Oleksandr Otenko
>>     <oleksandr.otenko at oracle.com
>>     <mailto:oleksandr.otenko at oracle.com>> wrote:
>>
>>         On 20/03/2015 18:12, Marko Topolnik wrote:
>>>         On Fri, Mar 20, 2015 at 5:45 PM, Oleksandr Otenko
>>>         <oleksandr.otenko at oracle.com
>>>         <mailto:oleksandr.otenko at oracle.com>> wrote:
>>>
>>>             No, that doesn't answer the question. You need to modify
>>>             how happens-before is built - because happens-before in
>>>             JMM and in some other model are two different
>>>             happens-befores. If you get rid of synchronization
>>>             order, then you need to explain which reads the write
>>>             will or will not synchronize-with.
>>>
>>>
>>>         I think it's quite simple: the read may synchronize-with any
>>>         write as long as that doesn't break happens-before consistency.
>>
>>         It seems quite naive, too. The problem is that currently the
>>         read synchronizes-with /all/ writes preceding it, but
>>         observes the value set by the /last/ write. Here you need to
>>         define somehow which write the read observes - you need to
>>         somehow define which of the writes is "last" and what the
>>         other readers are allowed to think about it.
>>
>>         It doesn't seem to be explained in one sentence.
>>
>>
>>     It is a quite lightweight exercise to rigorously specify this in
>>     terms of Lamport's clocks; but I concede that, lacking a shared
>>     intuition, it will take more than a sentence to communicate. I
>>     hesitate to turn this into a treatise on the application of
>>     Lamport's clocks to the JMM, so I'm letting it rest.
>>
>>>             I am only involved in this discussion because you said
>>>             it isn't IRIW, but I see all signs that it is. I
>>>             remember the discussion here doubting that IRIW should
>>>             be supported, and I appreciate the arguments, but
>>>             without the specification it is difficult to continue a
>>>             meaningful discussion.
>>>
>>>
>>>         That's strange to hear since I have pointed out exactly why
>>>         it's not IRIW: if we broaden the definition such that it
>>>         covers my case, then we must accept that Intel allows IRIW
>>>         to happen because it explicitly excludes the writing thread
>>>         from the guarantee which is supposed to disallow it.
>>
>>         Rwt6 and Rrt6 are reduntant. If you remove them, it becomes
>>         IRIW. Rwt6 only witnesses the particular ordering of some
>>         operations in IRIW - it forbids some of the outcomes from
>>         IRIW, but doesn't add new ones. Rrt6 is meaningless.
>>
>>
>>     IRIW involves four independent threads and six events. My example
>>     involves only three threads, so there must be something wrong in
>>     calling it "exactly IRIW". Apparently you have in mind some quite
>>     flexible definition of IRIW, but I cannot second-guess what it
>>     might be.
>>
>>     ---
>>     Marko
>
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150323/44f3ab15/attachment.html>

From marko at hazelcast.com  Mon Mar 23 13:42:18 2015
From: marko at hazelcast.com (Marko Topolnik)
Date: Mon, 23 Mar 2015 18:42:18 +0100
Subject: [concurrency-interest] Enforcing total sync order on modern
	hardware
In-Reply-To: <551046C4.7020301@oracle.com>
References: <CALtZ-o47wWjeVXtn=zx8t4vau6D5T-qFnA3Vm2AGcFUm2S_PVw@mail.gmail.com>
	<550805ED.30003@oracle.com>
	<CALtZ-o7TCBcFhv3BpjCWQsGoYh+rFbB83sutZa8DXCtB2V16HA@mail.gmail.com>
	<1501970.3n0TOdWmYW@d-allen>
	<CALtZ-o5vvaA0=LrAfaVS2mYqKOfctBhGGQMj4oMckpq-_cg0zQ@mail.gmail.com>
	<550B0245.50606@oracle.com>
	<CALtZ-o4K8i2QrS3OzApai=CCv9oFpbtZDZgN0V8M4cZRy5ocTg@mail.gmail.com>
	<550B1222.3060704@oracle.com> <550B1629.50203@oracle.com>
	<CALtZ-o5ike96VAMeCUM1coqZtyB6OH20qpA-0Dn+qzAJPDgy8A@mail.gmail.com>
	<550B388C.2070808@oracle.com>
	<CALtZ-o7Qe9uMX1G_8Gm0DfkpyFOqC9680azEm1Sss3kXGdoiJA@mail.gmail.com>
	<550C079D.4020407@oracle.com>
	<CALtZ-o7SoNgww5vOsTmcK0Hxo5r2nnGUvby=uSsNve_FLQ0j4Q@mail.gmail.com>
	<550C4E98.3040208@oracle.com>
	<CALtZ-o6MCR3EWG5FRc_LNUn1Znt_251pZHM7NVCaha+pJKM1Dg@mail.gmail.com>
	<550C6C53.6040308@oracle.com>
	<CALtZ-o7zop9RS4RKaVuSwpr+iuadENYOsv49rGVZv_84n0AQLg@mail.gmail.com>
	<55100CAC.9000806@oracle.com>
	<CALtZ-o5nbO7puxFObMrvAp6X4dWi01g=2GPArUhEbraO-tcxVA@mail.gmail.com>
	<551046C4.7020301@oracle.com>
Message-ID: <CALtZ-o641aNPn35z7df5vL40CS02hGwjrMK+Tx+_EuBzLqxzCA@mail.gmail.com>

On Mar 23, 2015 6:01 PM, "Oleksandr Otenko" <oleksandr.otenko at oracle.com>
wrote:
>
> IRIW results apply to any thread doing the reading. The existence of the
fourth thread only generalizes the result.

This is incorrect: the essential property of IRIW is that it constructs a
result for which no sequentially consistent explanation exists. It is the
minimal example to reproduce the issue of interest, therefore none of its
parts is optional.

>
> It seems this branch of the conversation is pointless.
>
> Alex
>
>
> On 23/03/2015 15:56, Marko Topolnik wrote:
>>
>> So your analogy to IRIW is established by introducing a whole new
reading thread. Such an analogy fails to capture the essence of my
scenario: I am interested precisely in the case where the "reading-writing"
thread observes its own writes in addition to the "timer" thread's writes.
The goal is to analyze the tension between the desire to win performance
through store forwarding and the need to stay sequentially consistent. It
was my impression that the distributed nature of QPI messaging would result
in the processors grabbing more of the liberties allowed by Intel's
specification, which specifically excludes my scenario from the ordering
guarantee. As Aleksey pointed out, this is not the case because an MFENCE
instruction provides a stronger guarantee than that: the coherence layer
will have resolved the value at the stored location before the load
instruction asks for its value.
>>
>> ---
>> Marko
>>
>> On Mon, Mar 23, 2015 at 1:53 PM, Oleksandr Otenko <
oleksandr.otenko at oracle.com> wrote:
>>>
>>> Out of all outcomes IRIW permits, choose those that have the fourth
thread observe 1 then 0 - ie there exists a thread which observed Wv1 occur
before T9. Now you are looking at your case with three threads. Your case
does not add more outcomes, only chooses a subset of those in IRIW.
>>>
>>>
>>> Alex
>>>
>>>
>>> On 20/03/2015 22:05, Marko Topolnik wrote:
>>>>
>>>> On Fri, Mar 20, 2015 at 7:52 PM, Oleksandr Otenko <
oleksandr.otenko at oracle.com> wrote:
>>>>>
>>>>> On 20/03/2015 18:12, Marko Topolnik wrote:
>>>>>>
>>>>>> On Fri, Mar 20, 2015 at 5:45 PM, Oleksandr Otenko <
oleksandr.otenko at oracle.com> wrote:
>>>>>>>
>>>>>>> No, that doesn't answer the question. You need to modify how
happens-before is built - because happens-before in JMM and in some other
model are two different happens-befores. If you get rid of synchronization
order, then you need to explain which reads the write will or will not
synchronize-with.
>>>>>>
>>>>>>
>>>>>> I think it's quite simple: the read may synchronize-with any write
as long as that doesn't break happens-before consistency.
>>>>>
>>>>>
>>>>> It seems quite naive, too. The problem is that currently the read
synchronizes-with all writes preceding it, but observes the value set by
the last write. Here you need to define somehow which write the read
observes - you need to somehow define which of the writes is "last" and
what the other readers are allowed to think about it.
>>>>>
>>>>> It doesn't seem to be explained in one sentence.
>>>>
>>>>
>>>> It is a quite lightweight exercise to rigorously specify this in terms
of Lamport's clocks; but I concede that, lacking a shared intuition, it
will take more than a sentence to communicate. I hesitate to turn this into
a treatise on the application of Lamport's clocks to the JMM, so I'm
letting it rest.
>>>>>>>
>>>>>>> I am only involved in this discussion because you said it isn't
IRIW, but I see all signs that it is. I remember the discussion here
doubting that IRIW should be supported, and I appreciate the arguments, but
without the specification it is difficult to continue a meaningful
discussion.
>>>>>>
>>>>>>
>>>>>> That's strange to hear since I have pointed out exactly why it's not
IRIW: if we broaden the definition such that it covers my case, then we
must accept that Intel allows IRIW to happen because it explicitly excludes
the writing thread from the guarantee which is supposed to disallow it.
>>>>>
>>>>>
>>>>> Rwt6 and Rrt6 are reduntant. If you remove them, it becomes IRIW.
Rwt6 only witnesses the particular ordering of some operations in IRIW - it
forbids some of the outcomes from IRIW, but doesn't add new ones. Rrt6 is
meaningless.
>>>>
>>>>
>>>> IRIW involves four independent threads and six events. My example
involves only three threads, so there must be something wrong in calling it
"exactly IRIW". Apparently you have in mind some quite flexible definition
of IRIW, but I cannot second-guess what it might be.
>>>>
>>>> ---
>>>> Marko
>>>
>>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150323/76b313b2/attachment-0001.html>

From mike at axiak.net  Mon Mar 23 15:11:30 2015
From: mike at axiak.net (Mike Axiak)
Date: Mon, 23 Mar 2015 15:11:30 -0400
Subject: [concurrency-interest] Getting a list of results from a list of
	CompletableFutures
Message-ID: <CAKUW7qHF-Pygfe0c2M1M7hcJfFTXO7vh-Q7c6mrax2ExG5x+xw@mail.gmail.com>

Hi,

I love the CompletableFuture API, and I understand the signature for
anyOf(), but I don't quite understand why allOf()'s interface is so
limited. Am I missing something, or could the following work:

    public static <T> CompletableFuture<List<T>>
allOf(Collection<CompletableFuture<? extends T>> futures) {
      CompletableFuture<List<T>> result = new CompletableFuture<>();
      CompletableFuture<Void> impl =
CompletableFuture.allOf(futures.toArray(new
CompletableFuture[futures.size()]));
      impl.thenAccept((v) -> {
        List<Exception> exceptions =
futures.stream().filter(CompletableFuture::isCompletedExceptionally).map((future)
-> {
          try {
            future.getNow(null);
          } catch (Exception e) {
            return e;
          }
          return null;
        }).filter(Objects::nonNull).collect(Collectors.toList());
          if (exceptions.isEmpty()) {
          result.complete(futures.stream().map((future) -> {
            try {
              return future.get();
            } catch (InterruptedException | ExecutionException e) {
              throw new RuntimeException(e);
            }
          }).collect(Collectors.toList()));
        } else {
          impl.completeExceptionally(new MultiFutureException(exceptions));
        }
      });
      return result;
    }
        public static class MultiFutureException extends Exception {
      private final List<Exception> exceptionsCauses;
        public MultiFutureException(List<Exception> exceptionsCauses) {
        this.exceptionsCauses = exceptionsCauses;
      }
    }

Best,
Mike
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150323/fcd4c84a/attachment.html>

From oleksandr.otenko at oracle.com  Tue Mar 24 07:42:10 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Tue, 24 Mar 2015 11:42:10 +0000
Subject: [concurrency-interest] Enforcing total sync order on modern
	hardware
In-Reply-To: <CALtZ-o641aNPn35z7df5vL40CS02hGwjrMK+Tx+_EuBzLqxzCA@mail.gmail.com>
References: <CALtZ-o47wWjeVXtn=zx8t4vau6D5T-qFnA3Vm2AGcFUm2S_PVw@mail.gmail.com>	<1501970.3n0TOdWmYW@d-allen>	<CALtZ-o5vvaA0=LrAfaVS2mYqKOfctBhGGQMj4oMckpq-_cg0zQ@mail.gmail.com>	<550B0245.50606@oracle.com>	<CALtZ-o4K8i2QrS3OzApai=CCv9oFpbtZDZgN0V8M4cZRy5ocTg@mail.gmail.com>	<550B1222.3060704@oracle.com>	<550B1629.50203@oracle.com>	<CALtZ-o5ike96VAMeCUM1coqZtyB6OH20qpA-0Dn+qzAJPDgy8A@mail.gmail.com>	<550B388C.2070808@oracle.com>	<CALtZ-o7Qe9uMX1G_8Gm0DfkpyFOqC9680azEm1Sss3kXGdoiJA@mail.gmail.com>	<550C079D.4020407@oracle.com>	<CALtZ-o7SoNgww5vOsTmcK0Hxo5r2nnGUvby=uSsNve_FLQ0j4Q@mail.gmail.com>	<550C4E98.3040208@oracle.com>	<CALtZ-o6MCR3EWG5FRc_LNUn1Znt_251pZHM7NVCaha+pJKM1Dg@mail.gmail.com>	<550C6C53.6040308@oracle.com>	<CALtZ-o7zop9RS4RKaVuSwpr+iuadENYOsv49rGVZv_84n0AQLg@mail.gmail.com>	<55100CAC.9000806@oracle.com>	<CALtZ-o5nbO7puxFObMrvAp6X4dWi01g=2GPArUhEbraO-tcxVA@mail.gmail.com>	<551046C4.7020301@oracle.com>
	<CALtZ-o641aNPn35z7df5vL40CS02hGwjrMK+Tx+_EuBzLqxzCA@mail.gma!
	il.com>
Message-ID: <55114D92.7010605@oracle.com>

17.4.3
> A set of actions is /sequentially consistent/ if all actions occur in 
> a total order ...(plus details of how the total order relates to 
> program order etc)

It looks like your objection goes against this. IRIW works because it 
must have the writes observed by all threads in the same order - due to 
the writes and reads forming a total order, even if they are independent 
- which is the cornerstone of /sequential consistency/.

I don't know how you measure that the example is minimal, because in 
some sense it is also maximal - the minimal case being Dekker's 
invariant with two threads writing one and reading the other variable.

T1: x=1;
r0=x; // may fuse with x=1 - then you get the canonical form of the example
r1=y;

T2: y=1;
r2=y; // may fuse with y=1
r3=x;

Alex

On 23/03/2015 17:42, Marko Topolnik wrote:
>
>
> On Mar 23, 2015 6:01 PM, "Oleksandr Otenko" 
> <oleksandr.otenko at oracle.com <mailto:oleksandr.otenko at oracle.com>> wrote:
> >
> > IRIW results apply to any thread doing the reading. The existence of 
> the fourth thread only generalizes the result.
>
> This is incorrect: the essential property of IRIW is that it 
> constructs a result for which no sequentially consistent explanation 
> exists. It is the minimal example to reproduce the issue of interest, 
> therefore none of its parts is optional.
>
> >
> > It seems this branch of the conversation is pointless.
> >
> > Alex
> >
> >
> > On 23/03/2015 15:56, Marko Topolnik wrote:
> >>
> >> So your analogy to IRIW is established by introducing a whole new 
> reading thread. Such an analogy fails to capture the essence of my 
> scenario: I am interested precisely in the case where the 
> "reading-writing" thread observes its own writes in addition to the 
> "timer" thread's writes. The goal is to analyze the tension between 
> the desire to win performance through store forwarding and the need to 
> stay sequentially consistent. It was my impression that the 
> distributed nature of QPI messaging would result in the processors 
> grabbing more of the liberties allowed by Intel's specification, which 
> specifically excludes my scenario from the ordering guarantee. As 
> Aleksey pointed out, this is not the case because an MFENCE 
> instruction provides a stronger guarantee than that: the coherence 
> layer will have resolved the value at the stored location before the 
> load instruction asks for its value.
> >>
> >> ---
> >> Marko
> >>
> >> On Mon, Mar 23, 2015 at 1:53 PM, Oleksandr Otenko 
> <oleksandr.otenko at oracle.com <mailto:oleksandr.otenko at oracle.com>> wrote:
> >>>
> >>> Out of all outcomes IRIW permits, choose those that have the 
> fourth thread observe 1 then 0 - ie there exists a thread which 
> observed Wv1 occur before T9. Now you are looking at your case with 
> three threads. Your case does not add more outcomes, only chooses a 
> subset of those in IRIW.
> >>>
> >>>
> >>> Alex
> >>>
> >>>
> >>> On 20/03/2015 22:05, Marko Topolnik wrote:
> >>>>
> >>>> On Fri, Mar 20, 2015 at 7:52 PM, Oleksandr Otenko 
> <oleksandr.otenko at oracle.com <mailto:oleksandr.otenko at oracle.com>> wrote:
> >>>>>
> >>>>> On 20/03/2015 18:12, Marko Topolnik wrote:
> >>>>>>
> >>>>>> On Fri, Mar 20, 2015 at 5:45 PM, Oleksandr Otenko 
> <oleksandr.otenko at oracle.com <mailto:oleksandr.otenko at oracle.com>> wrote:
> >>>>>>>
> >>>>>>> No, that doesn't answer the question. You need to modify how 
> happens-before is built - because happens-before in JMM and in some 
> other model are two different happens-befores. If you get rid of 
> synchronization order, then you need to explain which reads the write 
> will or will not synchronize-with.
> >>>>>>
> >>>>>>
> >>>>>> I think it's quite simple: the read may synchronize-with any 
> write as long as that doesn't break happens-before consistency.
> >>>>>
> >>>>>
> >>>>> It seems quite naive, too. The problem is that currently the 
> read synchronizes-with all writes preceding it, but observes the value 
> set by the last write. Here you need to define somehow which write the 
> read observes - you need to somehow define which of the writes is 
> "last" and what the other readers are allowed to think about it.
> >>>>>
> >>>>> It doesn't seem to be explained in one sentence.
> >>>>
> >>>>
> >>>> It is a quite lightweight exercise to rigorously specify this in 
> terms of Lamport's clocks; but I concede that, lacking a shared 
> intuition, it will take more than a sentence to communicate. I 
> hesitate to turn this into a treatise on the application of Lamport's 
> clocks to the JMM, so I'm letting it rest.
> >>>>>>>
> >>>>>>> I am only involved in this discussion because you said it 
> isn't IRIW, but I see all signs that it is. I remember the discussion 
> here doubting that IRIW should be supported, and I appreciate the 
> arguments, but without the specification it is difficult to continue a 
> meaningful discussion.
> >>>>>>
> >>>>>>
> >>>>>> That's strange to hear since I have pointed out exactly why 
> it's not IRIW: if we broaden the definition such that it covers my 
> case, then we must accept that Intel allows IRIW to happen because it 
> explicitly excludes the writing thread from the guarantee which is 
> supposed to disallow it.
> >>>>>
> >>>>>
> >>>>> Rwt6 and Rrt6 are reduntant. If you remove them, it becomes 
> IRIW. Rwt6 only witnesses the particular ordering of some operations 
> in IRIW - it forbids some of the outcomes from IRIW, but doesn't add 
> new ones. Rrt6 is meaningless.
> >>>>
> >>>>
> >>>> IRIW involves four independent threads and six events. My example 
> involves only three threads, so there must be something wrong in 
> calling it "exactly IRIW". Apparently you have in mind some quite 
> flexible definition of IRIW, but I cannot second-guess what it might be.
> >>>>
> >>>> ---
> >>>> Marko
> >>>
> >>>
> >>
> >
>

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150324/a782e27b/attachment-0001.html>

From TEREKHOV at de.ibm.com  Tue Mar 24 08:30:35 2015
From: TEREKHOV at de.ibm.com (Alexander Terekhov)
Date: Tue, 24 Mar 2015 13:30:35 +0100
Subject: [concurrency-interest] Enforcing total sync order on
	modern	hardware
In-Reply-To: <55114D92.7010605@oracle.com>
References: <CALtZ-o47wWjeVXtn=zx8t4vau6D5T-qFnA3Vm2AGcFUm2S_PVw@mail.gmail.com>	<CALtZ-o5vvaA0=LrAfaVS2mYqKOfctBhGGQMj4oMckpq-_cg0zQ@mail.gmail.com>
	<550B0245.50606@oracle.com>	<CALtZ-o4K8i2QrS3OzApai=CCv9oFpbtZDZgN0V8M4cZRy5ocTg@mail.gmail.com>
	<550B1222.3060704@oracle.com>	<550B1629.50203@oracle.com>	<CALtZ-o5ike96VAMeCUM1coqZtyB6OH20qpA-0Dn+qzAJPDgy8A@mail.gmail.com>
	<550B388C.2070808@oracle.com>	<CALtZ-o7Qe9uMX1G_8Gm0DfkpyFOqC9680azEm1Sss3kXGdoiJA@mail.gmail.com>
	<550C079D.4020407@oracle.com>	<CALtZ-o7SoNgww5vOsTmcK0Hxo5r2nnGUvby=uSsNve_FLQ0j4Q@mail.gmail.com>
	<550C4E98.3040208@oracle.com>	<CALtZ-o6MCR3EWG5FRc_LNUn1Znt_251pZHM7NVCaha+pJKM1Dg@mail.gmail.com>
	<550C6C53.6040308@oracle.com>	<CALtZ-o7zop9RS4RKaVuSwpr+iuadENYOsv49rGVZv_84n0AQLg@mail.gmail.com>
	<55100CAC.9 <55114D92.7010605@oracle.com>
Message-ID: <OFDA2D560D.9BC3824A-ONC1257E12.0043C10D-C1257E12.0044B96F@de.ibm.com>

IRIW is about TSO, not SC (which is TSO + store-load barrier). And TSO is
release-acquire consistency + write atomicity.

> the cornerstone of sequential consistency

is awfully expensive store-load barrier.

PMFJI


Oleksandr Otenko <oleksandr.otenko at oracle.com>@cs.oswego.edu on 24.03.2015
12:42:10

Sent by:	concurrency-interest-bounces at cs.oswego.edu


To:	Marko Topolnik <marko at hazelcast.com>, concurrency-interest
       <Concurrency-interest at cs.oswego.edu>
cc:
Subject:	Re: [concurrency-interest] Enforcing total sync order on modern
       hardware


17.4.3
      A set of actions is sequentially consistent if all actions occur in a
      total order ...(plus details of how the total order relates to
      program order etc)

It looks like your objection goes against this. IRIW works because it must
have the writes observed by all threads in the same order - due to the
writes and reads forming a total order, even if they are independent -
which is the cornerstone of sequential consistency.

I don't know how you measure that the example is minimal, because in some
sense it is also maximal - the minimal case being Dekker's invariant with
two threads writing one and reading the other variable.

T1: x=1;
r0=x; // may fuse with x=1 - then you get the canonical form of the example
r1=y;

T2: y=1;
r2=y; // may fuse with y=1
r3=x;

Alex

On 23/03/2015 17:42, Marko Topolnik wrote:



      On Mar 23, 2015 6:01 PM, "Oleksandr Otenko" <
      oleksandr.otenko at oracle.com> wrote:
      >
      > IRIW results apply to any thread doing the reading. The existence
      of the fourth thread only generalizes the result.


      This is incorrect: the essential property of IRIW is that it
      constructs a result for which no sequentially consistent explanation
      exists. It is the minimal example to reproduce the issue of interest,
      therefore none of its parts is optional.


      >
      > It seems this branch of the conversation is pointless.
      >
      > Alex
      >
      >
      > On 23/03/2015 15:56, Marko Topolnik wrote:
      >>
      >> So your analogy to IRIW is established by introducing a whole new
      reading thread. Such an analogy fails to capture the essence of my
      scenario: I am interested precisely in the case where the
      "reading-writing" thread observes its own writes in addition to the
      "timer" thread's writes. The goal is to analyze the tension between
      the desire to win performance through store forwarding and the need
      to stay sequentially consistent. It was my impression that the
      distributed nature of QPI messaging would result in the processors
      grabbing more of the liberties allowed by Intel's specification,
      which specifically excludes my scenario from the ordering guarantee.
      As Aleksey pointed out, this is not the case because an MFENCE
      instruction provides a stronger guarantee than that: the coherence
      layer will have resolved the value at the stored location before the
      load instruction asks for its value.
      >>
      >> ---
      >> Marko
      >>
      >> On Mon, Mar 23, 2015 at 1:53 PM, Oleksandr Otenko <
      oleksandr.otenko at oracle.com> wrote:
      >>>
      >>> Out of all outcomes IRIW permits, choose those that have the
      fourth thread observe 1 then 0 - ie there exists a thread which
      observed Wv1 occur before T9. Now you are looking at your case with
      three threads. Your case does not add more outcomes, only chooses a
      subset of those in IRIW.
      >>>
      >>>
      >>> Alex
      >>>
      >>>
      >>> On 20/03/2015 22:05, Marko Topolnik wrote:
      >>>>
      >>>> On Fri, Mar 20, 2015 at 7:52 PM, Oleksandr Otenko <
      oleksandr.otenko at oracle.com> wrote:
      >>>>>
      >>>>> On 20/03/2015 18:12, Marko Topolnik wrote:
      >>>>>>
      >>>>>> On Fri, Mar 20, 2015 at 5:45 PM, Oleksandr Otenko <
      oleksandr.otenko at oracle.com> wrote:
      >>>>>>>
      >>>>>>> No, that doesn't answer the question. You need to modify how
      happens-before is built - because happens-before in JMM and in some
      other model are two different happens-befores. If you get rid of
      synchronization order, then you need to explain which reads the write
      will or will not synchronize-with.
      >>>>>>
      >>>>>>
      >>>>>> I think it's quite simple: the read may synchronize-with any
      write as long as that doesn't break happens-before consistency.
      >>>>>
      >>>>>
      >>>>> It seems quite naive, too. The problem is that currently the
      read synchronizes-with all writes preceding it, but observes the
      value set by the last write. Here you need to define somehow which
      write the read observes - you need to somehow define which of the
      writes is "last" and what the other readers are allowed to think
      about it.
      >>>>>
      >>>>> It doesn't seem to be explained in one sentence.
      >>>>
      >>>>
      >>>> It is a quite lightweight exercise to rigorously specify this in
      terms of Lamport's clocks; but I concede that, lacking a shared
      intuition, it will take more than a sentence to communicate. I
      hesitate to turn this into a treatise on the application of Lamport's
      clocks to the JMM, so I'm letting it rest.
      >>>>>>>
      >>>>>>> I am only involved in this discussion because you said it
      isn't IRIW, but I see all signs that it is. I remember the discussion
      here doubting that IRIW should be supported, and I appreciate the
      arguments, but without the specification it is difficult to continue
      a meaningful discussion.
      >>>>>>
      >>>>>>
      >>>>>> That's strange to hear since I have pointed out exactly why
      it's not IRIW: if we broaden the definition such that it covers my
      case, then we must accept that Intel allows IRIW to happen because it
      explicitly excludes the writing thread from the guarantee which is
      supposed to disallow it.
      >>>>>
      >>>>>
      >>>>> Rwt6 and Rrt6 are reduntant. If you remove them, it becomes
      IRIW. Rwt6 only witnesses the particular ordering of some operations
      in IRIW - it forbids some of the outcomes from IRIW, but doesn't add
      new ones. Rrt6 is meaningless.
      >>>>
      >>>>
      >>>> IRIW involves four independent threads and six events. My
      example involves only three threads, so there must be something wrong
      in calling it "exactly IRIW". Apparently you have in mind some quite
      flexible definition of IRIW, but I cannot second-guess what it might
      be.
      >>>>
      >>>> ---
      >>>> Marko
      >>>
      >>>
      >>
      >


_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest




From marko at hazelcast.com  Tue Mar 24 08:46:58 2015
From: marko at hazelcast.com (Marko Topolnik)
Date: Tue, 24 Mar 2015 13:46:58 +0100
Subject: [concurrency-interest] Enforcing total sync order on modern
	hardware
In-Reply-To: <OFDA2D560D.9BC3824A-ONC1257E12.0043C10D-C1257E12.0044B96F@de.ibm.com>
References: <CALtZ-o47wWjeVXtn=zx8t4vau6D5T-qFnA3Vm2AGcFUm2S_PVw@mail.gmail.com>
	<CALtZ-o5vvaA0=LrAfaVS2mYqKOfctBhGGQMj4oMckpq-_cg0zQ@mail.gmail.com>
	<550B0245.50606@oracle.com>
	<CALtZ-o4K8i2QrS3OzApai=CCv9oFpbtZDZgN0V8M4cZRy5ocTg@mail.gmail.com>
	<550B1222.3060704@oracle.com> <550B1629.50203@oracle.com>
	<CALtZ-o5ike96VAMeCUM1coqZtyB6OH20qpA-0Dn+qzAJPDgy8A@mail.gmail.com>
	<550B388C.2070808@oracle.com>
	<CALtZ-o7Qe9uMX1G_8Gm0DfkpyFOqC9680azEm1Sss3kXGdoiJA@mail.gmail.com>
	<550C079D.4020407@oracle.com>
	<CALtZ-o7SoNgww5vOsTmcK0Hxo5r2nnGUvby=uSsNve_FLQ0j4Q@mail.gmail.com>
	<550C4E98.3040208@oracle.com>
	<CALtZ-o6MCR3EWG5FRc_LNUn1Znt_251pZHM7NVCaha+pJKM1Dg@mail.gmail.com>
	<550C6C53.6040308@oracle.com>
	<CALtZ-o7zop9RS4RKaVuSwpr+iuadENYOsv49rGVZv_84n0AQLg@mail.gmail.com>
	<55114D92.7010605@oracle.com>
	<OFDA2D560D.9BC3824A-ONC1257E12.0043C10D-C1257E12.0044B96F@de.ibm.com>
Message-ID: <CALtZ-o7Fq-u5rs3ccYoA3TjBSW0F8p3YYdcm=OzzM2gadH7+nQ@mail.gmail.com>

On Tue, Mar 24, 2015 at 1:30 PM, Alexander Terekhov <TEREKHOV at de.ibm.com>
wrote:

> IRIW is about TSO, not SC (which is TSO + store-load barrier). And TSO is
> release-acquire consistency + write atomicity.
>

That's a good observation---I wasn't sure if TSO implied an ordering across
the entire multiprocessor system, or an ordering of each individual
processor's stores. If it is generally understood that it means the former,
then it's all IRIW needs to be safe. However, IRIW specifically avoids my
scenario where one of the reading processors is also the writer an thus
excluded from the TSO guarantees as provided by Intel, for example.

---
Marko
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150324/d97b9d1e/attachment.html>

From oleksandr.otenko at oracle.com  Tue Mar 24 08:47:14 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Tue, 24 Mar 2015 12:47:14 +0000
Subject: [concurrency-interest] Enforcing total sync order on modern
	hardware
In-Reply-To: <OFDA2D560D.9BC3824A-ONC1257E12.0043C10D-C1257E12.0044B96F@de.ibm.com>
References: <CALtZ-o47wWjeVXtn=zx8t4vau6D5T-qFnA3Vm2AGcFUm2S_PVw@mail.gmail.com>	<CALtZ-o5vvaA0=LrAfaVS2mYqKOfctBhGGQMj4oMckpq-_cg0zQ@mail.gmail.com>	<550B0245.50606@oracle.com>	<CALtZ-o4K8i2QrS3OzApai=CCv9oFpbtZDZgN0V8M4cZRy5ocTg@mail.gmail.com>	<550B1222.3060704@oracle.com>	<550B1629.50203@oracle.com>	<CALtZ-o5ike96VAMeCUM1coqZtyB6OH20qpA-0Dn+qzAJPDgy8A@mail.gmail.com>	<550B388C.2070808@oracle.com>	<CALtZ-o7Qe9uMX1G_8Gm0DfkpyFOqC9680azEm1Sss3kXGdoiJA@mail.gmail.com>	<550C079D.4020407@oracle.com>	<CALtZ-o7SoNgww5vOsTmcK0Hxo5r2nnGUvby=uSsNve_FLQ0j4Q@mail.gmail.com>	<550C4E98.3040208@oracle.com>	<CALtZ-o6MCR3EWG5FRc_LNUn1Znt_251pZHM7NVCaha+pJKM1Dg@mail.gmail.com>	<550C6C53.6040308@oracle.com>	<CALtZ-o7zop9RS4RKaVuSwpr+iuadENYOsv49rGVZv_84n0AQLg@mail.gmail.com>	<55100CAC.9
	<55114D92.7010605@oracle.com>
	<OFDA2D560D.9BC3824A-ONC1257E12.0043C10D-C1257E12.0044B96F@de.ibm.com>
Message-ID: <55115CD2.3000208@oracle.com>

I don't know why you call IRIW a test of TSO, if total order of reads is 
also important. Don't forget the load-load barrier, which is implicit on 
x86, and a small theorem about what happens when you don't order 
concurrent reads totally.

Alex

On 24/03/2015 12:30, Alexander Terekhov wrote:
> IRIW is about TSO, not SC (which is TSO + store-load barrier). And TSO is
> release-acquire consistency + write atomicity.
>
>> the cornerstone of sequential consistency
> is awfully expensive store-load barrier.
>
> PMFJI
>
>
> Oleksandr Otenko <oleksandr.otenko at oracle.com>@cs.oswego.edu on 24.03.2015
> 12:42:10
>
> Sent by:	concurrency-interest-bounces at cs.oswego.edu
>
>
> To:	Marko Topolnik <marko at hazelcast.com>, concurrency-interest
>         <Concurrency-interest at cs.oswego.edu>
> cc:
> Subject:	Re: [concurrency-interest] Enforcing total sync order on modern
>         hardware
>
>
> 17.4.3
>        A set of actions is sequentially consistent if all actions occur in a
>        total order ...(plus details of how the total order relates to
>        program order etc)
>
> It looks like your objection goes against this. IRIW works because it must
> have the writes observed by all threads in the same order - due to the
> writes and reads forming a total order, even if they are independent -
> which is the cornerstone of sequential consistency.
>
> I don't know how you measure that the example is minimal, because in some
> sense it is also maximal - the minimal case being Dekker's invariant with
> two threads writing one and reading the other variable.
>
> T1: x=1;
> r0=x; // may fuse with x=1 - then you get the canonical form of the example
> r1=y;
>
> T2: y=1;
> r2=y; // may fuse with y=1
> r3=x;
>
> Alex
>
> On 23/03/2015 17:42, Marko Topolnik wrote:
>
>
>
>        On Mar 23, 2015 6:01 PM, "Oleksandr Otenko" <
>        oleksandr.otenko at oracle.com> wrote:
>        >
>        > IRIW results apply to any thread doing the reading. The existence
>        of the fourth thread only generalizes the result.
>
>
>        This is incorrect: the essential property of IRIW is that it
>        constructs a result for which no sequentially consistent explanation
>        exists. It is the minimal example to reproduce the issue of interest,
>        therefore none of its parts is optional.
>
>
>        >
>        > It seems this branch of the conversation is pointless.
>        >
>        > Alex
>        >
>        >
>        > On 23/03/2015 15:56, Marko Topolnik wrote:
>        >>
>        >> So your analogy to IRIW is established by introducing a whole new
>        reading thread. Such an analogy fails to capture the essence of my
>        scenario: I am interested precisely in the case where the
>        "reading-writing" thread observes its own writes in addition to the
>        "timer" thread's writes. The goal is to analyze the tension between
>        the desire to win performance through store forwarding and the need
>        to stay sequentially consistent. It was my impression that the
>        distributed nature of QPI messaging would result in the processors
>        grabbing more of the liberties allowed by Intel's specification,
>        which specifically excludes my scenario from the ordering guarantee.
>        As Aleksey pointed out, this is not the case because an MFENCE
>        instruction provides a stronger guarantee than that: the coherence
>        layer will have resolved the value at the stored location before the
>        load instruction asks for its value.
>        >>
>        >> ---
>        >> Marko
>        >>
>        >> On Mon, Mar 23, 2015 at 1:53 PM, Oleksandr Otenko <
>        oleksandr.otenko at oracle.com> wrote:
>        >>>
>        >>> Out of all outcomes IRIW permits, choose those that have the
>        fourth thread observe 1 then 0 - ie there exists a thread which
>        observed Wv1 occur before T9. Now you are looking at your case with
>        three threads. Your case does not add more outcomes, only chooses a
>        subset of those in IRIW.
>        >>>
>        >>>
>        >>> Alex
>        >>>
>        >>>
>        >>> On 20/03/2015 22:05, Marko Topolnik wrote:
>        >>>>
>        >>>> On Fri, Mar 20, 2015 at 7:52 PM, Oleksandr Otenko <
>        oleksandr.otenko at oracle.com> wrote:
>        >>>>>
>        >>>>> On 20/03/2015 18:12, Marko Topolnik wrote:
>        >>>>>>
>        >>>>>> On Fri, Mar 20, 2015 at 5:45 PM, Oleksandr Otenko <
>        oleksandr.otenko at oracle.com> wrote:
>        >>>>>>>
>        >>>>>>> No, that doesn't answer the question. You need to modify how
>        happens-before is built - because happens-before in JMM and in some
>        other model are two different happens-befores. If you get rid of
>        synchronization order, then you need to explain which reads the write
>        will or will not synchronize-with.
>        >>>>>>
>        >>>>>>
>        >>>>>> I think it's quite simple: the read may synchronize-with any
>        write as long as that doesn't break happens-before consistency.
>        >>>>>
>        >>>>>
>        >>>>> It seems quite naive, too. The problem is that currently the
>        read synchronizes-with all writes preceding it, but observes the
>        value set by the last write. Here you need to define somehow which
>        write the read observes - you need to somehow define which of the
>        writes is "last" and what the other readers are allowed to think
>        about it.
>        >>>>>
>        >>>>> It doesn't seem to be explained in one sentence.
>        >>>>
>        >>>>
>        >>>> It is a quite lightweight exercise to rigorously specify this in
>        terms of Lamport's clocks; but I concede that, lacking a shared
>        intuition, it will take more than a sentence to communicate. I
>        hesitate to turn this into a treatise on the application of Lamport's
>        clocks to the JMM, so I'm letting it rest.
>        >>>>>>>
>        >>>>>>> I am only involved in this discussion because you said it
>        isn't IRIW, but I see all signs that it is. I remember the discussion
>        here doubting that IRIW should be supported, and I appreciate the
>        arguments, but without the specification it is difficult to continue
>        a meaningful discussion.
>        >>>>>>
>        >>>>>>
>        >>>>>> That's strange to hear since I have pointed out exactly why
>        it's not IRIW: if we broaden the definition such that it covers my
>        case, then we must accept that Intel allows IRIW to happen because it
>        explicitly excludes the writing thread from the guarantee which is
>        supposed to disallow it.
>        >>>>>
>        >>>>>
>        >>>>> Rwt6 and Rrt6 are reduntant. If you remove them, it becomes
>        IRIW. Rwt6 only witnesses the particular ordering of some operations
>        in IRIW - it forbids some of the outcomes from IRIW, but doesn't add
>        new ones. Rrt6 is meaningless.
>        >>>>
>        >>>>
>        >>>> IRIW involves four independent threads and six events. My
>        example involves only three threads, so there must be something wrong
>        in calling it "exactly IRIW". Apparently you have in mind some quite
>        flexible definition of IRIW, but I cannot second-guess what it might
>        be.
>        >>>>
>        >>>> ---
>        >>>> Marko
>        >>>
>        >>>
>        >>
>        >
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


From marko at hazelcast.com  Tue Mar 24 08:54:03 2015
From: marko at hazelcast.com (Marko Topolnik)
Date: Tue, 24 Mar 2015 13:54:03 +0100
Subject: [concurrency-interest] Enforcing total sync order on modern
	hardware
In-Reply-To: <55115CD2.3000208@oracle.com>
References: <CALtZ-o47wWjeVXtn=zx8t4vau6D5T-qFnA3Vm2AGcFUm2S_PVw@mail.gmail.com>
	<CALtZ-o5vvaA0=LrAfaVS2mYqKOfctBhGGQMj4oMckpq-_cg0zQ@mail.gmail.com>
	<550B0245.50606@oracle.com>
	<CALtZ-o4K8i2QrS3OzApai=CCv9oFpbtZDZgN0V8M4cZRy5ocTg@mail.gmail.com>
	<550B1222.3060704@oracle.com> <550B1629.50203@oracle.com>
	<CALtZ-o5ike96VAMeCUM1coqZtyB6OH20qpA-0Dn+qzAJPDgy8A@mail.gmail.com>
	<550B388C.2070808@oracle.com>
	<CALtZ-o7Qe9uMX1G_8Gm0DfkpyFOqC9680azEm1Sss3kXGdoiJA@mail.gmail.com>
	<550C079D.4020407@oracle.com>
	<CALtZ-o7SoNgww5vOsTmcK0Hxo5r2nnGUvby=uSsNve_FLQ0j4Q@mail.gmail.com>
	<550C4E98.3040208@oracle.com>
	<CALtZ-o6MCR3EWG5FRc_LNUn1Znt_251pZHM7NVCaha+pJKM1Dg@mail.gmail.com>
	<550C6C53.6040308@oracle.com>
	<CALtZ-o7zop9RS4RKaVuSwpr+iuadENYOsv49rGVZv_84n0AQLg@mail.gmail.com>
	<55114D92.7010605@oracle.com>
	<OFDA2D560D.9BC3824A-ONC1257E12.0043C10D-C1257E12.0044B96F@de.ibm.com>
	<55115CD2.3000208@oracle.com>
Message-ID: <CALtZ-o5xyMOHbg4Y5tyFd+-P=cqezP==5_2fcSB4jc1qut6aPw@mail.gmail.com>

On Tue, Mar 24, 2015 at 1:47 PM, Oleksandr Otenko <
oleksandr.otenko at oracle.com> wrote:

> I don't know why you call IRIW a test of TSO, if total order of reads is
> also important. Don't forget the load-load barrier, which is implicit on
> x86, and a small theorem about what happens when you don't order concurrent
> reads totally.
>

But note that you still don't need a total _global_ order of reads for
IRIW, it just has to be preserved locally for each processor. Dekker's
idiom exercises this more strongly because there has to be global ordering
between stores and loads by different processors.

---
Marko
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150324/b490fa39/attachment-0001.html>

From TEREKHOV at de.ibm.com  Tue Mar 24 08:54:59 2015
From: TEREKHOV at de.ibm.com (Alexander Terekhov)
Date: Tue, 24 Mar 2015 13:54:59 +0100
Subject: [concurrency-interest] Enforcing total sync order on modern
	hardware
In-Reply-To: <55115CD2.3000208@oracle.com>
References: <CALtZ-o47wWjeVXtn=zx8t4vau6D5T-qFnA3Vm2AGcFUm2S_PVw@mail.gmail.com>	<550B0245.50606@oracle.com>
	<CALtZ-o4K8i2QrS3OzApai=CCv9oFpbtZDZgN0V8M4cZRy5ocTg@mail.gmail.com>	<550B1222.3060704@oracle.com>
	<550B1629.50203@oracle.com>	<CALtZ-o5ike96VAMeCUM1coqZtyB6OH20qpA-0Dn+qzAJPDgy8A@mail.gmail.com>
	<550B388C.2070808@oracle.com>	<CALtZ-o7Qe9uMX1G_8Gm0DfkpyFOqC9680azEm1Sss3kXGdoiJA@mail.gmail.com>
	<550C079D.4020407@oracle.com>	<CALtZ-o7SoNgww5vOsTmcK0Hxo5r2nnGUvby=uSsNve_FLQ0j4Q@mail.gmail.com>
	<550C4E98.3040208@oracle.com>	<CALtZ-o6MCR3EWG5FRc_LNUn1Znt_251pZHM7NVCaha+pJKM1Dg@mail.gmail.com>
	<550C6C53.6040308@oracle.com>	<CALtZ-o7zop9RS4RKaVuSwpr+iuadENYOsv49rGVZv_84n0AQLg@mail.gmail.com>
	<55100CAC.9 <55114D92.7010605@oracle.com <55115CD2.3000208@oracle.com>
Message-ID: <OF3A93C71E.ED213406-ONC1257E12.0046EACA-C1257E12.0046F528@de.ibm.com>

http://lists.boost.org/Archives/boost/2011/08/185134.php

Oleksandr Otenko <oleksandr.otenko at oracle.com> on 24.03.2015 13:47:14

To:	Alexander Terekhov/Germany/IBM at IBMDE
cc:	Marko Topolnik <marko at hazelcast.com>, concurrency-interest
       <Concurrency-interest at cs.oswego.edu>
Subject:	Re: [concurrency-interest] Enforcing total sync order on modern
       hardware


I don't know why you call IRIW a test of TSO, if total order of reads is
also important. Don't forget the load-load barrier, which is implicit on
x86, and a small theorem about what happens when you don't order
concurrent reads totally.

Alex

On 24/03/2015 12:30, Alexander Terekhov wrote:
> IRIW is about TSO, not SC (which is TSO + store-load barrier). And TSO is
> release-acquire consistency + write atomicity.
>
>> the cornerstone of sequential consistency
> is awfully expensive store-load barrier.
>
> PMFJI
>
>
> Oleksandr Otenko <oleksandr.otenko at oracle.com>@cs.oswego.edu on
24.03.2015
> 12:42:10
>
> Sent by:		 concurrency-interest-bounces at cs.oswego.edu
>
>
> To:		 Marko Topolnik <marko at hazelcast.com>, concurrency-interest
>         <Concurrency-interest at cs.oswego.edu>
> cc:
> Subject:		 Re: [concurrency-interest] Enforcing total sync order on
modern
>         hardware
>
>
> 17.4.3
>        A set of actions is sequentially consistent if all actions occur
in a
>        total order ...(plus details of how the total order relates to
>        program order etc)
>
> It looks like your objection goes against this. IRIW works because it
must
> have the writes observed by all threads in the same order - due to the
> writes and reads forming a total order, even if they are independent -
> which is the cornerstone of sequential consistency.
>
> I don't know how you measure that the example is minimal, because in some
> sense it is also maximal - the minimal case being Dekker's invariant with
> two threads writing one and reading the other variable.
>
> T1: x=1;
> r0=x; // may fuse with x=1 - then you get the canonical form of the
example
> r1=y;
>
> T2: y=1;
> r2=y; // may fuse with y=1
> r3=x;
>
> Alex
>
> On 23/03/2015 17:42, Marko Topolnik wrote:
>
>
>
>        On Mar 23, 2015 6:01 PM, "Oleksandr Otenko" <
>        oleksandr.otenko at oracle.com> wrote:
>        >
>        > IRIW results apply to any thread doing the reading. The
existence
>        of the fourth thread only generalizes the result.
>
>
>        This is incorrect: the essential property of IRIW is that it
>        constructs a result for which no sequentially consistent
explanation
>        exists. It is the minimal example to reproduce the issue of
interest,
>        therefore none of its parts is optional.
>
>
>        >
>        > It seems this branch of the conversation is pointless.
>        >
>        > Alex
>        >
>        >
>        > On 23/03/2015 15:56, Marko Topolnik wrote:
>        >>
>        >> So your analogy to IRIW is established by introducing a whole
new
>        reading thread. Such an analogy fails to capture the essence of my
>        scenario: I am interested precisely in the case where the
>        "reading-writing" thread observes its own writes in addition to
the
>        "timer" thread's writes. The goal is to analyze the tension
between
>        the desire to win performance through store forwarding and the
need
>        to stay sequentially consistent. It was my impression that the
>        distributed nature of QPI messaging would result in the processors
>        grabbing more of the liberties allowed by Intel's specification,
>        which specifically excludes my scenario from the ordering
guarantee.
>        As Aleksey pointed out, this is not the case because an MFENCE
>        instruction provides a stronger guarantee than that: the coherence
>        layer will have resolved the value at the stored location before
the
>        load instruction asks for its value.
>        >>
>        >> ---
>        >> Marko
>        >>
>        >> On Mon, Mar 23, 2015 at 1:53 PM, Oleksandr Otenko <
>        oleksandr.otenko at oracle.com> wrote:
>        >>>
>        >>> Out of all outcomes IRIW permits, choose those that have the
>        fourth thread observe 1 then 0 - ie there exists a thread which
>        observed Wv1 occur before T9. Now you are looking at your case
with
>        three threads. Your case does not add more outcomes, only chooses
a
>        subset of those in IRIW.
>        >>>
>        >>>
>        >>> Alex
>        >>>
>        >>>
>        >>> On 20/03/2015 22:05, Marko Topolnik wrote:
>        >>>>
>        >>>> On Fri, Mar 20, 2015 at 7:52 PM, Oleksandr Otenko <
>        oleksandr.otenko at oracle.com> wrote:
>        >>>>>
>        >>>>> On 20/03/2015 18:12, Marko Topolnik wrote:
>        >>>>>>
>        >>>>>> On Fri, Mar 20, 2015 at 5:45 PM, Oleksandr Otenko <
>        oleksandr.otenko at oracle.com> wrote:
>        >>>>>>>
>        >>>>>>> No, that doesn't answer the question. You need to modify
how
>        happens-before is built - because happens-before in JMM and in
some
>        other model are two different happens-befores. If you get rid of
>        synchronization order, then you need to explain which reads the
write
>        will or will not synchronize-with.
>        >>>>>>
>        >>>>>>
>        >>>>>> I think it's quite simple: the read may synchronize-with
any
>        write as long as that doesn't break happens-before consistency.
>        >>>>>
>        >>>>>
>        >>>>> It seems quite naive, too. The problem is that currently the
>        read synchronizes-with all writes preceding it, but observes the
>        value set by the last write. Here you need to define somehow which
>        write the read observes - you need to somehow define which of the
>        writes is "last" and what the other readers are allowed to think
>        about it.
>        >>>>>
>        >>>>> It doesn't seem to be explained in one sentence.
>        >>>>
>        >>>>
>        >>>> It is a quite lightweight exercise to rigorously specify this
in
>        terms of Lamport's clocks; but I concede that, lacking a shared
>        intuition, it will take more than a sentence to communicate. I
>        hesitate to turn this into a treatise on the application of
Lamport's
>        clocks to the JMM, so I'm letting it rest.
>        >>>>>>>
>        >>>>>>> I am only involved in this discussion because you said it
>        isn't IRIW, but I see all signs that it is. I remember the
discussion
>        here doubting that IRIW should be supported, and I appreciate the
>        arguments, but without the specification it is difficult to
continue
>        a meaningful discussion.
>        >>>>>>
>        >>>>>>
>        >>>>>> That's strange to hear since I have pointed out exactly why
>        it's not IRIW: if we broaden the definition such that it covers my
>        case, then we must accept that Intel allows IRIW to happen because
it
>        explicitly excludes the writing thread from the guarantee which is
>        supposed to disallow it.
>        >>>>>
>        >>>>>
>        >>>>> Rwt6 and Rrt6 are reduntant. If you remove them, it becomes
>        IRIW. Rwt6 only witnesses the particular ordering of some
operations
>        in IRIW - it forbids some of the outcomes from IRIW, but doesn't
add
>        new ones. Rrt6 is meaningless.
>        >>>>
>        >>>>
>        >>>> IRIW involves four independent threads and six events. My
>        example involves only three threads, so there must be something
wrong
>        in calling it "exactly IRIW". Apparently you have in mind some
quite
>        flexible definition of IRIW, but I cannot second-guess what it
might
>        be.
>        >>>>
>        >>>> ---
>        >>>> Marko
>        >>>
>        >>>
>        >>
>        >
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>




From marko at hazelcast.com  Tue Mar 24 09:02:03 2015
From: marko at hazelcast.com (Marko Topolnik)
Date: Tue, 24 Mar 2015 14:02:03 +0100
Subject: [concurrency-interest] Enforcing total sync order on modern
	hardware
In-Reply-To: <OF3A93C71E.ED213406-ONC1257E12.0046EACA-C1257E12.0046F528@de.ibm.com>
References: <CALtZ-o47wWjeVXtn=zx8t4vau6D5T-qFnA3Vm2AGcFUm2S_PVw@mail.gmail.com>
	<550B0245.50606@oracle.com>
	<CALtZ-o4K8i2QrS3OzApai=CCv9oFpbtZDZgN0V8M4cZRy5ocTg@mail.gmail.com>
	<550B1222.3060704@oracle.com> <550B1629.50203@oracle.com>
	<CALtZ-o5ike96VAMeCUM1coqZtyB6OH20qpA-0Dn+qzAJPDgy8A@mail.gmail.com>
	<550B388C.2070808@oracle.com>
	<CALtZ-o7Qe9uMX1G_8Gm0DfkpyFOqC9680azEm1Sss3kXGdoiJA@mail.gmail.com>
	<550C079D.4020407@oracle.com>
	<CALtZ-o7SoNgww5vOsTmcK0Hxo5r2nnGUvby=uSsNve_FLQ0j4Q@mail.gmail.com>
	<550C4E98.3040208@oracle.com>
	<CALtZ-o6MCR3EWG5FRc_LNUn1Znt_251pZHM7NVCaha+pJKM1Dg@mail.gmail.com>
	<550C6C53.6040308@oracle.com>
	<CALtZ-o7zop9RS4RKaVuSwpr+iuadENYOsv49rGVZv_84n0AQLg@mail.gmail.com>
	<55115CD2.3000208@oracle.com>
	<OF3A93C71E.ED213406-ONC1257E12.0046EACA-C1257E12.0046F528@de.ibm.com>
Message-ID: <CALtZ-o4AO9jdodquge3SrEsxhgtpjeRFSdbAF_Mf+iNf+BrY+g@mail.gmail.com>

On Tue, Mar 24, 2015 at 1:54 PM, Alexander Terekhov <TEREKHOV at de.ibm.com>
wrote:

> http://lists.boost.org/Archives/boost/2011/08/185134.php
>
> A == 1 && B == 0 && C == 1 && D == 0 is possible without remote write
> atomicity.


It is also possible if (A = X; B = Y;) can be reordered into (B = Y; A =
X;), even if the writes were in order. That's what Oleksandr was getting
at, I believe.

---
Marko
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150324/0ee1ecbe/attachment.html>

From TEREKHOV at de.ibm.com  Tue Mar 24 09:26:49 2015
From: TEREKHOV at de.ibm.com (Alexander Terekhov)
Date: Tue, 24 Mar 2015 14:26:49 +0100
Subject: [concurrency-interest] Enforcing total sync order on modern
	hardware
In-Reply-To: <CALtZ-o4AO9jdodquge3SrEsxhgtpjeRFSdbAF_Mf+iNf+BrY+g@mail.gmail.com>
References: <CALtZ-o47wWjeVXtn=zx8t4vau6D5T-qFnA3Vm2AGcFUm2S_PVw@mail.gmail.com>	<550B1222.3060704@oracle.com>
	<550B1629.50203@oracle.com>	<CALtZ-o5ike96VAMeCUM1coqZtyB6OH20qpA-0Dn+qzAJPDgy8A@mail.gmail.com>
	<550B388C.2070808@oracle.com>	<CALtZ-o7Qe9uMX1G_8Gm0DfkpyFOqC9680azEm1Sss3kXGdoiJA@mail.gmail.com>
	<550C079D.4020407@oracle.com>	<CALtZ-o7SoNgww5vOsTmcK0Hxo5r2nnGUvby=uSsNve_FLQ0j4Q@mail.gmail.com>
	<550C4E98.3040208@oracle.com>	<CALtZ-o6MCR3EWG5FRc_LNUn1Znt_251pZHM7NVCaha+pJKM1Dg@mail.gmail.com>
	<550C6C53.6040308@oracle.com>	<CALtZ-o7zop9RS4RKaVuSwpr+iuadENYOsv49rGVZv_84n0AQLg@mail.gmail.com>
	<55115CD2.3000208@oracle.com>	<OF3A93C71E.ED213406-ONC1257E12.0046EACA-C1257E12.0046F528@de.ibm.com>
	<CALtZ-o4AO9jdodquge3SrEsxhgtpjeRFSdbAF_Mf+iNf+BrY+g@mail.gmail.com>
Message-ID: <OF3D4C19AE.726C50FC-ONC1257E12.004823BD-C1257E12.0049DF3C@de.ibm.com>

"no local reordering" (load-load barrier or .acq or even more
precise .hoist-load-barrier*** label for loads) was implied.

***) means unidirectional .acq barrier not affecting subsequent stores -
think of a read-write lock in read-only mode (with read-dependent writes
ordered by dependencies just like on all existing archs AFAIK).

Marko Topolnik <marko at hazelcast.com> on 24.03.2015 14:02:03

To:	Alexander Terekhov/Germany/IBM at IBMDE
cc:	Oleksandr Otenko <oleksandr.otenko at oracle.com>, concurrency-interest
       <Concurrency-interest at cs.oswego.edu>
Subject:	Re: [concurrency-interest] Enforcing total sync order on modern
       hardware


On Tue, Mar 24, 2015 at 1:54 PM, Alexander Terekhov <TEREKHOV at de.ibm.com>
wrote:
      http://lists.boost.org/Archives/boost/2011/08/185134.php

      A == 1 && B == 0 && C == 1 && D == 0 is possible without remote
      write
      atomicity.

It is also possible if (A = X; B = Y;)?can be reordered into (B = Y; A =
X;), even if the writes were in order. That's what Oleksandr was getting
at, I believe.

---
Marko



From oleksandr.otenko at oracle.com  Tue Mar 24 10:32:07 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Tue, 24 Mar 2015 14:32:07 +0000
Subject: [concurrency-interest] Enforcing total sync order on modern
	hardware
In-Reply-To: <CALtZ-o5xyMOHbg4Y5tyFd+-P=cqezP==5_2fcSB4jc1qut6aPw@mail.gmail.com>
References: <CALtZ-o47wWjeVXtn=zx8t4vau6D5T-qFnA3Vm2AGcFUm2S_PVw@mail.gmail.com>	<CALtZ-o5vvaA0=LrAfaVS2mYqKOfctBhGGQMj4oMckpq-_cg0zQ@mail.gmail.com>	<550B0245.50606@oracle.com>	<CALtZ-o4K8i2QrS3OzApai=CCv9oFpbtZDZgN0V8M4cZRy5ocTg@mail.gmail.com>	<550B1222.3060704@oracle.com>	<550B1629.50203@oracle.com>	<CALtZ-o5ike96VAMeCUM1coqZtyB6OH20qpA-0Dn+qzAJPDgy8A@mail.gmail.com>	<550B388C.2070808@oracle.com>	<CALtZ-o7Qe9uMX1G_8Gm0DfkpyFOqC9680azEm1Sss3kXGdoiJA@mail.gmail.com>	<550C079D.4020407@oracle.com>	<CALtZ-o7SoNgww5vOsTmcK0Hxo5r2nnGUvby=uSsNve_FLQ0j4Q@mail.gmail.com>	<550C4E98.3040208@oracle.com>	<CALtZ-o6MCR3EWG5FRc_LNUn1Znt_251pZHM7NVCaha+pJKM1Dg@mail.gmail.com>	<550C6C53.6040308@oracle.com>	<CALtZ-o7zop9RS4RKaVuSwpr+iuadENYOsv49rGVZv_84n0AQLg@mail.gmail.com>	<55114D92.7010605@oracle.com>	<OFDA2D560D.9BC3824A-ONC1257E12.0043C10D-C1257E12.0044B96F@de.ibm.com>	<55115CD2.3000208@oracle.com>
	<CALtZ-o5xyMOHbg4Y5tyFd+-P=cqezP==5_2fcSB4jc1qut6aPw@mail.gmail.com>
Message-ID: <55117567.4070809@oracle.com>

On 24/03/2015 12:54, Marko Topolnik wrote:
> On Tue, Mar 24, 2015 at 1:47 PM, Oleksandr Otenko 
> <oleksandr.otenko at oracle.com <mailto:oleksandr.otenko at oracle.com>> wrote:
>
>     I don't know why you call IRIW a test of TSO, if total order of
>     reads is also important. Don't forget the load-load barrier, which
>     is implicit on x86, and a small theorem about what happens when
>     you don't order concurrent reads totally.
>
>
> But note that you still don't need a total _global_ order of reads for 
> IRIW, it just has to be preserved locally for each processor.
Some, but not all, reorderings of concurrent reads are indistinguishable 
from a certain fixed global total order of reads. What does this give 
you apart from freedom of implementation?


> Dekker's idiom exercises this more strongly because there has to be 
> global ordering between stores and loads by different processors.

Global ordering in Dekker's idiom you are talking about is also present 
in IRIW. Dekker's idiom specifies more in program order:

T1:
x=1;
r0=x;  // program order po(r0=x, x=1); in IRIW it is in synchronization 
order, so may be so(r0=x, x=1) or the other way around
r1=y;  // "global order" between r1=y and y=1 executed by different 
processors in Dekker's idiom is also present in IRIW

T2:
y=1;
r2=y;
r3=x;

The outcome r1==0, r3==0 is disallowed through the same reasoning in 
Dekker's idiom as in IRIW: since r1==0, y=1 must not happen before r1=y; 
then r3=x, being after y=1 in program order, must also not precede r1=y, 
and transitively cannot precede x=1. You need this step that from "Y 
must not precede X" follows that "X happens-before Y" - but that's what 
makes the order total.

In IRIW you have the same chain of reasoning. Even if you get rid of 
synchronization order, but keep synchronizes-with and transitive 
closure, then from observing r0==1 and r2==1 you still have the same 
edges. The key here still is the conclusion that since r1==0, y=1 must 
not happen before r1=y, which is the only part that requires total 
ordering - cannot be partially ordered. But it is exactly the same bit 
that cannot be partially ordered in Dekker's idiom: r1=y happens-before 
y=1 /because/ */otherwise/* r1=y /should/ observe 1. If you don't have 
mutual exclusion in ordering r1=y and y=1 in Dekker's idiom, why would 
the outcome r1==0, r3==0 be forbidden?


I've seen others criticizing IRIW as being not realistic, but can you 
support Dekker's idiom (which I reckon /is/ realistic) without proving 
IRIW at the same time? I wonder how that's possible. Or what gets 
forsaken. Transitive closure goes? Ok, I see that happening; but then it 
is not about total order as such.

Alex

>
> ---
> Marko

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150324/8b2f0bbf/attachment-0001.html>

From TEREKHOV at de.ibm.com  Tue Mar 24 11:42:50 2015
From: TEREKHOV at de.ibm.com (Alexander Terekhov)
Date: Tue, 24 Mar 2015 16:42:50 +0100
Subject: [concurrency-interest] Enforcing total sync order on
	modern	hardware
In-Reply-To: <55117567.4070809@oracle.com>
References: <CALtZ-o47wWjeVXtn=zx8t4vau6D5T-qFnA3Vm2AGcFUm2S_PVw@mail.gmail.com>	<550B0245.50606@oracle.com>
	<CALtZ-o4K8i2QrS3OzApai=CCv9oFpbtZDZgN0V8M4cZRy5ocTg@mail.gmail.com>	<550B1222.3060704@oracle.com>
	<550B1629.50203@oracle.com>	<CALtZ-o5ike96VAMeCUM1coqZtyB6OH20qpA-0Dn+qzAJPDgy8A@mail.gmail.com>
	<550B388C.2070808@oracle.com>	<CALtZ-o7Qe9uMX1G_8Gm0DfkpyFOqC9680azEm1Sss3kXGdoiJA@mail.gmail.com>
	<550C079D.4020407@oracle.com>	<CALtZ-o7SoNgww5vOsTmcK0Hxo5r2nnGUvby=uSsNve_FLQ0j4Q@mail.gmail.com>
	<550C4E98.3040208@oracle.com>	<CALtZ-o6MCR3EWG5FRc_LNUn1Znt_251pZHM7NVCaha+pJKM1Dg@mail.gmail.com>
	<550C6C53.6040308@oracle.com>	<CALtZ-o7zop9RS4RKaVuSwpr+iuadENYOsv49rGVZv_84n0AQLg@mail.gmail.com>
	<55114D92.7010605@oracle.com>	<OFDA2D560 <55117567.4070809@oracle.com>
Message-ID: <OF46F097B9.BA1B93D1-ONC1257E12.0055A21A-C1257E12.00565344@de.ibm.com>

Dekker is about store-load barrier and not about write atomicity (there is
only one observer of writes).

IRIW is about write atomicity and not about store-load barrier.

Oleksandr Otenko <oleksandr.otenko at oracle.com>@cs.oswego.edu on 24.03.2015
15:32:07

Sent by:	concurrency-interest-bounces at cs.oswego.edu


To:	Marko Topolnik <marko at hazelcast.com>
cc:	concurrency-interest <Concurrency-interest at cs.oswego.edu>, Alexander
       Terekhov/Germany/IBM at IBMDE
Subject:	Re: [concurrency-interest] Enforcing total sync order on modern
       hardware


On 24/03/2015 12:54, Marko Topolnik wrote:
      On Tue, Mar 24, 2015 at 1:47 PM, Oleksandr Otenko <
      oleksandr.otenko at oracle.com> wrote:
            I don't know why you call IRIW a test of TSO, if total order of
            reads is also important. Don't forget the load-load barrier,
            which is implicit on x86, and a small theorem about what
            happens when you don't order concurrent reads totally.

      But note that you still don't need a total _global_ order of reads
      for IRIW, it just has to be preserved locally for each processor.
Some, but not all, reorderings of concurrent reads are indistinguishable
from a certain fixed global total order of reads. What does this give you
apart from freedom of implementation?


      Dekker's idiom exercises this more strongly because there has to be
      global ordering between stores and loads by different processors.

Global ordering in Dekker's idiom you are talking about is also present in
IRIW. Dekker's idiom specifies more in program order:

T1:
x=1;
r0=x;? // program order po(r0=x, x=1); in IRIW it is in synchronization
order, so may be so(r0=x, x=1) or the other way around
r1=y;? // "global order" between r1=y and y=1 executed by different
processors in Dekker's idiom is also present in IRIW

T2:
y=1;
r2=y;
r3=x;

The outcome r1==0, r3==0 is disallowed through the same reasoning in
Dekker's idiom as in IRIW: since r1==0, y=1 must not happen before r1=y;
then r3=x, being after y=1 in program order, must also not precede r1=y,
and transitively cannot precede x=1. You need this step that from "Y must
not precede X" follows that "X happens-before Y" - but that's what makes
the order total.

In IRIW you have the same chain of reasoning. Even if you get rid of
synchronization order, but keep synchronizes-with and transitive closure,
then from observing r0==1 and r2==1 you still have the same edges. The key
here still is the conclusion that since r1==0, y=1 must not happen before
r1=y, which is the only part that requires total ordering - cannot be
partially ordered. But it is exactly the same bit that cannot be partially
ordered in Dekker's idiom: r1=y happens-before y=1 because otherwise r1=y
should observe 1. If you don't have mutual exclusion in ordering r1=y and
y=1 in Dekker's idiom, why would the outcome r1==0, r3==0 be forbidden?


I've seen others criticizing IRIW as being not realistic, but can you
support Dekker's idiom (which I reckon is realistic) without proving IRIW
at the same time? I wonder how that's possible. Or what gets forsaken.
Transitive closure goes? Ok, I see that happening; but then it is not about
total order as such.

Alex


      ---
      Marko
_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest




From oleksandr.otenko at oracle.com  Tue Mar 24 11:48:24 2015
From: oleksandr.otenko at oracle.com (Oleksandr Otenko)
Date: Tue, 24 Mar 2015 15:48:24 +0000
Subject: [concurrency-interest] Enforcing total sync order on modern
	hardware
In-Reply-To: <55117567.4070809@oracle.com>
References: <CALtZ-o47wWjeVXtn=zx8t4vau6D5T-qFnA3Vm2AGcFUm2S_PVw@mail.gmail.com>	<550B0245.50606@oracle.com>	<CALtZ-o4K8i2QrS3OzApai=CCv9oFpbtZDZgN0V8M4cZRy5ocTg@mail.gmail.com>	<550B1222.3060704@oracle.com>	<550B1629.50203@oracle.com>	<CALtZ-o5ike96VAMeCUM1coqZtyB6OH20qpA-0Dn+qzAJPDgy8A@mail.gmail.com>	<550B388C.2070808@oracle.com>	<CALtZ-o7Qe9uMX1G_8Gm0DfkpyFOqC9680azEm1Sss3kXGdoiJA@mail.gmail.com>	<550C079D.4020407@oracle.com>	<CALtZ-o7SoNgww5vOsTmcK0Hxo5r2nnGUvby=uSsNve_FLQ0j4Q@mail.gmail.com>	<550C4E98.3040208@oracle.com>	<CALtZ-o6MCR3EWG5FRc_LNUn1Znt_251pZHM7NVCaha+pJKM1Dg@mail.gmail.com>	<550C6C53.6040308@oracle.com>	<CALtZ-o7zop9RS4RKaVuSwpr+iuadENYOsv49rGVZv_84n0AQLg@mail.gmail.com>	<55114D92.7010605@oracle.com>	<OFDA2D560D.9BC3824A-ONC1257E12.0043C10D-C1257E12.0044B96F@de.ibm.com>	<55115CD2.3000208@oracle.com>	<CALtZ-o5xyMOHbg4Y5tyFd+-P=cqezP==5_2fcSB4jc1qut6aPw@mail.gmail.com>
	<55117567.4070809@oracle.com>
Message-ID: <55118748.2010109@oracle.com>

I must add that of course the totality of order between r1=y and y=1 is 
not the totality of order of all reads and writes - it is only the 
representation of atomicity of reads and writes of the same variable, 
which is reasonable to assume in any system.

What I stress on, is that IRIW results do /not/ require the order of 
/all/ reads and writes to be total - IRIW results can be proven from 
just /transitive closure/ of program order and synchronizes-with (even 
if you choose to construct the latter in some way different from JMM - 
not invoking total synchronization order).

Alex

On 24/03/2015 14:32, Oleksandr Otenko wrote:
> On 24/03/2015 12:54, Marko Topolnik wrote:
>> On Tue, Mar 24, 2015 at 1:47 PM, Oleksandr Otenko 
>> <oleksandr.otenko at oracle.com <mailto:oleksandr.otenko at oracle.com>> wrote:
>>
>>     I don't know why you call IRIW a test of TSO, if total order of
>>     reads is also important. Don't forget the load-load barrier,
>>     which is implicit on x86, and a small theorem about what happens
>>     when you don't order concurrent reads totally.
>>
>>
>> But note that you still don't need a total _global_ order of reads 
>> for IRIW, it just has to be preserved locally for each processor.
> Some, but not all, reorderings of concurrent reads are 
> indistinguishable from a certain fixed global total order of reads. 
> What does this give you apart from freedom of implementation?
>
>
>> Dekker's idiom exercises this more strongly because there has to be 
>> global ordering between stores and loads by different processors.
>
> Global ordering in Dekker's idiom you are talking about is also 
> present in IRIW. Dekker's idiom specifies more in program order:
>
> T1:
> x=1;
> r0=x;  // program order po(r0=x, x=1); in IRIW it is in 
> synchronization order, so may be so(r0=x, x=1) or the other way around
> r1=y;  // "global order" between r1=y and y=1 executed by different 
> processors in Dekker's idiom is also present in IRIW
>
> T2:
> y=1;
> r2=y;
> r3=x;
>
> The outcome r1==0, r3==0 is disallowed through the same reasoning in 
> Dekker's idiom as in IRIW: since r1==0, y=1 must not happen before 
> r1=y; then r3=x, being after y=1 in program order, must also not 
> precede r1=y, and transitively cannot precede x=1. You need this step 
> that from "Y must not precede X" follows that "X happens-before Y" - 
> but that's what makes the order total.
>
> In IRIW you have the same chain of reasoning. Even if you get rid of 
> synchronization order, but keep synchronizes-with and transitive 
> closure, then from observing r0==1 and r2==1 you still have the same 
> edges. The key here still is the conclusion that since r1==0, y=1 must 
> not happen before r1=y, which is the only part that requires total 
> ordering - cannot be partially ordered. But it is exactly the same bit 
> that cannot be partially ordered in Dekker's idiom: r1=y 
> happens-before y=1 /because/ */otherwise/* r1=y /should/ observe 1. If 
> you don't have mutual exclusion in ordering r1=y and y=1 in Dekker's 
> idiom, why would the outcome r1==0, r3==0 be forbidden?
>
>
> I've seen others criticizing IRIW as being not realistic, but can you 
> support Dekker's idiom (which I reckon /is/ realistic) without proving 
> IRIW at the same time? I wonder how that's possible. Or what gets 
> forsaken. Transitive closure goes? Ok, I see that happening; but then 
> it is not about total order as such.
>
> Alex
>
>>
>> ---
>> Marko
>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150324/445c5dee/attachment.html>

From marko at hazelcast.com  Tue Mar 24 13:44:16 2015
From: marko at hazelcast.com (Marko Topolnik)
Date: Tue, 24 Mar 2015 18:44:16 +0100
Subject: [concurrency-interest] Enforcing total sync order on modern
	hardware
In-Reply-To: <55117567.4070809@oracle.com>
References: <CALtZ-o47wWjeVXtn=zx8t4vau6D5T-qFnA3Vm2AGcFUm2S_PVw@mail.gmail.com>
	<CALtZ-o5vvaA0=LrAfaVS2mYqKOfctBhGGQMj4oMckpq-_cg0zQ@mail.gmail.com>
	<550B0245.50606@oracle.com>
	<CALtZ-o4K8i2QrS3OzApai=CCv9oFpbtZDZgN0V8M4cZRy5ocTg@mail.gmail.com>
	<550B1222.3060704@oracle.com> <550B1629.50203@oracle.com>
	<CALtZ-o5ike96VAMeCUM1coqZtyB6OH20qpA-0Dn+qzAJPDgy8A@mail.gmail.com>
	<550B388C.2070808@oracle.com>
	<CALtZ-o7Qe9uMX1G_8Gm0DfkpyFOqC9680azEm1Sss3kXGdoiJA@mail.gmail.com>
	<550C079D.4020407@oracle.com>
	<CALtZ-o7SoNgww5vOsTmcK0Hxo5r2nnGUvby=uSsNve_FLQ0j4Q@mail.gmail.com>
	<550C4E98.3040208@oracle.com>
	<CALtZ-o6MCR3EWG5FRc_LNUn1Znt_251pZHM7NVCaha+pJKM1Dg@mail.gmail.com>
	<550C6C53.6040308@oracle.com>
	<CALtZ-o7zop9RS4RKaVuSwpr+iuadENYOsv49rGVZv_84n0AQLg@mail.gmail.com>
	<55114D92.7010605@oracle.com>
	<OFDA2D560D.9BC3824A-ONC1257E12.0043C10D-C1257E12.0044B96F@de.ibm.com>
	<55115CD2.3000208@oracle.com>
	<CALtZ-o5xyMOHbg4Y5tyFd+-P=cqezP==5_2fcSB4jc1qut6aPw@mail.gmail.com>
	<55117567.4070809@oracle.com>
Message-ID: <CALtZ-o6uJNWBdRXT_2_DCKv+p5yx=_sDBLE0pdGVkC85fM_VHQ@mail.gmail.com>

On Tue, Mar 24, 2015 at 3:32 PM, Oleksandr Otenko <
oleksandr.otenko at oracle.com> wrote:

> The outcome r1==0, r3==0 is disallowed through the same reasoning in
> Dekker's idiom as in IRIW: since r1==0, y=1 must not happen before r1=y;
> then r3=x, being after y=1 in program order, must also not precede r1=y,
> and transitively cannot precede x=1. You need this step that from "Y must
> not precede X" follows that "X happens-before Y" - but that's what makes
> the order total.
>
> In IRIW you have the same chain of reasoning. Even if you get rid of
> synchronization order, but keep synchronizes-with and transitive closure,
> then from observing r0==1 and r2==1 you still have the same edges. The key
> here still is the conclusion that since r1==0, y=1 must not happen before
> r1=y, which is the only part that requires total ordering - cannot be
> partially ordered. But it is exactly the same bit that cannot be partially
> ordered in Dekker's idiom: r1=y happens-before y=1 *because* *otherwise*
> r1=y *should* observe 1. If you don't have mutual exclusion in ordering
> r1=y and y=1 in Dekker's idiom, why would the outcome r1==0, r3==0 be
> forbidden?
>

This the IRIW (1,0,1,0) result in a happens-before diagram:

T4: x=0 --> y=0 ----> r2=y --> r3=x
                   /--^
T3: x=0 --> y=0 --/-----> r0=x --> r1=y
                 /    /---^
T2: y=1 --------/    /
                    /
T1: x=1 -----------/

(note that I made the setting of initial values in T3 and T4 explicit)

As you can witness, there are no violations of the union of
synchronizes-with and program order ("transitive closure", even though that
term actually applies to something else). Therefore IRIW only violates
sequential consistency and does not violate what I assume under the term
"happens-before consistency".

BTW as for your earlier observation that IRIW is not minimal because Dekker
is smaller, note that the "issue of interest" for IRIW is the sequentially
inconsistent observation of mutually independent stores, as observed by
mutually independent loads. Here "mutually independent" means "done by
separate, independent threads". Clearly, IRIW is exactly the minimal
scenario for that.

---
Marko
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150324/8f76a680/attachment.html>

From TEREKHOV at de.ibm.com  Tue Mar 24 14:15:28 2015
From: TEREKHOV at de.ibm.com (Alexander Terekhov)
Date: Tue, 24 Mar 2015 19:15:28 +0100
Subject: [concurrency-interest] Enforcing total sync order on modern
	hardware
In-Reply-To: <CALtZ-o6uJNWBdRXT_2_DCKv+p5yx=_sDBLE0pdGVkC85fM_VHQ@mail.gmail.com>
References: <CALtZ-o47wWjeVXtn=zx8t4vau6D5T-qFnA3Vm2AGcFUm2S_PVw@mail.gmail.com>	<550B0245.50606@oracle.com>
	<CALtZ-o4K8i2QrS3OzApai=CCv9oFpbtZDZgN0V8M4cZRy5ocTg@mail.gmail.com>	<550B1222.3060704@oracle.com>
	<550B1629.50203@oracle.com>	<CALtZ-o5ike96VAMeCUM1coqZtyB6OH20qpA-0Dn+qzAJPDgy8A@mail.gmail.com>
	<550B388C.2070808@oracle.com>	<CALtZ-o7Qe9uMX1G_8Gm0DfkpyFOqC9680azEm1Sss3kXGdoiJA@mail.gmail.com>
	<550C079D.4020407@oracle.com>	<CALtZ-o7SoNgww5vOsTmcK0Hxo5r2nnGUvby=uSsNve_FLQ0j4Q@mail.gmail.com>
	<550C4E98.3040208@oracle.com>	<CALtZ-o6MCR3EWG5FRc_LNUn1Znt_251pZHM7NVCaha+pJKM1Dg@mail.gmail.com>
	<550C6C53.6040308@oracle.com>	<CALtZ-o7zop9RS4RKaVuSwpr+iuadENYOsv49rGVZv_84n0AQLg@mail.gmail.com>
	<55114D92.7010605@oracle.com>	<OFDA2D560
	<CALtZ-o6uJNWBdRXT_2_DCKv+p5yx=_sDBLE0pdGVkC85fM_VHQ@mail.gmail.com>
Message-ID: <OF56501874.6484FD81-ONC1257E12.00634692-C1257E12.00644C94@de.ibm.com>

again, IRIW is about write atomicity... minimal scenario for that needs
only three threads:

T1: X = 1;
T2: if (X) Y = 1;
T3: A = Y; B = X;


A == 1 && B == 0 is possible without write atomicity.

Marko Topolnik <marko at hazelcast.com> on 24.03.2015 18:44:16

To:	Oleksandr Otenko <oleksandr.otenko at oracle.com>
cc:	Alexander Terekhov/Germany/IBM at IBMDE, concurrency-interest
       <Concurrency-interest at cs.oswego.edu>
Subject:	Re: [concurrency-interest] Enforcing total sync order on modern
       hardware


On Tue, Mar 24, 2015 at 3:32 PM, Oleksandr Otenko <
oleksandr.otenko at oracle.com> wrote:
      The outcome r1==0, r3==0 is disallowed through the same reasoning in
      Dekker's idiom as in IRIW: since r1==0, y=1 must not happen before
      r1=y; then r3=x, being after y=1 in program order, must also not
      precede r1=y, and transitively cannot precede x=1. You need this step
      that from "Y must not precede X" follows that "X happens-before Y" -
      but that's what makes the order total.

      In IRIW you have the same chain of reasoning. Even if you get rid of
      synchronization order, but keep synchronizes-with and transitive
      closure, then from observing r0==1 and r2==1 you still have the same
      edges. The key here still is the conclusion that since r1==0, y=1
      must not happen before r1=y, which is the only part that requires
      total ordering - cannot be partially ordered. But it is exactly the
      same bit that cannot be partially ordered in Dekker's idiom: r1=y
      happens-before y=1 because otherwise r1=y should observe 1. If you
      don't have mutual exclusion in ordering r1=y and y=1 in Dekker's
      idiom, why would the outcome r1==0, r3==0 be forbidden?

This the IRIW (1,0,1,0) result in a happens-before diagram:

T4: x=0 --> y=0 ----> r2=y --> r3=x
? ? ? ? ? ? ? ? ? ?/--^
T3: x=0 --> y=0 --/-----> r0=x --> r1=y
? ? ? ? ? ? ? ? ?/ ? ?/---^
T2: y=1 --------/ ? ?/
? ? ? ? ? ? ? ? ? ? /
T1: x=1 -----------/

(note that I made the setting of initial values in T3 and T4 explicit)

As you can witness, there are no violations of the union of
synchronizes-with and program order ("transitive closure", even though that
term actually applies to something else). Therefore IRIW only violates
sequential consistency and does not violate what I assume under the term
"happens-before consistency".

BTW as for your earlier observation that IRIW is not minimal because Dekker
is smaller, note that the "issue of interest" for IRIW is the sequentially
inconsistent observation of mutually independent stores, as observed by
mutually independent loads. Here "mutually independent" means "done by
separate, independent threads". Clearly, IRIW is exactly the minimal
scenario for that.

---
Marko




From marko at hazelcast.com  Tue Mar 24 15:30:41 2015
From: marko at hazelcast.com (Marko Topolnik)
Date: Tue, 24 Mar 2015 20:30:41 +0100
Subject: [concurrency-interest] Enforcing total sync order on modern
	hardware
In-Reply-To: <OF56501874.6484FD81-ONC1257E12.00634692-C1257E12.00644C94@de.ibm.com>
References: <CALtZ-o47wWjeVXtn=zx8t4vau6D5T-qFnA3Vm2AGcFUm2S_PVw@mail.gmail.com>
	<550B0245.50606@oracle.com>
	<CALtZ-o4K8i2QrS3OzApai=CCv9oFpbtZDZgN0V8M4cZRy5ocTg@mail.gmail.com>
	<550B1222.3060704@oracle.com> <550B1629.50203@oracle.com>
	<CALtZ-o5ike96VAMeCUM1coqZtyB6OH20qpA-0Dn+qzAJPDgy8A@mail.gmail.com>
	<550B388C.2070808@oracle.com>
	<CALtZ-o7Qe9uMX1G_8Gm0DfkpyFOqC9680azEm1Sss3kXGdoiJA@mail.gmail.com>
	<550C079D.4020407@oracle.com>
	<CALtZ-o7SoNgww5vOsTmcK0Hxo5r2nnGUvby=uSsNve_FLQ0j4Q@mail.gmail.com>
	<550C4E98.3040208@oracle.com>
	<CALtZ-o6MCR3EWG5FRc_LNUn1Znt_251pZHM7NVCaha+pJKM1Dg@mail.gmail.com>
	<550C6C53.6040308@oracle.com>
	<CALtZ-o7zop9RS4RKaVuSwpr+iuadENYOsv49rGVZv_84n0AQLg@mail.gmail.com>
	<55114D92.7010605@oracle.com>
	<CALtZ-o6uJNWBdRXT_2_DCKv+p5yx=_sDBLE0pdGVkC85fM_VHQ@mail.gmail.com>
	<OF56501874.6484FD81-ONC1257E12.00634692-C1257E12.00644C94@de.ibm.com>
Message-ID: <CALtZ-o6SAxtBwZ94G3NN1-HRw4eme4mOFiT-AnYWgJ+uHNb4zw@mail.gmail.com>

On Tue, Mar 24, 2015 at 7:15 PM, Alexander Terekhov <TEREKHOV at de.ibm.com>
wrote:

> again, IRIW is about write atomicity... minimal scenario for that needs
> only three threads:
>
> T1: X = 1;
> T2: if (X) Y = 1;
> T3: A = Y; B = X;
>
> A == 1 && B == 0 is possible without write atomicity.
>

But this dispenses with the essential feature of IRIW, which is the
independency of two writes so that any other thread can receive them in
arbitrary order without breaching causality. In your example you have
second write dependent on the first, introducing a clear happens-before
edge between X = 1 and Y = 1. Put another way, a system may pass this test
and still fail IRIW. Therefore they cannot be considered equivalent.

---
Marko


> Marko Topolnik <marko at hazelcast.com> on 24.03.2015 18:44:16
>
> To:     Oleksandr Otenko <oleksandr.otenko at oracle.com>
> cc:     Alexander Terekhov/Germany/IBM at IBMDE, concurrency-interest
>        <Concurrency-interest at cs.oswego.edu>
> Subject:        Re: [concurrency-interest] Enforcing total sync order on
> modern
>        hardware
>
>
> On Tue, Mar 24, 2015 at 3:32 PM, Oleksandr Otenko <
> oleksandr.otenko at oracle.com> wrote:
>       The outcome r1==0, r3==0 is disallowed through the same reasoning in
>       Dekker's idiom as in IRIW: since r1==0, y=1 must not happen before
>       r1=y; then r3=x, being after y=1 in program order, must also not
>       precede r1=y, and transitively cannot precede x=1. You need this step
>       that from "Y must not precede X" follows that "X happens-before Y" -
>       but that's what makes the order total.
>
>       In IRIW you have the same chain of reasoning. Even if you get rid of
>       synchronization order, but keep synchronizes-with and transitive
>       closure, then from observing r0==1 and r2==1 you still have the same
>       edges. The key here still is the conclusion that since r1==0, y=1
>       must not happen before r1=y, which is the only part that requires
>       total ordering - cannot be partially ordered. But it is exactly the
>       same bit that cannot be partially ordered in Dekker's idiom: r1=y
>       happens-before y=1 because otherwise r1=y should observe 1. If you
>       don't have mutual exclusion in ordering r1=y and y=1 in Dekker's
>       idiom, why would the outcome r1==0, r3==0 be forbidden?
>
> This the IRIW (1,0,1,0) result in a happens-before diagram:
>
> T4: x=0 --> y=0 ----> r2=y --> r3=x
>                    /--^
> T3: x=0 --> y=0 --/-----> r0=x --> r1=y
>                  /    /---^
> T2: y=1 --------/    /
>                     /
> T1: x=1 -----------/
>
> (note that I made the setting of initial values in T3 and T4 explicit)
>
> As you can witness, there are no violations of the union of
> synchronizes-with and program order ("transitive closure", even though that
> term actually applies to something else). Therefore IRIW only violates
> sequential consistency and does not violate what I assume under the term
> "happens-before consistency".
>
> BTW as for your earlier observation that IRIW is not minimal because Dekker
> is smaller, note that the "issue of interest" for IRIW is the sequentially
> inconsistent observation of mutually independent stores, as observed by
> mutually independent loads. Here "mutually independent" means "done by
> separate, independent threads". Clearly, IRIW is exactly the minimal
> scenario for that.
>
> ---
> Marko
>
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150324/dd874f2d/attachment.html>

From lukeisandberg at gmail.com  Tue Mar 24 15:35:32 2015
From: lukeisandberg at gmail.com (Luke Sandberg)
Date: Tue, 24 Mar 2015 12:35:32 -0700
Subject: [concurrency-interest] ForkJoinPool cap on number of threads
Message-ID: <CAO9V1MJ05OgL9ERksB=_Eep35Euvk8RMQbm8hSoWmrbKzOVBSQ@mail.gmail.com>

Hi,
I'm trying to use ForkJoinPool and I am concerned that the maximum number
of threads it will create is capped at 32767.  Whereas I would like to set
a much lower cap (4-16).

You can configure the FJP with a target parallelism but i don't know if it
is possible to actually put a cap on the max parallelism.

In my specific case i will just be using it as an ExecutorService
(basically execute() and shutdown()) with the 'asyncMode' bit set to
'true', and will _not_ be using the Fork/Join framework.  (I want to see if
my application can get better performanced via decreased contention and
better cache locality).  Based on my reading of the docs i think this means
that it will not create any compensating threads since i would not be using
the managed blocking facilities.

Does anyone have any experience with using this in production?  I am
concerned that 'compensating' threads could cause my servers to run out of
memory for thread stacks (at 1MB a piece they aren't cheap)' and my
applications run in a shared hosting environment with strict caps on per
process ram usage (If i go over my reservation, my server will be killed).
Currently, we manage this with ThreadPoolExecutor by exposing our maxSizes
to our configuration and using that to set reservations.

Thanks,
Luke
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150324/077276fc/attachment-0001.html>

From TEREKHOV at de.ibm.com  Tue Mar 24 16:12:14 2015
From: TEREKHOV at de.ibm.com (Alexander Terekhov)
Date: Tue, 24 Mar 2015 21:12:14 +0100
Subject: [concurrency-interest] Enforcing total sync order on
	modern	hardware
In-Reply-To: <CALtZ-o6SAxtBwZ94G3NN1-HRw4eme4mOFiT-AnYWgJ+uHNb4zw@mail.gmail.com>
References: <CALtZ-o47wWjeVXtn=zx8t4vau6D5T-qFnA3Vm2AGcFUm2S_PVw@mail.gmail.com>	<550B1222.3060704@oracle.com>
	<550B1629.50203@oracle.com>
	<CALtZ-o5ike96VAMeCUM1coqZtyB6OH20qpA-0Dn+qzAJPDgy8A@mail.gmail.com>	<550B388C.2070808@oracle.com>
	<CALtZ-o7Qe9uMX1G_8Gm0DfkpyFOqC9680azEm1Sss3kXGdoiJA@mail.gmail.com>	<550C079D.4020407@oracle.com>
	<CALtZ-o7SoNgww5vOsTmcK0Hxo5r2nnGUvby=uSsNve_FLQ0j4Q@mail.gmail.com>	<550C4E98.3040208@oracle.com>
	<CALtZ-o6MCR3EWG5FRc_LNUn1Znt_251pZHM7NVCaha+pJKM1Dg@mail.gmail.com>	<550C6C53.6040308@oracle.com>
	<CALtZ-o7zop9RS4RKaVuSwpr+iuadENYOsv49rGVZv_84n0AQLg@mail.gmail.com>	<55114D92.7010605@oracle.com>
	<CALtZ-o6uJNWBdRXT_2_DCKv+p5yx=_sDBLE0pdGVkC85fM_VHQ@mail.gmail.com>	<OF5650187
	<CALtZ-o6SAxtBwZ94G3NN1-HRw4eme4mOFiT-AnYWgJ+uHNb4zw@mail.gmail.com>
Message-ID: <OFC2B82B44.4FF9890D-ONC1257E12.006E9DF9-C1257E12.006EFD4B@de.ibm.com>

"Put another way, a system may pass this test and still fail IRIW"

I don't think so, do you have a real world example?

Marko Topolnik <marko at hazelcast.com>@cs.oswego.edu on 24.03.2015 20:30:41

Sent by:	concurrency-interest-bounces at cs.oswego.edu


To:	Alexander Terekhov/Germany/IBM at IBMDE
cc:	concurrency-interest <Concurrency-interest at cs.oswego.edu>
Subject:	Re: [concurrency-interest] Enforcing total sync order on modern
       hardware


On Tue, Mar 24, 2015 at 7:15 PM, Alexander Terekhov <TEREKHOV at de.ibm.com>
wrote:
      again, IRIW is about write atomicity... minimal scenario for that
      needs
      only three threads:

      T1: X = 1;
      T2: if (X) Y = 1;
      T3: A = Y; B = X;

      A == 1 && B == 0 is possible without write atomicity.

But this dispenses with the essential feature of IRIW, which is the
independency of two writes so that any other thread can receive them in
arbitrary order without breaching causality. In your example you have
second write dependent on the first, introducing a clear happens-before
edge between X = 1 and Y = 1. Put another way, a system may pass this test
and still fail IRIW. Therefore they cannot be considered equivalent.

---
Marko

      Marko Topolnik <marko at hazelcast.com> on 24.03.2015 18:44:16

      To:? ? ?Oleksandr Otenko <oleksandr.otenko at oracle.com>
      cc:? ? ?Alexander Terekhov/Germany/IBM at IBMDE, concurrency-interest
      ? ? ? ?<Concurrency-interest at cs.oswego.edu>
      Subject:? ? ? ? Re: [concurrency-interest] Enforcing total sync order
      on modern
      ? ? ? ?hardware


      On Tue, Mar 24, 2015 at 3:32 PM, Oleksandr Otenko <
      oleksandr.otenko at oracle.com> wrote:
      ? ? ? The outcome r1==0, r3==0 is disallowed through the same
      reasoning in
      ? ? ? Dekker's idiom as in IRIW: since r1==0, y=1 must not happen
      before
      ? ? ? r1=y; then r3=x, being after y=1 in program order, must also
      not
      ? ? ? precede r1=y, and transitively cannot precede x=1. You need
      this step
      ? ? ? that from "Y must not precede X" follows that "X happens-before
      Y" -
      ? ? ? but that's what makes the order total.

      ? ? ? In IRIW you have the same chain of reasoning. Even if you get
      rid of
      ? ? ? synchronization order, but keep synchronizes-with and
      transitive
      ? ? ? closure, then from observing r0==1 and r2==1 you still have the
      same
      ? ? ? edges. The key here still is the conclusion that since r1==0,
      y=1
      ? ? ? must not happen before r1=y, which is the only part that
      requires
      ? ? ? total ordering - cannot be partially ordered. But it is exactly
      the
      ? ? ? same bit that cannot be partially ordered in Dekker's idiom:
      r1=y
      ? ? ? happens-before y=1 because otherwise r1=y should observe 1. If
      you
      ? ? ? don't have mutual exclusion in ordering r1=y and y=1 in
      Dekker's
      ? ? ? idiom, why would the outcome r1==0, r3==0 be forbidden?

      This the IRIW (1,0,1,0) result in a happens-before diagram:

      T4: x=0 --> y=0 ----> r2=y --> r3=x
      ? ? ? ? ? ? ? ? ? ?/--^
      T3: x=0 --> y=0 --/-----> r0=x --> r1=y
      ? ? ? ? ? ? ? ? ?/ ? ?/---^
      T2: y=1 --------/ ? ?/
      ? ? ? ? ? ? ? ? ? ? /
      T1: x=1 -----------/

      (note that I made the setting of initial values in T3 and T4
      explicit)

      As you can witness, there are no violations of the union of
      synchronizes-with and program order ("transitive closure", even
      though that
      term actually applies to something else). Therefore IRIW only
      violates
      sequential consistency and does not violate what I assume under the
      term
      "happens-before consistency".

      BTW as for your earlier observation that IRIW is not minimal because
      Dekker
      is smaller, note that the "issue of interest" for IRIW is the
      sequentially
      inconsistent observation of mutually independent stores, as observed
      by
      mutually independent loads. Here "mutually independent" means "done
      by
      separate, independent threads". Clearly, IRIW is exactly the minimal
      scenario for that.

      ---
      Marko


_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest




From marko at hazelcast.com  Tue Mar 24 16:27:42 2015
From: marko at hazelcast.com (Marko Topolnik)
Date: Tue, 24 Mar 2015 21:27:42 +0100
Subject: [concurrency-interest] Enforcing total sync order on modern
	hardware
In-Reply-To: <OFC2B82B44.4FF9890D-ONC1257E12.006E9DF9-C1257E12.006EFD4B@de.ibm.com>
References: <CALtZ-o47wWjeVXtn=zx8t4vau6D5T-qFnA3Vm2AGcFUm2S_PVw@mail.gmail.com>
	<550B1222.3060704@oracle.com> <550B1629.50203@oracle.com>
	<CALtZ-o5ike96VAMeCUM1coqZtyB6OH20qpA-0Dn+qzAJPDgy8A@mail.gmail.com>
	<550B388C.2070808@oracle.com>
	<CALtZ-o7Qe9uMX1G_8Gm0DfkpyFOqC9680azEm1Sss3kXGdoiJA@mail.gmail.com>
	<550C079D.4020407@oracle.com>
	<CALtZ-o7SoNgww5vOsTmcK0Hxo5r2nnGUvby=uSsNve_FLQ0j4Q@mail.gmail.com>
	<550C4E98.3040208@oracle.com>
	<CALtZ-o6MCR3EWG5FRc_LNUn1Znt_251pZHM7NVCaha+pJKM1Dg@mail.gmail.com>
	<550C6C53.6040308@oracle.com>
	<CALtZ-o7zop9RS4RKaVuSwpr+iuadENYOsv49rGVZv_84n0AQLg@mail.gmail.com>
	<55114D92.7010605@oracle.com>
	<CALtZ-o6uJNWBdRXT_2_DCKv+p5yx=_sDBLE0pdGVkC85fM_VHQ@mail.gmail.com>
	<CALtZ-o6SAxtBwZ94G3NN1-HRw4eme4mOFiT-AnYWgJ+uHNb4zw@mail.gmail.com>
	<OFC2B82B44.4FF9890D-ONC1257E12.006E9DF9-C1257E12.006EFD4B@de.ibm.com>
Message-ID: <CALtZ-o7ksHJSpc8EXrDO91=N-KPr8Gn_jw8OqLVRGFAbkU0pAg@mail.gmail.com>

On Tue, Mar 24, 2015 at 9:12 PM, Alexander Terekhov <TEREKHOV at de.ibm.com>
wrote:

> "Put another way, a system may pass this test and still fail IRIW"
>
> I don't think so, do you have a real world example?
>

All it takes is an architecture which doesn't enforce a global TSO for
independent writes, but does for dependent writes. Which, if any, chip has
those semantics I don't know, but its (non)existence certainly doesn't
(dis)prove anything. The tests will not suddenly become inequivalent the
day an architecture appears which differentiates between them.

---
Marko
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150324/75daec31/attachment.html>

From TEREKHOV at de.ibm.com  Tue Mar 24 23:01:36 2015
From: TEREKHOV at de.ibm.com (Alexander Terekhov)
Date: Wed, 25 Mar 2015 04:01:36 +0100
Subject: [concurrency-interest] Enforcing total sync order on
	modern	hardware
In-Reply-To: <CALtZ-o7ksHJSpc8EXrDO91=N-KPr8Gn_jw8OqLVRGFAbkU0pAg@mail.gmail.com>
References: <CALtZ-o47wWjeVXtn=zx8t4vau6D5T-qFnA3Vm2AGcFUm2S_PVw@mail.gmail.com>	<550B388C.2070808@oracle.com>
	<CALtZ-o7Qe9uMX1G_8Gm0DfkpyFOqC9680azEm1Sss3kXGdoiJA@mail.gmail.com>	<550C079D.4020407@oracle.com>
	<CALtZ-o7SoNgww5vOsTmcK0Hxo5r2nnGUvby=uSsNve_FLQ0j4Q@mail.gmail.com>	<550C4E98.3040208@oracle.com>
	<CALtZ-o6MCR3EWG5FRc_LNUn1Znt_251pZHM7NVCaha+pJKM1Dg@mail.gmail.com>	<550C6C53.6040308@oracle.com>
	<CALtZ-o7zop9RS4RKaVuSwpr+iuadENYOsv49rGVZv_84n0AQLg@mail.gmail.com>	<55114D92.7010605@oracle.com>
	<CALtZ-o6uJNWBdRXT_2_DCKv+p5yx=_sDBLE0pdGVkC85fM_VHQ@mail.gmail.com>	<CALtZ-o6SAxtBwZ94G3NN1-HRw4eme4mOFiT-AnYWgJ+uHNb4zw@mail.gmail.com>
	<OFC2B82B44.4FF9890D-ONC1257E12.006E9D
	<CALtZ-o7ksHJSpc8EXrDO91=N-KPr8Gn_jw8OqLVRGFAbkU0pAg@mail.gmail.com>
Message-ID: <OF751BC994.98F17D13-ONC1257E13.000FF42E-C1257E13.0010A19C@de.ibm.com>

????

T1: X = 1;
T2: if (X) Y = 1;
T3: A = Y; B = X;

is about atomicity of absolutely independent write T1: X = 1; just like in
IRIW.

To make it dependent just add another thread:

T0: Z = 1;
T1: if (Z) X = 1;
T2: if (X) Y = 1;
T3: A = Y; B = X;

Marko Topolnik <marko at hazelcast.com>@cs.oswego.edu on 24.03.2015 21:27:42

Sent by:	concurrency-interest-bounces at cs.oswego.edu


To:	Alexander Terekhov/Germany/IBM at IBMDE
cc:	concurrency-interest <Concurrency-interest at cs.oswego.edu>
Subject:	Re: [concurrency-interest] Enforcing total sync order on modern
       hardware


On Tue, Mar 24, 2015 at 9:12 PM, Alexander Terekhov <TEREKHOV at de.ibm.com>
wrote:
      "Put another way, a system may pass this test and still fail IRIW"

      I don't think so, do you have a real world example?

All it takes is an architecture which doesn't enforce a global TSO for
independent writes, but does for dependent writes. Which, if any, chip has
those semantics I don't know, but its (non)existence certainly doesn't
(dis)prove anything. The tests will not suddenly become inequivalent the
day an architecture appears which differentiates between them.

---
Marko_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest



From marko at hazelcast.com  Wed Mar 25 03:07:05 2015
From: marko at hazelcast.com (Marko Topolnik)
Date: Wed, 25 Mar 2015 08:07:05 +0100
Subject: [concurrency-interest] Enforcing total sync order on modern
	hardware
In-Reply-To: <OF751BC994.98F17D13-ONC1257E13.000FF42E-C1257E13.0010A19C@de.ibm.com>
References: <CALtZ-o47wWjeVXtn=zx8t4vau6D5T-qFnA3Vm2AGcFUm2S_PVw@mail.gmail.com>
	<550B388C.2070808@oracle.com>
	<CALtZ-o7Qe9uMX1G_8Gm0DfkpyFOqC9680azEm1Sss3kXGdoiJA@mail.gmail.com>
	<550C079D.4020407@oracle.com>
	<CALtZ-o7SoNgww5vOsTmcK0Hxo5r2nnGUvby=uSsNve_FLQ0j4Q@mail.gmail.com>
	<550C4E98.3040208@oracle.com>
	<CALtZ-o6MCR3EWG5FRc_LNUn1Znt_251pZHM7NVCaha+pJKM1Dg@mail.gmail.com>
	<550C6C53.6040308@oracle.com>
	<CALtZ-o7zop9RS4RKaVuSwpr+iuadENYOsv49rGVZv_84n0AQLg@mail.gmail.com>
	<55114D92.7010605@oracle.com>
	<CALtZ-o6uJNWBdRXT_2_DCKv+p5yx=_sDBLE0pdGVkC85fM_VHQ@mail.gmail.com>
	<CALtZ-o6SAxtBwZ94G3NN1-HRw4eme4mOFiT-AnYWgJ+uHNb4zw@mail.gmail.com>
	<CALtZ-o7ksHJSpc8EXrDO91=N-KPr8Gn_jw8OqLVRGFAbkU0pAg@mail.gmail.com>
	<OF751BC994.98F17D13-ONC1257E13.000FF42E-C1257E13.0010A19C@de.ibm.com>
Message-ID: <CALtZ-o4bNi_DgitvtaMNZbZ872r6gWjPZMpP1wf_qY8y2K2F6A@mail.gmail.com>

On Wed, Mar 25, 2015 at 4:01 AM, Alexander Terekhov <TEREKHOV at de.ibm.com>
wrote:

> T1: X = 1;
> T2: if (X) Y = 1;
> T3: A = Y; B = X;
>
> is about atomicity of absolutely independent write T1: X = 1; just like in
> IRIW.
>

IRIW requires all writes to be independent, not just one, and I have
provided a clear argument why that is important (testing for TSO of
independent writes). So I can't really make sense of your current argument.

BTW "write atomicity" just means that all the constituent bits are observed
to be written at once. Obviously, you don't have that meaning in mind, but
it is not crystal-clear what exactly you _do_ mean by it.

---
Marko
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150325/ac5316f7/attachment.html>

From viktor.klang at gmail.com  Wed Mar 25 05:06:54 2015
From: viktor.klang at gmail.com (Viktor Klang)
Date: Wed, 25 Mar 2015 10:06:54 +0100
Subject: [concurrency-interest] ForkJoinPool cap on number of threads
In-Reply-To: <CAO9V1MJ05OgL9ERksB=_Eep35Euvk8RMQbm8hSoWmrbKzOVBSQ@mail.gmail.com>
References: <CAO9V1MJ05OgL9ERksB=_Eep35Euvk8RMQbm8hSoWmrbKzOVBSQ@mail.gmail.com>
Message-ID: <CANPzfU8kmKXoHj83NG4cfNS6OGKzW-skQC8bsp-dNo-EXY9ZOQ@mail.gmail.com>

Hi Luke,

the newest version of FJP has a tighter bound on max number of threads
(search for maxSpares here:
http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/ForkJoinPool.java?view=markup
)

Another way of limiting is to have the ThreadFactory return null when no
more threads should be created.

On Tue, Mar 24, 2015 at 8:35 PM, Luke Sandberg <lukeisandberg at gmail.com>
wrote:

> Hi,
> I'm trying to use ForkJoinPool and I am concerned that the maximum number
> of threads it will create is capped at 32767.  Whereas I would like to set
> a much lower cap (4-16).
>
> You can configure the FJP with a target parallelism but i don't know if it
> is possible to actually put a cap on the max parallelism.
>
> In my specific case i will just be using it as an ExecutorService
> (basically execute() and shutdown()) with the 'asyncMode' bit set to
> 'true', and will _not_ be using the Fork/Join framework.  (I want to see if
> my application can get better performanced via decreased contention and
> better cache locality).  Based on my reading of the docs i think this means
> that it will not create any compensating threads since i would not be using
> the managed blocking facilities.
>
> Does anyone have any experience with using this in production?  I am
> concerned that 'compensating' threads could cause my servers to run out of
> memory for thread stacks (at 1MB a piece they aren't cheap)' and my
> applications run in a shared hosting environment with strict caps on per
> process ram usage (If i go over my reservation, my server will be killed).
> Currently, we manage this with ThreadPoolExecutor by exposing our maxSizes
> to our configuration and using that to set reservations.
>
> Thanks,
> Luke
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>


-- 
Cheers,
?
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150325/3451bc18/attachment-0001.html>

From TEREKHOV at de.ibm.com  Wed Mar 25 05:37:26 2015
From: TEREKHOV at de.ibm.com (Alexander Terekhov)
Date: Wed, 25 Mar 2015 10:37:26 +0100
Subject: [concurrency-interest] Enforcing total sync order on
	modern	hardware
In-Reply-To: <CALtZ-o4bNi_DgitvtaMNZbZ872r6gWjPZMpP1wf_qY8y2K2F6A@mail.gmail.com>
References: <CALtZ-o47wWjeVXtn=zx8t4vau6D5T-qFnA3Vm2AGcFUm2S_PVw@mail.gmail.com>	<550C079D.4020407@oracle.com>
	<CALtZ-o7SoNgww5vOsTmcK0Hxo5r2nnGUvby=uSsNve_FLQ0j4Q@mail.gmail.com>	<550C4E98.3040208@oracle.com>
	<CALtZ-o6MCR3EWG5FRc_LNUn1Znt_251pZHM7NVCaha+pJKM1Dg@mail.gmail.com>	<550C6C53.6040308@oracle.com>
	<CALtZ-o7zop9RS4RKaVuSwpr+iuadENYOsv49rGVZv_84n0AQLg@mail.gmail.com>	<55114D92.7010605@oracle.com>
	<CALtZ-o6uJNWBdRXT_2_DCKv+p5yx=_sDBLE0pdGVkC85fM_VHQ@mail.gmail.com>	<CALtZ-o6SAxtBwZ94G3NN1-HRw4eme4mOFiT-AnYWgJ+uHNb4zw@mail.gmail.com>
	<CALtZ-o7ksHJSpc8EXrDO91=N-KPr8Gn_jw8OqLVRGFAbkU0pAg@mail.gmail.com>	<OF751BC994.98F17D13-ONC1257E13.000FF42E-C1257E13.0010A19C@de.ibm.com>
	<CALtZ-o4bNi_DgitvtaMNZbZ872r6gWjPZMpP1wf_qY8y2K2F6A@mail.gmail.com>
Message-ID: <OF04FC52C3.11E38C7F-ONC1257E13.00348C44-C1257E13.0034DF30@de.ibm.com>

try google... quick search yielded

https://books.google.de/books?isbn=1608459578

Michael L. Scott - 2013 - ?Computers
Here the problem is not bypassing, but a lack of write atomicity?one thread
sees the value written by a store and another thread subsequently sees the
value ...


Marko Topolnik <marko at hazelcast.com>@cs.oswego.edu on 25.03.2015 08:07:05

Sent by:	concurrency-interest-bounces at cs.oswego.edu


To:	Alexander Terekhov/Germany/IBM at IBMDE
cc:	concurrency-interest <Concurrency-interest at cs.oswego.edu>
Subject:	Re: [concurrency-interest] Enforcing total sync order on modern
       hardware


On Wed, Mar 25, 2015 at 4:01 AM, Alexander Terekhov <TEREKHOV at de.ibm.com>
wrote:
      T1: X = 1;
      T2: if (X) Y = 1;
      T3: A = Y; B = X;

      is about atomicity of absolutely independent write T1: X = 1; just
      like in
      IRIW.

IRIW requires all writes to be independent, not just one, and I have
provided a clear argument why that is important (testing for TSO of
independent writes). So I can't really make sense of your current argument.

BTW "write atomicity" just means that all the constituent bits are observed
to be written at once. Obviously, you don't have that meaning in mind, but
it is not crystal-clear what exactly you _do_ mean by it.

---
Marko_______________________________________________
Concurrency-interest mailing list
Concurrency-interest at cs.oswego.edu
http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From marko at hazelcast.com  Wed Mar 25 05:49:39 2015
From: marko at hazelcast.com (Marko Topolnik)
Date: Wed, 25 Mar 2015 10:49:39 +0100
Subject: [concurrency-interest] Enforcing total sync order on modern
	hardware
In-Reply-To: <OF04FC52C3.11E38C7F-ONC1257E13.00348C44-C1257E13.0034DF30@de.ibm.com>
References: <CALtZ-o47wWjeVXtn=zx8t4vau6D5T-qFnA3Vm2AGcFUm2S_PVw@mail.gmail.com>
	<550C079D.4020407@oracle.com>
	<CALtZ-o7SoNgww5vOsTmcK0Hxo5r2nnGUvby=uSsNve_FLQ0j4Q@mail.gmail.com>
	<550C4E98.3040208@oracle.com>
	<CALtZ-o6MCR3EWG5FRc_LNUn1Znt_251pZHM7NVCaha+pJKM1Dg@mail.gmail.com>
	<550C6C53.6040308@oracle.com>
	<CALtZ-o7zop9RS4RKaVuSwpr+iuadENYOsv49rGVZv_84n0AQLg@mail.gmail.com>
	<55114D92.7010605@oracle.com>
	<CALtZ-o6uJNWBdRXT_2_DCKv+p5yx=_sDBLE0pdGVkC85fM_VHQ@mail.gmail.com>
	<CALtZ-o6SAxtBwZ94G3NN1-HRw4eme4mOFiT-AnYWgJ+uHNb4zw@mail.gmail.com>
	<CALtZ-o7ksHJSpc8EXrDO91=N-KPr8Gn_jw8OqLVRGFAbkU0pAg@mail.gmail.com>
	<OF751BC994.98F17D13-ONC1257E13.000FF42E-C1257E13.0010A19C@de.ibm.com>
	<CALtZ-o4bNi_DgitvtaMNZbZ872r6gWjPZMpP1wf_qY8y2K2F6A@mail.gmail.com>
	<OF04FC52C3.11E38C7F-ONC1257E13.00348C44-C1257E13.0034DF30@de.ibm.com>
Message-ID: <CALtZ-o5fxgV00wJBKPD_tL7ViqxAO62cGyPfa-xP=v5Toe_10Q@mail.gmail.com>

You have confused my complaint as being about the lack of any definition,
whereas I actually complained that there are several competing definitions
used by various sources. The most natural definition, at least for me, is
this:

When an atomic store is performed on a shared variable, no other thread can
observe the modification half-complete.


Source: http://preshing.com/20130618/atomic-vs-non-atomic-operations

---
Marko

On Wed, Mar 25, 2015 at 10:37 AM, Alexander Terekhov <TEREKHOV at de.ibm.com>
wrote:

> try google... quick search yielded
>
> https://books.google.de/books?isbn=1608459578
>
> Michael L. Scott - 2013 - ?Computers
> Here the problem is not bypassing, but a lack of write atomicity?one thread
> sees the value written by a store and another thread subsequently sees the
> value ...
>
>
> Marko Topolnik <marko at hazelcast.com>@cs.oswego.edu on 25.03.2015 08:07:05
>
> Sent by:        concurrency-interest-bounces at cs.oswego.edu
>
>
> To:     Alexander Terekhov/Germany/IBM at IBMDE
> cc:     concurrency-interest <Concurrency-interest at cs.oswego.edu>
> Subject:        Re: [concurrency-interest] Enforcing total sync order on
> modern
>        hardware
>
>
> On Wed, Mar 25, 2015 at 4:01 AM, Alexander Terekhov <TEREKHOV at de.ibm.com>
> wrote:
>       T1: X = 1;
>       T2: if (X) Y = 1;
>       T3: A = Y; B = X;
>
>       is about atomicity of absolutely independent write T1: X = 1; just
>       like in
>       IRIW.
>
> IRIW requires all writes to be independent, not just one, and I have
> provided a clear argument why that is important (testing for TSO of
> independent writes). So I can't really make sense of your current argument.
>
> BTW "write atomicity" just means that all the constituent bits are observed
> to be written at once. Obviously, you don't have that meaning in mind, but
> it is not crystal-clear what exactly you _do_ mean by it.
>
> ---
> Marko_______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150325/959a1fe6/attachment.html>

From TEREKHOV at de.ibm.com  Wed Mar 25 06:02:53 2015
From: TEREKHOV at de.ibm.com (Alexander Terekhov)
Date: Wed, 25 Mar 2015 11:02:53 +0100
Subject: [concurrency-interest] Enforcing total sync order on modern
	hardware
In-Reply-To: <CALtZ-o5fxgV00wJBKPD_tL7ViqxAO62cGyPfa-xP=v5Toe_10Q@mail.gmail.com>
References: <CALtZ-o47wWjeVXtn=zx8t4vau6D5T-qFnA3Vm2AGcFUm2S_PVw@mail.gmail.com>	<550C4E98.3040208@oracle.com>
	<CALtZ-o6MCR3EWG5FRc_LNUn1Znt_251pZHM7NVCaha+pJKM1Dg@mail.gmail.com>	<550C6C53.6040308@oracle.com>
	<CALtZ-o7zop9RS4RKaVuSwpr+iuadENYOsv49rGVZv_84n0AQLg@mail.gmail.com>	<55114D92.7010605@oracle.com>
	<CALtZ-o6uJNWBdRXT_2_DCKv+p5yx=_sDBLE0pdGVkC85fM_VHQ@mail.gmail.com>	<CALtZ-o6SAxtBwZ94G3NN1-HRw4eme4mOFiT-AnYWgJ+uHNb4zw@mail.gmail.com>
	<CALtZ-o7ksHJSpc8EXrDO91=N-KPr8Gn_jw8OqLVRGFAbkU0pAg@mail.gmail.com>	<OF751BC994.98F17D13-ONC1257E13.000FF42E-C1257E13.0010A19C@de.ibm.com>
	<CALtZ-o4bNi_DgitvtaMNZbZ872r6gWjPZMpP1wf_qY8y2K2F6A@mail.gmail.com>	<OF04FC52C3.11E38C7F-ONC1257E13.00348C44-C1257E13.0034D
	<CALtZ-o5fxgV00wJBKPD_tL7ViqxAO62cGyPfa-xP=v5Toe_10Q@mail.gmail.com>
Message-ID: <OF29135BA6.DFEFBAC1-ONC1257E13.0036BC56-C1257E13.003733CC@de.ibm.com>

thats about atomic 'stores' and 'loads'...  not 'write' atomicity... (note
different words)

Marko Topolnik <marko at hazelcast.com> on 25.03.2015 10:49:39

To:	Alexander Terekhov/Germany/IBM at IBMDE
cc:	concurrency-interest <Concurrency-interest at cs.oswego.edu>
Subject:	Re: [concurrency-interest] Enforcing total sync order on modern
       hardware


You have confused my complaint as being about the lack of any definition,
whereas I actually complained that there are several competing definitions
used by various sources. The most natural definition, at least for me, is
this:

    When an atomic store is performed on a shared variable, no other thread
    can observe the modification half-complete.

Source:?http://preshing.com/20130618/atomic-vs-non-atomic-operations

---
Marko

On Wed, Mar 25, 2015 at 10:37 AM, Alexander Terekhov <TEREKHOV at de.ibm.com>
wrote:
      try google... quick search yielded

      https://books.google.de/books?isbn=1608459578

      Michael L. Scott - 2013 - ?Computers
      Here the problem is not bypassing, but a lack of write atomicity?one
      thread
      sees the value written by a store and another thread subsequently
      sees the
      value ...


      Marko Topolnik <marko at hazelcast.com>@cs.oswego.edu on 25.03.2015 08
      :07:05

      Sent by:? ? ? ? concurrency-interest-bounces at cs.oswego.edu


      To:? ? ?Alexander Terekhov/Germany/IBM at IBMDE
      cc:? ? ?concurrency-interest <Concurrency-interest at cs.oswego.edu>
      Subject:? ? ? ? Re: [concurrency-interest] Enforcing total sync order
      on modern
      ? ? ? ?hardware


      On Wed, Mar 25, 2015 at 4:01 AM, Alexander Terekhov <
      TEREKHOV at de.ibm.com>
      wrote:
      ? ? ? T1: X = 1;
      ? ? ? T2: if (X) Y = 1;
      ? ? ? T3: A = Y; B = X;

      ? ? ? is about atomicity of absolutely independent write T1: X = 1;
      just
      ? ? ? like in
      ? ? ? IRIW.

      IRIW requires all writes to be independent, not just one, and I have
      provided a clear argument why that is important (testing for TSO of
      independent writes). So I can't really make sense of your current
      argument.

      BTW "write atomicity" just means that all the constituent bits are
      observed
      to be written at once. Obviously, you don't have that meaning in
      mind, but
      it is not crystal-clear what exactly you _do_ mean by it.

      ---
      Marko_______________________________________________
      Concurrency-interest mailing list
      Concurrency-interest at cs.oswego.edu
      http://cs.oswego.edu/mailman/listinfo/concurrency-interest


From lukeisandberg at gmail.com  Wed Mar 25 11:24:59 2015
From: lukeisandberg at gmail.com (Luke Sandberg)
Date: Wed, 25 Mar 2015 08:24:59 -0700
Subject: [concurrency-interest] ForkJoinPool cap on number of threads
In-Reply-To: <CANPzfU8kmKXoHj83NG4cfNS6OGKzW-skQC8bsp-dNo-EXY9ZOQ@mail.gmail.com>
References: <CAO9V1MJ05OgL9ERksB=_Eep35Euvk8RMQbm8hSoWmrbKzOVBSQ@mail.gmail.com>
	<CANPzfU8kmKXoHj83NG4cfNS6OGKzW-skQC8bsp-dNo-EXY9ZOQ@mail.gmail.com>
Message-ID: <CAO9V1MK9yYZcx=5CpO8S6txWrFnUjzimn2JU1hZ3DOdTevu8bA@mail.gmail.com>

Thanks!

Looks like i can have my ForkJoinThreadFactory return null to limit active
threads. (it would be nice if that was documented on ForkJoinThreadFactory)

In that snapshot it looks like the maxSpares limitation only applies to the
common pool, is there a plan to add a standard maxSize mechanism for user
pools?

On Wed, Mar 25, 2015 at 2:06 AM, Viktor Klang <viktor.klang at gmail.com>
wrote:

> Hi Luke,
>
> the newest version of FJP has a tighter bound on max number of threads
> (search for maxSpares here:
> http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/ForkJoinPool.java?view=markup
> )
>
> Another way of limiting is to have the ThreadFactory return null when no
> more threads should be created.
>
> On Tue, Mar 24, 2015 at 8:35 PM, Luke Sandberg <lukeisandberg at gmail.com>
> wrote:
>
>> Hi,
>> I'm trying to use ForkJoinPool and I am concerned that the maximum number
>> of threads it will create is capped at 32767.  Whereas I would like to set
>> a much lower cap (4-16).
>>
>> You can configure the FJP with a target parallelism but i don't know if
>> it is possible to actually put a cap on the max parallelism.
>>
>> In my specific case i will just be using it as an ExecutorService
>> (basically execute() and shutdown()) with the 'asyncMode' bit set to
>> 'true', and will _not_ be using the Fork/Join framework.  (I want to see if
>> my application can get better performanced via decreased contention and
>> better cache locality).  Based on my reading of the docs i think this means
>> that it will not create any compensating threads since i would not be using
>> the managed blocking facilities.
>>
>> Does anyone have any experience with using this in production?  I am
>> concerned that 'compensating' threads could cause my servers to run out of
>> memory for thread stacks (at 1MB a piece they aren't cheap)' and my
>> applications run in a shared hosting environment with strict caps on per
>> process ram usage (If i go over my reservation, my server will be killed).
>> Currently, we manage this with ThreadPoolExecutor by exposing our maxSizes
>> to our configuration and using that to set reservations.
>>
>> Thanks,
>> Luke
>>
>> _______________________________________________
>> Concurrency-interest mailing list
>> Concurrency-interest at cs.oswego.edu
>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>
>>
>
>
> --
> Cheers,
> ?
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150325/7b168d71/attachment-0001.html>

From michael.m.spiegel at gmail.com  Wed Mar 25 13:40:24 2015
From: michael.m.spiegel at gmail.com (Michael Spiegel)
Date: Wed, 25 Mar 2015 13:40:24 -0400
Subject: [concurrency-interest] ForkJoinPool cap on number of threads
In-Reply-To: <CAO9V1MK9yYZcx=5CpO8S6txWrFnUjzimn2JU1hZ3DOdTevu8bA@mail.gmail.com>
References: <CAO9V1MJ05OgL9ERksB=_Eep35Euvk8RMQbm8hSoWmrbKzOVBSQ@mail.gmail.com>
	<CANPzfU8kmKXoHj83NG4cfNS6OGKzW-skQC8bsp-dNo-EXY9ZOQ@mail.gmail.com>
	<CAO9V1MK9yYZcx=5CpO8S6txWrFnUjzimn2JU1hZ3DOdTevu8bA@mail.gmail.com>
Message-ID: <CANwu5-qcoJbZcZbGODRs-GM7V0sfQSjtHCYG6VfocKhzqn75YA@mail.gmail.com>

Maybe I am misunderstanding the question but a simple solution is just to
use a semaphore with a fixed number of permits and use it to limit the
number of tasks submitted.

On Wed, Mar 25, 2015 at 11:24 AM, Luke Sandberg <lukeisandberg at gmail.com>
wrote:

> Thanks!
>
> Looks like i can have my ForkJoinThreadFactory return null to limit active
> threads. (it would be nice if that was documented on ForkJoinThreadFactory)
>
> In that snapshot it looks like the maxSpares limitation only applies to
> the common pool, is there a plan to add a standard maxSize mechanism for
> user pools?
>
> On Wed, Mar 25, 2015 at 2:06 AM, Viktor Klang <viktor.klang at gmail.com>
> wrote:
>
>> Hi Luke,
>>
>> the newest version of FJP has a tighter bound on max number of threads
>> (search for maxSpares here:
>> http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/concurrent/ForkJoinPool.java?view=markup
>> )
>>
>> Another way of limiting is to have the ThreadFactory return null when no
>> more threads should be created.
>>
>> On Tue, Mar 24, 2015 at 8:35 PM, Luke Sandberg <lukeisandberg at gmail.com>
>> wrote:
>>
>>> Hi,
>>> I'm trying to use ForkJoinPool and I am concerned that the maximum
>>> number of threads it will create is capped at 32767.  Whereas I would like
>>> to set a much lower cap (4-16).
>>>
>>> You can configure the FJP with a target parallelism but i don't know if
>>> it is possible to actually put a cap on the max parallelism.
>>>
>>> In my specific case i will just be using it as an ExecutorService
>>> (basically execute() and shutdown()) with the 'asyncMode' bit set to
>>> 'true', and will _not_ be using the Fork/Join framework.  (I want to see if
>>> my application can get better performanced via decreased contention and
>>> better cache locality).  Based on my reading of the docs i think this means
>>> that it will not create any compensating threads since i would not be using
>>> the managed blocking facilities.
>>>
>>> Does anyone have any experience with using this in production?  I am
>>> concerned that 'compensating' threads could cause my servers to run out of
>>> memory for thread stacks (at 1MB a piece they aren't cheap)' and my
>>> applications run in a shared hosting environment with strict caps on per
>>> process ram usage (If i go over my reservation, my server will be killed).
>>> Currently, we manage this with ThreadPoolExecutor by exposing our maxSizes
>>> to our configuration and using that to set reservations.
>>>
>>> Thanks,
>>> Luke
>>>
>>> _______________________________________________
>>> Concurrency-interest mailing list
>>> Concurrency-interest at cs.oswego.edu
>>> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>>>
>>>
>>
>>
>> --
>> Cheers,
>> ?
>>
>
>
> _______________________________________________
> Concurrency-interest mailing list
> Concurrency-interest at cs.oswego.edu
> http://cs.oswego.edu/mailman/listinfo/concurrency-interest
>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150325/d9351984/attachment.html>

From martinrb at google.com  Wed Mar 25 14:23:03 2015
From: martinrb at google.com (Martin Buchholz)
Date: Wed, 25 Mar 2015 11:23:03 -0700
Subject: [concurrency-interest] ForkJoinPool cap on number of threads
In-Reply-To: <CAO9V1MK9yYZcx=5CpO8S6txWrFnUjzimn2JU1hZ3DOdTevu8bA@mail.gmail.com>
References: <CAO9V1MJ05OgL9ERksB=_Eep35Euvk8RMQbm8hSoWmrbKzOVBSQ@mail.gmail.com>
	<CANPzfU8kmKXoHj83NG4cfNS6OGKzW-skQC8bsp-dNo-EXY9ZOQ@mail.gmail.com>
	<CAO9V1MK9yYZcx=5CpO8S6txWrFnUjzimn2JU1hZ3DOdTevu8bA@mail.gmail.com>
Message-ID: <CA+kOe08BfhxaL--bmCDbb2OiRhDVCBr7eBf=tLpQ1zCr_ipAzg@mail.gmail.com>

On Wed, Mar 25, 2015 at 8:24 AM, Luke Sandberg <lukeisandberg at gmail.com>
wrote:

> Thanks!
>
> Looks like i can have my ForkJoinThreadFactory return null to limit active
> threads. (it would be nice if that was documented on ForkJoinThreadFactory)
>
>
I agree that we should make that clearer - there are existing snippets of
javadoc, but we should add something (perhaps even sample code) showing how
to impose a limit.


> In that snapshot it looks like the maxSpares limitation only applies to
> the common pool, is there a plan to add a standard maxSize mechanism for
> user pools?
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150325/71ac275f/attachment.html>

From martinrb at google.com  Wed Mar 25 15:07:06 2015
From: martinrb at google.com (Martin Buchholz)
Date: Wed, 25 Mar 2015 12:07:06 -0700
Subject: [concurrency-interest] ForkJoinPool cap on number of threads
In-Reply-To: <CA+kOe08BfhxaL--bmCDbb2OiRhDVCBr7eBf=tLpQ1zCr_ipAzg@mail.gmail.com>
References: <CAO9V1MJ05OgL9ERksB=_Eep35Euvk8RMQbm8hSoWmrbKzOVBSQ@mail.gmail.com>
	<CANPzfU8kmKXoHj83NG4cfNS6OGKzW-skQC8bsp-dNo-EXY9ZOQ@mail.gmail.com>
	<CAO9V1MK9yYZcx=5CpO8S6txWrFnUjzimn2JU1hZ3DOdTevu8bA@mail.gmail.com>
	<CA+kOe08BfhxaL--bmCDbb2OiRhDVCBr7eBf=tLpQ1zCr_ipAzg@mail.gmail.com>
Message-ID: <CA+kOe09TgvD4uM+QFnXQ4EC3VRpriG+Kpx35bJOidDPLiaonsw@mail.gmail.com>

TIL that ForkJoinPool.ForkJoinWorkerThreadFactory does not implement
ThreadFactory!
Nevertheless, we can align the specs to allow the former to return null,
like the latter.
This seems like small progress:

Index: src/main/java/util/concurrent/ForkJoinPool.java
===================================================================
RCS file:
/export/home/jsr166/jsr166/jsr166/src/main/java/util/concurrent/ForkJoinPool.java,v
retrieving revision 1.240
diff -u -r1.240 ForkJoinPool.java
--- src/main/java/util/concurrent/ForkJoinPool.java 23 Feb 2015 20:54:08
-0000 1.240
+++ src/main/java/util/concurrent/ForkJoinPool.java 25 Mar 2015 19:03:36
-0000
@@ -681,7 +681,8 @@
          * Returns a new worker thread operating in the given pool.
          *
          * @param pool the pool this thread works in
-         * @return the new worker thread
+         * @return the new worker thread, or {@code null} if the request
+         *         to create a thread is rejected
          * @throws NullPointerException if the pool is null
          */
         public ForkJoinWorkerThread newThread(ForkJoinPool pool);


On Wed, Mar 25, 2015 at 11:23 AM, Martin Buchholz <martinrb at google.com>
wrote:

>
>
> On Wed, Mar 25, 2015 at 8:24 AM, Luke Sandberg <lukeisandberg at gmail.com>
> wrote:
>
>> Thanks!
>>
>> Looks like i can have my ForkJoinThreadFactory return null to limit
>> active threads. (it would be nice if that was documented on
>> ForkJoinThreadFactory)
>>
>>
> I agree that we should make that clearer - there are existing snippets of
> javadoc, but we should add something (perhaps even sample code) showing how
> to impose a limit.
>
>
>> In that snapshot it looks like the maxSpares limitation only applies to
>> the common pool, is there a plan to add a standard maxSize mechanism for
>> user pools?
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150325/1928d24e/attachment.html>

From lukeisandberg at gmail.com  Wed Mar 25 16:25:11 2015
From: lukeisandberg at gmail.com (Luke Sandberg)
Date: Wed, 25 Mar 2015 13:25:11 -0700
Subject: [concurrency-interest] ForkJoinPool cap on number of threads
In-Reply-To: <CA+kOe09TgvD4uM+QFnXQ4EC3VRpriG+Kpx35bJOidDPLiaonsw@mail.gmail.com>
References: <CAO9V1MJ05OgL9ERksB=_Eep35Euvk8RMQbm8hSoWmrbKzOVBSQ@mail.gmail.com>
	<CANPzfU8kmKXoHj83NG4cfNS6OGKzW-skQC8bsp-dNo-EXY9ZOQ@mail.gmail.com>
	<CAO9V1MK9yYZcx=5CpO8S6txWrFnUjzimn2JU1hZ3DOdTevu8bA@mail.gmail.com>
	<CA+kOe08BfhxaL--bmCDbb2OiRhDVCBr7eBf=tLpQ1zCr_ipAzg@mail.gmail.com>
	<CA+kOe09TgvD4uM+QFnXQ4EC3VRpriG+Kpx35bJOidDPLiaonsw@mail.gmail.com>
Message-ID: <CAO9V1M+rxd5PUmhKTsr4Q03MaJOFqpKpJq7Jobu0g5gWVZbtJQ@mail.gmail.com>

I don't want to limit submisssions (requests queueing is fine, i also have
various mechanism to limit blocking, my uses should be largely
non-blocking), i need to limit the actual number of threads running.

Thanks martin!  I think that doc would be have been helpful (also the jdk 8
docs on FJP would have been useful, i am still on jdk7 and was looking at
the older docs).

On Wed, Mar 25, 2015 at 12:07 PM, Martin Buchholz <martinrb at google.com>
wrote:

> TIL that ForkJoinPool.ForkJoinWorkerThreadFactory does not implement
> ThreadFactory!
> Nevertheless, we can align the specs to allow the former to return null,
> like the latter.
> This seems like small progress:
>
> Index: src/main/java/util/concurrent/ForkJoinPool.java
> ===================================================================
> RCS file:
> /export/home/jsr166/jsr166/jsr166/src/main/java/util/concurrent/ForkJoinPool.java,v
> retrieving revision 1.240
> diff -u -r1.240 ForkJoinPool.java
> --- src/main/java/util/concurrent/ForkJoinPool.java 23 Feb 2015 20:54:08
> -0000 1.240
> +++ src/main/java/util/concurrent/ForkJoinPool.java 25 Mar 2015 19:03:36
> -0000
> @@ -681,7 +681,8 @@
>           * Returns a new worker thread operating in the given pool.
>           *
>           * @param pool the pool this thread works in
> -         * @return the new worker thread
> +         * @return the new worker thread, or {@code null} if the request
> +         *         to create a thread is rejected
>           * @throws NullPointerException if the pool is null
>           */
>          public ForkJoinWorkerThread newThread(ForkJoinPool pool);
>
>
> On Wed, Mar 25, 2015 at 11:23 AM, Martin Buchholz <martinrb at google.com>
> wrote:
>
>>
>>
>> On Wed, Mar 25, 2015 at 8:24 AM, Luke Sandberg <lukeisandberg at gmail.com>
>> wrote:
>>
>>> Thanks!
>>>
>>> Looks like i can have my ForkJoinThreadFactory return null to limit
>>> active threads. (it would be nice if that was documented on
>>> ForkJoinThreadFactory)
>>>
>>>
>> I agree that we should make that clearer - there are existing snippets of
>> javadoc, but we should add something (perhaps even sample code) showing how
>> to impose a limit.
>>
>>
>>> In that snapshot it looks like the maxSpares limitation only applies to
>>> the common pool, is there a plan to add a standard maxSize mechanism for
>>> user pools?
>>>
>>
>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://cs.oswego.edu/pipermail/concurrency-interest/attachments/20150325/c2a9e49d/attachment-0001.html>

